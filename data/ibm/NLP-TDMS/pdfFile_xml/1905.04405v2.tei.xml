<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language-Conditioned Graph Networks for Relational Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language-Conditioned Graph Networks for Relational Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Solving grounded language tasks often requires reasoning about relationships between objects in the context of a given task. For example, to answer the question "What color is the mug on the plate?" we must check the color of the specific mug that satisfies the "on" relationship with respect to the plate. Recent work has proposed various methods capable of complex relational reasoning. However, most of their power is in the inference structure, while the scene is represented with simple local appearance features. In this paper, we take an alternate approach and build contextualized representations for objects in a visual scene to support relational reasoning. We propose a general framework of Language-Conditioned Graph Networks (LCGN), where each node represents an object, and is described by a context-aware representation from related objects through iterative message passing conditioned on the textual input. E.g., conditioning on the "on" relationship to the plate, the object "mug" gathers messages from the object "plate" to update its representation to "mug on the plate", which can be easily consumed by a simple classifier for answer prediction. We experimentally show that our LCGN approach effectively supports relational reasoning and improves performance across several tasks and datasets. Our code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Grounded language comprehension tasks, such as visual question answering (VQA) or referring expression comprehension (REF), require finding the relevant objects in the scene and reasoning about certain relationships between them. For example in <ref type="figure">Figure 1</ref>, to answer the question is there a person to the left of the woman holding a blue umbrella, we must locate the relevant objects -person, woman and blue umbrella -and model the specified relationshipsto the left of and holding.</p><p>How should we build a model to perform reasoning in grounded language comprehension tasks? Prior works have explored various approaches from learning joint visualtextual representations (e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref>) to pooling over pairwise relationships (e.g. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>) or constructing explicit Question: Is there a person to the left of the woman holding a blue umbrella?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: Yes</head><p>Answer: No Question: Is the left-most person holding a red bag?  <ref type="figure">Figure 1</ref>. In this work, we create context-aware representations for objects by sending messages between relevant objects in a dynamic way that depends on the input language. In the left example, the first round of message passing updates object 2 with features of object 3 based on the woman holding a blue umbrella (green arrow), and the second round updates object 1 with object 2's features based on person to the left (red arrow). The final answer prediction can be made by a single attention hop over the most relevant object (blue box).</p><p>reasoning steps with modular or symbolic representations (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref>). Although these models are capable of performing complex relational inference, their scene representations are built upon local visual appearance features that do not contain much contextual information. Instead, they tend to rely heavily on manually designed inference structures or modules to perform reasoning about relationships, and are often specific to a particular task.</p><p>In this work, we propose an alternative way to facilitate reasoning with a context-aware scene representation, suitable for multiple tasks. Our proposed Language-Conditioned Graph Network (LCGN) model augments the local appearance feature of each entity in the scene with a relational contextualized feature. Our model is a graph network built upon visual entities in the scene, which collects relational information through multiple iterations of message passing between the entities. It dynamically determines which objects to collect information from on each round, by weighting the edges in the graph, and sends messages through the graph to propagate just the right amount of relational information. The key idea is to condition the message passing on the specific contextual relationships described in the input text. <ref type="figure">Figure 1</ref> illustrates this process, where the person would be represented not only by her local appearance, but also by contextualized features indicating her relationship to other relevant objects in the scene, e.g., left of a woman. Our contextualized representation can be easily plugged into task-specific models to replace standard local appearance features, facilitating reasoning with rich relational information. E.g. for the question answering task, it is sufficient to perform a single attention hop over the relevant object, whose representation is contextualized (e.g. blue box in <ref type="figure">Figure 1</ref>). Importantly, our scene representation is constructed with respect to the given reasoning task. An object in the scene may be involved in multiple relations in different contexts: in <ref type="figure">Figure 1</ref>, the person can be simultaneously left of a woman holding a blue umbrella, holding a white bag, and standing on a sidewalk. Rather than building a complete representation of all the first-and higher-order relational information for each object (which can be enormous and unnecessary), we focus the contextual representation on relational information that is helpful to the reasoning task by conditioning on the input text ( <ref type="figure">Figure 1</ref> left vs. right).</p><p>We apply our Language-Conditioned Graph Networks to two reasoning tasks with language inputs-Visual Question Answering (VQA) and Referring Expression Comprehension (REF). In these tasks, we replace the local appearancebased visual representations with the context-aware representations from our LCGN model, and demonstrate that our context-aware scene representations can be used as inputs to perform complex reasoning via simple task-specific approaches, with a consistent improvement over the local appearance features across different tasks and datasets. We obtain state-of-the-art results on the GQA dataset <ref type="bibr" target="#b18">[19]</ref> for VQA and the CLEVR-Ref+ dataset <ref type="bibr" target="#b25">[26]</ref> for REF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We first provide an overview of the reasoning tasks addressed in this paper. Then we review related work on graph networks and other contextualized representations. Finally, we discuss alternative approaches to reasoning problems.</p><p>Visual question answering (VQA) and referring expression comprehension (REF) VQA and REF are two popular tasks that require reasoning about image content. While in VQA the goal is to answer a question about an image <ref type="bibr" target="#b2">[3]</ref>, in REF one has to localize an image region that corresponds to a referring expression <ref type="bibr" target="#b28">[29]</ref>. While the real-world VQA dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> focuses more on perception than complex reasoning, the more recent synthetic CLEVR <ref type="bibr" target="#b19">[20]</ref> dataset is a standard benchmark for relational reasoning. An even more recent GQA dataset <ref type="bibr" target="#b18">[19]</ref> brings together the best of both worlds: real images and relational questions. It is built upon the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref> and construct the balanced question-answer pairs from scene graphs.</p><p>For REF, there are a number of standard benchmarks such as RefCOCO <ref type="bibr" target="#b44">[45]</ref> and RefCOCOg <ref type="bibr" target="#b28">[29]</ref>, with natural language referring expressions and images from the COCO dataset <ref type="bibr" target="#b24">[25]</ref>. However, many of the expressions in these datasets do not require resolving relations. Recently, a new CLEVR-Ref+ dataset <ref type="bibr" target="#b25">[26]</ref> has been proposed for REF. It is built using the CLEVR environment and involves very complex queries, aiming to assess the reasoning capabilities of existing models and find their limitations.</p><p>In this work we tackle both VQA and REF tasks on three datasets in total. Notably, in all cases, we use the same approach, Language-Conditioned Graph Network (LCGN), to build contextualized representations of objects/image regions. This shows the generality and effectiveness of our approach for various visual reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph networks and contextualized representations</head><p>Graph networks are powerful models that can perform relational inference through message passing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>. The core idea is to enable communication between image regions to build contextualized representations of these regions. Graph networks have been successfully applied to various tasks, from object detection <ref type="bibr" target="#b26">[27]</ref> and region classification <ref type="bibr" target="#b6">[7]</ref> to human-object interaction <ref type="bibr" target="#b32">[33]</ref> and activity recognition <ref type="bibr" target="#b13">[14]</ref>. Besides, self-attention models <ref type="bibr" target="#b37">[38]</ref> and non-local networks <ref type="bibr" target="#b40">[41]</ref> can also be cast as graph networks in a general sense. Below we review some of the recent works that rely on graph networks and other contextualized representations for VQA and REF.</p><p>A prominent work that introduced relational reasoning in VQA is <ref type="bibr" target="#b35">[36]</ref>, which proposes Relation Networks (RNs) for modeling relations between all pairs of objects, conditioned on a question. <ref type="bibr" target="#b5">[6]</ref> extends RNs with the Broadcasting Convolutional Network module, which globally broadcasts objects' visuo-spatial features. The first work to use graph networks in VQA is <ref type="bibr" target="#b36">[37]</ref>, which combines dependency parses of questions and scene graph representations of abstract scenes. <ref type="bibr" target="#b47">[48]</ref> proposes modeling structured visual attention over a Conditional Random Field on image regions. A recent work, <ref type="bibr" target="#b29">[30]</ref>, conditions on a question to learn a graph representation of an image, capturing object interactions with the relevant neighbours via spatial graph convolutions. Later, <ref type="bibr" target="#b4">[5]</ref> extends this idea to modeling spatialsemantic pairwise relations between all pairs of regions.</p><p>For the REF task, <ref type="bibr" target="#b39">[40]</ref> proposes Language-guided Graph Attention Networks, where attention over nodes and edges is guided by a referring expression, which is decomposed into subject, intra-class and inter-class relationships.</p><p>Our work is related to, yet distinct from, the approaches above. While <ref type="bibr" target="#b29">[30]</ref> predicts a sparsely connected graph (conditioned on the question) that remains fixed for each step of graph convolution, our LCGN model predicts dynamic edge weights to focus on different connections in each message passing iteration. Besides, <ref type="bibr" target="#b29">[30]</ref> is tailored to VQA and is non-trivial to adapt to REF (since it includes max-pooling over node representations). Compared to <ref type="bibr" target="#b4">[5]</ref>, instead of max-pooling over explicitly constructed pairwise vectors, our model predicts normalized edge weights that both improve computation efficiency in message passing and make it easier to visualize and inspect connections. Finally, <ref type="bibr" target="#b39">[40]</ref> is tailored to REF by modeling specific subject attention and inter-and intra-class relations, and does not gather higherorder relational information in an iterative manner. We propose a more general approach for scene representation that is applicable to both VQA and REF.</p><p>Reasoning models A multitude of approaches have been recently proposed to tackle visual reasoning tasks, such as VQA and REF. Neural Module Networks (NMNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> are multi-step models that build question-specific layouts and execute them against an image. NMNs have also been applied to REF, e.g. CMN <ref type="bibr" target="#b16">[17]</ref> and Stack-NMN <ref type="bibr" target="#b14">[15]</ref>. MAC <ref type="bibr" target="#b17">[18]</ref> performs multi-step reasoning while recording information in its memory. FiLM <ref type="bibr" target="#b31">[32]</ref> is an approach which modulates image representation with the given question via conditional batch normalization, and is extended in <ref type="bibr" target="#b41">[42]</ref> with a multi-step reasoning procedure where both modalities can modulate each other. QGHC <ref type="bibr" target="#b9">[10]</ref> predicts questiondependent convolution kernels to modulate visual features. DFAF <ref type="bibr" target="#b8">[9]</ref> introduces self-attention and co-attention mechanisms between visual features and question words, allowing information to flow across modalities. The Neural-Symbolic approach <ref type="bibr" target="#b42">[43]</ref> disentangles reasoning from image and language understanding, by first extracting symbolic representations from images and text, and then executing symbolic programs over them. MAttNet <ref type="bibr" target="#b43">[44]</ref>, a state-ofthe-art approach to REF, uses attention to parse an expression and ground it through several modules.</p><p>Our approach is not meant to substitute the aforementioned reasoning models, but to complement them. Our contextualized visual representation can be combined with other reasoning models to replace the local feature representation. A prominent reasoning model capable of addressing both VQA and REF is Stack-NMN <ref type="bibr" target="#b14">[15]</ref>, and we empirically compare to it in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Language-Conditioned Graph Networks</head><p>Given a visual scene and a textual input for a reasoning task such as VQA or REF, we propose to construct a contextualized representation for each entity in the scene that contains the relational information needed for the reasoning procedure specified in the language input. This contextualized representation is obtained in our novel Language-Conditioned Graph Networks (LCGN) model, through iterative message passing conditioned on the language input. It can be then used as input to a taskspecific output module such as a single-hop VQA classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Context-aware scene representation</head><p>For an image I and a textual input Q that represents a reasoning task, let N be the number of entities in the scene, where each entity can be a detected object or a spatial location on the convolutional feature map of the image. Let x loc i (where i = 1, ..., N ) be the local feature representation of the i-th entity, i.e. the i-th detected object's visual feature or the convolutional feature at the i-th location on the feature grid. We would like to output a context-aware representation x out i for each entity i conditioned on the textual input Q that contains the relational context associated with entity i. This is obtained through iterative message passing over T iterations with our Language-Conditioned Graph Networks, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>We use a fully-connected graph over the scene, where each node corresponds to an entity i as defined above, and there is a directed edge i → j between every pair of entities i and j. Each node i is represented by a local feature x loc i that is fixed during message passing, and a context feature x ctx i,t that is updated during each iteration t. A learned parameter is used as the initial context representation x ctx i,0 at t = 0 for all nodes, before the message passing starts.</p><p>Textual command extraction To incorporate the textual input in the iterative message passing, we build a textual command vector for each iteration t (where t = 1, ..., T ). Given a textual input Q for the reasoning task, such as a question in VQA or a query in REF, we extract a set of vectors {c t } from the text Q, using the same multi-step textual attention mechanism as in Stack-NMN <ref type="bibr" target="#b14">[15]</ref> and MAC <ref type="bibr" target="#b17">[18]</ref>. Specifically, Q is encoded into a sequence {h s } S s=1 and a summary vector q with a bi-directional LSTM as:</p><formula xml:id="formula_0">[h 1 , h 2 , ..., h S ] = BiLSTM(Q) and q = [h 1 ; h S ] (1)</formula><p>where S is the number of words in Q, and h s = [ − → h s ; ← − h s ] is the concatenation of the forward and backward hidden states for word s from the bi-directional LSTM output. At each iteration t, a textual attention α t,s is computed over the words, and the textual command c t is obtained from the textual attention as follows:</p><formula xml:id="formula_1">α t,s = Softmax s W 1 h s W (t) 2 ReLU (W 3 q) (2) c t = S s=1 α t,s · h s<label>(3)</label></formula><p>where is element-wise multiplication. Each c t can be seen as a textual command supplied during the t-th iteration. Unlike all other parameters that are shared across iterations, here W (t) 2 is learned separately for each iteration t. Language-conditioned message passing At the t-th iteration where t = 1, ..., T , we first build a joint representation of each entity. Then, we compute the (directed) connection j,i to each object i, which is collected by i to update its context feature x ctx i,t . The local feature x loc i and the final context feature x ctx i,T are combined into a joint context-aware feature x out i , which is used in simple task-specific output modules for VQA or REF.</p><p>weights w (t) j,i from every entity j (the sender, j = 1, ..., N ) to every entity i (the receiver, i = 1, ..., N ). Finally, each entity j sends a message vector m (t) j,i to each entity i, and each entity i sums up all of its incoming messages m (t) j,i to update its contextual representation from x ctx i,t−1 to x ctx i,t as described below.</p><p>Step 1. We build a joint representationx i,t for each node, by concatenating x loc i and x ctx i,t−1 and their elementwise product (after linear mapping) as</p><formula xml:id="formula_2">x i,t = x loc i ; x ctx i,t−1 ; W 4 x loc i W 5 x ctx i,t−1<label>(4)</label></formula><p>Step 2. We compute the directed connection weights w (t) j,i from node j (the sender) to node i (the receiver), conditioning on the textual command c t at iteration t. Here, the connection weights are normalized with a softmax function over j, so that the sender weights sum up to 1 for each receiver, i.e. N j=1 w (t) j,i = 1 for all i = 1, ..., N as follows:</p><formula xml:id="formula_3">w (t) j,i = Softmax j (W 6xi,t ) T ((W 7xj,t ) (W 8 c t )) (5)</formula><p>Step 3. Each node j sends a message m (t) j,i to each node i conditioning on the textual input c t and weighted by the connection weight w (t) j,i . Then, each node i sums up the incoming messages and updates its context representation:</p><formula xml:id="formula_4">m (t) j,i = w (t) j,i · ((W 9xj,t ) (W 10 c t ))<label>(6)</label></formula><p>x ctx</p><formula xml:id="formula_5">i,t = W 11   x ctx i,t−1 ; N j=1 m (t) j,i  <label>(7)</label></formula><p>A naive implementation would involve N 2 pairwise vectors m (t) j,i , which is inefficient for large N . We implement it more efficiently by building an N -row matrix M containing N unweighted messagesm</p><formula xml:id="formula_6">(t) j = (W 9xj,t ) (W 10 c t ) in Eqn. 6, which is left multiplied by the edge weight ma- trix E (where E ij = w (t) j,i ) to obtain the sums N j=1 m (t) j,i</formula><p>in Eqn. 7 for all nodes in a single matrix multiplication. With this implementation, we can train our LCGN model efficiently with N as large as 196 in our experiments.</p><p>Final representation We combine each entity's local feature x loc i and context feature x ctx i,T (after T iterations) as its final representation x out i :</p><formula xml:id="formula_7">x out i = W 12 x loc i ; x ctx i,T<label>(8)</label></formula><p>The x out i can be used as input to subsequent task-specific modules such as VQA or REF models, instead of the original local representation x loc i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Application to VQA and REF</head><p>To apply our LCGN model to language-based reasoning tasks such as Visual Question Answering (VQA) and Referring Expression Comprehension (REF), we build simple task-specific output modules based on the language input and the contextualized representation of each entity. Our LCGN model and the subsequent task-specific modules are jointly trained end-to-end.</p><p>A single-hop answer classifier for VQA The VQA task requires outputting an answer for an input image I and a question Q. We adopt the commonly used classification approach and build a single-hop attention model as a classifier to select one of the possible answers from the training set.</p><p>First, the question Q is encoded into a vector q with the Bi-LSTM in Eqn. 1. Then a single-hop attention β i is used over the objects to aggregate visual information, which is fused with q to predict the score vector y for each answer.</p><formula xml:id="formula_8">β i = Softmax i W 13 x out i (W 14 q) (9) y = W 15 ReLU W 16 N i=1 β i x out i ; q<label>(10)</label></formula><p>During training, a softmax or sigmoid classification loss is applied on the output scores y for answer classification.</p><p>GroundeR <ref type="bibr" target="#b34">[35]</ref> for REF The REF task requires outputting a target bounding box as the grounding result for an input referring expression Q. Here, we use a retrieval approach as in previous works and select one target entity from the N candidate entities in the scene (either object detection results or spatial locations on a convolutional feature map). To select the target object p from the N candidates, we encode expression Q to vector q as in Eqn 1 and build a model similar to the fully-supervised version of GroundeR <ref type="bibr" target="#b34">[35]</ref> to output a matching score r i for each entity i. In the case of using spatial locations on a convolutional feature map, we further output a 4-dimensional vector u to predict the bounding box offset from the feature grid location.</p><formula xml:id="formula_9">r i = W 17 x out i (W 18 q) (11) p = arg max i r i (12) u = W 19 x out p<label>(13)</label></formula><p>During training, we use a softmax loss over the scores r i among the N candidates to select the target entity p, and an L2 loss over the box offset u to refine the box location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We apply our LCGN model to two tasks -VQA and REF -for language-conditioned reasoning. For the VQA task, we evaluate on the GQA dataset <ref type="bibr" target="#b18">[19]</ref> and the CLEVR dataset <ref type="bibr" target="#b19">[20]</ref>, which both require resolving relations between objects. For the REF task, we evaluate on the CLEVR-Ref+ dataset <ref type="bibr" target="#b25">[26]</ref>. In particular, the CLEVR and CLEVR-Ref+ datasets contain many complicated questions or expressions with higher-order relations, such as the ball on the left of the object behind a blue cylinder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visual Question Answering (VQA)</head><p>Evaluation on the GQA dataset We first evaluate our LCGN model on the GQA dataset <ref type="bibr" target="#b18">[19]</ref> for visual question answering. The GQA dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref> and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs,  <ref type="bibr" target="#b33">[34]</ref>.</p><p>We apply our LCGN model together with the single-hop classifier ("single-hop + LCGN") in Sec. 3.2 for answer prediction. We use T = 4 rounds of message passing in our LCGN model, which takes approximately 20 hours to train using a single Titan Xp GPU. As a comparison to our LCGN model, we also train the single-hop classifier with only the local features x loc in Eqn. 9 ("single-hop").</p><p>We first experiment with using the released object detection features in the GQA dataset as our local features x loc , which is shown in <ref type="bibr" target="#b18">[19]</ref> to perform better than the convolutional grid features, and compare with previous works. <ref type="bibr" target="#b0">1</ref> Similar to MAC <ref type="bibr" target="#b17">[18]</ref>, we initialize question word embedding from GloVe <ref type="bibr" target="#b30">[31]</ref> and maintain an exponential moving average of model parameters during training. To facilitate spatial reasoning, we concatenate the Faster R-CNN object detection features with their corresponding box coordinates. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. By comparing "single-hop + LCGN" with "single-hop" in the last two rows, it can be seen that our LCGN model brings around 2% (absolute) improvement in accuracy, indicating that our LCGN model facilitates reasoning by replacing the local features x loc with the contextualized features x out containing rich relational information for the reasoning task. <ref type="figure" target="#fig_3">Figure 3</ref> shows question answering examples from our model on this dataset.</p><p>We compare with three previous approaches in <ref type="table" target="#tab_0">Table 1</ref>. CNN+LSTM <ref type="bibr" target="#b18">[19]</ref> and Bottom-Up <ref type="bibr" target="#b0">[1]</ref> are simple fusion approaches between the text and the image, using the released GQA convolutional grid features or object detection features respectively. The MAC model <ref type="bibr" target="#b17">[18]</ref> is a multi-step attention and memory model with specially designed control, reading and writing cells, and is trained on the same object detection features as our model. Our approach outperforms the MAC model that performs multi-step inference, obtain- ing the state-of-the-art results on the GQA dataset. We further apply our LCGN model to other types of local features, and experiment with using either the same 7×7×2048-dimensional convolutional grid features (where each x loc i is a feature map location and N = 49) as used in CNN+LSTM in <ref type="table" target="#tab_0">Table 1</ref> or an "oracle" symbolic local representation at both training and test time, based on a set of ground-truth objects along with their class and attribute annotations ("GT objects and attributes") in the scene graph data of the GQA dataset. In the latter setting with symbolic representation, we construct two one-hot vectors to represent each object's class and attributes, and concatenate them as each object's x loc i . <ref type="bibr" target="#b1">2</ref> The results are shown in <ref type="table" target="#tab_1">Table 2</ref>, where our LCGN model delivers consistent improvements over all three types of local feature representations.</p><p>Evaluation on the CLEVR dataset We also evaluate our LCGN model on the CLEVR dataset <ref type="bibr" target="#b19">[20]</ref>, a dataset for VQA with complicated relational questions, such as what number of other objects are there of the same size as the brown shiny object. Following previous works, we use the 14 × 14 × 1024 convolutional grid features extracted from the C4 block of an ImageNet-pretrained ResNet-101 network <ref type="bibr" target="#b12">[13]</ref> as the local features x loc on the CLEVR dataset (i.e. each x loc i is a feature map location and N = 196). Similar to our experiments on the GQA dataset, we apply our LCGN model together with the single-hop answer classifier and compare it with using only the local features in the answer classifier. We also compare with previous works that use only question-answer pairs as supervision (i.e. without relying on the functional program annotations in <ref type="bibr" target="#b19">[20]</ref>).</p><p>The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. It can be seen that the single-hop classifier only achieves 72.6% accuracy when using the local convolutional grid features x loc ("singlehop"), which is unsurprising since the CLEVR dataset often involves resolving multiple and higher-order relations beyond the capacity of the single-hop classifier alone. However, when trained together with the context-aware representation x out from our LCGN model, this same single-hop classifier ("single-hop + LCGN") achieves a significantly <ref type="bibr" target="#b1">2</ref> In this setting, we can only evaluate on the val split with public scene graph annotations. We note that this is the only setting where we use the scene graphs in the GQA dataset. In all other settings, we only use the images and question-answer pairs to train our models. Also, our model does not rely on the GQA question semantic step annotations in any settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Stack-NMN <ref type="bibr" target="#b14">[15]</ref> 93.0% RN <ref type="bibr" target="#b35">[36]</ref> 95.5% FiLM <ref type="bibr" target="#b31">[32]</ref> 97.6% MAC <ref type="bibr" target="#b17">[18]</ref> 98.9% NS-CL <ref type="bibr" target="#b27">[28]</ref> 99.2%  <ref type="figure" target="#fig_5">Figure 4</ref> shows question answering examples of our model. We further experiment with varying the number T of message passing iterations in our LCGN model. In addition, to isolate the effect of conditioning on textual inputs during message passing, we also train and evaluate a restricted version of LCGN without text conditioning ("single-hop + LCGN w/o txt"), by replacing the c t 's from Eqn 3 with a vector of all ones. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>, where it can be seen that using multiple rounds of iterations (T &gt; 1) leads to a significant performance increase, and it is crucial to incorporate the textual information c t into the message passing procedure. This is likely because the CLEVR dataset involves complicated questions that need multi-step context propagation. In addition, it is more efficient to collect the specific relational context relevant to the input question, instead of building a scene representation with a complete and unconditional knowledge base of all relational information that any input questions can query from.</p><p>Given that multi-round message passing (T &gt; 1) works better than using only a single round (T = 1), we further study whether it is beneficial to have dynamic connection weights w (t) j,i in Eqn. 5 that can be different in each iteration t to allow an object i to focus on different context objects j in different rounds. As a comparison, we train a restricted version of LCGN with static connection weights w j,i ("single-hop + LCGN w/ static w j,i "), where we only predict the weights w <ref type="bibr" target="#b0">(1)</ref> j,i in Eqn. 5 for the first round t = 1, and reuse it in all subsequent rounds (i.e. setting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Steps  <ref type="table" target="#tab_5">Table 4</ref>. Ablation on iteration steps T and whether to condition on the text or have dynamic weights, on the CLEVR validation split.</p><formula xml:id="formula_10">w (t) j,i = w (1)</formula><p>j,i for all t &gt; 1). From the last row of <ref type="table" target="#tab_5">Table 4</ref> it can be seen that there is a performance drop when restricting to static connection weights w j,i predicted only in the first round, and we also observe a similar (but larger) drop for the REF task in Sec. 4.2 and <ref type="table" target="#tab_5">Table 5</ref>. This suggests that it is better to have dynamic connections during each iteration, instead of first predicting a fixed connection structure on which iterative message passing is performed (e.g. <ref type="bibr" target="#b29">[30]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Referring Expression Comprehension (REF)</head><p>Our LCGN model provides a generic approach to building context-aware scene representations and is not restricted to a specific task such as VQA. We also apply our LGCN model to the referring expression comprehension (REF) task, where given a referring expression that describes an object in the scene, the model is asked to localize the target object with a bounding box.</p><p>We experiment with the CLEVR-Ref+ dataset <ref type="bibr" target="#b25">[26]</ref>, which contains similar images as in the CLEVR dataset <ref type="bibr" target="#b19">[20]</ref> and complicated referring expressions requiring relation resolution. On the CLEVR-Ref+ dataset, we evaluate with the bounding box detection task in <ref type="bibr" target="#b25">[26]</ref>, where the output is a bounding box of the target object and there is only one single target object described by the expression. A localization is consider correct if it overlaps with the ground-truth box with at least 50% IoU. Same as in our VQA experiments on the CLEVR dataset in Sec. 4.1, here we also use the 14×14×1024 convolutional grid features from ResNet-101 C4 block as our local features x loc (i.e. each x loc i is a feature map location and N = 196), with T = 4 rounds of message passing. The final target bounding box is predicted with a 4-dimensional bounding box offset vector u in Eqn. 13 from the selected grid location p in Eqn. 12.</p><p>We apply our LCGN model to build a context-aware representation x out conditioned on the input referring expression, which is used as input to our implementation of the GroundeR approach <ref type="bibr" target="#b34">[35]</ref> (Sec. 3.2) for bounding box prediction ("GroundeR + LCGN"). As a comparison, we train and evaluate the GroundeR model without our contextaware representation ("GroundeR"), using local features x loc as inputs in Eqn. 11. Similar to our experiments on the CLEVR dataset for VQA in Sec. 4.1, we also ablate our Method Accuracy Stack-NMN <ref type="bibr" target="#b14">[15]</ref> 56.5% SLR <ref type="bibr" target="#b45">[46]</ref> 57.7% MAttNet <ref type="bibr" target="#b43">[44]</ref> 60.9%</p><p>GroundeR <ref type="bibr" target="#b34">[35]</ref> 61.7% GroundeR + LCGN w/o txt 65.0% GroundeR + LCGN w/ static wj,i 71.4% GroundeR + LCGN (ours) 74.8% <ref type="table" target="#tab_5">Table 5</ref>. Performance on the CLEVR-Ref+ dataset for REF.</p><p>LCGN model with not conditioning on the input expression in message passing ("GroundeR + LCGN w/o txt") or using static connection weights w j,i predicted from the first round ("GroundeR + LCGN w/ static w j,i ").</p><p>The results are shown in <ref type="table" target="#tab_5">Table 5</ref>, where our contextaware scene representation from LCGN leads to approximately 13% (absolute) improvement in REF accuracy. Consistent with our observation on the VQA task, for the REF task we find it important for the message passing procedure to depend on the input expression, and allowing the model to have dynamic connection weights w (t) j,i that can differ for each round t. Our model outperforms previous work by a large margin, achieving the state-of-the-art performance for REF on the CLEVR-Ref+ dataset. <ref type="figure" target="#fig_7">Figure 5</ref> shows example predictions of our model on the CLEVR-Ref+ dataset.</p><p>In previous works, SLR <ref type="bibr" target="#b45">[46]</ref> and MAttNet <ref type="bibr" target="#b43">[44]</ref> are specifically designed for the REF task. SLR jointly trains an expression generation model (speaker) and an expression comprehension model (listener), and MAttNet relies on modular structure for subject, location and relation comprehension. While Stack-NMN <ref type="bibr" target="#b14">[15]</ref> is also a generic approach that is applicable to both the VQA task and the REF task, the major contribution of Stack-NMN is to construct an explicit step-wise inference procedure with compositional modules, and it relies on hand-designed module structures and local appearance-based scene representations. On the other hand, our work augments the scene representation with rich relational context. We show that our approach outperforms Stack-NMN on both the VQA and the REF tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose Language-Conditioned Graph Networks (LCGN), a generic approach to language-based reasoning tasks such VQA and REF. Instead of building task-specific inference procedures, our LCGN model constructs rich context-aware representations of the scene through iterative message passing. Experimentally, we show that the context-aware representations from our LCGN model can improve over the local appearance-based representations across various types of local features and multiple datasets, and it is crucial for the message passing procedure to depend on the language inputs. Acknowledgements. This work was partially supported by the Berkeley AI Research, the NSF and DARPA XAI.  j,i exceeds a threshold. The blue star on each line is the sender node j, and the line width corresponds to its connection weight. In the upper example, the person, the elephant and the fence propagate messages with each other, and fence receives messages from the elephant in t = 4. In the lower example, the frisbee collect messages from the dog as contextual information in multiple rounds, and is picked up by the single-hop classifier. The red star (along with the box) in the last column shows the object with the highest single-hop attention βi in Eqn. 9.  j,i similar to <ref type="figure" target="#fig_3">Figure 3</ref>, where the blue stars are the sender nodes. The last column shows the single-hop attention βi in Eqn. 9 over the N = 14 × 14 feature grid. In the upper example, in t = 1 the matte ball (leftmost) collects messages from the gray metal ball (of the same size), and then in t = 3 messages are propagated within the convolutional grids on the matte ball, possibly to refine the collected context from the gray ball. In the lower example, in t = 1 all four balls try to propagate messages within the convolutional grids of each ball region, and in t = 2 the three other balls (of the same size) receive messages from the rubber ball (leftmost) and are picked up by the single-hop classifier.  j,i similar to <ref type="figure" target="#fig_3">Figure 3</ref>, where the blue stars are the sender nodes. The last column shows the selected target grid location p on the N = 14 × 14 spatial grid (the red star) in Eqn. 12, along with the ground-truth (yellow) box and the predicted box (red box from bounding box regression u in Eqn. 13). In the upper example, the blue cube (the target object) collects messages from the two other objects in t = 2, and then the blue cube further collects messages from the big matte green cube on the left (which has the same shape) in t = 3. In the lower example, the green cube checks for other cubes by collecting messages from things on its right in t = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>In our implementation, we use d txt = 512 as the dimensionality of the textual vectors (such as h s , q, and c t ), and d ctx = 512 as the dimensionality of the context features x ctx i of each entity i. On the GQA dataset, we first reduce the dimensionality of the input local features x loc i (convolutional grid features, object detection features or GT objects and attributes in <ref type="table" target="#tab_1">Table 2</ref> of the main paper) to the same dimensionality d loc = 512 with a single fully-connected layer (without non-linearity). During training, we train with a sigmoid cross entropy loss and use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with a batch size of 128 and a learning rate of 3 × 10 −4 .</p><p>On the CLEVR dataset and the CLEVR-Ref+ dataset, we first apply a small two-layer convolutional network on the ResNet-101-C4 features to output a 14 × 14 × 512 feature map, so that the feature dimensionality at each location on the feature map is also reduced to d loc = 512. We use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with a batch size of 64 and a learning rate of 10 −4 . On the CLEVR dataset, we train with a softmax loss for answer classification. On the CLEVR-Ref+ dataset, we train with a softmax loss to select the target location p and an L2 loss (i.e. mean squared error loss) for the  bounding box offset u.</p><p>To facilitate reasoning about spatial relations such as "left" and "right", we also add spatial information to the local features. On the GQA dataset, when using object detection features or GT objects and attributes, we concatenate the local features with the bounding box coordinates of the corresponding objects. When using convolutional grid features (on GQA, CLEVR and CLEVR-Ref+), for each convolutional grid location (h, w), we concatenate the sinusoidal positional encoding <ref type="bibr" target="#b37">[38]</ref> of h and w to the convolutional channel output at (h, w).</p><p>The shapes of the parameters in our LCGN model are shown in <ref type="table" target="#tab_5">Table A</ref>.1. All our models are trained using a single Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative analysis on edge weights</head><p>We perform quantitative analysis on the learned edge weights {w j,i }, and measure how much they vary across different questions on the same image using the CLEVR dataset (where all images have exactly 10 associated questions). For each receiver node i, we associate it with a max-connected sender node j * = arg max j {w j,i }. Then, we count for each receiver node i in the image how many unique j * there are (across the 10 questions) -this number would be between 1 and 10 for each image; the higher number, the more variance in w j,i across questions. On average, each source node i is connected to 6.396 unique j * across the 10 questions on the same image, showing that the learned edge weights {w j,i } are largely dependent on the input questions. j,i exceeds a threshold. The blue star on each line is the sender node j. In these example, the objects of interest receive messages from other objects through those connections with high weights (the red lines). The red star (along with the box) in the last column shows the object with the highest attention βi in the single-hop VQA classifier in Sec. 3.2 of the main paper. The last two rows show two failure examples on the GQA dataset. Some failure cases are due to ambiguity in the answers in the GQA dataset (e.g. "woman" vs. "lady" in the last example). question: is the number of small cylinders behind the cyan thing greater than the number of cubes that are behind the green block? prediction: yes ground-truth: no question: how many other objects are the same shape as the purple metallic thing? prediction: 6 ground-truth: 7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional visualization examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3Figure 2 .</head><label>2</label><figDesc>Is there a man on the right of a person sitting on a chair holding a wine glass? We propose Language-Conditioned Graph Networks (LCGN) to address reasoning tasks such as VQA and REF. Our model constructs a context-aware representation x out i for each object i through iterative message passing conditioned on the input text. During message passing, each object i is represented by a local feature x loc i and a context feature x ctx i,t . In every iteration, each object j sends a message vector m (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>attention βi question: is the fence in front of the elephant green and metallic? prediction: yes ground-truth: yes question: the frisbee is on what animal? prediction: dog ground-truth: dog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples from our LCGN model on the validation split of the GQA dataset for VQA. In the middle 4 columns, each red line shows an edge j → i along the message passing paths (among the N detected objects) where the connection edge weight w (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>single-hop attention βiquestion: what color is the matte ball that is the same size as the gray metal thing? prediction: yellow ground-truth: yellow question: how many other things are the same size as the yellow rubber ball? prediction: 3 ground-truth: 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Examples from our LCGN model on the validation split of the CLEVR dataset for VQA. The middle 4 columns show the connection edge weights w (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>referring expression: any other things that are the same shape as the big matte thing(s) referring expression: the second one of the cube(s) from right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Examples from our LCGN model on the validation split of the CLEVR-Ref+ dataset for REF. The middle 4 columns show the connection edge weights w (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figures C. 1 Figure C. 1 .</head><label>11</label><figDesc>and C.2 show additional visualization examples for the VQA task on the GQA dataset and the CLEVR dataset, respectively. Figure C.3 shows additional examples for the REF task on the CLEVR-Ref+ dataset. attention βi question: are there carts near the pond? prediction: yes ground-truth: yes question: what color is the flag? prediction: white ground-truth: white question: what type of vehicle is in front of the hanging wires? prediction: train ground-truth: train question: on what does the man sit? prediction: bench ground-truth: bench question: are there both a tennis ball and a racket in the image? prediction: yes ground-truth: yes question: what vehicle is on the highway? prediction: truck ground-truth: ambulance question: who is holding the umbrella? prediction: woman ground-truth: lady Additional examples from our LCGN model on the validation split of the GQA dataset for VQA. In the middle 4 columns, each red line shows an edge j → i along the message passing paths (among the N detected objects) where the connection edge weight w (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>attention βi question: there is a small gray block ; are there any spheres to the left of it? prediction: yes ground-truth: yes question: is the purple thing the same shape as the large gray rubber thing? prediction: no ground-truth: no question: do the large metal sphere and the matte block have the same color? prediction: yes ground-truth: yes question: is there anything else that has the same material as the red thing? prediction: yes ground-truth: yes question: is there any other thing that is the same color as the cylinder? prediction: no ground-truth: no question: what number of other objects are there of the same size as the gray sphere? prediction: 5 ground-truth: 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure C. 2 .</head><label>2</label><figDesc>Additional examples from our LCGN model on the validation split of the CLEVR dataset for VQA. The middle 4 columns show the connection edge weights w(t) j,i similar to Figure C.1, where the blue stars are the sender nodes. The last column shows the attention βi in the single-hop VQA classifier in Sec. 3.2 of the main paper over the N = 14 × 14 feature grid. In these examples, the relevant objects in the question usually first propagate messages within the convolutional grids of the same object (possibly to form an object representation from the CNN features), and then the object of interest tends to collect messages from other context objects. The last two rows show two failure examples on the CLEVR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>VQA performance on the GQA dataset.<ref type="bibr" target="#b0">1</ref> the GQA dataset provides two types of pre-extracted visual features for each image -convolutional grid features of size 7 × 7 × 2048 extracted from a ResNet-101 network<ref type="bibr" target="#b12">[13]</ref> trained on ImageNet, and object detection features of size N</figDesc><table><row><cell>Method</cell><cell>val</cell><cell>Accuracy 1 test-dev</cell><cell>test</cell></row><row><cell>CNN+LSTM [19]</cell><cell>49.2%</cell><cell>-</cell><cell>46.6%</cell></row><row><cell>Bottom-Up [1]</cell><cell>52.2%</cell><cell>-</cell><cell>49.7%</cell></row><row><cell>MAC [18]</cell><cell>57.5%</cell><cell>-</cell><cell>54.1%</cell></row><row><cell>single-hop</cell><cell>62.0%</cell><cell>53.8%</cell><cell>54.4%</cell></row><row><cell cols="2">single-hop + LCGN (ours) 63.9%</cell><cell>55.8%</cell><cell>56.1%</cell></row></table><note>det × 2048 (where N det is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation on different local features on the GQA dataset.</figDesc><table><row><cell>Method</cell><cell>Local features</cell><cell cols="2">Accuracy val test-dev</cell></row><row><cell>single-hop</cell><cell>convolutional</cell><cell>55.0%</cell><cell>48.6%</cell></row><row><cell>single-hop + LCGN</cell><cell>grid features</cell><cell>55.3%</cell><cell>49.5%</cell></row><row><cell>single-hop</cell><cell cols="2">object features 62.0%</cell><cell>53.8%</cell></row><row><cell cols="3">single-hop + LCGN from detection 63.9%</cell><cell>55.8%</cell></row><row><cell>single-hop</cell><cell>GT objects</cell><cell>87.0%</cell><cell>n/a</cell></row><row><cell cols="3">single-hop + LCGN and attributes 2 90.2%</cell><cell>n/a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>VQA performance on the test split of the CLEVR dataset. We use T = 4 rounds of message passing in our LCGN model.</figDesc><table><row><cell>single-hop</cell><cell>72.6%</cell></row><row><cell>single-hop + LCGN (ours)</cell><cell>97.9%</cell></row><row><cell cols="2">higher accuracy of 97.9% comparable to several state-of-</cell></row><row><cell cols="2">the-art approaches on this dataset, showing that our LCGN</cell></row><row><cell cols="2">model is able to embed relational context information in its</cell></row><row><cell cols="2">output scene representation x out . Among previous works,</cell></row><row><cell cols="2">Stack-NMN [15] and MAC [18] rely on multi-step infer-</cell></row><row><cell cols="2">ence procedures to predict an answer. RN [36] pools over</cell></row><row><cell cols="2">all N 2 pairwise object-object vectors to collect relational</cell></row><row><cell cols="2">information in a single step. FiLM [32] modulates the</cell></row><row><cell cols="2">batch normalization parameters of a convolutional network</cell></row><row><cell cols="2">with the input question. NS-CL [28] learns symbolic rep-</cell></row><row><cell cols="2">resentations of the scene and uses quasi-logical reasoning.</cell></row></table><note>Except for Stack-NMN [15], most previous works are tai- lored to the VQA task, and it is non-trivial to apply them to other tasks such as REF, while our LCGN model provides a generic scene representation applicable to multiple tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc></figDesc><table><row><cell>(t) 2 .</cell></row></table><note>1. The parameter shapes in our LCGN model. All param- eters are shared across different time steps t, except for W</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We learned from the GQA dataset authors that its test-dev and test splits were collected differently from its train and val splits, with a noticeable domain shift from val to test-dev and test. We train on the train split and report results on three GQA splits (val, test-dev and test). The performance of previous work on val was obtained from the dataset authors.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>input image t = 1 t = 2 t = 3 t = 4 bounding box output referring expression: any other yellow shiny objects that have the same size as the first one of the objects from front referring expression: any other tiny objects that have the same material as the third one of the objects from left referring expression: the second one of the things from left referring expression: any other matte things that have the same shape as the first one of the red metal things from right referring expression: the first one of the things from front that are on the right side of the first one of the purple spheres from front referring expression: the second one of the shiny objects from front referring expression: any other matte things of the same shape as the fifth one of the rubber things from right referring expression: look at sphere that is right of the first one of the things from front; the second one of the objects from right that are in front of it </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Broadcasting convolutional network for visual relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simyung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7239" to="7248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question-guided hybrid convolution for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Classifying collisions with spatio-temporal action graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Brosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01233</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gqa: a new dataset for compositional question answering over realworld images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clevr-ref+: Diagnosing visual reasoning with referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure inference net: object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6985" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stathis</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cascaded mutual modulation for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural-symbolic vqa: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Pushmeet Kohli, and Josh Tenenbaum</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A joint speaker-listener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7282" to="7290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1291" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
