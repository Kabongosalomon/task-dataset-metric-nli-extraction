<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Strong Baseline for Fashion Retrieval with Person Re-Identification models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>1 Synerise</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Wieczorek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Information Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Michalowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Information Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wroblewska</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Information Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Information Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dabrowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Information Science</orgName>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Strong Baseline for Fashion Retrieval with Person Re-Identification models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">1 Synerise</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>clothes retrieval</term>
					<term>quadruplet loss</term>
					<term>person re-identification</term>
					<term>deep learning in fashion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fashion retrieval is the challenging task of finding an exact match for fashion items contained within an image. Difficulties arise from the fine-grained nature of clothing items, very large intra-class and inter-class variance. Additionally, query and source images for the task usually come from different domains -street photos and catalogue photos respectively. Due to these differences, a significant gap in quality, lighting, contrast, background clutter and item presentation exists between domains. As a result, fashion retrieval is an active field of research both in academia and the industry. Inspired by recent advancements in Person Re-Identification research, we adapt leading ReID models to be used in fashion retrieval tasks. We introduce a simple baseline model for fashion retrieval, significantly outperforming previous state-of-the-art results despite a much simpler architecture. We conduct in-depth experiments on Street2Shop and DeepFashion datasets and validate our results. Finally, we propose a cross-domain (cross-dataset) evaluation method to test the robustness of fashion retrieval models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fashion image retrieval, <ref type="bibr" target="#b2">3</ref> commonly associated with visual search, has received a lot of attention recently. This is an interesting challenge from both a commercial and academic perspective. The task of fashion retrieval is to find an exact or very similar products from the vendor's catalogue (gallery) to the given query image. Creating a model that can find similarities between content of the images is essential for the two basic visual-related products for the fashion industry: recommendations and search.</p><p>Visual recommendations (VR) -visual similarity needs to be found between a currently viewed product and other products in the database. Retrieval is done among the images from a single domain, which is a domain of catalogue photos.</p><p>Visual search (VS) -visual similarity between user taken/uploaded photo and the products' photos in the vendor's database. This case is a cross-domain retrieval as the photos from catalogue contains generally a single item and are taken by professionals using high-quality equipment, assuring proper background (usually solid white) and lighting conditions. On the other hand, photos taken by users are taken with a smartphone camera in uncontrolled lighting conditions and are of lower quality, noisy, with multiple persons and garments.</p><p>From a business perspective, especially for e-commerce retailers, these tools still have a lot of untapped potential. VS in particular can be used in various ways to enrich the customer experience and facilitate a more convenient search, since it may be easier and more natural for users to take a photo of a product and upload it directly via an app rather than to use textual search.</p><p>During work on VS solutions, we found that the task of visual search for clothes is in many ways analogous to Person Re-Identification problem (ReID). Yet we have not encountered any work that examines models specific to Person ReID tasks in the context of fashion image retrieval tasks. Thus, we decided to modify and apply approaches from Person ReID field to fashion retrieval.</p><p>There are four main differences between the ReID and VS problems. Firstly, in ReID tasks data may be regarded as homogeneous, since the query and gallery images are both taken by CCTV cameras, therefore they are from the same domain. The cameras, resolution, view angles, and other factors may vary within the domain. In fashion visual search tasks images come from two domains. One domain contains store catalogue photos taken by professionals at the same studio settings. The second domain consists of photos taken by users, which may vary due to lighting conditions, the quality of the camera, angles and focal length. Additionally, the content of the image may be photographed differently. Catalogue photos are often well aligned, the background is usually solid white or monochromatic and the fashion item is fully visible. In contrast, user images often contain multiple objects, a cluttered background and the fashion item is often occluded or cropped.</p><p>Secondly, ReID problems may use additional temporal information to narrow down the search space, e.g. <ref type="bibr" target="#b35">[32]</ref>, which adds an information stream unavailable in the fashion domain.</p><p>Moreover, fashion items, especially clothes, are highly deformable goods resulting in a high intra-class variance in appearance, further complicating appearance comparisons.</p><p>Finally, in ReID, the majority of images contain a whole person and can be relatively easily aligned. User images in fashion may differ both in the alignment, orientation and content. They may contain a whole person with multiple clothing items, upper or lower half of a person, clothes laying on the sofa etc.</p><p>We also pinpointed four major similarities between ReID and clothes retrieval. In both domains, the core problem is an instance retrieval task that relies heavily on fine-grain details, which makes both tasks complex and challenging. Secondly, both domains must implicitly consider a common subset of clothes &amp; materials deformations specific to human body poses and shapes. Moreover, in both domains the models have to deal with occlusion, lighting, angle and orientation differences. Finally, clothes are also an important factor in ReID task, especially when faces are not visible.</p><p>Due to the similarities and despite the differences of the two aforementioned tasks we decided to investigate how and which ReID solutions can be effectively applied to the fashion retrieval task. Our main contributions are:</p><p>-Recognition of deep analogies between Person ReID and Fashion Retrieval tasks. -A review of ReID models and their applicability to Fashion Retrieval tasks.</p><p>-Adjustments of ReID models for their application to the fashion domain.</p><p>-Thorough evaluation of the adapted ReID models on two commonly used fashion datasets. -A simple baseline model for fashion retrieval significantly outperforming state-of-the-art results for DeepFashion and Street2Shop datasets. -A cross-domain (cross-dataset) evaluation method to test robustness of the solutions.</p><p>In the following sections we provide a review of Person Re-Identification models and solutions used in Fashion Retrieval domain. We show the baselines for our comparisons (Section 2). Then, we provide an argument for our selection of applicable ReID models (Section 3). Subsequently, we describe our results, compare them with previous state-of-the-art approaches and provide a discussion (Section 4) of our findings. We conclude with our best achievements (Section 5). In supplement material we provide statistics of datasets used for training and evaluation, as well as additional samples from model outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section focuses mainly on the problems of extracting features from images and the retrieval process itself. Instance retrieval is basically a two-stage process: (1) encoding image/item into a n-dimensional embedding vector; (2) searching for the most similar product embedding to the query in the n-dimensional space under a chosen distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>The process of extracting features from an image may be defined as passing the pixel matrix through a function to obtain an embedding vector that encompasses all necessary information about the image in the encoded form. It is the first and most indispensable step towards image retrieval. Most research is devoted to improving the performance of this stage.</p><p>Over the years, the functions used for encoding image content have evolved. At first, hand-crafted features describing global image characteristics (e.g. histogram of oriented gradients -HOG) or local dependencies like SIFT (scaleinvariant feature transform) were used. These methods did work, however subsequently convolutional neural networks (CNNs) surpassed these methods in computer vision tasks and dominated the field.</p><p>One of the first works using CNN for an image retrieval task was <ref type="bibr" target="#b25">[24]</ref> that also managed to surpass the stat-of-the-art of approaches based on SIFT, HOG and other hand-crafted features. In <ref type="bibr" target="#b5">[6]</ref> authors used CNNs extracted features and trained a model using a triplet loss function, which was widespread by <ref type="bibr" target="#b27">[26]</ref> for face recognition task. By 'CNN extracted features' we mean the final output of either one or many convolutional layers that are arranged in various combinations (e.g. pyramid, sequence of layers). All of these combinations work as various learned filters for an image matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Retrieval</head><p>Image retrieval may be defined as a task of matching images based on their feature vectors/embeddings under a chosen distance metric. There are three main approaches when dealing with image retrieval using deep learning embeddings: direct representation, classification loss training and deep metric learning. We explain them in the following paragraphs.</p><p>Direct representation This is the simplest approach as it does not involve any training on the target dataset. In most cases, an ImageNet pre-trained neural network is used. To obtain the feature vector, an image is fed into the network and the output from one or more layers is treated as its embedding. To find the most similar images to the query, a cosine or other similarity metric is applied to the set of embeddings. Such an approach was used in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Classification loss training Another approach is to again take a pre-trained CNN and train it on the target dataset using a classification loss function. The difference to the direct representation is that the network is further trained (finetuned) on a target dataset. Thus, the embeddings should be optimized in a way to allow correct classification and in turn indirectly improve the retrieval quality.</p><p>This approach was also present in previously mentioned reference <ref type="bibr" target="#b13">[14]</ref>, where the authors used pre-trained AlexNet to learn the similarity between street and shop photos using cross-entropy loss over pair of images, classifying them as either matching or non-matching.</p><p>Deep metric learning Image retrieval formulated as a classification problem is not an optimal approach, as the classification is done mainly in the last fully connected (FC) layer of the neural network (NN), which is not used during the inference stage. Moreover, the gradient flow does not explicitly optimize the feature embeddings to be similar/dissimilar under the distance metric used for retrieval.</p><p>Deep metric learning treats the image retrieval problem somewhat as a clustering or ranking problem and optimizes the mutual arrangement of the embeddings in space. One of the most widely used approaches that pulls the same class closely and pushes away other class embeddings is using a triplet loss for training neural network. The triplet loss and its' modifications are described in our supplement in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fashion Image Understanding</head><p>Fashion image understanding has been an area of research for a while. The papers that introduced two modern large public datasets <ref type="bibr" target="#b41">[38]</ref>, <ref type="bibr" target="#b12">[13]</ref>, gave the start for a rapid development of machine learning applications in the fashion field. Over the years numerous fashion datasets were released by various authors: <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b46">[43]</ref>, <ref type="bibr" target="#b50">[47]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> accelerating the research and allowing a wider range of tasks to be tackled.</p><p>Clothes retrieval seems to be the most popular task <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b46">[43]</ref>, <ref type="bibr" target="#b42">[39]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, it is often combined with another task to attain a synergy effect, either with landmark detection (detecting keypoints of clothes) <ref type="bibr" target="#b20">[20]</ref>, attribute prediction <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[20]</ref>, or object detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Re-identification</head><p>The problem of Person Re-Identification, being an instance retrieval task, has undergone a similar evolution as the image retrieval domain, starting from using hand-crafted features <ref type="bibr" target="#b16">[17]</ref> and matching their embeddings under chosen distance metric. Currently CNN extracted features are the dominant approach.</p><p>Similarly to Section 2.2 a variety of loss functions are used in ReID problems: classification loss <ref type="bibr" target="#b45">[42]</ref>, verification loss <ref type="bibr" target="#b4">[5]</ref> and triplet loss <ref type="bibr" target="#b18">[19]</ref>. Triplet Loss performs best <ref type="bibr" target="#b8">[9]</ref> in most cases by a significant margin.</p><p>Due to the fact that the ReID problem focuses on images of people from CCTV cameras, who are mostly in a standing position, some works exploited this fact by adding this information during training. In <ref type="bibr" target="#b44">[41]</ref> the authors used horizontal pooling of image stripes, while <ref type="bibr" target="#b24">[23]</ref> modified a network architecture search algorithm specifically for ReID task, that also integrated the human body in an upright position during training. <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b26">[25]</ref> proposed using human/skeleton pose information along with global and local features for better alignment of features, thus, increasing retrieval performance.</p><p>Moreover, Person Re-Identification problem settings allow to implicitly or explicitly use spatio-temporal information, as time constraint allows to eliminate a portion of irrelevant images during retrieval stage <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Currently spatiotemporal approach <ref type="bibr" target="#b35">[32]</ref> tops the leaderboard in Person ReID task on Market1501 -common dataset for Person Re-Identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>The aim of our work was to investigate if and how ReID models can be successfully applied to a fashion retrieval task. Based on the performance of various models in Person Re-Identification task <ref type="bibr" target="#b3">4</ref> and their suitability for the fashion retrieval problem, we selected the most appropriate models. The motivation behind our final choice was threefold:</p><p>1. The chosen models should exhibit top performance on ReID datasets. 2. The chosen models should cover different approaches with regards to the network architecture, training regime etc. 3. The code had to be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ReID Models</head><p>Two models were chosen based on the above criteria.</p><p>First, <ref type="bibr" target="#b23">[22]</ref> presents an approach that combines the most efficient training approaches and sets a strong baseline for other ReID researchers, hence its name: ReID Strong Baseline (RST). Through a thorough evaluation it showed that by combining simple architecture, global features and using training tricks (warmup learning rate, random erasing augmentation, label smoothing, last stride, BNNeck, center loss; for more details see <ref type="bibr" target="#b23">[22]</ref>) one can surpass state-of-the-art performance in the ReID domain.</p><p>The second choice -OSNet-AIN <ref type="bibr" target="#b48">[45]</ref> uses features extracted from different scales, which seems to be a well-suited approach in a fine-grained instance retrieval. The authors devised a flexible fusion mechanism that allows the network to focus on local or global features that improves performance. This is a different approach from the first one, as both global and local features are used and the whole neural architecture was purpose-built for ReID task.</p><p>It is worth mentioning that some top scoring papers from website Paperswithcode were not chosen as they either required temporal data <ref type="bibr" target="#b35">[32]</ref> or made an implicit assumptions of spatial alignment of objects in images <ref type="bibr" target="#b24">[23]</ref>. Neither approach was suitable to the clothes retrieval task and the datasets we used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings for Our Experiments</head><p>To fully explore ReID models and their application to the fashion retrieval task we conducted a number of experiments with the two selected approaches. We explored several aspects with potential to influence the performance in the fashion domain:</p><p>backbone architecture (a feature extractor part of a model; see <ref type="figure" target="#fig_1">Figure 2</ref>) triplet and quadruplet loss functions in RST size of input images re-ranking (post-processing methods to improve accuracy of retrieval) cross-domain evaluation (training on one dataset and testing on another)</p><p>Using the selected ReID models we dealt with three loss functions. As for the OSNet-AIN, we followed the training regime proposed by its authors and we trained the model using only a classification loss. For the RST model we used a loss function that consisted of three elements: a classification loss, a triplet loss and a center loss. Additionally, we tested if by replacing the triplet loss with a quadruplet loss one could improve the performance. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SOTA Models for Clothes Retrieval vs RST Model</head><p>A detect-then-retrieve model for multi-domain fashion item retrieval <ref type="bibr" target="#b15">[16]</ref> showed SOTA performance for Street2Shop dataset. The model consists of Object Detector (Mask-RCNN), which is used to crop clothing items from shop images and a Retrieval Model that is built upon Resnet-50 backbone, but incorporates RMAC pipeline <ref type="bibr" target="#b5">[6]</ref>. The RMAC is a global representation of images that uses CNN as a local feature extractor. Local features are extracted and max-pooled from multiple multi-scale overlapping regions covering whole image, creating a single feature vector for each region. These region features are aggregated, normalized, transformed and then normalized again. Moreover, in <ref type="bibr" target="#b15">[16]</ref> an ensemble model was also created that consisted of two single models, one trained using triplet loss and other one using AP loss. Input image sizes used in <ref type="bibr" target="#b15">[16]</ref> were also much larger than used in our RST model as the images were resized to 800 pixels, which caused problem in processing them thought the network, since they were forced to process either a single image (AP loss) or a single triplet (triplet loss) at a time. For more details see <ref type="bibr" target="#b15">[16]</ref>.</p><p>Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid <ref type="bibr" target="#b14">[15]</ref> achieved SOTA performance on DeepFashion dataset. They based their solution on graphs and graph convolution. The architecture consists of three parts. First is a CNN network, which extracts multi-scale (pyramid) features from numerous, overlapping windows that cover the whole image. What is important two images of the same item are fed into the network street and shop images and they are processed together as a pair. Second part is the Similarity Computation, which computes region similarity between street and shop images for all possible local feature combinations at the same pyramid scale. Next, a graph is built. Computed region similarities are the nodes, the relations between region similarities are the edges. Scalar edges weights are computed automatically based on the node, incoming and outgoing edges. Finally, the convolution operations are applied to the graph and classification loss is used for training. The aim of the network is to classify the pair of images as depicting the same clothes or not. Input size of images was 224x224, which is slightly smaller than used in our best performing RST model. For more details see <ref type="bibr" target="#b14">[15]</ref>.</p><p>RST model Compared to the two models described above, the RST model can be characterised by its simplicity. Even though, the architecture is much simpler, the performance exceeds significantly current SOTA. The RST model consists of 3 parts. First, CNN backbone extracts features from images and global average pooling is applied to create global feature vectors. They are used for computing quadruplet and center loss. Next, the global feature vectors are normalized and we call them Images embeddings (see <ref type="figure" target="#fig_1">Figure 2</ref>). Images embeddings are used during training as input to a fully connected layer, while during inference stage (retrieval) they are used to compute similarity distance. In <ref type="figure" target="#fig_1">Figure 2</ref> we presents the RST model, which shows the pipeline and all parts of the architecture. It can be deemed as strikingly simple, yet it substantially exceeds current SOTA results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we describe our methodology and details of experiments we ran. We conducted evaluation of our approach and its settings on Street2Shop and DeepFashion datasets. <ref type="bibr" target="#b5">6</ref> For both Street2Shop and DeepFashion we cropped the clothing items of interest from the whole images, as both datasets contain ground-truth bounding boxes for street photos. Additionally DeepFashion contains bounding boxes for shop images, so we cropped them as well. Street2Shop does not provide bounding boxes for shop images, therefore we used the original, un-cropped shop images for the gallery set. These settings for street images are in line with those used in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b14">[15]</ref>, which we consider as the current SOTA for Street2Shop and DeepFashion respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>To adapt the RST model to fashion datasets we introduced some changes to the code, which were mostly required due to significant difference in size between ReID and clothes retrieval datasets. The size of DeepFashion and Street2Shop caused RAM and video RAM overflow, which was solved by using more memoryefficient methods. Moreover, we added some new functionalities and scripts. Our improvements are described in detail in the supplement.</p><p>Unfortunately, evaluation with re-ranking and without category constraint for Street2Shop was impossible despite improving the code efficiency, therefore we decided to conservatively estimate the results. The estimated results are marked with asterisk (*). For more details see the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ReID Models Comparison</head><p>In the first experiment we trained both ReID models to compare their performance on DeepFashion and Street2Shop datasets. For training we used 256x128 input images. <ref type="table" target="#tab_0">Table 1</ref> contains the results of the models and current state-of-theart approach. It can be seen that the RST model surpasses the current SOTA model and the OSNet-AIN performs worse than the other two approaches. The reason for poor performance of OSNet may be the fact that it is a lightweight network and built very specifically for ReID tasks.</p><p>Due to the large performance gap between RST and OSNet models, we decided to conduct further experiments using only the more promising RST model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Backbone Influence</head><p>In this section we inspect the influence of the used backbone on the results. The results of our experiments are shown in <ref type="table" target="#tab_1">Table 2</ref>. All runs during this experiment were performed on input images of size 256x128, using an RST approach with all tricks enabled and using quadruplet loss. All models were trained for 120 epochs on 1 GPU, with base learning rate 0.0001 and batch size 128. SeResNext-50 <ref type="bibr" target="#b9">[10]</ref>, EfficientNet-b2 and EfficientNet-b4 <ref type="bibr" target="#b33">[30]</ref> were trained on 2 GPUs using ModelParallel mode due to their size. Our findings are in line with backbone performance presented by the authors in <ref type="bibr" target="#b23">[22]</ref>, i.e. ResNet50-IBN-A <ref type="bibr" target="#b40">[37]</ref> is the best performing backbone. Regarding the results, we infer that such a large advantage in performance for Resnet50-IBNs may be caused by instance normalization, which reduces the impact of variations in contrast and lighting between street and shop photos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of Loss Functions</head><p>The results for the RST model using triplet and quadruplet loss are shown in <ref type="table" target="#tab_2">Table 3</ref>. It can be seen that the quadruplet loss in our test settings performed marginally better than the triplet loss, yet it brings an improvement at almost no cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Influence of Input Image Size</head><p>Even though the results achieved with 256x128 input images had already surpassed the current SOTA performance for many backbones (See <ref type="table" target="#tab_1">Table 2)</ref>, we decided to test if larger images would result in even higher performance. Outcomes of our experiments are presented in <ref type="table" target="#tab_3">Table 4</ref>. It can be seen that using larger images allows to further boost performance. In our settings, we achieved best results for input images of size 320x320. Using larger images (480x480) did not bring any advantage. Additional plots are available in the supplement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Influence of Re-ranking</head><p>One of the methods boosting performance tested in <ref type="bibr" target="#b23">[22]</ref> was re-ranking during test stage. Re-ranking is a post-processing method applied to raw retrieval results in order to improve the accuracy of the results. In our experiments we used kreciprocal encoding proposed by <ref type="bibr" target="#b47">[44]</ref> similarly to <ref type="bibr" target="#b23">[22]</ref>. <ref type="table">Table 5</ref> presents the comparison of RST models with and without using reranking. It can be seen that the results with re-ranking post-processing improved significantly for both datasets. The improvement is greatest for mAP and Acc@1, thus suggesting that re-ranking 'pushes' the positive samples closer to the start of the results list. Such behaviour seems desirable for real-world applications where a visual search user will obtain more relevant items at the top <ref type="table">Table 5</ref>. The comparison of performance for the model without and with re-ranking. For results marked with * see Section 4.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Street2Shop</head><p>DeepFashion mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20 No re-ranking <ref type="bibr" target="#b49">46</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparison to the State-of-the-art</head><p>In <ref type="table">Table 6</ref> we compare our results to the state-of-the-art on Street2Shop dataset <ref type="bibr" target="#b15">[16]</ref>. We list the best performing ensemble model from the aforementioned paper, as it achieved better results than their best single model. In comparison, we show the results of our best performing single model, which was trained using an RST approach, Resnet50-IBN-A backbone using quadruplet loss during training. It can be seen that generally our approach performs better by a large margin despite using only 320x320 input image size compared to 800x800 used in <ref type="bibr" target="#b15">[16]</ref>. In all categories and in overall performance our model clearly outperforms current state-of-the-art by a large margin. In two categories our model performs marginally worse, but it may be attributed to the fact that we use much smaller images. The categories with inferior performance consist of small items such as belts or eye-wear, where fine-grained details are more important and a high image resolution is beneficial. In our case cropped items are of very small dimensions, which may limit performance.</p><p>It is important to note that the mAP, Acc@1, Acc@20 reported by <ref type="bibr" target="#b15">[16]</ref> as overall performance are just average values across all categories for each metric. There, the retrieval stage was limited to products only from a specific category, thus limiting the choice of the model and making retrieval less challenging. We also report the performance in these settings for our model and for <ref type="bibr" target="#b15">[16]</ref> in the Average over categories row of <ref type="table">Table 6</ref> to allow fair comparison. Additionally, we propose an unconstrained evaluation, where we conduct retrieval from all gallery images, with no restrictions. Our results are in the Unconstrained retrieval row of <ref type="table">Table 6</ref>.</p><p>The large gap between Average over categories and Unconstrained retrieval derives from the fact that the well performing categories such as skirts and dresses are the most numerous ones, therefore they weigh more in the final results than categories that have few queries, such as belts. Hence, unconstrained retrieval is closer to a weighted average than the simple average named by us as Average over categories, where each category has equal weight when calculating the final results. <ref type="table">Table 6</ref>. Comparison of performance on Street2Shop dataset using mAP, Acc@1, and Acc@20 metrics. Best performance for each category is presented in bold per metric. For results marked with * see Section 4.1 Current SOTA <ref type="bibr" target="#b15">[16]</ref> Our Model Our Model Re-ranking Category mAP Acc@1 Acc@20 mAP Acc@1 Acc@20 mAP Acc@1 Acc@20 bags <ref type="bibr" target="#b24">23</ref>  <ref type="table">Table 7</ref> results on DeepFashion dataset are presented. Our model outperforms the results of <ref type="bibr" target="#b2">[3]</ref> in terms of Accuracy for all k values. Especially Acc@1 noted largest relative boost. <ref type="table">Table 7</ref>. Comparison of performance on DeepFashion dataset using Acc@1, Acc@20 and Acc@50 metrics. Best performance for each metric is presented in bold Current SOTA <ref type="bibr" target="#b14">[15]</ref> Our Model Our Model Re-ranking Acc@1 Acc@20 Acc@50 Acc@1 Acc@20 Acc@50 Acc@1 Acc@20 <ref type="formula">Acc@50</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Cross-domain Evaluation</head><p>Inspired by the Person Re-Identification domain, where cross-domain evaluation is often performed to measure robustness of the models <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b49">[46]</ref>, we decided to conduct such a test using our best performing RST model. Retrieval images with green border are the true match to the query. The top 10 most similar retrieval images are shown. It is worth noting that the retrieval was performed on the whole gallery dataset, with no pruning to the query item's category. Cross-domain evaluation consists of training a model on Dataset A and evaluation of its performance on Dataset B. In such settings training and test data distributions are different.</p><p>In <ref type="table" target="#tab_7">Table 8</ref> we present the results of our cross-domain experiments, both with and without re-ranking. It can be seen that cross-domain performance of the RST model on both datasets is much lower than when model is trained and tested on the same dataset -same domain (See <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref>). We suppose that such a large gap in performance between cross and same-domain evaluation is caused by a significant difference in data distribution of the two datasets. For example DeepFashion contains only three categories: lower, upper and full-body, while Street2Shop contains 11 fine-grained clothing categories.</p><p>Even though cross-domain evaluation results are far from our best performing models, they are on a reasonable level when compared to the current state-of-theart results for each dataset, thus, indicating that the RST model with Resnet50-IBN-A backbone can learn meaningful representation of garments that can be transferred between domains to a large extent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we examined the analogies and differences between Person Re-Identification research field and Fashion Retrieval, then examined the transferability of ReID models, and performed adjustments necessary to make them work for fashion items. Using the approach proposed by <ref type="bibr" target="#b23">[22]</ref> using only global features, a number of improvements and learning optimizations we achieved 54.8 mAP, 72.9 Acc@20 on Street2Shop and 47.3 Acc@1, 79.0 Acc@20 on DeepFashion dataset, establishing new state-of-the-art results for both. The performance on Street2Shop seems particularly robust, compared to previous state-of-the-art as our results were achieved on images several times smaller than the previous top-scoring models.</p><p>By analogy to <ref type="bibr" target="#b23">[22]</ref> we consider the results achieved by our model as a strong baseline for further clothes retrieval research. Despite a simple architecture, the baseline model outperforms previous solutions dedicated and customized specifically for fashion retrieval systems. Our results are a clear indica-tion that field of fashion retrieval can benefit from research established for Person Re-Identification. Additionally, we performed supplementary experiments and showed that quadruplet loss may bring further improvements at a negligible cost, and that re-ranking can further boost performance. Finally, we introduced cross-domain evaluation into clothes retrieval research to test robustness of fashion retrieval models.</p><p>1 Synerise 2 Faculty of Mathematics and Information Science, Warsaw University of Technology</p><p>In the following sections of supplementary material we provide loss function definitions used in fashion retrieval domain (Section 1) along with metrics used (Section 2). Then we describe two open datasets used in our tests which are commonly used in the domain (Section 3). We also demonstrate a few problems with the datasets along with our adjustments for the community to use them in more convenient way using the popular COCO format.</p><p>Finally we list a few examples of outputs of our models, and also examples with re-ranking technique and without them (Section 5).</p><p>In the following sections of supplementary material we provide loss function definitions used in fashion retrieval domain (Section 1) along with metrics used (Section 2). Then we describe two open datasets used in our tests which are commonly used in the domain (Section 3). We also demonstrate a few problems with the datasets along with our adjustments for the community to use them in more convenient way using the popular COCO format.</p><p>Finally we list a few examples of outputs of our models, and also examples with re-ranking technique and without them (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Loss functions</head><p>In the image retrieval task there are two loss functions commonly used: classification and triplet loss. Therefore, prevailing number of works in the image retrieval domain use a combination of a classification and a triplet loss for training deep learning models. Classification loss function is used to identify exact id of a person/garment (i.e. images of the same person/garment have the same id). It is a standard loss in classification tasks and in our case it is cross-entropy loss. Also all of the considered models use either of these two loss functions.</p><p>Deep metric learning treats the image retrieval problem somewhat as a clustering or ranking problem and optimizes the mutual arrangement of the embeddings in space. One of the most widely used approaches that pulls the same class closely and pushes away other class embeddings is using a triplet loss <ref type="bibr" target="#b27">[26]</ref> for training neural network.</p><p>Triplet loss is formulated as follows:</p><formula xml:id="formula_0">L triplet = f (A) − f (P ) 2 2 − f (A) − f (N ) 2 2 + α +<label>(1)</label></formula><p>where [z] + = max(z, 0) and f denotes learnt embedding function applied to all data points. A triplet loss consists of an anchor image (a query) A, a positive example P -the other image of the same object (in this paper -clothing item) present in the A image -and negative sample N , which is an image of a different object from that shown in the image A.</p><p>Learning NN with triplet loss minimizes the intra-class distance, between anchor and positive samples, and maximizes inter-class distance, between anchor and negative samples. The triplet loss proved to achieve state-of-the-art performance and became a standard in similarity learning tasks <ref type="bibr" target="#b36">[33]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In triplet loss strategy, the method of creating triplets is an important part of the training and influences the model performance immensely <ref type="bibr" target="#b39">[36]</ref>. In <ref type="bibr" target="#b27">[26]</ref> authors used semi-hard triplets, where negative samples are further away from the anchor than the positive samples, but still the loss value is positive, thus, it allows learning.</p><p>Most of the works we examined use online hard negative sampling to form a training triplet. This methods select such data points, so that the negative sample is closer to the anchor than the positive sample. As a results, the neutral network is given only the triplets that maximize the value of the loss function, therefore it is called 'hard'. This method of creating triplets proved to perform better than other sampling methods and is used in numerous works <ref type="bibr" target="#b29">[27]</ref>, <ref type="bibr" target="#b43">[40]</ref>, <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b34">[31]</ref>.</p><p>To further improve the triplet loss some authors either extends the number of tuples in the loss <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b37">[34]</ref> or/and propose novel sampling methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b39">[36]</ref>. However, the reported improvements are not high, thus, we did not use them in our experiments.</p><p>Triplet, and in general n-tuple-loss, aims to properly arrange embeddings in an n-dimensional space. While the triplet loss is a common choice in retrieval/ranking tasks, we also examined the quadruplet loss and its influence on the performance. Our implementation of the quadruplet loss follows one found in <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_1">L quad = f (A) − f (P ) 2 2 − f (A) − f (N 1 ) 2 2 + α 1 + + f (A) − f (P ) 2 2 − f (N 2 ) − f (N 1 ) 2 2 + α 2 + (2)</formula><p>where the first term is the same as in Equation 1, thus, it takes care of the pull-push relation between anchor, positive and negative samples. The second term demands the intra-class distance to be smaller than the maximum interclass distance in respect to a different probe -N 2 (N 2 = N 1 ⇒ N 2 and N 1 represents different garments/IDs). α 1 and α 2 are the margin values, which are set dynamically as in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In <ref type="bibr" target="#b23">[22]</ref> additionally center loss <ref type="bibr" target="#b38">[35]</ref> is used as one of the training tricks. It aims to pull same class embeddings together as the n-tuple-loss considers only relative distance between embeddings neglecting the distance absolute values. Center loss alleviate this problem by penalizing the distance between embeddings and their id/class center. Formula for center loss is as follows:</p><formula xml:id="formula_2">L center = 1 2 B j=1 f tj − c yj 2 2 (3)</formula><p>where y j denotes label of j-th image in the mini-batch. B is the batch size, f tj is an embedding of j-th image and c yj is the center of y j -th class features center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Metrics in fashion retrieval</head><p>To evaluate the performance of our approach we used metrics that we found most often in the related papers. Most widely used metric in retrieval tasks is Accuracy@k (Acc@k), formulated as:</p><formula xml:id="formula_3">Acc@k = 1 N N i=1 1 S + q ∩ S K q ,<label>(4)</label></formula><p>where N is the number of queries and 1 S + q ∩ S K q is an indicator function, which evaluates to 1 if the ground-truth image is within top-k retrieved results. k is usually set from 1 to 20. The metric measures if the retrieved item was among top-k proposals.</p><p>Second metric that we encountered in the papers was mAP , which is a mean average precision, that shows how well the retrieval is done on average. Though mAP values were rarely reported in clothes retrieval papers we decided to use this metric in our experiments along Acc@k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section we describe datasets used for evaluation. Apart from describing their statistics, we also explain the process of reformatting them and how they were processed during our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Street2Shop</head><p>The dataset was introduced by <ref type="bibr" target="#b13">[14]</ref> and became one of the most widely used datasets for evaluating clothes retrieval solutions. Therefore there is an abundance of works that present their results on the dataset, thus, providing a strong benchmark for our methods. It contains 404,683 shop photos and 20,357 street photos depicting 204,795 distinct clothing items <ref type="bibr" target="#b13">[14]</ref>. To allow compatibility across datasets and models we tested, we transformed the Street2Shop dataset to COCO-format, while keeping original train/test split and categorization. Annotations in COCO-format are available on our GitHub 3 .</p><p>In contrast to some authors <ref type="bibr" target="#b14">[15]</ref> we decided not to perform any data cleaning or hand-curating images/annotations, even though we encountered some erroneous annotations such as multiple annotations for a single-item image or bounding boxes placed in 'random' places (see examples in <ref type="figure">Fig. )</ref>).</p><p>We made such decision to allow a fair comparison with <ref type="bibr" target="#b15">[16]</ref>, which we found to have best performance on Street2Shop dataset, while it does not mention any data cleaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DeepFashion</head><p>The dataset contributed by <ref type="bibr" target="#b21">[21]</ref> contains over 800,00 images, but for our task we only used Consumer-to-shop Clothes Retrieval subset that consists of 33,881 distinct clothing items and total of 239,557 images, creating 195,540 pairs. We used results found in <ref type="bibr" target="#b2">[3]</ref> as our benchmark, since their were the best we found.</p><p>Similarly to Street2Shop dataset, DeepFashion is also not free from some defects, which we show in <ref type="figure" target="#fig_0">Figures 1, 2, 3, 4</ref>.</p><p>In Consumer-to-Shop subset of DeepFashion , we found out that the same products and even the same images were assigned different product identifiers. As a result, the retrieval performance is falsely understated compared to the real performance. Two examples of such errors are presented in the supplementary materials to this paper <ref type="figure" target="#fig_0">Fig. 1</ref>. A collection of photos that depict, we believe, the same clothing item -dress visible in the left most photo. Above each photo there are four pieces of information; from the top: item id, file name, category, subset name. Despite the fact that item ids should be unique for distinct garments, it seems that the same item have various ids assigned, which results in erroneous retrieval results presented in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Code improvements</head><p>As we mentioned in the main text, we encountered problems with vRAM and RAM overflow caused by the size of the datasets we tested. The RST model contains fully connected layers used for classification of identities/clothes. While ReID datasets the RST model was tested on Market-1501, DukeMTMC-reid contain 1501 and 1812 unique identities, the fashion datasets have an order of <ref type="figure" target="#fig_1">Fig. 2</ref>. Examples of retrieval for the query images with product ids from presented in <ref type="figure" target="#fig_0">Figure 1</ref> produced by our best model on 320x320 images. The images in the first column are query images, while the images on their right are the retrieval results with decreasing similarity towards the right side. Retrieval images with green border are the true match to the query. The top 10 most similar retrieval images are shown. It can be seen that some images that are just mirrored copies of the same image, yet only one of them is deemed as a true match. We believe it is an error in data annotation, which understates real retrieval performance. <ref type="figure" target="#fig_2">Fig. 3</ref>. A collection of photos that depict, we believe, the same clothing item -top visible in the left most photo. Above each photo there are four pieces of information; from the top: item id, file name, category, subset name. Despite the fact that item ids should be unique for distinct garments, it seems that the same item have various ids assigned, which results in erroneous retrieval results presented in <ref type="figure" target="#fig_3">Figure 4</ref>. Interestingly, the right most photos depicts a top that is plain white, which also seems to be incorrect compared the rest. <ref type="figure" target="#fig_3">Fig. 4</ref>. Examples of retrieval for the query images with product ids from presented in <ref type="figure" target="#fig_2">Figure 3</ref> produced by our best model on 320x320 images. The images in the first column are query images, while the images on their right are the retrieval results with decreasing similarity towards the right side. Retrieval images with green border are the true match to the query. The top 10 most similar retrieval images are shown. It can be seen that all results have first and second image the same, while in all cases only the latter is correct even though the former is also from 'top' category, thus, it seems to be pertaining exactly the same garment. magnitude more identities (clothes) roughly 10000-15000. As a result, the FC layer needs thousands neurons instead of hundreds, thus, it requires much more video RAM (vRAM. To address this problem we introduced two independent solutions:</p><p>1. Gradient accumulation -it allows to use smaller mini-batches in constrained vRAM settings when the mini-batch either do not fit into GPU memory or is too small causing the gradient descent to be volatile and prevent model from converging. Gradient accumulation splits original mini-batch into submini-batches, feeds them into network, compute gradients, but the model weights are updated after all sub-mini-batches. 2. ModelParallel mode -it is a distributed training technique that splits a single model across different devices. The data and gradients are moved between devices during forward and backward pass. It was implemented to allow us to test larger backbones that would not fit into a single GPU, use larger mini-batches and, thus speed up the training.</p><p>In the code version we used, Resnet-50-IBNs were not yet implemented, therefore we implemented both A and B variants by ourselves based on original implementation. Additionally, we expanded available backbones with the whole EfficientNet family based on the implementations available here. <ref type="bibr" target="#b3">4</ref> During evaluation step, especially when performing re-ranking, we encountered problem with RAM consumption. Again, the problem arises from the size of the clothes retrieval datasets we tested. To tackle the problem we introduced three solutions:</p><p>1. We introduced batch processing during both creating images' embeddings, to avoid vRAM overflow, and during computation of distance matrix for tens of thousands images. Originally, both operations were performed in one go. 2. Conducting evaluation with re-ranking for single categories for Street2Shop was still problematic due to large RAM requirements, so we used batch processing again, but, we appended intermediate results from batch computations of distance matrix to a file in a hard drive. During re-ranking itself we used Numpy function memmap to avoid reading the whole matrix into RAM, while still allowing RAM-like processing. 3. Unfortunately, evaluation with re-ranking and without category constraint was still impossible for Street2Shop , as the whole distance matrix over 400,000x400,000 floats, was again too big even when using memap function. We decided to conservatively estimate the results by calculating weighted average over categories and deducting a penalty term. The penalty term was computed for each metric separately using results from the model variant without re-ranking, as the maximum difference, between the weighted average over categories and Unconstrained Retrieval values.</p><p>Finally, we added Accuracy@k computation for specified k and a script that at the end of evaluation creates visualization of the results. It plots query image and top-k retrieved images. <ref type="figure" target="#fig_4">Fig. 5</ref>. Examples of retrieval on DeepFashion dataset produced by our best model on 320x320 images. The images in the first column are query images, while the images on their right are the retrieval results with decreasing similarity towards the right side. Retrieval images with green border are the true match to the query. The top 10 most similar retrieval images are shown. Two retrieval results are shown for each query image, one without and one with re-ranking. The top result from a pair is without re-ranking. <ref type="figure">Fig. 8</ref>. Examples of retrieval on Street2Shop dataset produced by our best model on 320x320 images. The images in the first column are query images, while the images on their right are the retrieval results with decreasing similarity towards the right side. Retrieval images with green border are the true match to the query. The top 10 most similar retrieval images are shown. Two retrieval results are shown for each query image, one without and one with re-ranking. The top result from a pair is without re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our result examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of images from ReID dataset, DeepFashion and Street2Shop . The difference in alignment, background and quality can be seen between ReID and clothes retrieval datasets, as well as between street and shop photos within a single dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>RST model architecture .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Two plots presenting influence of input image size on mAP and Acc@1 metric for DeepFashion and Street2Shop datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of retrieval on the Street2Shop dataset produced by our best model on 320x320 images. The images in the first column are query images, while the images on their right are the retrieval results with decreasing similarity towards the right side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of retrieval on DeepFashion dataset produced by our best model on 320x320 images. Retrieval and visualization settings are identical as inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of performance of models on Street2Shop and DeepFashion data. Best performance across models on a given dataset is in bold</figDesc><table><row><cell></cell><cell cols="2">Street2Shop</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell>Model</cell><cell cols="8">mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20 Acc@50</cell></row><row><cell>RST</cell><cell cols="2">37.2 42.3 61.1</cell><cell>66.5</cell><cell cols="3">35 30.8 62.3</cell><cell>69.4</cell><cell>78</cell></row><row><cell>OSNet-AIN</cell><cell>18.9 25.3</cell><cell>40.8</cell><cell cols="3">45.4 20.1 17.5</cell><cell>40.2</cell><cell>46.9</cell><cell>53.8</cell></row><row><cell cols="2">Current SOTA 29.7 34.4</cell><cell>-</cell><cell>60.4</cell><cell>-</cell><cell>27.5</cell><cell>-</cell><cell>65.3</cell><cell>75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results achieved by different backbones on Street2Shop and DeepFashion datasets compared to the current SOTA.</figDesc><table><row><cell></cell><cell cols="2">Street2Shop</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell><cell></cell></row><row><cell>Backbone</cell><cell cols="8">mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20 Acc@50</cell></row><row><cell>ResNet-50</cell><cell>32.0 36.6</cell><cell>55.3</cell><cell cols="3">60.6 32.4 28.1</cell><cell>58.3</cell><cell>65.5</cell><cell>74.2</cell></row><row><cell>SeResNet-50</cell><cell>30.5 34.6</cell><cell>53.1</cell><cell cols="3">58.7 31.3 27.0</cell><cell>57.8</cell><cell>65.4</cell><cell>74.4</cell></row><row><cell>SeResNeXt-50</cell><cell>31.9 36.9</cell><cell>54.5</cell><cell cols="3">59.7 32.2 27.8</cell><cell>58.5</cell><cell>66.0</cell><cell>74.6</cell></row><row><cell cols="3">ResNet50-IBN-A 37.2 42,3 61.1</cell><cell cols="3">66.5 35.0 30.8</cell><cell>62.3</cell><cell>69.4</cell><cell>78.0</cell></row><row><cell cols="2">ResNet50-IBN-B 36.9 41.9</cell><cell>60.6</cell><cell cols="3">65.1 32.2 28.1</cell><cell>58.4</cell><cell>65.8</cell><cell>74.7</cell></row><row><cell cols="2">EfficientNet-b1 28.8 35.1</cell><cell>52.1</cell><cell cols="3">56.7 28.5 23.4</cell><cell>49.8</cell><cell>56.8</cell><cell>66.1</cell></row><row><cell cols="2">EfficientNet-b2 29.4 34.0</cell><cell>50.8</cell><cell cols="3">56.2 24.1 20.4</cell><cell>45.9</cell><cell>53.2</cell><cell>62.6</cell></row><row><cell cols="2">EfficientNet-b4 31.8 38.2</cell><cell>55.6</cell><cell cols="3">60.2 26.8 23.1</cell><cell>59.1</cell><cell>57.4</cell><cell>66.7</cell></row><row><cell>Current SOTA</cell><cell>29.7 34.4</cell><cell>-</cell><cell>60.4</cell><cell>-</cell><cell>25.7</cell><cell>64.4</cell><cell>-</cell><cell>75.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Our results on Street2Shop and DeepFashion datasets achieved with triplet and quadruplet loss functions</figDesc><table><row><cell></cell><cell cols="2">Street2Shop</cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell cols="7">Loss function mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20</cell></row><row><cell cols="3">Quadruplet 37.2 42.3 61.1</cell><cell>66.5</cell><cell>35 30.8</cell><cell>62.3</cell><cell>69.4</cell></row><row><cell>Triplet</cell><cell>37.1 41.8</cell><cell>60.4</cell><cell cols="2">65.7 34.8 30.5</cell><cell>62.4</cell><cell>69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of performance of clothes retrieval with different input image sizes. All experiments were performed using Resnet50-IBN-A RST model with all tricks enabled, quadruplet loss function and without re-ranking</figDesc><table><row><cell></cell><cell cols="2">Street2Shop</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell>Input size</cell><cell cols="8">mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20 Acc@50</cell></row><row><cell>256x128</cell><cell>38.6 44.5</cell><cell>62.5</cell><cell cols="3">67.2 35.0 30.8</cell><cell>62.3</cell><cell>69.4</cell><cell>78.0</cell></row><row><cell>224x224</cell><cell>42.4 49.2</cell><cell>66.2</cell><cell cols="3">70.9 40.5 35.8</cell><cell>68.5</cell><cell>75.1</cell><cell>82.4</cell></row><row><cell>320x320</cell><cell cols="2">46.8 53.7 69.8</cell><cell cols="4">73.6 43.0 37.8 71.1</cell><cell>77.2</cell><cell>84.1</cell></row><row><cell>480x480</cell><cell>46.6 53.5</cell><cell>69.0</cell><cell cols="3">72.9 42.4 37.3</cell><cell>69.4</cell><cell>75.4</cell><cell>82.2</cell></row><row><cell cols="2">Current SOTA 29.7 34.4</cell><cell>-</cell><cell>60.4</cell><cell>-</cell><cell>27.5</cell><cell>-</cell><cell>65.3</cell><cell>76.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Comparison of the performance of the RST on cross-domain evaluation. DF → S2S means that the model was trained on DeepFashion dataset and tested on the Street2Shop test set. For results marked with * see Section 4.1</figDesc><table><row><cell></cell><cell cols="2">DF → S2S</cell><cell cols="2">S2S → DF</cell></row><row><cell></cell><cell cols="5">mAP Acc@1 Acc@10 Acc@20 mAP Acc@1 Acc@10 Acc@20</cell></row><row><cell cols="2">No re-ranking 26.2 37.7</cell><cell>49.5</cell><cell>53.3 20.9 18.6</cell><cell>40.7</cell><cell>47.2</cell></row><row><cell>Re-ranking</cell><cell cols="3">27.9* 37.4* 48.8* 51.7* 23.5 20.5</cell><cell>43.7</cell><cell>49.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Image retrieval pertains to finding similar images as a whole, while in the case of fashion, the task is an instance retrieval task, as we want to find a match to a single item contained in the image. In this work we use those two terms interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://paperswithcode.com/task/person-re-identification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Triplet and quadruplet loss functions and their modifications are described in our supplementary material in more detail.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">These datasets, used metrics and many visualizations are presented very precisely in our supplement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Mikolaj Wieczorek 1 , Andrzej Michalowski 1 , Anna Wroblewska 1,2[0000−0002−3407−7570] , and Jacek Dabrowski 1[0000−0002−1581−2365]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Linkwillbereleasedinthefinalsubmission</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/lukemelas/EfficientNet-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 6. More retrieval results on DeepFashion dataset without and with re-ranking.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 10. More retrieval results on Street2Shop dataset without and with re-ranking.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for: A Strong Baseline for Fashion Retrieval with Person</head><p>Re-Identification models</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01719</idno>
		<idno>arXiv: 1704.01719</idno>
		<ptr target="http://arxiv.org/abs/1704.01719" />
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint person re-identification and camera network topology inference in multiple cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2019.01.003</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1077314219300037" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrigues</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04652</idno>
		<idno>arXiv: 1810.04652</idno>
		<ptr target="http://arxiv.org/abs/1810.04652" />
		<title level="m">Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1901.07973</idno>
		<idno>arXiv: 1901.07973</idno>
		<ptr target="http://arxiv.org/abs/1901.07973" />
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Transfer Learning for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<idno>arXiv: 1611.05244</idno>
		<ptr target="http://arxiv.org/abs/1611.05244" />
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-end Learning of Deep Visual Representations for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07940</idno>
		<idno>arXiv: 1610.07940</idno>
		<ptr target="http://arxiv.org/abs/1610.07940" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srikhanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05750</idno>
		<idno>arXiv: 1906.05750</idno>
		<ptr target="http://arxiv.org/abs/1906.05750" />
		<title level="m">The iMaterialist Fashion Attribute Dataset</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K B</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01285</idno>
		<idno>arXiv: 1704.01285</idno>
		<ptr target="http://arxiv.org/abs/1704.01285" />
		<title level="m">Smart Mining for Deep Metric Learning</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<idno>arXiv: 1703.07737</idno>
		<ptr target="http://arxiv.org/abs/1703.07737" />
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<idno>arXiv: 1709.01507</idno>
		<ptr target="http://arxiv.org/abs/1709.01507" />
		<title level="m">Squeeze-and-Excitation Networks</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-Domain Image Retrieval with a Dual Attribute-Aware Ranking Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.127</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7410484/" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Camera Network Based Person Re-identification by Leveraging Spatial-Temporal Constraint and Multiple Cameras Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-27671-7_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-27671-715" />
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<editor>Liu, X.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="174" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.1778</idno>
		<idno>arXiv: 1401.1778</idno>
		<ptr target="http://arxiv.org/abs/1401.1778" />
		<title level="m">Large Scale Visual Recommendations From Street Fashion Images</title>
		<imprint>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Where to Buy It: Matching Street Clothing Photos in Online Shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.382</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7410739/" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="3343" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11754</idno>
		<idno>arXiv: 1908.11754</idno>
		<ptr target="http://arxiv.org/abs/1908.11754" />
		<title level="m">Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A detect-then-retrieve model for multi-domain fashion item retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kucer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6247939</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2012.6247939" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1063" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretable Multimodal Retrieval for Fashion Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3240508.3240646</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=3240508.3240646" />
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference on Multimedia Conference -MM &apos;18</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1571" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-Scale Triplet CNN for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="192" to="196" />
		</imprint>
	</monogr>
	<note>MM &apos;16, Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2964284.2967209</idno>
		<ptr target="https://doi.org/10.1145/2964284.2967209" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Fashion Analysis with Feature Map Upsampling and Landmark-Driven Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-11015-4_4</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-11015-4_4" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2018 Workshops</title>
		<editor>Leal-Taix, L., Roth, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11131</biblScope>
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/CVPR.2016.124</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7780493/" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Strong Baseline and Batch Normalization Neck for Deep Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08332</idno>
		<idno>arXiv: 1906.08332</idno>
		<ptr target="http://arxiv.org/abs/1906.08332" />
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-ReID: Searching for a Partaware ConvNet for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09776</idno>
		<idno>arXiv: 1903.09776 version: 4</idno>
		<ptr target="http://arxiv.org/abs/1903.09776" />
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2014.131</idno>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910029" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting><address><addrLine>OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
	<note>Columbus</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10378</idno>
		<idno>arXiv: 1711.10378</idno>
		<ptr target="http://arxiv.org/abs/1711.10378" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<ptr target="http://arxiv.org/abs/1503.03832" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training Region-Based Object Detectors With Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.html" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pose-driven Deep Convolutional Model for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2017.427</idno>
		<ptr target="https://arxiv.org/abs/1709.08325v1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.11946" />
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks. ICML abs/1905.11946</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishvakarma</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1903.00905</idno>
		<idno>arXiv: 1903.00905</idno>
		<ptr target="http://arxiv.org/abs/1903.00905" />
		<title level="m">MILDNet: A Lightweight Single Scaled Deep Ranking Architecture</title>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1812.03282</idno>
		<idno>arXiv: 1812.03282 version: 1</idno>
		<ptr target="http://arxiv.org/abs/1812.03282" />
		<title level="m">Spatial-Temporal Person Re-identification</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4661</idno>
		<idno>arXiv: 1404.4661</idno>
		<ptr target="http://arxiv.org/abs/1404.4661" />
		<title level="m">Learning Fine-grained Image Similarity with Deep Ranking</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03238</idno>
		<idno>arXiv: 1903.03238</idno>
		<ptr target="http://arxiv.org/abs/1903.03238" />
		<title level="m">Ranked List Loss for Deep Metric Learning</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Discriminative Feature Learning Approach for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-319-46478-7_31</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-319-46478-7_31" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2016</title>
		<editor>Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07567</idno>
		<idno>arXiv: 1706.07567</idno>
		<ptr target="http://arxiv.org/abs/1706.07567" />
		<title level="m">Sampling Matters in Deep Embedding Learning</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.437</idno>
		<ptr target="http://ieeexplore.ieee.org/document/6751549/" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3519" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clothing Co-Parsing by Joint Image Segmentation and Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.407</idno>
		<idno type="arXiv">arXiv:1502.00739</idno>
		<ptr target="http://arxiv.org/abs/1502.00739" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="3182" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2016.2603342</idno>
		<ptr target="https://doi.org/10.1109/LSP.2016.2603342" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<idno>arXiv: 1711.08184</idno>
		<ptr target="http://arxiv.org/abs/1711.08184" />
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MARS: A Video Benchmark for Large-Scale Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-452" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01394</idno>
		<idno>arXiv: 1807.01394</idno>
		<ptr target="http://arxiv.org/abs/1807.01394" />
		<title level="m">ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Re-ranking Person Re-identification with k-Reciprocal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.389</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8099872/" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06827</idno>
		<idno>arXiv: 1910.06827</idno>
		<ptr target="http://arxiv.org/abs/1910.06827" />
		<title level="m">Learning Generalisable Omni-Scale Representations for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00953</idno>
		<idno>arXiv: 1905.00953</idno>
		<ptr target="http://arxiv.org/abs/1905.00953" />
		<title level="m">Omni-Scale Feature Learning for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">FashionAI: A Hierarchical Dataset for Fashion Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
