<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVERSARIAL DROPOUT REGULARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ADVERSARIAL DROPOUT REGULARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by fooling a special domain critic network. However, a drawback of this approach is that the critic simply labels the generated features as in-domain or not, without considering the boundaries between classes. This can lead to ambiguous features being generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), to encourage the generator to output more discriminative features for the target domain. Our key idea is to replace the critic with one that detects non-discriminative features, using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvement over the state of the art. We also show that our approach can be used to train Generative Adversarial Networks for semi-supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transferring knowledge learned by deep neural networks on label-rich domains to new target domains is a challenging problem, especially when the source and target input distributions have different characteristics. For example, while simulated driving images rendered by games provide a rich source of labeled data for semantic segmentation, deep models trained on such source data do not transfer well to real target domains ( <ref type="figure">Fig. 1 (a-d)</ref>). When target-domain labels are unavailable for fine-tuning, unsupervised domain adaptation must be applied to improve the source model.</p><p>Recent methods for unsupervised domain adaptation attempt to reduce the discrepancy between the source and target features via adversarial learning <ref type="bibr" target="#b35">(Tzeng et al. (2014)</ref>; <ref type="bibr" target="#b5">Ganin &amp; Lempitsky (2014)</ref>). They divide the base network into a feature encoder G and classifier C, and add a separate domain classifier (critic) network D. The critic takes the features generated by G and labels them as either source-or target-domain. The encoder E is then trained with an additional adversarial loss that maximizes D's mistakes and thus aligns features across domains.</p><p>However, a major drawback of this approach is that the critic simply predicts the domain label of the generated point and does not consider category information. Thus the generator may create features that look like they came from the right domain, but are not discriminative. In particular, it can generate points close to class boundaries, as shown in <ref type="figure">Fig. 1(e)</ref>, which are likely to be mis-classified by the source model. We argue that to achieve good performance on the target data, the adaptation model must take the decision boundaries between classes into account while aligning features across domains ( <ref type="figure">Fig. 1(f)</ref>). The problem is, it is not clear how this can be accomplished without labels on target data.</p><p>In this paper, we propose a novel adversarial alignment technique that overcomes the above limitation and preserves class boundaries. We make the following observation: if the critic could detect points near the decision boundary, then the generator would have to avoid these areas of the feature space in order to fool the critic. Thus the critic would force the generator to create more discriminative features. How can we obtain such a critic? If we alter the boundary of the classifier C slightly and measure the change in the posterior class probability p(y|x), where y and x denote class and 1 arXiv:1711.01575v3 [cs.CV] 2 Mar 2018</p><p>Under review as a conference paper at ICLR 2018 <ref type="figure">Figure 1</ref>: <ref type="bibr">(a-d)</ref> An illustration of a deep model trained on simulated source training data failing to segment a real target domain image: (a) shows the target image, (b) is the ground truth segmentation into semantic categories <ref type="bibr">(car, road, etc)</ref>, (d) is the output of the unadapted source models, (e) improved segmentation obtained by our proposed ADR method. (e) Previous distribution matching methods do not consider the source decision boundary when aligning source and target feature points. (f) We propose to use the boundary information to achieve low-density separation of aligned points. input respectively, then samples near the decision boundary are likely to have the largest change. In fact, this posterior discrepancy is inversely proportional to the distance from the class boundary. We thus propose to maximize this posterior discrepancy to turn C into a critic sensitive to nondiscriminative points. We call this technique Adversarial Dropout Regularization. Here, dropout is not used in the standard way, which is to regularize the main classifier and make it insensitive to noise. Instead, we use dropout in an adversarial way, to transform the classifier into a critic sensitive to noise. Compared to previous adversarial feature alignment methods, where the distributions p(x) are aligned globally, our method aligns target features away from decision boundaries, as illustrated in <ref type="figure">Fig.1</ref></p><formula xml:id="formula_0">(f).</formula><p>Our ADR approach has several benefits. First, we train the generator G with feedback from the classifier C, in contrast to existing methods, which use an unrelated critic D. Second, our method is general and straightforward to apply to a variety of domain adaptation problems, such as classification and semantic segmentation. Finally, since ADR is trained to align distributions, it is also applicable to semi-supervised learning and training of generative models, such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">(Goodfellow et al. (2014a)</ref>). Through extensive experiments, we demonstrate the benefit of ADR over existing domain adaptation approaches, achieving state-of-the-art results in difficult domain shifts. We also show an application to semi-supervised learning using GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Domain Adaptation. Recent unsupervised domain adaptation (UDA) methods for visual data aim to align the feature distributions of the source and target domains ; ; <ref type="bibr" target="#b35">Tzeng et al. (2014)</ref>; <ref type="bibr" target="#b6">Ganin et al. (2016)</ref>; <ref type="bibr" target="#b18">Long et al. (2015b)</ref>; <ref type="bibr" target="#b38">Yan et al. (2017)</ref>; <ref type="bibr" target="#b20">Long et al. (2017)</ref>). Such methods are motivated by theoretical results stating that minimizing the divergence between domains will lower the upper bound of the error on target domain <ref type="bibr" target="#b0">(Ben-David et al. (2010)</ref>). Many works in deep learning utilize the technique of distribution matching in hidden layers of a network such as a CNN <ref type="bibr" target="#b35">(Tzeng et al. (2014)</ref>; <ref type="bibr" target="#b6">Ganin et al. (2016)</ref>; <ref type="bibr" target="#b18">Long et al. (2015b)</ref>). However, they measure the domain divergence based on the hidden features of the network without considering the relationship between its decision boundary and the target features, as we do.</p><p>Low-density Separation. Many semi-supervised learning (SSL) methods utilize the relationship between the decision boundary and unlabeled samples, a technique called low-density separation <ref type="bibr" target="#b1">(Chapelle &amp; Zien (2005)</ref>; <ref type="bibr" target="#b13">Joachims (1999)</ref>). By placing the boundary in the area where the unlabeled samples are sparse, these models aim to obtain discriminative representations. Our method aims to achieve low-density separation for deep domain adaptation and is related to entropy minimization for semi-supervised learning <ref type="bibr" target="#b9">(Grandvalet &amp; Bengio (2005)</ref>). <ref type="bibr" target="#b19">(Long et al. (2016)</ref>) used entropy minimization in their approach to directly measure how far samples are from a decision boundary by calculating entropy of the classifier's output. On the other hand, our method tries to achieve low-density separation by slightly moving the boundary and detecting target samples sensitive to the movement. As long as target samples features are robust to the movement, they will be allowed to exist relatively nearby the boundary compared to source samples, as <ref type="figure">Fig. 1</ref> shows.</p><p>In <ref type="bibr" target="#b19">(Long et al. (2016)</ref>) entropy minimization is just a part of the overall approach. To compare our ADR approach to entropy minimization directly, we use a new baseline method. To our knowledge, though this method has not been proposed by any previous works, it is easily achieved by modifying a method proposed by <ref type="bibr" target="#b30">(Springenberg (2015)</ref>). The generator tries to minimize the entropy of the target samples whereas the critic tries to maximize it. The entropy is directly measured by the output of the classifier. This baseline is similar to our approach in that the goal of the method is to achieve low-density separation.</p><p>Dropout. Dropout is a method that prevents the networks from overfitting <ref type="bibr" target="#b31">(Srivastava et al. (2014)</ref>) by randomly dropping units from the neural network during training. Effectively, dropout samples from an exponential number of different thinned networks at training time, which prevents units from co-adapting too much. At test time, predictions are obtained by using the outputs of all neurons. If the thinned networks are able to classify the samples accurately, the full network will as well. In other words, dropout encourages the network to be robust to noise. In our work, we use dropout to regularize the feature generation network G, but in an adversarial way. We train the critic C to be sensitive to the noise caused by dropout and use C to regularize G so that it generates noise-robust features. To our knowledge, this use of dropout is completely different from existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We assume that we have access to a labeled source image x s and a corresponding label y s drawn from a set of labeled source images {X s , Y s }, as well as an unlabeled target image x t drawn from unlabeled target images X t . We train a feature generation network G, which takes inputs x s or x t , and a network C that acts as both the main classifier and the critic. C takes features from G and classifies them into K classes, predicting a K-dimensional vector of logits {l 1 , l 2 , l 3 ...l K }. The logits are then converted to class probabilities by applying the softmax function. Namely, the probability that x is classified into class j is denoted by p(y = j|x) = exp(lj ) K k=1 exp(l k ) . We use the notation p(y|x) to denote the K-dimensional probabilistic output for input x.</p><p>The weights of G can be initialized either by pre-training on some auxiliary dataset (e.g., Imagenet), or with random weights. C uses random initialization. When acting as critic, C must detect the feature encodings of target samples near the decision boundary, while G must avoid generating features near that area to fool the critic. To detect the samples near the decision boundary, we propose to slightly perturb the boundary and measure the change in the posterior class probability p(y|x). The critic network C is then trained to increase the change while the feature generation network G is trained to decrease it. Through this adversarial training, G learns to generate target features far away from the decision boundary to decrease the change in posterior, as samples near the decision boundary are likely to have the largest change. In this section, we show how we utilize dropout to perturb the boundary in the critic and measure sensitivity. We then show how to construct the training procedure to ensure discriminativeness for source samples, without which the decision boundary would be meaningless. Finally, we give some intuition for our method and improve it based on this insight. As the training procedure used for GAN is slightly different from the one used in domain adaptation experiments, we describe it in the corresponding experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CLASSIFIER SELECTION VIA DROPOUT</head><p>Consider the standard training of a neural network using dropout. For every sample within a minibatch, each node of the network is removed with some probability, effectively selecting a different classifier for every sample during training. We harness this idea in a very simple way.</p><p>We forward input features G(x t ) to C twice, dropping different nodes each time and obtaining two different output vectors denoted as C 1 (G(x t )), C 2 (G(x t )). In other words, we are selecting two different classifiers C 1 and C 2 from C by dropout as in <ref type="figure">Fig. 2</ref>. In the figure, the corresponding posterior probabilities are indicated as p 1 (y|x t ), p 2 (y|x t ), abbreviated as p 1 and p 2 in the following discussion. In order to detect the change of predictions near the boundary, the critic tries to increase</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Feature Classifier Prediction Generator Update G to minimize sensitivity on target inputs (Fix C)</p><p>Update C to maximize sensitivity on target inputs (Fix G)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial learning Critic sampling using dropout</head><p>Train G, C on source inputs using classification loss <ref type="figure">Figure 2</ref>: Left: We train G, C with classification loss on source and sample a critic consisting of two classifiers using dropout. The critic's sensitivity is measured as the divergence between the class predictions of C 1 and C 2 on the same input. Right: Adversarial training iterates two steps: the critic tries to maximize the sensitivity while the generator tries to minimize it. the difference between the predictions of C 1 and C 2 . This difference corresponds to C's sensitivity to the noise caused by dropout.</p><formula xml:id="formula_1">x s Sample critic C 1 , C 2 p(y|x s )</formula><p>To measure the sensitivity d(p 1 , p 2 ) between the two obtained probabilistic outputs, we use the symmetric kullback leibler (KL) divergence. Formally, the divergence is calculated as</p><formula xml:id="formula_2">d(p 1 , p 2 ) = 1 2 (D kl (p 1 |p 2 ) + D kl (p 2 |p 1 ))<label>(1)</label></formula><p>where KL divergence between p and q is denoted as D kl (p|q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TRAINING PROCEDURE</head><p>In our approach, C works as both critic and classifier. There are the following three requirements in our method: 1) C and G must classify source samples correctly to obtain discriminative features; 2) C should maximize the sensitivity for target samples to detect the samples near the boundary; 3) G should learn to minimize the sensitivity to move target samples away from the boundary.</p><p>The training within the same mini-batch consists of the following three steps.</p><p>Step 1, in this step, C is trained as a classifier. C and G have to classify source samples correctly to obtain discriminative features. Thus, we update both networks' parameters based on the following standard classification loss. Given source labels y s and samples x s , the objective in this step is</p><formula xml:id="formula_3">min G,C L(X s , Y s ) = −E (xs,ys)∼(Xs,Ys) K k=1 1l [k=ys] log p(y|x s )<label>(2)</label></formula><p>Step 2, in this step, C is trained as a critic to detect target samples near the boundary. Two classifiers are sampled from C for each target sample using dropout twice to obtain p 1 and p 2 . Then, C's parameters are updated to maximize the sensitivity as measured by Eq. 1. Since C should learn discriminative features for source samples, in addition to the sensitivity term, we add Eq. 2. We experimentally confirmed that this term is essential to obtain good performance.</p><formula xml:id="formula_4">min C L(X s , Y s ) − L adv (X t ) (3) L adv (X t ) = E xt∼Xt [d(p 1 , p 2 )]<label>(4)</label></formula><p>Step 3, in order to obtain representations where target samples are placed far from the decision boundary, G is trained to minimize sensitivity. Here we do not add the categorical loss for source samples as in Step 2, as the generator is able to obtain discriminative features without it. We also assume that target samples should be uniformly distributed among the possible K classes and add a corresponding conditional entropy term to the generator objective.</p><formula xml:id="formula_5">min G L adv (X t ) + E xt∼Xt K k=1 p(y = k|x t ) log p(y|x t )<label>(5)</label></formula><p>We update the parameters of C and G in every step following the defined objectives. We experimentally found it beneficial to repeat Step 3 n times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INSIGHT AND IMPROVEMENT</head><p>Our ADR approach encourages different neurons of the classifier to learn different characteristics of the input (see Sec. 4.1.) The output is the combination of shared and unshared nodes, therefore, to maximize the sensitivity, the unshared nodes must learn different features of target samples. As learning proceeds, each neuron in C will capture different characteristics. At the same time, to minimize the sensitivity, G learns to extract pure categorical information. If G outputs features which are not related to categorical information, such as texture, slight contrast or difference of color, C will utilize them to maximize sensitivity.</p><p>The trained classifier will be sensitive to the perturbation of targets caused by dropout. We note that our approach is contrary to methods called adversarial example training <ref type="bibr" target="#b8">(Goodfellow et al. (2014b)</ref>; <ref type="bibr" target="#b23">Miyato et al. (2016)</ref>) which train the classifier to be robust to adversarial examples. They utilize input noise which can deceive or change the output of the classifier, and incorporate it to obtain a good classifier. Our ADR method encourages feature generator to obtain noise-robust target features. However, with regard to the classifier, it is trained to be sensitive to noise. To improve the final accuracy, we learn another classifier C that is not trained to be sensitive to the noise. C takes features generated by G and is trained with classification loss on source samples. The loss of C is not used to update G. We compare the accuracy of C and C in experiments on image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>4.1 EXPERIMENT ON TOY DATA Experimental Setting. In this experiment, we observe the decision boundary obtained by each neuron to demonstrate that ADR encourages the neurons to learn different input characteristics. We use synthetic "two moons" data for this problem. Two dimensional samples from two classes are generated as source samples. Target samples are obtained by rotating the source samples. In our setting, the rotation was set to 30 degrees and data was generated with scikit-learn <ref type="bibr" target="#b25">(Pedregosa et al. (2011)</ref>). We train a six-layered fully-connected network; the lower 3 layers are used as feature generator, and upper 3 layers are used as classifier. We used Batch Normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy (2015)</ref>) and ReLU as activation function. The number of neurons are [2,5,5] for feature generator, [5,5,2] for classifier. We visualize the boundary obtained from each neuron in last layer by removing the output of all other neurons. Results. We show the learned boundary in <ref type="figure" target="#fig_0">Fig. 3</ref>. In the baseline model trained only with source samples (top row), two of five neurons do not seem to learn an effective boundary, and three neurons learn a similar boundary. On the other hand, in our method (bottom row), although two neurons do not seem to learn any meaningful boundary, three neurons learn a distinctive boundaries. Each neuron is trained to be sensitive to the noise caused by target samples. The final decision boundary (rightmost column) classifies most target samples correctly. The accuracy of our proposed method is 96% whereas the accuracy of the non-adapted model was 84%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">UNSUPERVISED DOMAIN ADAPTATION FOR CLASSIFICATION</head><p>Experiments on Digits Classification. We evaluate our model on adaptation between digits datasets. We use MNIST (LeCun et al. <ref type="formula" target="#formula_2">(1998)</ref>), SVHN <ref type="bibr" target="#b24">(Netzer et al. (2011)</ref>) and USPS datasets and follow the protocol of unsupervised domain adaptation used by <ref type="bibr" target="#b36">(Tzeng et al. (2017)</ref>). We assume no labeled target samples and use fixed hyper-parameters for all experiments, unlike other works that use a target validation set <ref type="bibr" target="#b28">(Saito et al. (2017)</ref>). The number of iterations for Step 3 was fixed at n = 4. We used the same network architecture as in <ref type="bibr" target="#b36">(Tzeng et al. (2017)</ref>), but inserted a Batch Normalization layer before the activation layer to stabilize the training. We used Adam <ref type="bibr" target="#b14">(Kingma &amp; Ba (2014)</ref>) for optimizer and set the learning rate to 2.0 × 10 −4 . We set the learning rate to a value commonly reported in the GAN literature.</p><p>We compare our approach to several existing methods and to the entropy minimization baseline (ENT) obtained by modifying (Springenberg <ref type="formula" target="#formula_2">(2015)</ref>). Due to space limitations, we provide a detailed explanation of this baseline in the appendix. <ref type="table" target="#tab_1">Table 1</ref> demonstrate that ADR obtains better performance than existing methods. In particular, on the challenging adaptation task from SVHN to MNIST, our method achieves much better accuracy than previously reported. <ref type="figure" target="#fig_2">Fig. 5</ref> shows the learning curve of each experiment. As sensitivity loss increases, the target accuracy improves. This means that as critic C learns to detect the non-discriminative samples, feature generator G learns to fool it, resulting in improved accuracy. In addition, we can see that the sensitivity of source samples increases too. As mentioned in Sec 3.3, the critic network should learn to capture features which are not very important for classification, such as texture or slight edges, and it seems to also capture such information in source samples. The accuracy of the classifier C (denoted by red), which is trained not to be sensitive to the noise, is almost always better than the accuracy of the critic network. In adaptation from SVHN to MNIST <ref type="figure" target="#fig_2">(Fig. 5(c)</ref>), the accuracy of the critic often suffers as it becomes too sensitive to the noise caused by dropout. On the other hand, the accuracy shown by the red line is stable. Our ENT baseline shows good performance compared to other existing methods. This result indicates the effectiveness of methods based on entropy minimization. In <ref type="figure" target="#fig_1">Fig. 4</ref>, we compare our proposed method and ENT in terms of entropy of target samples. Our method clearly decreases the entropy, because target samples are moved away from the decision boundary. Yet, its behavior is different from ENT. Interestingly, the entropy is made smaller than ENT in case of adaptation from USPS to MNIST <ref type="figure" target="#fig_1">(Fig. 4(b)</ref>) though ENT directly minimizes the entropy and our method does not. On the SVHN to MNIST task <ref type="figure" target="#fig_1">(Fig.  4(c)</ref>), the entropy of ADR is larger than ENT, which indicates that our method places the target samples closer to the decision boundary than ENT does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in</head><p>Experiments on Object Classification. We next evaluate our method on fine-tuning a pretrained CNN. We use a new domain adaptation benchmark called the VisDA Challenge <ref type="bibr" target="#b26">(Peng et al. (2017)</ref>)   which focuses on the challenging task of adapting from synthetic to real images. The source domain consists of 152,409 synthetic 2D images from 12 object classes rendered from 3D models. The validation and test target domains consists of real images, which belong to the same classes. We used the validation domain (55,400 images) as our target domain in an unsupervised domain adaptation setting.</p><p>We evaluate our model on fine-tuning networks pretrained on ImageNet <ref type="bibr" target="#b3">(Deng et al. (2009)</ref>): ResNet101 ) and ResNext <ref type="bibr" target="#b37">(Xie et al. (2016)</ref>). For the feature generator, we use the pre-trained CNN after removing the top fully connected layer. For the classification network, we use a three-layered fully connected network. <ref type="table" target="#tab_3">Table 2</ref> shows that our method outperformed other distribution matching methods and our new baseline (ENT) in finetuning both networks by a large margin. ENT did not achieve better performance than existing methods, though improvement over the source only model was observed. Although this method performed well on digits, it does not work as well here, possibly because of the larger shift between very different domains. In this experiment, after training G and C, we retrained a classifier C just on the features generated by G due to GPU memory limitations, and observed improvement in both networks. Image Segmentation experiments. Next, we apply our method to adaptation for semantic image segmentation. Image segmentation is different from classification in that we classify each pixel in the image. To evaluate the performance on segmentation, the synthetic GTA5 (Richter et al. <ref type="formula" target="#formula_2">(2016)</ref>) dataset is used as source, and real CityScape <ref type="bibr" target="#b2">(Cordts et al. (2016)</ref>) dataset is used as target. Previous work tackled this problem by matching distributions of each pixel's feature in a middle layer of the network <ref type="bibr" target="#b11">(Hoffman et al. (2016)</ref>). In this work, we apply ADR by calculating sensitivity between all pixels. The training procedure is exactly the same as in classification experiments.   <ref type="bibr" target="#b5">(Ganin &amp; Lempitsky (2014)</ref>) and <ref type="bibr" target="#b18">(Long et al. (2015b)</ref>) respectively. Ours (retrain classifier) means the classifier retrained for our proposed generator as we mentioned in Sec 3.3. Our proposed method shows much better performance than existing methods. We use the pretrained ResNet50, and utilize an FCN <ref type="bibr" target="#b17">(Long et al. (2015a)</ref>) based network architecture. For the feature generator, we use the pretrained network without fully-connected layers. For the classifier, we use a fully-convolutional network with dropout layers. Due to limited memory, the batch size is set to 1. We include details of the network architecture in our supplementary material. For comparison, we train a domain classifier based model for our network (DANN). We build a domain classifier network for the features of each pixel following <ref type="bibr" target="#b11">(Hoffman et al. (2016)</ref>).</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we show the qualitative comparison with existing methods. ADR clearly improves mean IoU compared to the source-only and competing models. We illustrate the improvement on example input images, ground truth labels, images segmented by Source Only model and our method in <ref type="figure" target="#fig_6">Fig. 7</ref>. While the Source Only model seems to suffer from domain shift, ADR generates a clean segmentation. These experiments demonstrate the effectiveness of ADR on semantic segmentation. Although we implemented ENT in this setting, the accuracy was much worse than the Source Only model with a mIoU of 15.0. The ENT method does not seem to work well on synthetic-to-real shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SEMI-SUPERVISED LEARNING USING GANS</head><p>In this section, we demonstrate the effectiveness of our method in training a Generative Adversarial Network (GAN) applied to semi-supervised learning. We follow the method proposed by <ref type="bibr" target="#b30">(Springenberg (2015)</ref>; <ref type="bibr" target="#b40">Salimans et al. (2016)</ref>), who use a K-class classification network as a critic to train a GAN in the semi-supervised setting. <ref type="table">Method  road  sidewalk  building   wall  fence  pole  t light  t sign  veg  terrain  sky  person  rider  car  truck  bus  train  mbike  bike  mIoU  Source Only</ref>    <ref type="bibr" target="#b5">(Ganin &amp; Lempitsky (2014)</ref>) and <ref type="bibr" target="#b11">(Hoffman et al. (2016)</ref> respectively. Approach. In contrast to the domain adaptation setting, here G tries to generate images which fool the critic C. Also, in this setting, we are given labeled and unlabeled real images from the same domain. Then, we train the critic to classify labeled images correctly and to move unlabeled images far from the decision boundary. To achieve this, we propose to train the critic with the following objective:</p><formula xml:id="formula_6">min C L C = L(X L , Y L ) + L adv (X u ) − L adv (X g ) + E xu∼Xu K k=1 p(y = k|x u ) log p(y|x u ) (6) L adv (X u ) = E xu∼Xu [d(C 1 (G(x u )), C 2 (G(x u )))] (7) L adv (X g ) = E xg∼X G [d(C 1 (G(x g )), C 2 (G(x g )))]<label>(8)</label></formula><p>where X L denotes the subset of labeled samples, X u denotes unlabeled ones and X g denotes images generated by G. The critic is trained to minimize the loss on labeled samples in the first term.</p><p>Since unlabeled images should be far away from the decision boundary and should be distributed uniformly among the classes, we add the second and fourth term. The third term encourages the critic to detect fake images generated near the boundary.</p><p>The objective of G is as follows,</p><formula xml:id="formula_7">min G L adv (X g ) + ||E xg∼Xg f (x g ) − E xu∼Xu f (x u )|| 2<label>(9)</label></formula><p>where the second term encourages generated images to be similar to real images, which is known to be effective to stabilize the training. The first term encourages the generator to create fake images which should be placed far away from the boundary. Such images should be similar to real images because they are likely to be assigned to some class with high probability. Here, we update C and G same number of times.</p><p>Experiment. We evaluate our proposed GAN training method by using SVHN and CIFAR10 datasets, using the critic network architecture from ). In the experiment on SVHN, we replaced Weight Normalization with Batch Normalization for C. Also, in the experiment on CIFAR10, we construct a classifier from a middle layer of the critic, which is not incorporated into the adversarial training step. This is motivated by the insight that the critic in our method is trained to be too sensitive to the dropout noise as we explained in Sec 3.3.</p><p>Results. From <ref type="figure" target="#fig_7">Fig. 8(a)</ref>, we can see that ADR seems to generate realistic SVHN images. Some images are significantly blurred, but most of images are clear and diverse. As for generated CIFAR10 images, they do not seem as realistic, but some objects appear in most images. In <ref type="table">Table 4</ref>, we can SVHN (% errors) CIFAR (% errors) Labeled Only SDGM <ref type="bibr" target="#b21">(Maaløe et al. (2016)</ref> 16.61 ± 0.24 -CatGAN (Springenberg <ref type="formula" target="#formula_2">(2015)</ref>  <ref type="table">Table 4</ref>: Comparison with state-of-the-art methods on two benchmark datasets. Only methods without data augmentation are included. We used the same critic architecture as used in ImpGAN.</p><p>see that the accuracy of the critic trained by our method has better performance than other models for SVHN. For CIFAR10, the accuracy was comparable to other state-of-the-art methods. This demonstrates that our proposed ADR approach is effective for training semi-supervised GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduced a novel approach for aligning feature distributions called Adversarial Dropout Regularization, which learns to generate discriminative features for the target domain. The method consists of a critic network that can detect samples near the boundary and a feature generator that fools the critic. Our approach is general, applies to a variety of tasks, and does not require domain labels. In extensive domain adaptation experiments, our method outperformed baseline methods, including entropy minimization, and achieved state-of-the-art results on three datasets. It also proved effective when applied to Generative Adversarial Networks for semi-supervised learning.</p><p>Similar to our method, we have critic networks C and generator G. C classifies samples into K class. C is trained to maximize the entropy of target samples, which encourages to move the target samples near the boundary. Then, G is trained to minimize the entropy of them. Thus, G tries to move target samples away from the boundary.</p><p>The only difference from our method is that we used entropy term for adversarial training loss. That is, in this method, we replace our sensitivity term d(p 1 , p 2 ) in Eq. 4 with entropy of the classifier output. The adversarial loss for this baseline method is a following one.</p><formula xml:id="formula_8">L adv (X t ) = E xt∼Xt [H[p(y|x t )]<label>(10)</label></formula><p>H[p(y|x t )] = − K k=1 p(y = k|x t ) log p(y = k|x t )</p><p>The hyperparameter n, how many times we update G for adversarial loss in one mini-batch, is set as n = 4. Experimentally, it worked well for all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DIGITS CLASSIFICATION TRAINING DETAIL</head><p>We follow the protocol used in <ref type="bibr" target="#b36">(Tzeng et al. (2017)</ref>). For adaptation from SVHN to MNIST, we used standard training splits of each datasets as training data. For evaluation, we used test splits of MNIST. For the adaptation between MNIST and USPS, we sampled 2000 images from MNIST and 1800 images from USPS. In these experiments, we composed the mini-batch half from source and half from target samples. The batchsize was set as 128 for both source and target. We report the score after repeating Step 1∼3 (please see Sec 3.2) 20000 times. For our baseline, we used exactly the same network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C OBJECT CLASSIFICATION TRAINING DETAIL</head><p>In this experiment, SGD with learning rate 1.0 × 10 −3 is used to optimize the parameters. For the finetuning of ResNet101, we set batch-size as 32. Due to the limit of GPU memory, we set it as 24 in finetuning ResNext model. We report the score after 20 epochs training. In order to train MMD model, we use 5 RBF kernels with the following standard deviation parameters: σ = [0.1, 0.05, 0.01, 0.0001, 0.00001]</p><p>We changed the number of the kernels and their parameters, but we could not observe significant performance difference. We report the performance after 5 epochs. We could not see any improvement after the epoch.</p><p>To train a model <ref type="bibr" target="#b5">(Ganin &amp; Lempitsky (2014)</ref>), we used two-layered domain classification networks. Experimentally, we did not see any improvement when the network architecture is changed. According to the original method <ref type="bibr" target="#b5">(Ganin &amp; Lempitsky (2014)</ref>), learning rate is decreased every iteration. However, in our experiment, we could not see improvement, thus, we fixed learning rate 1.0 × 10 −3 . We report the accuracy after 1 epoch. The accuracy dropped significantly after the first epoch. We assume this is due to the large domain difference between synthetic and real images.</p><p>For our new baseline, ENT, we used the same hyperparameter as we used for our proposed method. Since the accuracy of ENT drops significantly after around 5 epochs, we report the accuracy after 5 epoch updates.</p><p>D SEGMENTATION EXPERIMENTS DETAIL <ref type="figure">Figure 9</ref>: Overview of architecture for semantic segmentation</p><p>We modified <ref type="bibr">FCN Long et al. (2015a)</ref> architecture suitable for ResNet structure. The features from ResBlock 2∼4 and the first convolution layer and maxpooling layer are used in our implementation. In <ref type="figure">Fig. 9</ref>, we show how we integrated the features of each layers. We regard the layers of ResNet50 as generator and rest of the networks, namely convolution and upsampling layers as a critic network. The input images were resized to 512x1024 due to the limit of GPU memory. For the same reason, the batchsize was set to one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(Best viewed in color) Toy Experiment. Top row: Model trained without adaptation. Columns 1-5show the decision boundary obtained by keeping one neuron in the last hidden layer and removing the rest. Red points are source samples of class one, green points are class two. Black points are target samples. The yellow region indicates where the samples are classified as class one, cyan region class two. We see that the neurons do not learn very diverse features. Column 6 shows the boundary obtained by keeping all 5 neurons. Bottom row: Boundaries learned by the model adapted by our adversarial dropout method. Unlike the top row, here neurons 3,4,5 learn diverse features which result in diverse boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Relationship between sensitivity loss on target (blue line), on source (yellow line), and accuracy (red: accuracy of C , green: accuracy of C) during training on digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparision of entropy of ours (blue line) with ENT (yellow line). The entropy is calculated on target samples by using the output of the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>visualizes the target features obtained by G with the pretrained model, model fine-tuned on source, and our ADR method. While the embedding of the source only model does not separate classes well due to domain shift, we can see clearly improved separation with ADR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of VisDA-classification (12 classes) target features using T-SNE (Maaten &amp; Hinton (2008)): (a) features obtained by the Imagenet-pretrained (Deng et al. (2009)) ResNext model not finetuned on VisDA; (b) features from the ResNext model fine-tuned only on VisDA source samples without any adaptation; (c) features obtained by ResNext adapted by our ADR method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of results on two real images. Clockwise from upper left: Original image; Ground truth; Segmented image before adaptation; Segmented image after adaptation by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Examples of generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on digits datasets. Please note that † means the result obtained using a few labeled target samples for validation. The reported accuracy of our method is obtained from C . ENT is our proposed baseline method. We implement entropy minimization based adversarial training as we mentioned in Appendix A.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on Visda2017 classification datasets. DANN and MMD are distribution alignment methods proposed by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on adaptation from GTA5 → Cityscapes. DANN and FCN Wild denote methods proposed by</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>We would like to thank Trevor Darrel for his great advice on our paper. First author's stay at Boston University is partially supported by scholarship of the University of Tokyo. The work was partially funded by the ImPACT Program of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan), and was partially supported by CREST, JST. Saenko was supported by IARPA and NSF grants CCF-1723379 and IIS-1724237.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auxiliary deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">Visda: The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks can be applied to achieve our goal too</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Critic is trained to classify real samples into K classes. They also trained critic to move unlabeled real images away from the boundary by minimizing entropy of the critic&apos;s output. Generated fake images are moved near the boundary by maximizing the entropy. On the other hand, generator is trained to generate fake images which should be placed away from the boundary. This kind of method can be easily applied to domain adaptation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>used small number of labeled samples to train critic. We would like to describe the method along with our problem setting</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">We set the batch size as 100 and used Adam with learning rate 2.0 × 1.0 −4 for optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gan Training</surname></persName>
		</author>
		<editor>Salimans et al.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>After the conv6 layer of the critic, we constructed a classifier which was not concerned with adversarial learning process</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
