<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
							<email>liangjian92@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="institution">National University of Singapore (NUS)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Partial Transfer Learning</term>
					<term>Domain Adaptation</term>
					<term>Adversarial Alignment</term>
					<term>Uncertainty Propagation</term>
					<term>Object Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000−0003−3890−1894] , Yunbo Wang 2[0000−0002−6215−8888] , Dapeng Hu 1 , Ran He 3[0000−0002−3807−991X] , and Jiashi Feng 1[0000−0001−6843−0064]</p><p>Abstract. This work addresses the unsupervised domain adaptation problem, especially in the case of class labels in the target domain being only a subset of those in the source domain. Such a partial transfer setting is realistic but challenging and existing methods always suffer from two key problems, negative transfer and uncertainty propagation. In this paper, we build on domain adversarial learning and propose a novel domain adaptation method BA 3 US with two new techniques termed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty Suppression (AUS), respectively. On one hand, negative transfer results in misclassification of target samples to the classes only present in the source domain. To address this issue, BAA pursues the balance between label distributions across domains in a fairly simple manner. Specifically, it randomly leverages a few source samples to augment the smaller target domain during domain alignment so that classes in different domains are symmetric. On the other hand, a source sample would be denoted as uncertain if there is an incorrect class that has a relatively high prediction score, and such uncertainty easily propagates to unlabeled target data around it during alignment, which severely deteriorates adaptation performance. Thus we present AUS that emphasizes uncertain samples and exploits an adaptive weighted complement entropy objective to encourage incorrect classes to have uniform and low prediction scores. Experimental results on multiple benchmarks demonstrate our BA 3 US surpasses state-of-the-arts for partial domain adaptation tasks. Code is available at https://github.com/tim-learn/BA3US.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past two decades, many research efforts have been devoted to unsupervised domain adaptation (UDA), which aims to leverage labeled source do-main data to learn to classify unlabeled target domain data. Typically, existing UDA methods minimize the discrepancy between two domains by matching their statistical distribution moments <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref> or by domain adversarial learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref>. Once the domain shift is mitigated, source classifiers can be easily transferred to the target domain even with no labeled target domain data available. However, both UDA strategies always assume that different domains share the same label space. Such an assumption may not hold in practice and target domain labels may be only a subset of source domain labels. This introduces an unsupervised partial domain adaptation (PDA) problem that receives increasing research attention recently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The PDA problem is challenging since source-only classes may occur in the target domain during distribution alignment, which is well-known as class mismatch that potentially causes negative transfer. Several previous PDA approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> mitigate negative transfer by jointly filtering out source-only classes and promote positive transfer by matching the data distribution in the shared classes. Samples from source-only classes are expected to have lower weights in the adaptation module such that the marginal distributions of two domains can be aligned well. However, it is rather risky to rule out the source-only classes, especially when the estimation of label distribution in the target domain is inaccurate.</p><p>To match two non-identical label spaces, we view the PDA problem from a new perspective and propose to augment the target label space to be the same as the source label space. Specifically, we develop a simple balanced alignment solution, termed Balanced Adversarial Alignment (BAA), that borrows fewer and fewer samples from the source domain to the target domain within an iterative adversarial learning framework. We expect that the augmented target domain looks much more similar to the source domain w.r.t. the label distribution, and the challenging PDA problem can be transformed to a well-studied UDA task. To focus on the originally shared classes, we propose to filter out the source-only classes via a class-level weighting scheme meanwhile, making the large UDA task more compact.</p><p>Besides, existing domain adaptation methods always employ a conventional cross-entropy loss to merely promote the prediction score of ground-truth classes but neglect to suppress those of incorrect classes, which may result in a new problem termed uncertainty propagation. Intuitively, if incorrect classes have relatively high prediction scores for source data, some wrong classes would possibly have the largest prediction scores for the aligned target data around them. This problem is quite critical but has been always ignored in the domain adaptation field. To circumvent the issue, we develop an uncertainty suppression solution termed Adaptive Uncertainty Suppression (AUS) that exploits complement entropy <ref type="bibr" target="#b6">[7]</ref> in the labeled source domain to prohibit possibly high prediction scores from incorrect classes. Specifically, we emphasize more the uncertain samples corresponding to smaller cross-entropy loss (confidence) and propose a confidence-weighted complement entropy objective in addition to the primary cross-entropy objective.</p><p>Generally, our baseline is built on the seminal domain adversarial networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and exploits conditional entropy minimization and an entropy-aware weight strategy <ref type="bibr" target="#b25">[26]</ref>. In this paper, we equip the baseline model with two proposed techniques mentioned above and finally formulate a unified framework BA 3 US which well addresses the negative transfer and uncertainty propagation problems in partial domain adaptation. We also empirically discover that the uncertainty suppression technique works well for vanilla closed-set domain adaptation.</p><p>To sum up, we make the following contributions. To our best knowledge, this is the first work that tackles partial domain adaptation by augmenting the target domain and transforming it into a UDA-like problem. The proposed balanced augmentation technique is fairly simple and works very well for PDA tasks. Besides, we address an overlooked issue in this field named uncertainty propagation by designing an adaptive weighted complement entropy for the source domain. Extensive results demonstrate that our approach yields new state-ofthe-art results on several visual benchmark datasets, including Office31 <ref type="bibr" target="#b33">[34]</ref>, Office-Home <ref type="bibr" target="#b41">[42]</ref>, and ImageNet-Caltech <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The past two decades have witnessed remarkable progress in domain adaptation. Interested readers can refer to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b18">19]</ref> for taxonomy and survey.</p><p>Unsupervised Domain Adaptation (UDA). Compared with its supervised counterpart, UDA is more practical and challenging since no labeled data in the target domain are available. Recently, deep convolutional neural networks have achieved great success for visual recognition tasks, and we focus on deep UDA methods in this work. They can be categorized into three main groups. The first group aims to minimize the domain discrepancy by matching different statistic moments like maximum mean discrepancy (MMD) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref> and higher-order moment matching <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref>. The second group that is widely used introduces a domain discriminator and exploits the idea of adversarial learning <ref type="bibr" target="#b11">[12]</ref> to encourage domain confusion so that the discriminator can not decide which domain the data come from. Some typical examples are <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2]</ref>. A third group is a reconstruction-based approach, assuming the reconstruction of both source and target domain samples to be important and helpful. Among them, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref> utilize encoder-decoder reconstruction and adversarial reconstruction, respectively. Despite their success for vanilla UDA, they are easily stuck by negative transfer for PDA due to the mismatched marginal label distributions.</p><p>Partial Domain Adaptation (PDA). In reality, PDA can be considered as a special case of imbalanced domain adaptation, where the target label distribution is quite dissimilar to that of the source domain. Until recent years, <ref type="bibr" target="#b30">[31]</ref> first introduces an imbalanced scenario where label numbers of the source and target domains are not the same, which draws the attention of many researchers <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. Different from shallow methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref>, deep methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30]</ref> are mainly based on the domain adversarial learning framework and achieve promising recognition accuracy. Selective adversarial network (SAN) <ref type="bibr" target="#b3">[4]</ref> exploits a multi-discriminator domain adversarial network and tries to select source-only classes by imposing different localized weights on different discriminators. Importance weighted adversarial nets (IWAN) <ref type="bibr" target="#b48">[49]</ref> apply only one domain discriminator and weigh each source sample with the probability of being a target sample. Partial adversarial domain adaptation (PADA) <ref type="bibr" target="#b4">[5]</ref> estimates the target label distribution and then feeds the class-wise weights to both the source discriminator and the domain discriminator, while two weighted inconsistency-reduced networks (TWINs) <ref type="bibr" target="#b29">[30]</ref> leverage two independent networks to estimate the target label distribution and minimize the domain difference measured by the classifiers' inconsistency on the target samples. Deep Residual Correction Network (DRCN) <ref type="bibr" target="#b20">[21]</ref> proposes a weighted class-wise matching strategy to explicitly align target data with the most relevant source subclasses. Recently, Example Transfer Network (ETN) <ref type="bibr" target="#b5">[6]</ref> jointly learns domaininvariant representations across domains and a progressive weighting scheme to quantify the transferability of source examples, which achieves state-of-the-art results on several benchmark datasets. Generally, all the PDA methods above attempt to filter out the large source domain to match the small target domain. Comparatively, our method tries to augment the small target domain to match the source domain from a different perspective.</p><p>Data Synthesis and Augmentation. Recently, synthesis and augmentation techniques like CycleGAN <ref type="bibr" target="#b50">[51]</ref> and mixup <ref type="bibr" target="#b47">[48]</ref> are favored by UDA and semi-supervised learning methods for improving performance. For example, <ref type="bibr" target="#b15">[16]</ref> directly exploits CycleGAN to generate target-like images from source samples to narrow the domain shift for adaptive semantic segmentation. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43]</ref> extend mixup to domain adaptation and generate pseudo training samples via interpolating between certain source samples and uncertain target samples. To some degree, target augmentation in this paper is like a special case of mixup where the mixup coefficient is always binary. However, the motivations are totally different. Our method considers neither the interpolated semantic label nor the interpolated domain label for a classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We elaborate on the proposed framework for partial domain adaptation (PDA) in this section. First, we give definitions and notations. We follow the protocol of unsupervised PDA where we have a labeled source domain dataset</p><formula xml:id="formula_0">D s = {(x s i , y s i )} ns i=1 , x s i ∈ R d and an unlabeled target domain dataset D t = {(x t i )} nt i=1 , x t i ∈ R d</formula><p>during the training stage. These two domains have different feature distributions: p s (x s ) = p t (x t ) due to the domain shift. Notably, different from vanilla UDA, the target labels are a subset of the source labels for PDA: Y t ⊆ Y s , and C denotes the total number of classes in Y s .</p><p>We aim to learn a deep neural network h : X → Y that consists of two components: h = g • f . Here f : X → Z denotes the feature extractor and g : Z → Y denotes a class predictor. Since we target at learning domain-invariant features, the prediction function is assumed identical, i.e., g = g s = g t . For simplicity, we also share the feature extractor f for different domains. We introduce an adversarial classifier D : Z → {0, 1} to mitigate the distribution discrepancy across domains as explained later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain Adversarial Learning Revisited</head><p>Generative adversarial network (GAN) <ref type="bibr" target="#b11">[12]</ref> learns two competing components: the discriminator D and the generator F which play a minimax two-player game, where F tries to fool D by generating examples that are as realistic as possible and D tries to classify the generated samples as fake. Inspired by the idea of GAN, domain adversarial neural networks (DANN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> develops a two-player game for UDA, where one player D(·; θ d ) (i.e., domain discriminator) tries to distinguish the source domain datum from that of the target domain and the other player F (·; θ f ) (i.e., feature extractor) is trained to confuse the domain discriminator D(·; θ d ). Generally, the minimax game of DANN is formulated as</p><formula xml:id="formula_1">min θ f ,θg max θ d L cls (θ f , θ g ) + λ L adv (θ f , θ d ), L adv (θ f , θ d ) = 1 n s ns i=1 log[D(F (x s i ))] + 1 n t nt j=1 log[1 − D(F (x t j ))], L cls (θ f , θ g ) = 1 n s ns i=1 l ce (G(F (x s i )), y s i ),<label>(1)</label></formula><p>where l ce (·, ·) represents the softmax cross-entropy loss, G(·; θ g ) denotes the source classifier, F (·; θ f ) represents the domain-shared feature extractor, and λ is a hyper-parameter to trade-off the source risk and domain adversary. Different from a label flipping step in GAN, a gradient reversal layer (GRL) is further defined in <ref type="bibr" target="#b8">[9]</ref> to optimize the objective in Eq. (1). Due to its simplicity and effectiveness, the idea of domain adversarial learning has been adopted in many previous domain adaptation works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref>. As shown in Eq. (1), each sample from both source and target domains is equally involved in the adversarial loss L adv , which seems not reasonable. If we only have samples distributed in the margin of the classifier (called 'hard' samples) from different domains and pursue domain alignment via them, it may perform badly for those 'easy' samples across domains. As such, we expect those 'hard' samples and 'easy' samples to own lower and higher weights during domain adversarial alignment. Specifically, we quantify the difficulty via the entropy criterion H(h) = − C c=1 h c log(h c ) and adopt the same weighting strategy as <ref type="bibr" target="#b25">[26]</ref> to impose an entropy-aware weight w(x) = 1 + e −H(h(x)) on each sample,</p><formula xml:id="formula_2">L e adv (θ f , θ d ) = 1 n s ns i=1 w(x s i ) log[D(F (x s i ))] + 1 n t nt j=1 w(x t j ) log[1 − D(F (x t j ))].<label>(2)</label></formula><p>Obviously, if there is no domain shift, UDA (including PDA) degenerates to a typical semi-supervised learning problem. On one hand, we aim to mitigate the domain shift; on the other hand, this motivates us to adopt the popular entropy minimization principle <ref type="bibr" target="#b12">[13]</ref> in semi-supervised learning to the PDA task. It is  <ref type="figure">Fig. 1</ref>. Architecture of our domain adaptation method. There are three modules: a shared feature extractor F , a classifier G and a domain discriminator D. Different from domain adversarial learning <ref type="bibr" target="#b8">[9]</ref>, it contains two extra components with marked red border, i.e., balanced augmentation and weighted complement entropy.</p><p>desirable that all the unlabeled target samples have highly-confident predictions. This is encouraged via the following conditional entropy term,</p><formula xml:id="formula_3">L ent (θ f , θ y ) = 1 n t nt j=1 H(G(F (x t j ))).<label>(3)</label></formula><p>Inspired by the observation in <ref type="bibr" target="#b3">[4]</ref> that redundant information is not beneficial for adaptation, we adopt the following optimization objective of Entropy-regularized DANN (E-DANN) as an initial model of our method,</p><formula xml:id="formula_4">min θ f ,θg max θ d L w cls (θ f , θ g ) + αL ent (θ f , θ g ) + λL e adv (θ f , θ d ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">L w cls (θ f , θ g ) = 1 ns ns i=1 m(y s i )l ce (G(F (x s i ))</formula><p>, y s i ), m denotes the normalized estimated class-level weight vector via the target domain, and α, λ are two empirical trade-off parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Balanced Adversarial Alignment (BAA)</head><p>The joint distribution shift is the actual root to negative transfer <ref type="bibr" target="#b44">[45]</ref>. For example, the marginal label distributions are not symmetric in PDA, and thus sourceonly classes are prone to be matched with target classes, resulting in a negative transfer problem. Generally, a class-level source re-weighting scheme sounds a natural choice for PDA, since it is expected to filter out source-only classes and promote positive transfer between the shared classes across domains. Previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> resort to target predictions to generate class-level weights and effectively avoid negative transfer to some degree. Ideally, PDA with a weighting scheme behaves like a small UDA problem, but it heavily relies on accurate target predictions to calculate suitable weights.</p><p>In contrast, we propose an extremely simple scheme as shown in <ref type="figure">Fig. 1</ref> for the challenging distribution alignment in PDA. Intuitively, we pursue the balance between different label distributions across domains with quite an opposite solution, i.e., augmenting the target domain using original source samples instead of weighting the source domain. This idea looks weird but is actually reasonable because we readily turn the PDA task into a large UDA-like task and the negative transfer effects caused by source-only classes can be well alleviated. The detailed formulation of balanced augmentation (alignment) is shown below,</p><formula xml:id="formula_6">L ba adv (θ f , θ d ) = 1 n s ns i=1 w(x s i )m(y s i ) log[D(F (x s i ))] + 1 n t nt j=1 w(x t j ) log[1 − D(F (x t j ))] + ρ n s ns i=1 w(x s i )m(y s i ) log[1 − D(F (x s i ))].</formula><p>(5) Specifically, we adopt a progressive strategy for the target augmentation scheme. We borrow the source samples with different ratios, and the ratio ρ gradually decreases to 0 as the number of iterations increases. This is because the learned feature representations in early iterations are not quite transferable and we need more source samples to avoid class mismatch. When the learned features are desirably discriminative and transferable, the estimation of class-level weights becomes more accurate, making the augmentation trivial.</p><p>Note that we borrow original samples from the source domain rather than exploiting a generative model like CycleGAN <ref type="bibr" target="#b50">[51]</ref> to synthesize target-like source images. The reason is that obtaining data-dependent translation models between such heterogeneous domains is quite time-consuming and we find translation does not even improve the adaptation results. Previous DA methods focus on strengthening the feature transferability by developing various domain alignment strategies, but they mostly ignore the feature discriminability and simply use the conventional crossentropy loss in the labeled source domain to learn the features. In that case, even though the domain shift is mitigated, the classifier may perform worse on target data. This is because source classes are not equally separated from each other, which may lead to the propagation of the confusion (uncertainty) to target predictions and thus confusing features in the target domain, which is termed as the uncertainty propagation problem. Take a 3-way classification problem as an example. There is no class overlap in the source domain, but class 1 and 2 may be close to each other. A toy example of how class 1 and class 2 behave is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> where a few samples lie close to the decision boundary between class 1 and class 2. During domain alignment, some unlabeled target data are enforced to match these source data, which would be easily misclassified. However, such a critical problem has always been overlooked in prior literature. Looking back at the cross-entropy loss l ce (ŷ, y) = − i y i log(ŷ i ), it only exploits the information from the ground-truth class while ignoring that from other incorrect classes. For example, the source output [0.6, 0.3, 0.1] is more uncertain than [0.6, 0.2, 0.2], but both have the same cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Uncertainty Suppression (AUS)</head><p>Though several previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> incorporate virtual adversarial training <ref type="bibr" target="#b31">[32]</ref> in the classification term to implicitly increase the margin between different classes, more computation complexity is required and several parameters need to be tuned. Inspired by <ref type="bibr" target="#b6">[7]</ref>, we exploit a complement entropy that expects uniform and low prediction scores for incorrect classes for labeled source samples. To accurately suppress uncertainty, we further place more emphasis on the uncertain samples that own smaller cross-entropy loss (confidence) and propose a confidence-weighted complement entropy objective below,</p><formula xml:id="formula_7">L w wce (θ f , θ g ) = 1 n s log(K − 1) ns i=1 m(y s i )l wce (G(F (x s i )), y s i ), where l wce (ŷ, y) = (1 −ŷ a ) ξ j =aŷ j 1 −ŷ a log(ŷ j 1 −ŷ a ),<label>(6)</label></formula><p>where ξ is a hyper-parameter and a is the index of ground-truth class in y. Different from the complementary training strategy in <ref type="bibr" target="#b6">[7]</ref>, we exploit the adaptive weighted complement entropy L wce as a regularizer, which is more efficient. We also assign a class-level weight for each sample in Eq. (6) like that in L w cls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unified Minimax Optimization Problem of BA 3 US</head><p>Finally, we integrate all the terms in Eqs. (4, 5, 6) on both source and target samples to avoid negative transfer and uncertainty prorogation and derive a unified framework for PDA. The overall min-max objective is formulated as</p><formula xml:id="formula_8">min θ f ,θg max θ d L w cls (θ f , θ g ) + αL ent (θ f , θ g ) + βL w wce (θ f , θ g ) + λL ba adv (θ f , θ d ),<label>(7)</label></formula><p>where β is another trade-off hyper-parameter to balance the complement entropy and the cross-entropy term. Again, m ∝ nt j=1 G(F (x t i )) is the normalized estimated class-level weight vector. To optimize the objective above, we follow <ref type="bibr" target="#b8">[9]</ref> to introduce a gradient reversal layer and adopt the same progressive strategy for parameter λ, i.e., increasing λ from 0 to 1 as the number of iterations grows.</p><p>Our method is closely related to DANN <ref type="bibr" target="#b8">[9]</ref>, sharing similar formalism of the domain adaptation theory <ref type="bibr" target="#b0">[1]</ref> that the expected target risk T (H) on the target examples is bounded by the source risk S (H) and other two terms below,</p><formula xml:id="formula_9">T (H) ≤ S (H) + | S (H, H * ) − T (H, H * )| + [ S (H * ) + T (H * )],<label>(8)</label></formula><p>where H * = arg min x∈H [ S (x) + T (x)] is the ideal joint hypothesis for the combined risk. DANN further discovers the second term H∆H-distance can be upper bounded by error of the domain adversarial classifier. As we do not have labels of the target domain, we expect the entropy minimization term on the target domain to help reduce the last term. Besides, the proposed complement entropy objective alleviates uncertainty propagation to make the weight estimation more accurate, and thus our method would turn PDA into a small UDA task. In this way, we can expect that our method minimizes the empirical target risk E (xt,yt)∼pt [f (x t ) = y t ]. The detailed optimization is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets. Office31 dataset <ref type="bibr" target="#b33">[34]</ref> includes images of 31 object classes from three different domains, i.e., Amazon, DSLR, and Webcam. We follow the standard protocol used in <ref type="bibr" target="#b4">[5]</ref> and pick up images of 10 categories shared by Office31 and Caltech256 <ref type="bibr" target="#b13">[14]</ref> as target domains. Office-Home dataset <ref type="bibr" target="#b41">[42]</ref> consists of 4 different domains with each containing 65 kinds of everyday objects, i.e., Artistic, Clipart, Product images, and Real World images. Likewise, we follow <ref type="bibr" target="#b4">[5]</ref> to select the first 25 categories (in alphabetic order) in each domain as a partial target domain. ImageNet-Caltech is a large-scale object recognition dataset that consists of two subsets, ImageNet-1K <ref type="bibr" target="#b32">[33]</ref> and Caltech256 <ref type="bibr" target="#b13">[14]</ref>. Here we use images from the public validation set of ImageNet-1K for the target domain.</p><p>In reality, each source domain contains 1,000 and 256 classes, and each target domain contains 84 classes. Baseline methods. We utilize all the source and target samples and report the average classification accuracy and standard deviation over 3 random trials. A → B means A is the source domain and B is the partial target domain. For comprehensive comparison, we provide the recognition results of our methods including E-DANN, Ours (w/ BAA) and Ours (BA 3 US) on each dataset, and compare them with some popular UDA methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26]</ref> and existing PDA methods, including SAN <ref type="bibr" target="#b3">[4]</ref>, IWAN <ref type="bibr" target="#b48">[49]</ref>, PADA <ref type="bibr" target="#b4">[5]</ref>, SSPDA <ref type="bibr" target="#b2">[3]</ref>, MWPDA <ref type="bibr" target="#b16">[17]</ref>, DRCN <ref type="bibr" target="#b20">[21]</ref>, ETN <ref type="bibr" target="#b5">[6]</ref>, and SAFN <ref type="bibr" target="#b45">[46]</ref>. Implementation details. If not specified, all the methods adopt ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as backbone. We fine-tune the pre-trained ImageNet model in PyTorch using a NVIDIA Titan X (12 GB memory). The adversarial layer and classifier layer are trained through back-propagation, and the learning rate of the classifier layer is 10 times that of lower layers. We adopt mini-batch SGD with momentum of 0.9 and the learning rate annealing strategy as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>: the learning rate is adjusted by η p = η 0 (1 +αp) −β , where p denotes the training progress changing from 0 to 1, and η 0 = 0.01,α = 10,β = 0.75 as suggested by <ref type="bibr" target="#b25">[26]</ref>. Based on the number of classes and balancing analysis in <ref type="bibr" target="#b6">[7]</ref>, we use β = 5 for Office31 and ImageNet-Caltech, and β = 1 for Office-Home. Besides, we set the number of intervals N/N u to 10, and N u is set to 200, 500, and 4,000 for Office31, Office-Home, and ImageNet-Caltech, respectively. Note that our method does not use the ten-crop technique <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> at the evaluation phase for better performance. <ref type="table" target="#tab_6">Table 1</ref>. Accuracy (%) on Office-Home dataset for partial domain adaptation via ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. The best in bold red; the second best in italic blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results for Partial Domain Adaptation</head><p>The results on three object recognition datasets including Office-Home, Of-fice31 and ImageNet-Caltech for PDA are shown in <ref type="table" target="#tab_6">Table 1</ref> and 2, with some baseline results directly reported from ETN <ref type="bibr" target="#b5">[6]</ref> with the same protocol. Obviously, BA 3 US achieves the best or second-best results on 10 out of 12 transfer tasks on the Office-Home dataset, and Ours (w/ BAA) and SAFN obtain the second and third best results, respectively. Regarding the average accuracy, BA 3 US advances the state-of-the-art result on Office-Home in SAFN <ref type="bibr" target="#b45">[46]</ref> by 5.78%, from 71.83% to 75.98%. For two specific tasks Cl →Ar and Pr →Rw, BA 3 US merely performs slightly worse than the best method. Besides, a stateof-the-art UDA approach CDAN+E <ref type="bibr" target="#b25">[26]</ref> performs worse than ResNet-50, which implies the difficulty of the partial transfer task. Further compared with UDA methods, even PDA methods like IWAN <ref type="bibr" target="#b48">[49]</ref> and PADA <ref type="bibr" target="#b4">[5]</ref> do not work well, which again indicates the partial transfer is quite challenging.</p><p>On the small-scale Office31 dataset, BA 3 US again obtains the best average accuracy and performs the best in 3 out of 6 transfer tasks. For transfer tasks from a large source domain A to small target domains (D, W ), BA 3 US remarkably outperforms other PDA methods. BA 3 US performs slightly worse for the D → A task because the target domain D is very small, making the proposed target augmentation and adaptive uncertainty suppression techniques inefficient. On the large-scale ImageNet-Caltech dataset, BA 3 US performs the best for both transfer tasks and still holds the best average accuracy with significant improvements. Moreover, UDA methods do not always perform better than ResNet-50, which implies they may suffer from the negative transfer problem.</p><p>Besides the ResNet-50 backbone network, we further investigate the effectiveness of BA 3 US with another backbone network VGG-16 <ref type="bibr" target="#b35">[36]</ref> on Office31 and compare it with state-of-the-art methods in <ref type="table">Table 3</ref>. It can be clearly observed that BA 3 US achieves the best or second-best results for all the tasks, significantly advancing the average accuracy from 93.88% to 95.84%. Compared with the results in <ref type="table">Table 2</ref>, we find BA 3 US (97.81%→95.84%) is also robust than ETN (96.73%→93.88%) w.r.t. the change of the backbone network. <ref type="table">Table 2</ref>. Accuracy (%) on Office31 and ImageNet-Caltech for partial domain adaptation via ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. The best in bold red; the second best in italic blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Office31</p><formula xml:id="formula_10">ImageNet-Caltech A → D A → W D → A D → W W → A W → D Avg. I → C C → I Avg.</formula><p>ResNet-50 <ref type="bibr" target="#b14">[15]</ref>   <ref type="table">Table 3</ref>. Accuracy (%) on Office31 for partial domain adaptation via VGG-16 <ref type="bibr" target="#b35">[36]</ref>. The best in bold red; the second best in italic blue. Ablation study. As shown in Tables 1 and 2, we provide the results of E-DANN and Ours (w/ BAA) along with BA 3 US on three datasets. Ours (w/ BAA) extends E-DANN by using the proposed balanced adversarial alignment technique in Sec. 3.2 instead, while Ours (BA 3 US) extends Ours (w/ BAA) by considering the proposed adaptive complement entropy objective in Eq. <ref type="bibr" target="#b5">(6)</ref>. Firstly, the baseline method E-DANN always performs well for partial domain adaptation since it removes the irrelevant classes in the source classification term like <ref type="bibr" target="#b4">[5]</ref>. Secondly, results on all three datasets demonstrate that Ours (BA 3 US) performs better than Ours (w/ BAA) and Ours (w/ BAA) performs better than E-DANN, which verify the effectiveness of two proposed techniques in Sec. 3.2 and Sec. 3.3.</p><formula xml:id="formula_11">Method A → D A → W D → A D → W W → A W → D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results for Closed-set Domain Adaptation</head><p>This section further investigates the effectiveness of the proposed uncertainty suppression technique for vanilla closed-set domain adaptation. Here we con-sider integrating them with DANN and CDAN <ref type="bibr" target="#b25">[26]</ref> respectively, and compare our methods with state-of-the-art UDA approaches including <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref> on the most-favored Office-Home dataset. As shown in <ref type="table">Table 4</ref>, the proposed method BA 3 US built on CDAN obtains the best average accuracy and ranks the top two in 9 out of 12 different transfer tasks. It is obvious that the adaptive uncertainty suppression technique works well for closed-set domain adaptation, advancing the average accuracy from 67.6% to 68.7% and from 68.0% to 69.2%. In fact, we also study the effectiveness of balanced adversarial alignment but find it hardly improve the performance since the label distributions in UDA have already been symmetric. <ref type="table">Table 4</ref>. Accuracy (%) on Office-Home dataset for vanilla unsupervised domain adaptation via ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. Methods * utilize augmentation during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results for Partial Domain Adaptation</head><p>We study our methods with different numbers of target classes in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. The performance decreases when the number is larger than 15, and BA 3 US always obtains the best results. As expected, the proposed augmentation technique becomes important when the number of target classes is small. Weight visualization. As stated before, the weighting scheme plays an important role in PDA. Thus we investigate the quality of the estimated classlevel weight m in Algorithm 1. As shown in <ref type="figure" target="#fig_2">Fig. 3(b-c)</ref>, we plot the estimated weights of BA 3 US for the two specific tasks. Since the Cl domain and Ar domain are quite different, making the estimation challenging. This is also evidenced in other PDA methods in <ref type="table" target="#tab_6">Table 1</ref> since the accuracy is around 55%. For the relatively easy task A→D, the weight estimation seems accurate, resulting in high classification accuracy.</p><p>Parameter Sensitivity. We study the sensitivity of parameters ξ and β of BA 3 US on the Office-Home and Office31 datasets. The mean accuracy is reported in <ref type="table" target="#tab_4">Table 5</ref> and <ref type="table" target="#tab_5">Table 6</ref>, respectively. Note that the complement entropy <ref type="bibr" target="#b6">[7]</ref> can be considered as a special case of the proposed adaptive one where ξ = 0. The results indicate using an adaptive objective is much better, and the performance is relatively stable. Regarding the parameter β, the accuracy around 1.0 and 5.0 is also stable, implying our method is not sensitive.   Convergence performance. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we study the convergence performance of the proposed methods for Ar→Cl and A→D. Obviously, the 'source only' method works worse without the domain alignment module, and E-DANN performs much better than it and quickly converges after 1,000 iterations. Besides, both BA 3 US and Ours (w/ BAA) obtain similar promising results, and BA 3 US performs slightly better since it further considers the adaptive complement entropy to diminish the uncertainty in source predictions. Feature visualization. We plot in <ref type="figure" target="#fig_4">Fig. 5</ref> the t-SNE embeddings <ref type="bibr" target="#b27">[28]</ref> of the features learned by 'source only', E-DANN and BA 3 US for two different transfer tasks. It is easy to discover that the features of target data are rather confusing in 'no adaptation' while the balanced alignment module helps mitigate the domain gap and the adaptive uncertainty suppression module helps increase the discrimination of the features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We develop a novel adversarial learning-based method BA 3 US for partial domain adaptation, which well addresses two key problems, negative transfer and uncertainty propagation. To tackle the asymmetric label distributions, BA 3 US offers a very simple solution by augmenting the target domain with samples from the source domain. Then, it uncovers an overlooked issue in the field termed uncertainty propagation and designs an adaptive complement entropy objective to well suppress the uncertainty in source predictions. Empirical results show it also works well for vanilla closed-set domain adaptation. Further experiments have validated that BA 3 US improves existing methods with substantial gains, establishing new state-of-the-art. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Mitigating the effects of uncertainty propagation from source. [blue: source, red: target, gray: adversarial alignment.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>of Ar→Cl (b) estimated weight of Ar→Cl (c) estimated weight of A→D (a) Accuracy with varying number of target classes. (b-c) Estimated class-level weights. Yellow bins denote ground-truth classes. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Ar → Cl (Office-Home) (b) A → D (Office31) Convergence analysis of proposed methods on two different transfer tasks. Accuracy (%) is given w.r.t. number of iterations. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Ours (w/ BAA) (d) Ours (BA 3 US) t-SNE visualizations for two transfer tasks A→D (upper row) and Ar→Cl (bottom row). Blue: source data; red: target data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ResNet-50<ref type="bibr" target="#b14">[15]</ref> 46.33 67.51 75.87 59.14 59.94 62.73 58.22 41.79 74.88 ±0.50 ±0.33 ±0.27 ±0.46 ±1.39 ±0.52 ±0.31 ±0.46 ±0.78 ±0.37 ±0.83 ±0.20 ±0.45 ±0.12 ±0.19 ±0.19 ±1.10 ±0.59 ±0.19 ±0.37 ±0.22 ±0.65 ±0.51 ±0.26 ±0.28 ±0.45 ±0.32 ±0.67 ±2.52 ±0.30 ±0.32 ±0.30 ±0.32 ±0.62 ±0.46 ±0.39 Ours (BA 3 US) 60.62 83.16 88.39 71.75 72.79 83.40 75.45 61.59 86.53 79.25 62.80 86.05 75.98 ± 0.45 ±0.12 ±0.19 ±0.19 ±1.10 ±0.59 ±0.19 ±0.37 ±0.22 ±0.65 ±0.51 ±0.26</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.40</cell><cell>48.18</cell><cell>74.17 61.35</cell></row><row><cell>ADDA [40]</cell><cell>45.23 68.79</cell><cell>79.21</cell><cell>64.56 60.01 68.29 57.56 38.89 77.45</cell><cell>70.28</cell><cell>45.23</cell><cell>78.32 62.82</cell></row><row><cell>CDAN+E [26]</cell><cell>47.52 65.91</cell><cell>75.65</cell><cell>57.07 54.12 63.42 59.60 44.30 72.39</cell><cell>66.02</cell><cell>49.91</cell><cell>72.80 60.73</cell></row><row><cell>IWAN [49]</cell><cell>53.94 54.45</cell><cell>78.12</cell><cell>61.31 47.95 63.32 54.17 52.02 81.28</cell><cell>76.46</cell><cell>56.75</cell><cell>82.90 63.56</cell></row><row><cell>SAN [4]</cell><cell>44.42 68.68</cell><cell>74.60</cell><cell>67.49 64.99 77.80 59.78 44.72 80.07</cell><cell>72.18</cell><cell>50.21</cell><cell>78.66 65.30</cell></row><row><cell>PADA [5]</cell><cell>51.95 67.00</cell><cell>78.74</cell><cell>52.16 53.78 59.03 52.61 43.22 78.79</cell><cell>73.73</cell><cell>56.60</cell><cell>77.09 62.06</cell></row><row><cell>SSPDA [3]</cell><cell>52.02 63.64</cell><cell>77.95</cell><cell>65.66 59.31 73.48 70.49 51.54 84.89</cell><cell>76.25</cell><cell>60.74</cell><cell>80.86 68.07</cell></row><row><cell>MWPDA [17]</cell><cell>55.39 77.53</cell><cell>81.27</cell><cell>57.08 61.03 62.33 68.74 56.42 86.67</cell><cell>76.70</cell><cell>57.67</cell><cell>80.06 68.41</cell></row><row><cell>ETN [6]</cell><cell>59.24 77.03</cell><cell>79.54</cell><cell>62.92 65.73 75.01 68.29 55.37 84.37</cell><cell>75.72</cell><cell>57.66</cell><cell>84.54 70.45</cell></row><row><cell>DRCN [21]</cell><cell>54.00 76.40</cell><cell>83.00</cell><cell>62.10 64.50 71.00 70.80 49.80 80.50</cell><cell>77.50</cell><cell>59.10</cell><cell>79.90 69.00</cell></row><row><cell>SAFN [46]</cell><cell>58.93 76.25</cell><cell>81.42</cell><cell>70.43 72.97 77.78 72.36 55.34 80.40</cell><cell>75.81</cell><cell>60.42</cell><cell>79.92 71.83</cell></row><row><cell>E-DANN</cell><cell>54.05 74.12</cell><cell>84.06</cell><cell>67.06 64.95 75.15 71.29 53.09 83.42</cell><cell>76.00</cell><cell>58.17</cell><cell>81.53 70.24</cell></row><row><cell>Ours (w/ BAA)</cell><cell cols="3">56.20 79.55 86.21 70.86 69.94 81.06 72.51 57.91 86.47</cell><cell>77.10</cell><cell>59.34</cell><cell>83.64 73.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>83.44±1.12 75.59±1.09 83.92±0.95 96.27±0.85 84.97±0.86 98.09±0.74 87.05 69.69±0.78 71.29±0.74 70.49 ADDA [40] 83.41±0.17 75.67±0.17 83.62±0.14 95.38±0.23 84.25±0.13 99.85±0.12 87.03 71.82±0.45 69.32±0.41 70.57 CDAN+E [26] 77.07±0.90 80.51±1.20 93.58±0.07 98.98±0.00 91.65±0.00 98.09±0.00 89.98 72.45±0.07 72.02±0.13 72.24 IWAN [49] 90.45±0.36 89.15±0.37 95.62±0.29 99.32±0.32 94.26±0.25 99.36±0.24 94.69 78.06±0.40 73.33±0.46 75.70 SAN [4] 94.27±0.28 93.90±0.45 94.15±0.36 99.32±0.52 88.73±0.44 99.36±0.12 94.96 77.75±0.36 75.26±0.42 76.51 PADA [5] 82.17±0.37 86.54±0.31 92.69±0.29 99.32±0.45 95.41±0.33 100.0±0.00 92.69 75.03±0.36 70.48±0.03±0.22 94.52±0.20 96.21±0.27 100.0±0.00 94.64±0.24 100.0±0.00 96.73 83.23±0.24 74.93±0.44 79.08 E-DANN 92.36±0.00 93.22±0.00 94.61±0.05 100.0±0.00 94.71±0.05 98.73±0.00 95.60 78.31±0.81 77.69±0.25 78.00 Ours (w/ BAA) 93.63±0.00 93.90±0.00 94.89±0.09 100.0±0.00 94.78±0.00 100.0±0.00 96.20 82.97±0.49 79.34±0.08 81.16 Ours (BA 3 US) 99.36±0.00 98.98±0.28 94.82±0.05 100.0±0.00 94.99±0.08 98.73±0.00 97.81 84.00±0.15 83.35±0.28 83.68</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44 72.76</cell></row><row><cell>SSPDA [3]</cell><cell>90.87</cell><cell>91.52</cell><cell>90.61</cell><cell>92.88</cell><cell>94.36</cell><cell>98.94</cell><cell>93.20 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MWPDA [17] 95.12</cell><cell>96.61</cell><cell>95.02</cell><cell>100.0</cell><cell>95.51</cell><cell>100.0</cell><cell>97.05 -</cell><cell>-</cell><cell>-</cell></row><row><cell>DRCN [21]</cell><cell>86.00</cell><cell>88.05</cell><cell>95.60</cell><cell>100.0</cell><cell>95.80</cell><cell>100.0</cell><cell>94.30 75.30</cell><cell>78.90</cell><cell>77.10</cell></row><row><cell>ETN [6]</cell><cell>95.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>43±0.48 60.34±0.84 72.96±0.56 97.97±0.63 79.12±0.54 99.36±0.36 81.03 IWAN [49] 90.95±0.33 82.90±0.31 89.57±0.24 79.75±0.26 93.36±0.22 88.53±0.16 87.51 SAN [4] 90.70±0.20 83.39±0.36 87.16±0.23 99.32±0.45 91.85±0.35 100.0±0.00 92.07 PADA [5] 81.73±0.34 86.05±0.36 93.00±0.24 100.0±0.00 95.26±0.27 100.0±0.00 92.54 ETN [6] 89.43±0.17 85.66±0.16 95.93±0.23 100.0±0.00 92.28±0.20 100.0±0.00 93.88 Ours (BA 3 US) 95.54±0.00 89.83±0.00 94.92±0.05 99.32±0.00 95.41±0.00 100.0±0.00 95.84</figDesc><table><row><cell></cell><cell>Avg.</cell></row><row><cell>VGG-16 [36]</cell><cell>76.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Sensitivity of parameter ξ. Office-Home 75.32 75.88 76.28 76.10 76.10 75.81 75.98 Office31 97.68 97.71 97.65 97.67 97.64 97.84 97.81</figDesc><table><row><cell>Avg. (%)</cell><cell>0.0 [7] 0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Sensitivity of parameter β. Office-Home 73.40 73.58 75.09 75.98 75.69 73.25 Office31 96.20 96.13 96.50 96.63 97.81 97.83</figDesc><table><row><cell>Avg. (%)</cell><cell>0.0</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>5.0</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo code of our method termed BA 3 US. Labeled source domain Ds, unlabeled target domain Dt; Parameters: Total training iterations N , updating interval Nu, batch size Bs = 36, ρ0 = 1/4, ξ = 1, α = 0.1, β ∈ {1, 5}, λ; Initialize the model parameters θ f , θg, θ d ; Initialize the class-level weight vector m, mi = 1/C; for i = 1 to N do Obtain Bs samples from Ds and Dt, respectively; Obtain ρBs random samples from Ds; Update θ f , θg, θ d by optimizing Eq. (7) and gradient reversaral layer; if i % Nu == 0 then update the class-level weight vector m; % note that this step is ignored in closed-set domain adaptation calcuate Lent in Eq. (3) for model selection; ρ ← ρ0 (1 -Nu/N ); end end Output: Target outputs corresponding to the minimal value of Lent.</figDesc><table /><note>Input:</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>J. Feng was partially supported by NUS ECRA FY17 P08, AISG-100E2019-035, and MOE Tier 2 MOE2017-T2-2-151. The authors also thank Quanhong Fu for her help to improve the technical writing aspect of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tackling partial domain adaptation with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05199</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complement objective training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR (2019) 2, 8, 9</title>
		<meeting>ICLR (2019) 2, 8, 9</meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<title level="m">Domain adaptation for visual applications: A comprehensive survey</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-weight partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A review of single-source unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<publisher>In press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual correction network for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In press, 2020) 4, 9</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregating randomized clustering-promoting invariant projections for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervised centroid shift: A simple and efficient approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Virtual mixup training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04215</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Twins: Two weighted inconsistency-reduced networks for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07405</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with imbalanced crossdomain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">Harry</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hubert Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Frank Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domainconstraint transfer coding for imbalanced unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by augmented distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transfer adaptation learning: A decade survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04687</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
