<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polarity Loss for Zero-shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Polarity Loss for Zero-shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Zero-shot object detection</term>
					<term>Zero-shot learning</term>
					<term>Deep neural networks</term>
					<term>Object detection</term>
					<term>Loss function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional object detection models require large amounts of training data. In comparison, humans can recognize previously unseen objects by merely knowing their semantic description. To mimic similar behaviour, zero-shot object detection aims to recognize and localize 'unseen object instances by using only their semantic information. The model is first trained to learn the relationships between visual and semantic domains for seen objects, later transferring the acquired knowledge to totally unseen objects. This setting gives rise to the need for correct alignment between visual and semantic concepts, so that the unseen objects can be identified using only their semantic attributes. In this paper, we propose a novel loss function called 'Polarity loss', that promotes correct visual-semantic alignment for an improved zero-shot object detection. On one hand, it refines the noisy semantic embeddings via metric learning on a 'Semantic vocabulary' of related concepts to establish a better synergy between visual and semantic domains. On the other hand, it explicitly maximizes the gap between positive and negative predictions to achieve better discrimination between seen, unseen and background objects. Our approach is inspired by embodiment theories in cognitive science, that claim human semantic understanding to be grounded in past experiences (seen objects), related linguistic concepts (word vocabulary) and visual perception (seen/unseen object images). We conduct extensive evaluations on MS-COCO and Pascal VOC datasets, showing significant improvements over state of the art. Our code and evaluation protocols available at: https://github.com/salman-h-khan/PL-ZSD Release</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E xisting deep learning models generally perform well in a fully supervised setting with large amounts of annotated data available for training. Since large-scale supervision is expensive and cumbersome to obtain in many real-world scenarios, the investigation of learning under reduced supervision is an important research problem. In this pursuit, human learning provides a remarkable motivation since humans can learn with very limited supervision. Inspired by human learning, zero-shot learning (ZSL) aims to reason about objects that have never been seen before using only their semantics. A successful ZSL system can help pave the way for life-long learning machines that intelligently discover new objects and incrementally enhance their knowledge.</p><p>Traditional ZSL literature only focuses on 'recognizing' unseen objects. Since real-world objects only appear as a part of a complete scene, the newly introduced zero-shot object detection (ZSD) task <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> considers a more practical setting where the goal is to simultaneously 'locate and recognize' unseen objects. This task offers new challenges for the existing object detection frameworks, the most important being the accurate alignment between visual and semantic concepts. If a sound alignment between the two heterogeneous domains is achieved, the unseen objects can be detected at inference using only their semantic representations. Further, a correct alignment can help the detector in differentiating between the background and the previously In this work, we propose a single-stage object detection pipeline underpinned by a novel objective function named Polarity loss. Our approach focuses on learning the complex interplay between visual and semantic domains such that the unseen objects (alongside the seen ones) can be accurately detected and localized. To this end, the proposed loss simultaneously achieves two key objectives during end-toend training process. First, it learns a new vocabulary metric that improves the semantic embeddings to better align them with the visual concepts. Such a dynamic adaptation of semantic representations is important since the original unsupervised semantic embeddings (e.g., word2vec) are noisy and thereby complicate the visual-semantic alignment. Secondly, it directly maximizes the margin between positive and negative detections to encourage the model to make confident predictions. This constraint not only improves visual-semantic alignment (through maximally separating class predictions) but also helps distinguish between background and unseen classes.</p><p>Our ZSD approach is distinct from existing state-of-theart <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> due to its direct emphasis on achieving better visual-semantic alignment. For example, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> use fixed semantics derived from unsupervised learning methods that can induce noise in the embedding space. In order to overcome noise in the semantic space, <ref type="bibr" target="#b4">[5]</ref> resorts to using detailed semantic descriptions of object classes instead of single vectors corresponding to class names. Further, the lack of explicit margins between positive and negative predictions leads to confusion between background and unseen classes <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In summary, our main contributions are: object-background imbalance and achieve maximal separation between positive and negative predictions. • Using an external vocabulary of words, our approach learns to associate semantic concepts with both seen and unseen objects. This helps to resolve confusion between unseen classes and background, and to appropriately reshape the noisy word embeddings. • A new seen-unseen split on the MS-COCO dataset that respects practical considerations such as diversity and rarity among unseen classes. • Extensive experiments on the old and new splits for MS-COCO and Pascal VOC which give absolute gains of 9.3 and 7.6 in mAP over <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively. A preliminary version of this work appeared in <ref type="bibr" target="#b5">[6]</ref>, where the contribution is limited to a single formulation of the polarity loss. Moreover, the experiments only consider word2vec as word embedding. In the current paper, we extended our work as follows: <ref type="bibr" target="#b0">(1)</ref> An alternate formulation of Polarity loss (Sec. 3.2). (2) Motivation of the proposed new split based on MSCOCO dataset for zero-shot detection (Sec. 5). (3) A detailed discussion on related works in the literature (Sec. 2). (4) More experiments and ablation studies e.g., with multiple semantic embeddings including Word2vec, GloVe and FastText, a validation study on hyperparameters and studying the impact of changing overlap threshold (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot learning (ZSL):</head><p>The earliest efforts were based on manually annotated attributes as a mid-level semantic representation <ref type="bibr" target="#b6">[7]</ref>. This approach resulted in a decent performance on fine-grained recognition tasks but to eliminate strong attribute supervision; researchers start exploring unsupervised word-vector based techniques <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Independent of the source of semantic information, a typical ZSL method needs to map both visual and semantic features to a common space to properly align information available in both domains. This can be achieved in three ways: (a) transform the image feature to semantic feature space <ref type="bibr" target="#b9">[10]</ref>, (b) map the semantic feature to image feature space <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> or, (c) map both image or semantic features to a common intermediate latent space <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To apply ZSL in practice, a few notable problem settings are: (1) transductive ZSL <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>: making use of unlabeled unseen data during training, (2) generalized ZSL <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>: classifying seen and unseen classes together, (3) domain adaptation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>: learning a projection function to adapt unseen target to seen source domain, (4) class-attribute association <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>: relating unsupervised semantics to human recognizable attributes. In this paper, our focus is not only recognition but also simultaneous localization of unseen objects which is a significantly complex and challenging problem.</p><p>Object detection: End-to-end trainable deep learning models have set new performance records on object detection benchmarks. The most popular deep architectures can be categorized into double stage networks (e.g., Faster-RCNN <ref type="bibr" target="#b21">[22]</ref>, RFCN <ref type="bibr" target="#b22">[23]</ref>) and single stage networks (e.g., SSD <ref type="bibr" target="#b23">[24]</ref>, YOLO <ref type="bibr" target="#b24">[25]</ref>). Generally, double-stage detectors achieve high accuracy, while single-stage detectors work   <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> instead of region proposal network (RPN), focal loss <ref type="bibr" target="#b27">[28]</ref> instead of traditional crossentropy loss, non-rectangular region selection <ref type="bibr" target="#b28">[29]</ref> instead of rectangular bounding boxes, designing backbone architectures <ref type="bibr" target="#b29">[30]</ref> instead of ImageNet pre-trained networks (e.g., VGG <ref type="bibr" target="#b30">[31]</ref>/ResNet <ref type="bibr" target="#b31">[32]</ref>). In this paper, we attempt to extend object detection to the next level: detecting unseen objects which are not observed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot object detection (ZSD):</head><p>The ZSL literature is predominated by classification approaches that focus on single <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> or multi-label <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> recognition. The extension of conventional ZSL approaches to zero-shot object localization/detection is relatively less investigated. Among previous attempts, Li et al. <ref type="bibr" target="#b36">[37]</ref> learned to segment attribute locations which can locate unseen classes. <ref type="bibr" target="#b37">[38]</ref> used a shared shape space to segment novel objects that look like seen objects. These approaches are useful for classification but not extendable to ZSD. <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> proposed novel object localization based on natural language description. Few other methods located unseen objects with weak image-level labels <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>. However, none of them perform ZSD. Very recently, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> investigated the ZSD problem. These methods can detect unseen objects using box annotations of seen objects. Among them, <ref type="bibr" target="#b1">[2]</ref> proposed a feature based approach where object proposals are generated by edge-box <ref type="bibr" target="#b41">[42]</ref>, and <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> modified the object detection frameworks <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref> to adapt ZSD settings. In another work, <ref type="bibr" target="#b4">[5]</ref> use textual description of seen/unseen classes to obtain semantic representations that are used for zero-shot object detection. Here, we propose a new loss formulation that can greatly benefit single stage zero-shot object detectors.</p><p>Generalized Zero-shot object detection (GZSD): This setting aims to detect both seen and unseen objects during inference. The key difference from generalized zero-shot recognition is that multiple objects can co-exist in a single image, thereby posing a challenge for the detector. In contrast, in the recognition setting either a seen or an unseen label is assigned since a single category is assumed per image. Only a handful of previous works in the literature reported GZSD results <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b42">[43]</ref> reported GZSD results while addressing transductive zero-shot detection and unseen object captioning problems, respectively. In the current work, we compare GZSD results with <ref type="bibr" target="#b1">[2]</ref> who evaluate on GZSD problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">POLARITY LOSS</head><p>We first introduce the proposed Polarity Loss that builds upon Focal loss <ref type="bibr" target="#b27">[28]</ref> for generic object detection. Focal loss only promotes correct prediction, whereas a sound ZSL system should also learn to minimize projections on representative vectors for negative classes. Our proposed loss jointly maximizes projection on correct classes and minimizes the alignment with incorrect ones (Sec. 3.2). Furthermore, it allows reshaping the noisy class semantics using a metric learning approach (Sec. 3.3). This approach effectively allows distinction between background vs. unseen classes and promotes better overall alignment between visual and semantic concepts. Below in Sec. 3.1, we provide a brief background followed by a description of proposed loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Balanced Cross-Entroy vs. Focal loss</head><p>Consider a binary classification task where y ∈ {0, 1} denotes the ground-truth class and p ∈ [0, 1] is the prediction probability for the positive class (i.e., y = 1). The standard binary cross-entropy (CE) formulation gives:</p><formula xml:id="formula_0">CE(p, y) = −α t log p t , p t = p, if y = 1, 1 − p, otherwise.<label>(1)</label></formula><p>where, α is a loss hyper-parameter representing inverse class frequency and the definition of α t is analogous to p t . In the object detection case, the object vs. background ratio is significantly high (e.g., 10 −3 ). Using a weight factor α is a traditional way to address this strong imbalance. However, being independent of the model's prediction, this approach treats both well-classified (easy) and poorly-classified (hard) cases equally. It favors easily classified examples to dominate the gradient and fails to differentiate between easy and hard examples. To address this problem, Lin et al. <ref type="bibr" target="#b27">[28]</ref> proposed 'Focal loss' (FL):</p><formula xml:id="formula_1">FL(p, y) = −α t (1 − p t ) γ log p t ,<label>(2)</label></formula><p>where, γ ∈ [0, 5] is a loss hyper-parameter that dictates the slope of cross entropy loss (a large value denotes higher slope). This alignment requires the training procedure to (1) push visual features close to their ground-truth embedding vector and (2) push them away from all negative class vectors. FL can only perform (1) but cannot enforce (2) during the training of ZSD. Therefore, although FL is well-suited for traditional seen object detection, but not for the ZSD scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Max-margin Formulation</head><p>To address the above-mentioned shortcomings, we propose a margin maximizing loss formulation that is particularly suitable for ZSD. This formulation is generalizable and can work with loss functions other than Eqs. 1 and 2. However, for the sake of comparison with the best model, we base our analysis on the state of the art FL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Objective Function</head><p>Multi-class Loss: Consider that a given training set {x, y} i contains N examples belonging to C object classes plus an additional background class. For the multi-label prediction case, the problem is treated as a sum of individual binary cross-entropy losses where each output neuron decides whether a sample belongs to a particular object class or not. Assume, y = {y i ∈ {0, 1}} ∈ R C and p = {p i ∈ [0, 1]} ∈ R C denotes the ground-truth label and prediction vectors respectively, and the background class is denoted by y = 0 ∈ R C . Then, the FL for a single box proposal is:</p><formula xml:id="formula_2">L = i −α i t (1 − p i t ) γ log p i t .<label>(3)</label></formula><p>Polarity Loss: Suppose, for a given bounding box feature containing an th object class, p represents the prediction value for the ground-truth object class, i.e., y = 1, see <ref type="table" target="#tab_2">Table 1</ref>. Note that p = 0 for the background class (where y i = 0; ∀i). Ideally, we would like to maximize the predictions for ground-truth classes and simultaneously minimize prediction scores for all other classes. We propose to explicitly maximize the margin between predictions for positive and negative classes to improve the visual-semantic alignment for ZSD (see <ref type="figure" target="#fig_2">Fig. 3</ref>). This leads to a new loss function that we term as 'Polarity Loss' (PL):</p><formula xml:id="formula_3">L P L = i f p (p i − p )FL(p i , y i ),<label>(4)</label></formula><p>where, f p is a monotonic penalty function. For any prediction, p i where =i, the difference p i −p represents the disparity between the true class prediction and the prediction for the negative class. The loss function enforces a large negative margin to push predictions p i and p further apart. Thus, for an object anchor case, the above objective enforces p &gt;p i , while for background case 0&gt;p i i.e., all p i 's are pushed towards zero (since p =0).</p><p>Our Penalty Function: f p should necessarily be a 'monotonically increasing' function. It offers a small penalty if the gap p i −p is low and a large penalty if the gap is high. This constraint enforces that p i &lt; p . In this paper, we implement f p with a β parameterized sigmoid function:</p><formula xml:id="formula_4">f p (p i − p ) = 1 1 + exp(−β(p i − p ))<label>(5)</label></formula><p>(a) Object case: p = .8  For the case when p i =p , the FL part guides the loss because f p becomes a constant. We choose a sigmoid form for f p because the difference (p i −p ) ∈ [−1, 1] and f p can be bounded by [0, 1], similar to α t or the (1−p t ) factor of FL. Note that, it is not compulsory to stick with this particular choice of f p . We also test a softplus-based function for f p in the next subsection.</p><formula xml:id="formula_5">p i .1 .8 .9 y i 0 1 0 p i t .9 .8 .1 p i − p -.7 0 .1 loss L L H (b) Background case: p = 0 p i .1 .8 .9 y i 0 0 0 p i t .9 .2 .1 p i − p .1 .8 .9 loss L H H</formula><p>Final Objective: The final form of the loss is:</p><formula xml:id="formula_6">L P L (p, y) = i −α i t (1 − p i t ) γ log p i t 1 + exp(−β(p i − p ))</formula><p>, where,</p><formula xml:id="formula_7">p i t = p i , if y i = 1 1 − p i , otherwise p = p i y i = 1 ,<label>(6)</label></formula><p>where, · denotes the Iverson bracket. Later in Sec. 3.3, we describe our vocabulary-based metric learning approach to obtain p in the above Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Analysis and Insights A Toy Example:</head><p>We explain the proposed loss with a toy example in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>. When an anchor box belongs to an 'object' and p i t ≥ .5 (high) then p i −p ≤ 0 (low). From <ref type="figure" target="#fig_1">Fig. 2</ref>, both a multi-class loss and the penalty function find low loss which eventually calculates a low loss. Similarly, when p i t &lt; .5 (low), p i −p &gt; 0 (high), which evaluates to a high loss. When an anchor belongs to 'background', p i −p ≥ 0 and a high p i results in a high value for both multi-class loss and the penalty function and vice versa. In this way, the penalty function always supports multi-class loss based on the disparity between the current prediction and groundtruth class's prediction.</p><p>Polarity Loss Properties: The PL has two intriguing properties. (a) Word-vectors alignment: For ZSL, generally visual features are projected onto the semantic word vectors. A high projection score indicates proper alignment with a FL pushes visual features close to their ground-truth. Thus, intraclass distances are minimized but inter-class distances are not considered. This works well for seen class separation, but is not optimal for unseen classes because inter-class distances must be increased to ensure unseen class separability. Our PL ensures this requisite.</p><p>word-vector. The overall goal of training is to achieve good alignment between the visual feature and its corresponding word-vector and an inverse alignment with all other wordvectors. In our proposed loss, FL(·) and f p perform the direct and inverse alignment respectively. <ref type="figure" target="#fig_2">Fig. 3</ref> shows visual features before and after this alignment. (b) Class imbalance: The penalty function f p follows a trend similar to α t and (1−p t ) γ . It means that f p assigns a low penalty to well-classified/easy examples and a high penalty to poorlyperformed/hard cases. It greatly helps in tackling class imbalance for single stage detectors where negative boxes heavily outnumber positive detections.</p><p>Alternative formulation of polarity loss: We have used a sigmoid based penalty function to implement f p (p i − p ) in our proposed polarity loss. Now, we present an alternative implementation of the penalty function based on the softplus function. In <ref type="figure" target="#fig_3">Fig. 4</ref>, we illustrate the shapes of both sigmoid and softplus based penalty functions. Both the functions increase penalty when p i − p moves from −1 to 1. However, the softplus is relatively smoother than sigmoid. Also, softplus has a flexibility to assign a penalty &gt; 1 for any poorly classified examples whereas sigmoid can penalize at most 1. The formulation for the softplus based penalty function is as follows,</p><formula xml:id="formula_8">f p (p i − p ) = log 1 + e β (p i −p )<label>(7)</label></formula><p>where, β is the loss hyper-parameter. The final polarity loss with the softplus based penalty function is the following:</p><formula xml:id="formula_9">L P L (p, y) = i −α i t (1 − p i t ) γ log 1 + e β (p i −p ) log p i t , p i t = p i , if y i = 1 1 − p i , otherwise p = p i y i = 1 ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vocabulary Metric Learning</head><p>Apart from proper visual-semantic alignment and class imbalance, a significant challenge for ZSD is the inherent noise in the semantic space. In this paper, we propose a new 'vocabulary metric learning' approach to improve the quality of word vectors for ZSL tasks. For brevity of expression, we restrict our discussion to the case of classification probability prediction or bounding box regression for a single anchor. For the classification case, metric learning is considered as a part of Polarity loss in Eq. 6. Suppose, the visual feature of that anchor, a is φ(a) = f , where φ represents the detector network. The total number of seen classes is S and a matrix W s ∈ R S×d denotes all the d-dimensional word vectors of S seen classes arranged row-wise. The detector network φ is augmented with FC layers towards the head to transform the visual feature f to have the same dimension as the word vectors, i.e., f ∈ R d . In <ref type="figure" target="#fig_4">Fig. 5</ref>, we describe several ways to learn the alignment function between visual features and semantic information. We elaborate these further below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning with Word-vectors</head><p>For the traditional detection case, shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, the visual features f are transformed with a learnable FC layer W d ∈ R S×d , followed by a sigmoid/softmax activation (σ) to calculate S prediction probabilities, p d = σ(W d f ). This approach works well for traditional object detection, but it is not suitable for the zero-shot setting as the transformation W d cannot work with unseen object classes. A simple extension of the traditional detection framework to the zero-shot setting is possible by replacing trainable weights of the FC layers, W d , by the non-trainable seen word vectors W s ( <ref type="figure" target="#fig_4">Fig. 5(b)</ref>). Keeping this layer frozen, we allow projection of the visual feature f to the word embedding space to calculate prediction scores p s :</p><formula xml:id="formula_10">p s = σ(W s f )<label>(9)</label></formula><p>This projection aligns visual features with the word vector of the corresponding true class. The intuition is that rather than directly learning a prediction score from visual features (in <ref type="figure" target="#fig_4">Fig 5(a)</ref>), it is better to learn a correspondence between the visual features with word vectors before the prediction. Challenges with Basic Approach: Although the configuration described in <ref type="figure" target="#fig_4">Fig. 5(b)</ref> delivers a basic solution to zero-shot detection, it suffers from several limitations.</p><p>(1) Fixed Representations: With a fixed embedding W s , the network cannot update the semantic representations and has limited flexibility to properly align visual and semantic domains. (2) Limited word embeddings: The word embedding space is usually learned using billions of words from unannotated texts which results in noisy word embeddings. Understanding the semantic space with only S word vectors is therefore unstable and insufficient to model visual-semantic  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>relationships. (3)</head><p>Unseen-background confusion: In ZSD, one common problem is that the model confuses unseen objects with background since it has not seen any visual instances of unseen classes <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Learning with vocabulary metric</head><p>To address the above gaps, we propose to learn a more expressive and flexible semantic domain representation. Such a representation can lead to a better alignment between visual features and word vectors. Precisely, we propose a vocabulary metric method summarized in <ref type="figure" target="#fig_4">Fig. 5</ref>(c) that takes advantage of the word vectors of a pre-defined vocabulary, D ∈ R v×d (where v is the number of words in the vocabulary). By relating the given class semantics with dictionary atoms, the proposed approach provides an inherent mechanism to update class-semantics optimally for ZSD. Now, we calculate the prediction score as follows:</p><formula xml:id="formula_11">p v = σ(δ(W s M D)f )<label>(10)</label></formula><p>Here, M ∈ R d×v represents the learnable parameters which connect seen word vectors with the vocabulary and δ(.) is a tanh activation function. M can be interpreted as learned  <ref type="figure">Fig. 7</ref>: Network architecture for ZSD. The green colored layer implements Eq. 9 (Our-PL-word) or 10 (Our-PL-vocab).</p><p>attention over the dictionary. With such an attention, the network can understand the semantic space better and learn a rich representation because it considers more linguistic examples (vocabulary words) inside the semantic space. Simultaneously, it helps the network to update the word embedding space for better alignment with visual features. Further, it reduces unseen-background confusion since the network can relate visual features more accurately with a diverse set of linguistic concepts. We visualize word vectors before and after the update in <ref type="figure" target="#fig_5">Fig. 6</ref> (a) and (b) respectively.</p><p>Here, we emphasize that the previous attempts to use such an external vocabulary have their respective limitations. For example, <ref type="bibr" target="#b19">[20]</ref> considered a limited set of attributes while <ref type="bibr" target="#b20">[21]</ref> used several disjoint training stages. These approaches are therefore not end-to-end trainable. Further, they only investigate the recognition problem.</p><p>Regression branch with semantics: Eq. 10 allows our network to predict seen class probabilities at the classification branch directly using semantic information from the vocabulary metric. Similarly, we also apply such semantics in the regression branch with some additional trainable FC layers. In our experiments, we show that adding semantics in this manner leads to further improvement in ZSD. It shows that the predicted regression box can benefit from the semantic information that improves the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ARCHITECTURE DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-stage Detector:</head><p>Our proposed ZSD framework is specially designed to work with single-stage detectors. The primary motivation is the direct connection between anchor classification and localization that ensures a strong feedback for both tasks. For this study, we choose a recent unified single architecture, RetinaNet <ref type="bibr" target="#b27">[28]</ref> to implement our proposed method. RetinaNet is the best detector known for its high speed (on par with single-stage detectors) and high accuracy (outperforming two-stage detectors). In <ref type="figure">Fig. 7</ref>, we illustrate the overall architecture of the model. In addition to a novel loss formulation, we also perform modifications to the RetinaNet architecture to link visual features (from ResNet50 <ref type="bibr" target="#b31">[32]</ref>) with semantic information. To adapt this network to ZSL setting, we perform simple modifications in both classification and box regression subnets to consider word-vectors (with vocabulary metric) during training (see <ref type="figure">Fig. 7</ref>).</p><p>RetinaNet has one backbone network called Feature Pyramid Network (FPN) <ref type="bibr" target="#b25">[26]</ref> and two task-specific subnetwork branches for classification and box regression. FPN extracts rich, multi-scale features for different anchor boxes from an image to detect objects at different scales. For each pyramid level, we use anchors at {1:2,1:1,2:1} aspect ratios with sizes {2 0 , 2 1/3 , 2 2/3 } totaling to A=9 anchors per level, covering an area of 32 2 to 512 2 pixels. The classification and box-regression subnetworks attempt to predict the onehot target ground-truth vector of size S and box parameters of size four respectively. We consider an anchor box as an object if it gets an intersection-over-union (IoU) ratio &gt; 0.5 with a ground-truth bounding box.</p><p>Modifications to RetinaNet: Suppose, a feature map at a given pyramid level has C channels. For the classification subnet, we first apply four conv layers with C filters of size 3 × 3, followed by ReLU, similar to RetinaNet. Afterward, we apply a 3 × 3 conv layer with d × A filters to convert visual features to the dimension of word vectors, d. Next, we apply a custom layer which projects image features onto the word vectors. We also apply a sigmoid activation function to the output of the projection. This custom layer may have fixed parameters like <ref type="figure" target="#fig_2">Fig. 3</ref>(b) or trainable parameters like 3(c) of the main paper with vocabulary metric. These operations are formulated as: p s = σ(W s f ) or p v = σ(δ(W s M D)f ) depending on the implementation of the custom layer. Similarly, for the box-regression branch, we attach another 3 × 3 convolution layer with C filters and ReLU non-linearity, followed by 3 × 3 convolution with d filters and the custom layer to get the projection response. Finally, another convolution with 4A to predict a relative offset between the anchor and ground-truth box. In this way, the box-prediction branch gets semantic information of word-vectors to predict offsets for regression. Note that, similar to <ref type="bibr" target="#b27">[28]</ref>, the classification and regression branches do not share any parameters, however, they have a similar structure.</p><p>Training: We train the classification subnet branch with our proposed loss defined in Eq. 6. Similar to <ref type="bibr" target="#b27">[28]</ref>, to address the imbalance between hard and easy examples, we normalize the total classification loss (calculated from ∼100k anchors) by the total number of object/positive anchor boxes rather than the total number of anchors. We use standard smooth L 1 loss for the box-regression subnet branch. The total loss is the sum of the loss of both branches.  Inference: For seen object detection, a simple forward pass predicts both confidence scores and bounding boxes in the classification and box-regression subnetworks respectively. Note that we only consider a fixed number (e.g., 100) of boxes from RPN having confidence greater than 0.05 for inference. Moreover, we apply Non-Maximum Suppression (NMS) with a threshold of 0.5 to obtain final detections. We select the final detections that satisfy a seen score-threshold (t s ). To detect unseen objects, we use the following equation, followed by an unseen score-thresholding with a relatively lower value (t u &lt; t s ) 1 :</p><formula xml:id="formula_12">p u = W u W T s σ(δ(W s M D)f )<label>(11)</label></formula><p>where, W u ∈ R U×d contains unseen class word vectors. For generalized zero-shot object detection (GZSD), we simply consider all detected seen and unseen objects together. In our experiments, we report performances for traditional seen, zero-shot unseen detection and GZSD. One can notice that our architecture predicts a bounding box for every anchor which is independent of seen classes. It enables the network to predict bounding boxes dedicated to unseen objects. Previous attempts like <ref type="bibr" target="#b0">[1]</ref> detect seen objects first and then attempt to classify those detections to unseen objects based on semantic similarity. By contrast, our model allows detection of unseen bounding boxes that are different to those seen. Reduced description of unseen: All seen semantics vectors are not necessary to describe an unseen objects <ref type="bibr" target="#b9">[10]</ref>. Thus, we only consider the top T predictions, p v ∈ R T from σ(δ(W s M D)f ) and the corresponding seen word vectors, W s ∈ R T×d to predict unseen scores. For the reduced case,</p><formula xml:id="formula_13">p u = W u W T s p v .</formula><p>In the experiments, we vary the number of the closest seen T from 5 to S and find that a relatively small value of T (e.g., 5) performs better than using all available T = S seen word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We evaluate our method with MS-COCO(2014) <ref type="bibr" target="#b45">[46]</ref> and Pascal VOC (2007/12) <ref type="bibr" target="#b46">[47]</ref>. With 1. Empirically, we found ts=0.3 and tu=0.1 generally work well. tu is kept smaller to counter classifier bias towards unseen classes due to the lack of visual examples during training. 80 object classes, MS-COCO includes 82,783 training and 40,504 validation images. For the ZSD task, only unseen class performance is of interest. As the test data labels are not known, the ZSD evaluation is done on a subset of validation data. MS-COCO(2014) has more validation images than any later versions which motivates us to use it. For Pascal VOC, we use the train set of 2007 and 2012 for training and use validation+test set of 2007 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Issues with existing MS-COCO split:</head><p>Recently, <ref type="bibr" target="#b1">[2]</ref> proposed a split of seen/unseen classes for MS-COCO (2014). It considers 73, 774 training images from 48 seen classes and 6608 test images from 17 unseen classes. The split criteria were the cluster embedding of class semantics and synset WordNet hierarchy <ref type="bibr" target="#b47">[48]</ref>. We identify two practical drawbacks of this split: (1) Because all 63 classes are not used as seen, this split does not take full advantage of training images/annotations, (2) Because of choosing unseen classes based on wordvector clustering it cannot guarantee the desired diverse nature of the unseen set. For example, this split does not choose any classes from 'outdoor' supercategory of MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed seen/unseen split on MS-COCO:</head><p>To address these issues, we propose a more realistic split of MS-COCO for ZSD. Following the practical consideration of unseen classes discussed in <ref type="bibr" target="#b0">[1]</ref> i.e. rarity and diverseness, we follow the following steps: (1) We sort classes of each superclass in ascending order based on the total number of instances in the training set. (2) For each superclass, we pick 20% rare classes as unseen which results in 15 unseen and 65 seen classes. Note that the superclass information is only used to create a diverse seen/unseen split, and never used during training. (3) Being zero-shot, we remove all the images from the training set where at least one unseen class appears to create a training set of 62,300 images. (4) For testing ZSD, we select 10,098 images from the validation set where at least one instance of an unseen class is present. The total number of unseen bounding boxes is 16,388. We use both seen and unseen annotation together for this set to perform GZSD. (5) We prepare another list of 38,096 images from the validation set where at least one occurrence of the seen instance is present to test traditional detection performance on seen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Seen / GZSD Unseen ZSD Seen Unseen HM Split in <ref type="bibr" target="#b1">[2]</ref>   In <ref type="figure" target="#fig_6">Fig. 8</ref>, we present all 80 MS-COCO classes in sorted order across each super-category based on the number of instance/bounding boxes inside the training set. Choosing 20% low-instance classes from each super-category ensures the rarity and diverseness for the chosen unseen classes. In this paper, we report results on both our and <ref type="bibr" target="#b1">[2]</ref> settings.</p><p>Pascal VOC Split: For Pascal VOC 2007/12 <ref type="bibr" target="#b46">[47]</ref>, we follow the settings of <ref type="bibr" target="#b2">[3]</ref>. We use 16 seen and 4 unseen classes from total 20 classes. We utilize 2072 and 3909 train images from Pascal VOC 2007 and 2012 respectively after ignoring images containing any instance of unseen classes. For testing, we use 1402 val+test images from Pascal VOC 2007 where any unseen class appears at least once.</p><p>Vocabulary: We choose vocabulary atoms from 5018 Flickr tags in NUS-WIDE <ref type="bibr" target="#b48">[49]</ref>. We only remove MS-COCO class names and tags that have no word vectors. This vocabulary covers a wide variety of objects, attributes, scene types, actions, and visual concepts.</p><p>Semantic embedding: For MS-COCO classes and vocabulary words, we use 2 normalized 300 dimensional unsupervised word2vec <ref type="bibr" target="#b7">[8]</ref>, GloVe <ref type="bibr" target="#b8">[9]</ref> and FastText <ref type="bibr" target="#b49">[50]</ref> vectors obtained from billions of words from unannotated texts like Wikipedia. For Pascal VOC <ref type="bibr" target="#b46">[47]</ref> classes, we use average 64 dimension binary per-instance attribute annotation of all training images from aPY dataset <ref type="bibr" target="#b50">[51]</ref>. Unless mentioned otherwise, we use word2vec in our experiments.</p><p>Evaluation metric: Being an object detection problem, we evaluate using mean average precision (mAP) at a particular IoU. Unless mentioned otherwise, we use IoU= 0.5. Notably, <ref type="bibr" target="#b2">[3]</ref> use the Recall measure for evaluations, however since recall based evaluation does not penalize a method for the wrongly predicted bounding boxes, we only recommend mAP based evaluation for ZSD. To evaluate GZSD, we report the harmonic mean (HM) of mAP and recall <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>Implementation details: We implement FPN with a basic ResNet50 <ref type="bibr" target="#b31">[32]</ref>. All images are rescaled to make their smallest side 800px. We train the FL-basic method using the original RetinaNet architecture with only training images of seen classes so that the pre-trained network does not get influenced by unseen instances. Then, to train Our-FL-word, we use the pre-trained weights to initialize the common layers of our framework. We ini-tialize all other uncommon layers with a uniform random distribution. Similarly, we train Our-PL-word and Our-FL-vocab upon the training of Our-FL-word. Finally, we train Our-PL-vocab using the pre-trained network of Our-FL-vocab. We train each network for 500k iterations keeping a single image in the minibatch. The only exception while training with our proposed loss is to train for 100k iterations instead of 500k. Each training time varies from 72 to 96 hours using a single Tesla P100 GPU. For optimization, we use Adam optimizer with learning rate 10 −5 , β 1 = 0.9 and β 2 = 0.999. We implement this framework with Keras library.</p><p>Validation strategy: α, γ and β are the hyper-parameters of the proposed polarity loss. Among them, α, γ are also present in the focal loss. Therefore, we choose α = 0.25, γ = 2.0 as recommended by the original RetinaNet paper <ref type="bibr" target="#b27">[28]</ref>. β is the only new hyper-parameter introduced in the polarity loss. We conduct a validation experiment on seen classes to perform the traditional detection task. In <ref type="table" target="#tab_8">Table 3</ref>, we tested β = {5, 10, 15, 20, 30} and picked β = 5 for our loss as it performed the best among all the considered values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods:</head><p>We rigorously evaluate our proposed ZSD method on both <ref type="bibr" target="#b1">[2]</ref> split (48/17) and our new (65/15) split of MS-COCO. We provide a brief description of all compared methods: (a) SB <ref type="bibr" target="#b1">[2]</ref>: This method extracts pretrained Inception-ResNet-v2 features from Edge-Box object proposals. It applies a standard max-margin loss to align visual features to semantic embeddings via linear projections. (b) DSES <ref type="bibr" target="#b1">[2]</ref>: In addition to SB, DSES augments extra bounding boxes other than MSCOCO objects. As <ref type="bibr" target="#b1">[2]</ref> reported recall performances, we also report recall results (in addition to mAP) to compare with this method. (c) Baseline: This method trains an exact RetinaNet model. Thus, it does not use any word vectors during training. To extend this approach to perform ZSD, we apply this formula to calculate unseen scores: p u =W u W T s p d where p d represents top T seen prediction scores for the reduced description of unseen. (d) Ours: This method is our final proposal using vocabulary and polarity loss <ref type="figure">(Fig. 7)</ref>.     Overall Results: <ref type="figure" target="#fig_7">Fig. 9</ref> presents overall performance on ZSD and GZSD tasks across different comparison methods with two different seen/unseen split of MS-COCO. In addition to mAP, we also report recall (RE) to compare with <ref type="bibr" target="#b1">[2]</ref>. With 48/17 settings, our method (and baseline) beats <ref type="bibr" target="#b1">[2]</ref> (SB and DSES) in both the ZSD and GZSD by a significantly large margin. Similarly, in 65/15 split, we outperform our baseline by a margin 3.92 mAP <ref type="bibr">(12.40 vs. 8.48)</ref> in ZSD task and 4.15 harmonic-mAP in GZSD task <ref type="bibr">(18.18 vs. 14.03</ref>). This improvement is the result of end-to-end learning, the inclusion of the vocabulary metric to update word vectors and the proposed loss in our method. We report results on GloVe and FastText word vectors in the <ref type="table" target="#tab_11">Table 4</ref> Hyper-parameter Sensitivity Analysis: We study the sensitivity of our model to loss hyper-parameters γ, α and β. First, we vary γ ∈ [0, 5] and α ∈ [.25, 1] keeping β=20. In <ref type="figure" target="#fig_9">Fig. 10</ref> (left), we report mAP using different parameter settings for ZSD and GZSD. Our model works best with α=.25 and γ=2.0 which are also the recommended values in FL. We also vary β from 1-30 to see its effect on ZSD in <ref type="figure" target="#fig_9">Fig. 10 (Right-a)</ref>. This parameter controls the steepness of the penalty function f p in Eq. 5. Notably β=20 provides correct steepness to estimate a penalty for incorrect predictions. Our loss can also work reasonably well with balanced CE (i.e., without FL when γ=0). We show this in <ref type="figure" target="#fig_9">Fig. 10(Rightb)</ref>. With a low α of 0.05, our method can achieve around 10% mAP. It shows that our penalty function can effectively balance object/background and easy/hard cases.</p><p>Ablation Studies: In <ref type="figure" target="#fig_10">Fig. 11</ref>(Left), we report results on different variants of our method. Our-FL-word: This method is based on the architecture in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>   Our-PL-vocab: The method uses our proposed framework in <ref type="figure">Fig. 7</ref> with vocabulary metric learning in the custom layer and is learned with polarity loss (without vocabulary metric learning). Our observations: (1) Our-FL-word works better than Baseline for ZSD and GZSD because the former uses word vectors during training whereas the later does not adopt semantics. By contrast, in GZSD-seen detection cases, Baseline outperforms Our-FL-word because the use of unsupervised semantics (word vectors) during training in Our-FL-word introduces noise in the network which degrades the seen mAP. (2) From Our-FL-word to Our-PL-word unseen mAP improves because of the proposed loss which increases inter-class and reduces intraclass differences. It brings better visual-semantic alignment than FL ( <ref type="figure" target="#fig_2">Fig. 3).</ref> (3) Our-PL-vocab further improves the ZSD performance. Here, the vocabulary metric helps the word vectors to update based on visual similarity and allows features to align better with semantics. Semantics in Box-regression Subnet: Our framework can be trained without semantics in the box-regression subnet. In <ref type="figure" target="#fig_10">Fig. 11 (Right)</ref>, we compare performance with and without using word vectors in the regression branch using FL and our loss. We observe that using word vectors in regression branch helps to improve the performance of ZSD.</p><p>Alternative formulation: In the first row of <ref type="table" target="#tab_12">Table 5</ref>, we report performance of Our-PL-vocab using this alternative polarity loss with β = 5 and word2vec as semantic information. With this alternative formulation, we achieve a performance quite close to that of sigmoid-based polarity loss.</p><p>Choice of Semantic Representation: In addition to word2vec as semantic word vectors reported in the main paper, we also experiment with GloVe <ref type="bibr" target="#b8">[9]</ref> and FastText <ref type="bibr" target="#b49">[50]</ref> as word vectors. We report those performances in <ref type="table" target="#tab_11">Table  4</ref>. We notice that Glove (glo) and FastText (ftx) achieve respectable performance, although they do not work as well     as word2vec. However, in all cases, Our-PL-vocab beats Our-FL-vocab on ZSD in both cases. Varying T and IoU: In Sec. 5 of the main paper, we discussed the reduced description of an unseen class based on closely related seen classes. We experiment with the behavior of our model by varying a different number of close seen classes. One can notice in <ref type="figure" target="#fig_1">Fig. 12(a)</ref>, a smaller number of close seen classes (e.g., 5) results in relatively better performance than using more seen classes (e.g., 10−65) during ZSD prediction. This behavior is related with the average number of classes per superclass (which is 7.2 for MS-COCO excluding 'person') because dissimilar classes from a different superclass may not contribute towards describing a particular unseen class. Thus, we use only five close seen classes to describe an unseen in all our experiments. In <ref type="figure" target="#fig_1">Fig. 12(b)</ref>, we report the impact of choosing a different IoU ratio (from 0.2 to 0.6) in ZSD. As expected, lower IoUs result in better performance than higher ones. As practiced in object detection literature, we use IoU= 0.5 for all other experiments in this paper.</p><p>Traditional detection: We report traditional detection performance of different versions of our framework in <ref type="figure" target="#fig_2">Fig. 13</ref>. As a general observation, it is clear that making detection explicitly dependent on semantic information hurts the detector's performance on 'seen' classes (traditional detection). This is consistent with the common belief that training directly on the desired output space in an end-to-end supervised setting achieves a better performance <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Consistently, we notice that FL-basic achieves the best performance because it is free from the noisy word vectors.</p><p>Our-FL-word performs relatively worse than FL-basic because of using noise word vectors as class semantics inside the network. Then, word vectors of vocabulary texts further reduce the performance in Our-FL-vocab. Our proposed loss (Our-PL-word and Our-PL-vocab cases) aligns visual features to noisy word vectors better than FL which is valuable for zero-shot learning but slightly degrades the seen performance. Similarly, we notice that while modifying the word embeddings, the vocabulary metric focuses more on proper visual-semantic alignment that is very helpful for ZSD but performs lower for the seen/traditional detection setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pascal VOC results:</head><p>To compare with YOLO-based ZSD Demirel et al. <ref type="bibr" target="#b2">[3]</ref>, we adopt their exact settings with Pascal VOC 2007 and 2012. Note that, their approach used attribute vectors as semantics from <ref type="bibr" target="#b50">[51]</ref>. As such attribute vectors are not available for our vocabulary list, we compare this approach with only using fixed attribute vectors inside our network. Our method beats Demirel et al. <ref type="bibr" target="#b2">[3]</ref> by a large margin (57.9 vs 63.5 on traditional detection, 54.5 vs 62.1 on unseen detection). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose an end-to-end trainable framework for ZSD based on a new loss formulation. Our proposed polarity loss promotes correct alignment between visual and semantic domains via max-margin constraints and by dynamically refining the noisy semantic representations. Further, it penalizes an example considering background vs. object imbalance, easy vs. hard cases and inter-class vs. intra-class relations. We qualitatively demonstrate that in the learned semantic embedding space, word vectors become well-distributed and visually similar classes reside close together. We propose a realistic seen-unseen split on the MS-COCO dataset to evaluate ZSD methods. In our experiments, we have outperformed several recent state-ofthe-art methods on both ZSD and GZSD tasks across the MS-COCO and Pascal VOC 2007 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Plots of multi-class loss (left) and penalty function (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>tSNE plot of visual features from 8 unseen classes projected onto semantic space using (a) FL &amp; (b) PL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of sigmoid (left) vs. softplus (right) based penalty functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Traditional basic approach with learnable W d , (b) Inserting word vectors as a fixed embedding Ws, (c) learnable word vectors with vocabulary metric δ(WsM D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>2D tSNE<ref type="bibr" target="#b44">[45]</ref> embedding of word2vec: (a) before (b) after modification based on vocabulary metric with our loss. Word-vectors are more evenly distributed in (b) than (a). Also, visually similar classes for example, apple/banana/orange/broccoli, cell phone/remote/laptop/mouse and handbag/backpack/umbrella are embedded more closely in (b) than (a). Super-category annotations are used for visualization only, not during our training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Instances of each class in the MS-COCO dataset (except class 'person'). Tall bars are clipped for better visualization. The class bars with black border are selected as unseen classes. We choose 20% rarest classes from each superclass as unseen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>(left) Overall performance on MS-COCO. Hyper-parameters are set on the validation set: β=5, IoU=0.5. mAP = mean average precision and RE = recall (@100). The top part shows results on [2] split and the lower part shows results on our proposed split. Ours achieves best performance in terms of mAP on unseen classes. (Right) Qualitative examples of ZSD (top row) and GZSD (bottom row). Pink and yellow box represent unseen and seen detections respectively. More qualitative results are presented in Fig. 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Demirel 26</head><label>26</label><figDesc>et al. [3] 57.9 54.5 68.0 72.0 74.0 48.0 41.0 61.0 48.0 25.0 48.0 73.0 75.0 71.0 73.0 33.0 59.0 57.0 55.0 82.0 55.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Parameter sensitivity analysis: (Left) Varying α and γ with a fixed β=20. (Right-a) Impact of varying β, (Right-b) varying α with γ=0 to see the behavior of our loss with only balanced CE. Note that the actual hyper-parameters choice is made on a validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Ablation studies with β = 20: (Left) Comparison of different variant of our approach, best method denoted with * . (Right) Impact of word-vectors in the regression branch. with focal loss. It uses static word vectors during training. But, it cannot update vectors based on visual features. Our-PL-word: Same architecture as of Our-FL-word but training is done with our proposed polarity loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>(a) Impact of selecting close seen (b) Impact of IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FFig. 13 :</head><label>13</label><figDesc>Traditional detection performance on 65 seen classes of MSCOCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 :</head><label>14</label><figDesc>Qualitative results of ZSD (two top rows) and GZSD (two bottom rows). Pink and yellow boxes represent unseen and seen detections respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>A toy example. Intermediate computations for Polarity Loss are shown. Low (L) values are shown in green while High (H) values are shown in red. A mismatch between (p i and y</figDesc><table /><note>i ) + a close match between (y i and y ) results in a high loss.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>/18.65 40.46/43.69 2.88/17.89 5.38/25.38</figDesc><table><row><cell>(↓)</cell><cell></cell><cell cols="4">(mAP/RE) (mAP/RE) (mAP/RE) (mAP/RE)</cell></row><row><cell>SB [2]</cell><cell>48/17</cell><cell>0.70/24.39</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSES [2]</cell><cell>48/17</cell><cell>0.54/27.19</cell><cell>-/15.02</cell><cell>-/15.32</cell><cell>-/15.17</cell></row><row><cell>ZSD-Textual [5]</cell><cell>48/17</cell><cell>-/34.3</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell></row><row><cell cols="4">Baseline 6.99Ours 48/17 48/17 10.01/43.56 35.92/38.24</cell><cell>4.12/26.32</cell><cell>7.39/31.18</cell></row><row><cell>Proposed Split (↓)</cell><cell></cell><cell>mAP/RE</cell><cell>mAP/RE</cell><cell>mAP/RE</cell><cell>mAP/RE</cell></row><row><cell>Baseline</cell><cell>65/15</cell><cell>8.48/20.44</cell><cell>36.96/40.09</cell><cell cols="2">8.66/20.45 14.03/27.08</cell></row><row><cell>Ours</cell><cell>65/15</cell><cell cols="4">12.40/37.72 34.07/36.38 12.40/37.16 18.18/36.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 :</head><label>2</label><figDesc>mAP scores of Pascal VOC'07. Italic classes are unseen.</figDesc><table><row><cell>β(→)</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell></row><row><cell>mAP</cell><cell>48.6</cell><cell>47.9</cell><cell>47.8</cell><cell>47.6</cell><cell>47.5</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>Validation study on traditional detection.</figDesc><table><row><cell>γ</cell><cell>α</cell><cell cols="4">GZSD ZSD Seen Unseen HM</cell></row><row><cell>0</cell><cell>1</cell><cell>6.6</cell><cell>31.9</cell><cell>6.6</cell><cell>10.9</cell></row><row><cell>0</cell><cell>.75</cell><cell>2.7</cell><cell>27.4</cell><cell>2.7</cell><cell>4.9</cell></row><row><cell cols="2">0.1 .75</cell><cell>5.4</cell><cell>27.9</cell><cell>5.4</cell><cell>9.0</cell></row><row><cell cols="2">0.2 .75</cell><cell>7.3</cell><cell>31.4</cell><cell>7.3</cell><cell>11.8</cell></row><row><cell cols="2">0.5 .50</cell><cell>8.4</cell><cell>30.6</cell><cell>8.4</cell><cell>13.1</cell></row><row><cell cols="3">1.0 .25 11.6</cell><cell>31.3</cell><cell>11.6</cell><cell>16.9</cell></row><row><cell cols="3">2.0 .25 12.6</cell><cell>33.0</cell><cell>12.6</cell><cell>18.3</cell></row><row><cell cols="2">5.0 .25</cell><cell>9.1</cell><cell>33.6</cell><cell>9.1</cell><cell>14.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>and trained</figDesc><table><row><cell>Method</cell><cell>ZSD</cell><cell cols="2">Seen Unseen GZSD</cell><cell>HM</cell><cell></cell><cell>10 15</cell></row><row><cell>Baseline Our-FL-word Our-PL-word</cell><cell cols="2">8.48 10.80 37.56 36.96 12.02 33.28</cell><cell>8.66 10.80 12.02</cell><cell>14.03 16.77 17.66</cell><cell>mAP</cell><cell>5</cell><cell>w/o word in regrs word in regrs word+vocab in regrs</cell></row><row><cell cols="3">Our-PL-vocab* 12.62 32.99</cell><cell>12.62</cell><cell>18.26</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our-FL</cell><cell>Our-PL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 4 :</head><label>4</label><figDesc>More results on ZSD with different word-vectors (GloVe, FasText and Word2Vec).</figDesc><table><row><cell>Method</cell><cell>Penalty Function</cell><cell>Seen/Unseen</cell><cell>Word Vector</cell><cell>ZSD</cell><cell>seen</cell><cell>GZSD unseen</cell><cell>HM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(mAP)</cell><cell>(mAP)</cell><cell>(mAP)</cell><cell>(mAP)</cell></row><row><cell>Our-PL-vocab</cell><cell>softplus</cell><cell>65/15</cell><cell>w2v</cell><cell>12.17</cell><cell>32.12</cell><cell>12.18</cell><cell>17.66</cell></row><row><cell>Our-PL-vocab</cell><cell>sigmoid</cell><cell>65/15</cell><cell>w2v</cell><cell>12.62</cell><cell>32.99</cell><cell>12.62</cell><cell>18.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison of ZSD performance with softplus and sigmoid based penalty functions.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07113</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zeroshot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8690" to="8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>NIPS (C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified approach for conventional zero-shot, generalized zero-shot, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="5652" to="5667" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Zero-shot learning via latent space encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transductive unbiased embedding for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive zero-shot learning with a self-training dictionary approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2908" to="2919" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal cycle-consistent generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-invariant projection learning for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1019" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic discovery, association estimation and learning of semantic attributes for a thousand categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H J S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">SSD: Single Shot MultiBox Detector</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>cited By 107</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recent advances in zero-shot recognition: Toward data-efficient understanding of visual content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="112" to="125" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning -a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilabel zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for zero-shot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attributes make sense on segmented objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Straight to shapes: Real-time detection of encoded shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07932</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiple instance visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<editor>ECCV (D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="391" to="405" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image captioning with unseen objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nuswide: A real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno>pp. 48:1- 48:9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR &apos;09</title>
		<meeting>the ACM International Conference on Image and Video Retrieval, CIVR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
