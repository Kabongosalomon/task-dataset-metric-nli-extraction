<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
						</author>
						<title level="a" type="main">NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Activity Understanding</term>
					<term>Video Analysis</term>
					<term>3D Action Recognition</term>
					<term>RGB+D Vision</term>
					<term>Deep Learning</term>
					<term>Large-Scale Benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding. [The dataset is available at: http:// rose1.ntu.edu.sg/ Datasets/ actionRecognition.asp.]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE development of depth sensors, e.g., Microsoft Kinect, Intel RealSense, and Asus Xtion, enables us to obtain effective 3D structure information of the objects and scenes <ref type="bibr" target="#b0">[1]</ref>. This empowers the computer vision solutions to move important steps towards 3D vision, such as 3D object recognition <ref type="bibr" target="#b3">[2]</ref>, 3D scene understanding <ref type="bibr" target="#b4">[3]</ref>, and 3D activity analysis <ref type="bibr" target="#b5">[4]</ref>, etc. Unlike RGB video-based activity analysis <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, 3D action recognition suffers from the lack of large-scale benchmark datasets. There are no publicly shared sources like YouTube to supply "in-thewild" 3D video samples of a realistically various set of action classes. This limits our ability to build large-sized benchmarks to train, evaluate, and compare the strengths of different approaches, especially the recent data-hungry techniques like deep learning methods. To the best of our knowledge, all the currently available 3D action recognition benchmarks have limitations in various aspects.</p><p>First is the small number of subjects and narrow range of performers' ages. This can significantly limit the intraclass variation of the actions. The constitution of human activities depend on the gender, age, physical condition, and even cultural aspects of the subjects. As a result, variation of human subjects is crucial for building a realistic action recognition benchmark.</p><p>The second factor is the limited number of action categories. When only a small set of classes are available, each can be very distinguishable by finding a simple motion pattern or even by the appearance of an interacted object. However, when the number of classes grows, similar motion patterns and interacted objects will be shared among different classes, which makes the action recognition much more challenging.</p><p>The third limitation is the highly restricted camera views. In most of the current datasets, the samples are captured from a front view with a fixed camera viewpoint. In some others, the views are often bounded to fixed front and side views using multiple cameras at the same time.</p><p>The fourth factor is the limited variation of the collection environments (e.g., backgrounds) which can also be important to achieve a sensible activity analysis dataset.</p><p>Finally and most importantly, the very limited number of video samples hinders the application of the advanced datadriven learning methods to this problem. Though several attempts have been done <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b11">[10]</ref>, they mostly suffer from over-fitting and have to scale down the size of their learning models. Therefore, they clearly need many more samples to generalize and perform better on the testing videos.</p><p>In order to overcome these limitations, a large-scale benchmark dataset, NTU RGB+D 120 dataset, is developed for 3D human activity analysis.</p><p>The proposed dataset consists of 114, 480 RGB+D video samples that are captured from 106 distinct human subjects. We have collected RGB videos, depth sequences, skeleton data (3D locations of 25 major body joints), and infrared frames using Microsoft Kinect v2. The action samples are captured from 155 different camera viewpoints. The subjects in this dataset are in a wide range of age distribution (from TABLE 1: Comparison of the proposed NTU RGB+D 120 dataset and some of the other publicly available datasets for 3D action recognition. Our dataset provides many more video samples, action classes, human subjects, and camera views in comparison with other available datasets for RGB+D action recognition. It is worth mentioning that most of the other datasets were collected on a single or few backgrounds and under fixed illumination condition, while there is high variation of environmental conditions in our dataset, which uses 96 different backgrounds and contains significant illumination variation.</p><p>10 to 57) and from different cultural backgrounds (15 countries), which brings very realistic variation to the quality of actions. We also provide the ambiance inconstancy by capturing the dataset under various environmental conditions (96 different backgrounds with illumination variation). The large amount of variation in subjects, views, and backgrounds makes it possible to have more sensible crosssubject and cross-setup evaluations for various 3D-based action analysis methods. The proposed dataset will help the community to move steps forward in 3D human activity analysis, and make it possible to apply data-hungry methods, such as deep learning techniques, for this task.</p><p>The performance of state-of-the-art 3D action recognition approaches is evaluated on our dataset, which shows the capability of applying deep models for activity analysis with the suggested cross-subject and cross-setup evaluation criteria. We also evaluate the performance of fusion across different data modalities, e.g., RGB, depth, and skeleton data, for activity analysis, since they provide appearance and geometrical information respectively, and are complementary for more accurate action recognition.</p><p>In this paper, we also investigate a novel one-shot 3D action recognition problem based on the proposed dataset. An Action-Part Semantic Relevance-aware (APSR) framework is proposed to handle this task by utilizing the semantic relevance between each body part and each action class at the distributed word embedding level <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>. Human actions can be represented by a combination of the movements of different body parts <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref>, and the importance degrees of body parts' motion are not equal for recognizing different action categories. As a result, we need to put more emphasis on the more relevant body parts when recognizing an action performed by a human. In this paper, we show that the name (text description) of the novel action class can assist in the identification of the relevant body parts, and by exploiting the semantic relevance between the action's and body part's descriptions as a guidance, the relevant body parts of the novel action categories can be emphasized, and thus the one-shot recognition performance is improved.</p><p>The rest of this paper is organized as follows. Section 2 reviews the current 3D-based human action recognition approaches and benchmarks. Section 3 introduces the proposed dataset, its structure, and the defined evaluation criteria. Section 4 presents the proposed APSR framework for one-shot 3D human action recognition. Section 5 shows the experimental evaluations with our benchmark. Finally, section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We briefly review publicly available benchmark datasets and recent methods for 3D human activity analysis in this section. Here we introduce a limited number of the most famous ones. Readers are referred to these survey papers <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref> for a more extensive list of the current 3D activity analysis datasets and methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Activity Analysis Datasets</head><p>After the release of the Microsoft Kinect <ref type="bibr" target="#b38">[37]</ref>, several datasets have been collected to conduct research on 3D human action recognition and to evaluate different methods in this field.</p><p>The MSR-Action3D dataset <ref type="bibr" target="#b12">[11]</ref> was the earliest which opened up the research in depth-based action analysis. The samples of this dataset were limited to depth sequences of gaming actions, e.g., forward punching, side boxing, forward kicking, side kicking, tennis swinging, tennis serving, golf swinging, etc. Later, the skeleton data was added to this dataset. The skeletal information includes the 3D locations of 20 different joints at each frame. A decent number of methods have been evaluated on this benchmark, and the recent ones reported close to saturation accuracies <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>.</p><p>The MSR-DailyActivity dataset <ref type="bibr" target="#b15">[14]</ref> was among the most challenging benchmarks in this field. It contains 320 samples of 16 daily activities with higher intra-class variation. The small number of samples and fixed camera viewpoints are the limitations of this dataset. Some reported results on this dataset also achieved very high accuracies <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b44">[43]</ref>.</p><p>The RGBD-HuDaAct dataset <ref type="bibr" target="#b14">[13]</ref> was one of the largest datasets. It contains RGB and depth sequences of 1189 videos of 12 human daily actions (plus one background class), with high variation in time lengths. The special characteristic of this dataset is the synced and aligned RGB and depth channels, which enables local multi-modal analysis of RBGD 1 signals.</p><p>The CAD-60 <ref type="bibr" target="#b13">[12]</ref> and CAD-120 <ref type="bibr" target="#b18">[17]</ref> datasets contain RGB, depth, and skeleton data of human actions. The special characteristic of these datasets is the variety of camera views. Unlike most of the other datasets, the cameras in these two datasets were not bound to front-view or sideviews. However, the limited number of video samples (60 and 120) is the downside of them.</p><p>The 3D Action Pairs dataset <ref type="bibr" target="#b19">[18]</ref> was proposed to provide multiple pairs of action classes. Each pair contains very closely related actions with differences along temporal axis, e.g., pick up/put down a box, push/pull a chair, put on/take off a hat, etc. State-of-the-art methods <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b45">[44]</ref> achieved perfect accuracy on this benchmark.</p><p>The Northwestern-UCLA <ref type="bibr" target="#b21">[20]</ref> and the Multiview 3D Event <ref type="bibr" target="#b20">[19]</ref> datasets used more than one depth sensors at the same time to collect multi-view representations of the same action, and scale up the number of samples.</p><p>The G3D <ref type="bibr" target="#b46">[45]</ref> and PKUMMD <ref type="bibr" target="#b47">[46]</ref> datasets were introduced for activity analysis in continuous sequences, which respectively contain 210 and 1076 videos. In each dataset, the videos were collected in the same environment.</p><p>It is worth mentioning that there are more than 40 datasets for 3D human action recognition <ref type="bibr" target="#b32">[31]</ref>. Though each of them provided important challenges of human activity analysis, they have limitations in some aspects. TABLE 1 shows the comparison between some of the current datasets and our large-scale RGB+D action recognition dataset.</p><p>By summarizing the contributions of our dataset over the existing ones, NTU RGB+D 120 dataset has: (1) many more action classes; (2) many more video samples for each action class; (3) much more intra-class variation, e.g., poses, 2: Comparison of the dataset version introduced in this paper and the one released in our preliminary conference paper <ref type="bibr" target="#b48">[47]</ref>. The top three rows show a comparison of the sizes of these two versions. The bottom two rows show a comparison of the recognition accuracies when evaluating several methods on it. The methods <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref> are evaluated by using the suggested two evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Preliminary <ref type="bibr">[</ref> interacted objects, ages and cultural backgrounds of the actors; (4) many more collection environments, e.g., different backgrounds and illumination conditions; (5) more camera views; (6) more camera-to-subject distance variation; <ref type="bibr" target="#b8">(7)</ref> used Kinect v2 which provides more accurate depth-maps and 3D joints, especially in a multi-camera setup, compared to the previous version of Kinect. This work is an extension of our previous conference paper <ref type="bibr" target="#b48">[47]</ref>. In <ref type="bibr" target="#b48">[47]</ref>, we introduced the preliminary version of our dataset that contains 60 action classes. In this paper, we significantly extend it and build the NTU RGB+D 120 dataset that is much larger and provides much more variation of environmental conditions, subjects, and camera views, etc. It also provides more challenges of 3D human activity analysis. A brief comparison between these two versions is shown in TABLE 2. Besides, in this paper, we propose a new framework for one-shot 3D action recognition based on the proposed NTU RGB+D 120 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Action Recognition Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D action recognition by hand-crafted features.</head><p>Since the introduction of the first few benchmarks, such as the MSR-Action3D <ref type="bibr" target="#b12">[11]</ref> and MSR-DailyActivity <ref type="bibr" target="#b15">[14]</ref> datasets, a decent number of feature extraction and classifier learning methods have been proposed and evaluated based on them.</p><p>Oreifej and Liu <ref type="bibr" target="#b19">[18]</ref> proposed to calculate the fourdimensional normals (X-Y-depth-time) from depth sequences, and accumulate them on spatio-temporal cubes as quantized histograms over 120 vertices of a regular polychoron. Yang and Tian <ref type="bibr" target="#b51">[50]</ref> proposed supernormal vectors as aggregated dictionary-based codewords of fourdimensional normals over space-time grids. The work of <ref type="bibr" target="#b25">[24]</ref> introduced histograms of oriented principle components of depth cloud points to extract robust features against viewpoint variations. Lu et al. <ref type="bibr" target="#b41">[40]</ref> applied τ test-based binary range-sample features on depth maps and achieved robust representation against noise, scaling, camera views, and background clutter.</p><p>To have view-invariant representations of the actions, features can be extracted from the 3D body joint positions <ref type="bibr" target="#b52">[51]</ref>, <ref type="bibr" target="#b53">[52]</ref> which are available for each frame. Evangelidis et al. <ref type="bibr" target="#b54">[53]</ref> divided the body into part-based joint quadruples, and encoded the configuration of each part with a succinct 6D feature vector, so called skeletal quads. To aggregate the skeletal quads, they applied Fisher vectors, and classified the samples by a linear SVM. In <ref type="bibr" target="#b55">[54]</ref>, different skeleton configurations were represented as points on a Lie group. Actions as time-series of skeletal configurations were encoded as curves on this manifold. The work of <ref type="bibr" target="#b43">[42]</ref> utilized group sparsity-based class-specific dictionary coding with geometric constraints to extract skeleton features.</p><p>In most of the 3D action recognition scenarios, there are more than one modality of information, and combining different data modalities can help to improve the classification accuracy. Ohn-Bar and Trivedi <ref type="bibr" target="#b56">[55]</ref> combined second order joint-angle similarity representations of skeletons with a modified two step HOG feature on spatio-temporal depth maps to build global representation of each video sample, and utilized a linear SVM to classify the actions. Wang et al. <ref type="bibr" target="#b57">[56]</ref> combined Fourier temporal pyramids of skeletal information with local occupancy pattern features extracted from depth maps, and applied a data mining framework to discover the most discriminative combinations of body joints. A structured sparsity-based multi-modal feature fusion technique was introduced by <ref type="bibr" target="#b58">[57]</ref> for action recognition in RGB+D domain. In <ref type="bibr" target="#b59">[58]</ref>, random decision forests were utilized for learning and feature pruning over a combination of depth and skeleton-based features. Hu et al. <ref type="bibr" target="#b42">[41]</ref> introduced a joint heterogeneous feature learning framework by combing RGB, depth, and skeleton data for human activity recognition. The work of <ref type="bibr" target="#b39">[38]</ref> proposed hierarchical mixed norms to fuse different features and select most informative body parts in a joint learning framework.</p><p>3D action recognition with deep networks. Recently, deep learning based-approaches have been proposed for 3D human activity analysis <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b60">[59]</ref>, <ref type="bibr" target="#b61">[60]</ref>. Specifically, many of them have been evaluated based on the preliminary version <ref type="bibr" target="#b48">[47]</ref> of our dataset, or pre-trained on it for transfer learning for other tasks <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b62">[61]</ref>, <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b64">[63]</ref>, <ref type="bibr" target="#b65">[64]</ref>, <ref type="bibr" target="#b66">[65]</ref>, <ref type="bibr" target="#b67">[66]</ref>, <ref type="bibr" target="#b68">[67]</ref>, <ref type="bibr" target="#b69">[68]</ref>, <ref type="bibr" target="#b70">[69]</ref>, <ref type="bibr" target="#b71">[70]</ref>, <ref type="bibr" target="#b72">[71]</ref>, <ref type="bibr" target="#b73">[72]</ref>, <ref type="bibr" target="#b74">[73]</ref>, <ref type="bibr" target="#b75">[74]</ref>, <ref type="bibr" target="#b76">[75]</ref>, <ref type="bibr" target="#b77">[76]</ref>, <ref type="bibr" target="#b78">[77]</ref>, <ref type="bibr" target="#b79">[78]</ref>, <ref type="bibr" target="#b80">[79]</ref>.</p><p>Some approaches used recurrent neural networks (RNNs) to model the motion dynamics and context dependencies for 3D human action recognition. Du et al. <ref type="bibr" target="#b10">[9]</ref> proposed a multi-layer RNN framework for 3D action recognition based on a hierarchy of skeleton-based inputs. Liu et al. <ref type="bibr" target="#b49">[48]</ref> introduced a Spatio-Temporal LSTM network by modeling the context information in both temporal and spatial dimensions. Zhang et al. <ref type="bibr" target="#b81">[80]</ref> added a view-adaptation scheme to the LSTM network to regulate the observation viewpoints. Luo et al. <ref type="bibr" target="#b62">[61]</ref> proposed an unsupervised learning method by using a LSTM encoder-decoder framework for action recognition in RGB+D videos.</p><p>Convolutional neural networks (CNNs) have also been applied to 3D human action recognition. Wang et al. <ref type="bibr">[81]</ref> proposed a "scene flow to action map" representation for RGB+D based action recognition with CNNs. Ke et al. <ref type="bibr" target="#b83">[82]</ref> transformed the 3D skeleton data to ten feature arrays and input them to CNNs for action recognition. Rahmani et al. <ref type="bibr" target="#b67">[66]</ref> designed a deep CNN model to transfer the visual appearance of human body-parts acquired from different views to a view-invariant space for depth-based activity analysis. Beside RNNs and CNNs, some other deep models have also been introduced for 3D human action recognition. Huang et al. <ref type="bibr" target="#b63">[62]</ref> incorporated Lie group structure into a deep architecture for skeleton-based action recognition. Tang et al. <ref type="bibr" target="#b78">[77]</ref> applied deep progressive reinforcement learning to distil the informative frames in the video sequences. Rahmani and Mian <ref type="bibr" target="#b84">[83]</ref> introduced a nonlinear knowledge transfer model to transform different views of the human actions to a canonical view for action classification.</p><p>One-shot 3D Action Recognition. Plenty of advanced techniques, such as metric learning <ref type="bibr" target="#b85">[84]</ref> and meta learning <ref type="bibr" target="#b86">[85]</ref>, <ref type="bibr" target="#b87">[86]</ref>, have been introduced for one-shot object recognition and image classification <ref type="bibr" target="#b88">[87]</ref>, <ref type="bibr" target="#b89">[88]</ref>, <ref type="bibr" target="#b90">[89]</ref>. In the context of 3D activity analysis, there are also a few attempts on oneshot-based learning.</p><p>Fanello et al. <ref type="bibr" target="#b91">[90]</ref> used 3D-HOF and Global-HOG features for one-shot action recognition in RGB+D videos. Wan et al. <ref type="bibr" target="#b92">[91]</ref> extracted mixed features around the sparse keypoints that are robust to scale, rotation and partial occlusions. Konečnỳ et al. <ref type="bibr" target="#b93">[92]</ref> combined HOG and HOF descriptors together with the dynamic time warping technique for one-shot RGB+D-based action recognition.</p><p>Different from these works, a simple yet effective APSR framework is introduced in this paper, which emphasizes the features of the relevant body parts by considering the semantic relevance of the action class and each body part, for one-shot 3D action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE NTU RGB+D 120 DATASET</head><p>In this section, we introduce the details of the proposed NTU RGB+D 120 action recognition dataset and the defined evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data Modalities</head><p>We use Microsoft Kinect sensors to collect our dataset. We collect four major data modalities acquired by this sensor, namely, the depth maps, the 3D joint information, the RGB frames, and the infrared (IR) sequences. The depth maps are sequences of two dimensional depth values in millimeters. To maintain all the information, we apply lossless compression for each individual frame. The resolution of each depth frame is 512 × 424.</p><p>The joint information consists of 3-dimensional locations of 25 major body joints for each detected and tracked human body in the scene. The corresponding pixels on RGB frames and depth maps are also provided for each body joint. The configuration of these joints is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The RGB videos are recorded in the provided resolution of 1920 × 1080.</p><p>The infrared sequences are also collected and stored frame by frame at the resolution of 512 × 424.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Action Classes</head><p>We have 120 action categories in total, which are divided into three major groups, including 82 daily actions (eating, writing, sitting down, moving objects, etc), 12 health-related actions (blowing nose, vomiting, staggering, falling down, etc), and 26 mutual actions (handshaking, pushing, hitting, hugging, etc).</p><p>Compared to the preliminary version <ref type="bibr" target="#b48">[47]</ref> of our dataset, the proposed NTU RGB+D 120 dataset contains many more action classes. Here we summarise the characteristics of the newly added actions compared to the actions in the preliminary version: (1) Fine-grained hand/finger motions. Most of the actions in the preliminary version of our dataset have significant body and hand motions, while the newly added classes in this extended version contain some actions that have fine-grained hand and finger motions, such as "make ok sign" and "snapping fingers". (2) Fine-grained objectrelated individual actions. The newly added actions include some fine-grained object-involved single-person actions, in which the body movements are not significant and the sizes of the involved objects are relatively small, such as "counting money" and "play magic cube". (3) Object-related mutual actions. Most of the two-person mutual actions in the preliminary version do not involve objects. In this extended version, some of the newly added mutual actions involve the interactions with objects, such as "wield knife towards other person" and "hit other person with object". (4) Different actions with similar posture patters but with different motion speeds. In this extended version, there are some different actions that have similar posture patterns but have different motion speeds. For example, "grab other person's stuff" is a newly added action, and its main difference compared to "touch other person's pocket (steal)" is the motion speed. (5) Different actions with similar body motions but with different objects involved. There are some different actions that have very similar body motions but involve different objects. For example, the motions in the newly added action "put on bag/backpack" are similar to those in the existing action "put on jacket". (6) Different actions with similar objects involved but with different body motions.</p><p>Among the newly added actions, there are also some different classes that share the same interacted objects, such as "put on bag/backpack" and "take something out of a bag/backpack".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Subjects</head><p>We invited 106 distinct subjects to our dataset collection sessions. These subjects are from 15 different countries. Their ages are between 10 and 57, and heights are between 1.3m and 1.9m. <ref type="figure">Fig. 9</ref> illustrates the variety of the subjects in age, gender, and height. Each subject is assigned a consistent ID number over the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Collection Setups</head><p>We use 32 collection setups to build our dataset, and over different setups, we change the location and background. Specifically, in each setup, we use three cameras at the same time to capture three different horizontal views for the same action sample. The three cameras are set up at the same height yet from three different horizontal angles:</p><formula xml:id="formula_0">−45 • , 0 • , +45 • .</formula><p>Each subject is asked to perform each action twice, once towards the left camera, and once towards the right camera. In this way, in each collection setup, we capture two front views, one left side view, one right side view, one left side 45 degrees view, and one right side 45 degrees view. The three cameras are assigned consistent camera numbers in our dataset, where camera 1 always observes the 45 degrees views, while cameras 2 and 3 observe the front and side views.</p><p>To further increase the number of camera views, over different collection setups, we also change the vertical heights of the cameras and their distances to the subjects, as reported in TABLE 3. All the camera and setup numbers are provided for each video sample in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benchmark Evaluations</head><p>To have standard evaluations for the methods to be tested on this benchmark, we define precise criteria for two types of action classification evaluation. For each criterion, we report the classification accuracy in percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cross-Subject Evaluation</head><p>For cross-subject evaluation, the 106 subjects are split into training and testing groups. Each group consists of 53 subjects. The IDs of the training subjects in this evaluation are: <ref type="bibr">1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38, 45, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 70, 74, 78, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92, 93, 94, 95, 97, 98, 100, 103.</ref> The remaining subjects are reserved for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-Setup Evaluation</head><p>For cross-setup evaluation, we pick all the samples with even collection setup IDs for training, and those with odd setup IDs for testing, i.e., 16 setups are used for training, and the other 16 setups are reserved for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APSR FRAMEWORK FOR ONE-SHOT 3D AC-TION RECOGNITION</head><p>Existing works <ref type="bibr" target="#b88">[87]</ref>, <ref type="bibr" target="#b89">[88]</ref> show that once some categories have been learned, the knowledge gained in this process can be abstracted and used to learn novel classes efficiently, even if only one learning example per new class is given (i.e., via one-shot learning). Since the samples of certain categories may be difficult to collect <ref type="bibr" target="#b33">[32]</ref>, one-shot visual recognition becomes an important research branch in computer vision.</p><p>In this section, we introduce the one-shot 3D action recognition scenario based on our proposed dataset, and show how a large auxiliary set could be used to assist the one-shot recognition for the novel classes. Specifically, an Action-Part Semantic Relevance-aware (APSR) framework is proposed for more reliable one-shot 3D action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">One-Shot Recognition on NTU RGB+D 120</head><p>We define the one-shot 3D action recognition scenario on the proposed NTU RGB+D 120 dataset as follows.</p><p>We split the full dataset into two parts: the auxiliary set and the evaluation set. There are no overlaps of classes between these two sets. The auxiliary set contains multiple action classes and samples, and these samples can be used for learning (e.g., learning a feature generation network). The evaluation set consists of the novel action classes for one-shot recognition evaluation, and one sample from each novel class is picked as the exemplar, while the remaining samples are used to test the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">APSR Framework</head><p>Previous works <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b94">[93]</ref> have shown that the importance degrees of the features from different body parts are not the same in analyzing different actions. For example, the features extracted from the leg joints are more relevant in recognizing the action "kicking", compared to those from other body parts. Therefore, it is intuitive to identify the body parts that are more relevant to the action performed in a video sequence, and correspondingly emphasize their features to achieve reliable recognition performance. However, in our one-shot recognition scenario, identifying the relevant body parts of the novel actions is difficult, as learning to generalize beyond the single specific exemplar is often very challenging <ref type="bibr" target="#b89">[88]</ref>.</p><formula xml:id="formula_1">Temporal (frame) Spatial (body part) F p-1,t F p,t F p,t-1 F P,T</formula><p>In this paper, a simple yet effective APSR framework, which can be used to generalize to the novel action categories, is introduced for one-shot recognition. The APSR framework emphasizes the relevant body parts for each new class of actions by considering the semantic relevance between the action and each body part based on their descriptions. Specifically, we design a network to generate the features of each body part, and then perform weighted pooling over these features with the guidance of relevance scores in the word embedding space. Finally, the pooling result is used to perform one-shot recognition.</p><p>Feature Generation Network. The feature generation network is adopted from the 2D Spatio-Temporal LSTM (ST-LSTM) <ref type="bibr" target="#b95">[94]</ref> designed for 3D activity analysis, which models the context and dependency information in both temporal dimension (over different frames) and spatial dimension (over different body parts). Readers are referred to <ref type="bibr" target="#b95">[94]</ref> for more details about the mechanism of ST-LSTM.</p><p>The original ST-LSTM <ref type="bibr" target="#b95">[94]</ref> models the context information via a single pass. To produce a more discriminative set of features for each part, we introduce bidirectional context passing (similar to bidirectional LSTM <ref type="bibr" target="#b96">[95]</ref>) for our feature generation network, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The input to our feature generation network is an action sample (here we input its skeleton data for efficiency), and the outputs are the features of all body parts at each frame. Concretely, at the unit (p, t) of this network, the input is the 3D coordinates of the skeletal joint of the body part (p) in the frame (t), and the output is the feature (F p,t ) representing this body part at this frame. By incorporating the spatiotemporal context information into each part, the obtained feature set F = {F p,t | p ∈ {1, ..., P }, t ∈ {1, ..., T }} is powerful for representing each body part (p) at each frame (t) in the performing of the action instance, where P is the number of body parts, and T is the number of frames used for each video sample.</p><p>Semantic Relevance. The proposed method is inspired by the recent works on word embedding in natural language processing <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b97">[96]</ref>. In these works, each word is mapped into a continuous embedding space, and two semantically relevant words will have large cosine similarity in this space <ref type="bibr" target="#b97">[96]</ref>, <ref type="bibr" target="#b98">[97]</ref>, <ref type="bibr" target="#b99">[98]</ref>. By pre-training on a massive natural language corpus, these models demonstrate their Action: "make ok sign" Body part: "right hand" <ref type="figure">Fig. 3</ref>: Illustration of estimating the semantic relevance score between the novel action's text description (name) and the body part's text description (name). Here we take the action "make ok sign" with the body part "right hand" as an example. Each word in a text description is fed to the pre-trained Word2Vec model to produce its embedding (a 300-dimensional vector), and the representation of a text description is obtained by averaging the embedding of all the words in it. Finally, the semantic relevance is estimated by calculating the cosine similarity between the two representations.</p><p>superior ability in preserving the semantic relations among different words, and thus have been successfully transferred to different tasks, such as document classification <ref type="bibr" target="#b100">[99]</ref>, image classification <ref type="bibr" target="#b89">[88]</ref>, and image/video caption generation <ref type="bibr" target="#b101">[100]</ref>.</p><p>This motivates us to utilize the prior knowledge about relevant body parts for recognizing new action classes, and the semantic relevance (cosine similarity) between the novel action's name and each body part's name in the embedding space can be used as prior information.</p><p>Specifically, the powerful Word2Vec model <ref type="bibr" target="#b29">[28]</ref> that is pre-trained on a large corpus is used in our method. When given a new action, we estimate the relevance score (cosine similarity) between this action and each body part based on their text descriptions (e.g., "make ok sign" versus "right hand"), by using the pre-trained Word2Vec model. For a description consisting of multiple words, its representation is obtained by averaging the embedding of all the words of it. If the estimated relevance score is negative, we reset it to zero. The method of estimating the semantic relevance score is illustrated in <ref type="figure">Fig. 3</ref>.</p><p>As shown in <ref type="figure">Fig. 4</ref>, the relevant body parts of the novel actions can be reliably indicated by using this method.</p><p>After we obtain the semantic relevance score (r c,p ) between the action class (c) and each body part (p), we normalize the score as: s c,p = r c,p / P u=1 r c,u to ensure that the total score of all body parts for this action class is 1 after normalization. Therefore, the action-part relevance score set (S c ) of the action class (c) is obtained as: S c = {s c,p | p ∈ {1, ..., P }}, which will be used as prior information for the feature generation network training and one-shot recognition evaluation.</p><p>Training. In our framework, we train the feature generation network on the auxiliary set that does not contain samples from the novel action categories. To train this network, at each unit (p, t) (see <ref type="figure" target="#fig_1">Fig. 2</ref>), we feed the produced body part feature F p,t to the softmax classifier for action classification, similar to <ref type="bibr" target="#b95">[94]</ref>. This indicates that at each unit, a prediction of the action class is produced based on the "hopping one foot jumping" "head": 0.11 "left hand": 0.29 "right hand": 0.27 "left foot": 0.58 "right foot": 0.59 "wield knife towards other person" "head": 0.24 "left hand": 0.39 "right hand": 0.41 "left foot": 0.19 "right foot": 0.21 … … <ref type="figure">Fig. 4</ref>: Examples of semantic relevance scores between action's name and each body part's name. In the 1st and 3rd columns, the body parts (joints) with larger scores are labeled with red circles. In the 2nd and 4th columns, we show the scores of several body parts. These scores are obtained by using pre-trained word embedding model <ref type="bibr" target="#b29">[28]</ref>. Semantically, "right foot" and "left foot" are very relevant to "hopping, one foot jumping", while "right hand" and "left hand" are relevant to "wield knife towards other person".</p><p>body part feature (F p,t ). We train the feature generation network with the classifiers in an end-to-end manner by using the following loss function:</p><formula xml:id="formula_2">L = P p=1 T t=1 s c,p l(c,ĉ p,t )<label>(1)</label></formula><p>where l(c,ĉ p,t ) is the negative log-likelihood loss measuring the difference between the true class label, c, of the sample and the prediction result,ĉ p,t , at the unit (p, t). The semantic relevance score s c,p is used here as the weight of the classification loss at the units (p, ·) that correspond to the body part p, i.e., more relevant parts are given larger loss weights. This drives the network to learn more discriminative features on the more relevant body parts of each action class. Evaluation. After training the network on the auxiliary set, we feed each sample from the evaluation set to the feature generation network to produce a feature set (F) for this sample, which represents the features of different body parts at each frame. Note that during evaluation, we remove the classifiers and only use the produced features, since the classes for evaluation are not contained in the auxiliary training set.</p><p>We perform weighted pooling over the obtained features (F) of each sample to generate an aggregated representation. Let f (a, b) denote the aggregated representation of the sample a when using b as the weights for feature pooling. Then the aggregated representation of an exemplar (Ω) of a novel class is calculated as:</p><formula xml:id="formula_3">f (Ω, S cΩ ) = P p=1 T t=1 s cΩ,p F p,t<label>(2)</label></formula><p>where c Ω denotes the class label of the exemplar Ω, and S cΩ is the action-part relevance score set of the action class c Ω .</p><p>Here the semantic relevance scores are used as weights to guide the aggregated representation of the sample Ω and to emphasize the features from more relevant body parts. To test a sample (i), we calculate the distance between this sample (i) and an exemplar (Ω) as:</p><formula xml:id="formula_4">D(i, Ω) = D cos f (i, S cΩ ), f (Ω, S cΩ )<label>(3)</label></formula><p>where D cos is the cosine distance between the two representations (vectors). Note that when calculating the distance of i to the exemplar Ω, we generate its aggregated representation as f (i, S cΩ ), which indicates the weighted pooling on the feature set of i is based on the relevance score set of the exemplar's class c Ω . For each testing sample, we calculate its distances to all the exemplars from all action categories by using Eq. (3), and then perform classification using the nearest neighbour classifier, as in <ref type="bibr" target="#b102">[101]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, some state-of-the-art methods that were designed for 3D action recognition are evaluated based on the suggested cross-subject and cross-setup criteria. Then the action recognition performances achieved by adopting different data modalities are compared. The performance of the proposed APSR framework for one-shot 3D action recognition is also evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Evaluations of 3D Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Evaluation of state-of-the-art methods</head><p>Twelve state-of-the-art 3D action recognition methods are evaluated on our dataset, namely, the Part-Aware LSTM <ref type="bibr" target="#b48">[47]</ref>, the Soft RNN <ref type="bibr" target="#b103">[102]</ref>, the Dynamic Skeleton <ref type="bibr" target="#b27">[26]</ref>, the Spatio-Temporal LSTM <ref type="bibr" target="#b49">[48]</ref>, the Internal Feature Fusion <ref type="bibr" target="#b95">[94]</ref>, the GCA-LSTM <ref type="bibr" target="#b31">[30]</ref>, the Multi-Task Learning Network <ref type="bibr" target="#b50">[49]</ref>, the FSNet <ref type="bibr" target="#b104">[103]</ref>, <ref type="bibr" target="#b109">[108]</ref>, the Skeleton Visualization <ref type="bibr" target="#b105">[104]</ref>, the Two-Stream Attention LSTM <ref type="bibr" target="#b106">[105]</ref>, the Multi-Task CNN with RotClips <ref type="bibr">[106]</ref>, and the Body Pose Evolution Map method <ref type="bibr" target="#b108">[107]</ref>. Using the cross-subject and cross-setup evaluation criteria, the results of these methods are reported in TABLE 4.</p><p>Among these approaches, the method in <ref type="bibr" target="#b27">[26]</ref> uses the Fourier temporal pyramid <ref type="bibr" target="#b57">[56]</ref> and the hand-crafted features for action classification. The methods in <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b95">[94]</ref>, <ref type="bibr" target="#b103">[102]</ref>, <ref type="bibr" target="#b106">[105]</ref> all use RNN/LSTM for 3D action recognition, and the approaches in <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b104">[103]</ref>, <ref type="bibr" target="#b105">[104]</ref>, <ref type="bibr">[106]</ref>, <ref type="bibr" target="#b108">[107]</ref> all use convolutional networks for spatial and temporal modeling. Specifically, the evaluations of <ref type="bibr" target="#b105">[104]</ref>, <ref type="bibr" target="#b108">[107]</ref> are performed by using the efficient MobileNet <ref type="bibr" target="#b110">[109]</ref> as the base model, and skeleton data as input. All the evaluated implementations are from the original authors of the corresponding papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation of using different data modalities</head><p>Since multiple data modalities are provided in our dataset, we also evaluate the performance of using different data modalities (e.g., RGB, depth, and skeleton data) as input for action recognition, and report the results in TABLE 5.</p><p>In this table, the accuracy on RGB video is achieved by learning visual and motion (optical flow) features at each frame of the RGB video by training the VGG model <ref type="bibr" target="#b111">[110]</ref>, and performing classification by averaging the classification results of all frames, similarly to <ref type="bibr" target="#b112">[111]</ref>. The accuracy on depth video is obtained by using the similar method as the RGB video, and training the VGG model based on the depth data. The accuracy on 3D skeleton data is achieved by using the method in <ref type="bibr" target="#b49">[48]</ref>.</p><p>We observe that when using the RGB or depth video as input, the performance of the cross-setup evaluation is weaker than that of the cross-subject one. The performance disparity can be justified as: <ref type="bibr" target="#b0">(1)</ref> In the cross-setup evaluation scenario, the heights and distances of the cameras are not the same over different setups. This indicates camera viewpoints are different, and thus the appearance of the actions can be significantly different. <ref type="bibr" target="#b3">(2)</ref> The background is also changing across different setups. However, when using the RGB or depth frames as input, the methods are more prone to learn from the appearance or view-dependent motion patterns.</p><p>By using the 3D skeleton data as input, the method performs better in the cross-setup evaluation. This is partially because the method using 3D skeleton data is stronger to generalize among different views, since the 3D skeletal representation is view-invariant in essence. However, it is still prone to errors of the body tracker.</p><p>We also evaluate the performance of fusing multiple data modalities for action recognition. The results in TA-BLE 5 show that compared to the method of using a single modality, fusing multiple modalities bring a performance improvement, since they contain complementary and discriminative appearance and geometrical information, which is useful for activity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation of using different sizes of training set</head><p>In the aforementioned experiments, the evaluated deep learning models, such as Part-Aware LSTM <ref type="bibr" target="#b48">[47]</ref> and FSNet <ref type="bibr" target="#b104">[103]</ref>, are trained with the defined large training set from our NTU RGB+D 120 dataset. Here we call this training set as "full training set".</p><p>To evaluate the effect of the training set size on the recognition performance, we use different ratios of training samples from the full training set for network training. We then evaluate the action recognition performance on the same testing set based on the cross-setup evaluation criterion.</p><p>We take six methods <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b95">[94]</ref>, <ref type="bibr" target="#b104">[103]</ref>, <ref type="bibr" target="#b106">[105]</ref> as examples, and show their results achieved by using different sizes of training set in <ref type="figure" target="#fig_3">Fig. 5</ref>. The results show that when more samples are used for network training, the action recognition accuracies of all these methods increase obviously. For example, when using 20% of the samples from the full training set for training, the accuracy of FSNet is 40.6%, while the accuracy reaches 62.4% when the full training set is used for network training.</p><p>We also evaluate the performance of using different sizes of training data for action recognition with different data modalities, and show the results in <ref type="figure" target="#fig_5">Fig. 6</ref>. The results in this figure also show the benefit of using more data for network training to achieve better action recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Detailed analysis according to data modalities</head><p>Here we analyze the results obtained by using different data modalities in detail. The skeleton data modality is evaluated using the Spatio-Temporal LSTM <ref type="bibr" target="#b49">[48]</ref>. The RGB and depth data modalities are both evaluated using the two-stream framework <ref type="bibr" target="#b112">[111]</ref>. Modality fusion is performed by fusing the results of the three modalities. The results of different methods, which are designed for 3D human activity analysis, using the cross-subject and cross-setup evaluation criteria on the NTU RGB+D 120 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cross-Subject Accuracy Cross-Setup Accuracy   We first plot the confusion matrices of different data modalities. Specifically, we show the confusion matrix of the RGB modality as an example in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>We then perform action-wise analysis for different data modalities. Considering the large number of action cate-  gories, for each data modality, we analyze the action classes that have high recognition accuracies (top 10 accurate classes), and the actions that are easily misclassified to other classes (top 10 confused action pairs). We show the results in TABLE 6. Based on the results in TABLE 6, we find that the actions that have significant motions and discriminative posture   patterns could be more accurately recognized. For example, the actions "walk apart from each other" and "walk towards each other", which have very discriminative and significant motions, are both in the top 10 accurate actions when using any of the three data modalities as input. We also observe that when using skeleton data as input, the actions that involve interactions with objects may be easily misclassified. For example, "play magic cube" is often confused with "counting money", and "handshaking" can also be misclassified to "giving something to other person", as shown in TABLE 6. This is possibly because the actions in each pair have similar human motion patterns, and the perception of the existences of the objects and their appearances is important for accurately recognizing these actions. However, the appearance information of the objects is ignored by the recognition model when using skeleton information only. In contrast, when we perform action recognition based on the RGB or depth data that captures the object information, many object-related actions could be accurately recognized. For example, "carry things with other person" and "move heavy objects" are both in the top 10 accurate actions of the RGB and depth data modalities.</p><p>Although RGB and depth data modalities both have good ability in representing the object-related actions, the actions with object involved may be misclassified when the same (or similar) objects are shared by different actions, where the objects may even mislead the classification. For example, when using RGB data as input, "ball up paper" tend to be misclassified to "fold paper", and "reading" and "writing" can also be confused, as shown in TABLE 6. An interesting observation is that for RGB data modality, many</p><p>samples of "open a box" are misclassified to "fold paper", while "open a box" is in the top 10 accurate actions for the depth data modality. This performance disparity could be explained as: in the RGB image, the appearances of the box and the paper can be similar, but the depth data can well represent the 3D shape information of the objects, thus depth data is more powerful in distinguishing the box from paper. As a result, the action "open a box" can be more accurately recognized by using depth data than by using RGB data. We also observe that "put on jacket", which is classified well with RGB data, is easily confused with "put on bag/backpack" with depth data. This may be because the jacket and the backpack can be more easily distinguished from their color and texture information, than from their 3D shape information.</p><p>As shown in TABLE 6, "kick backward" is in the top 10 accurate actions of both skeleton and depth data modalities, while "kick backward" is easily confused to "side kick" when using RGB data. A possible reason is that both the depth data modality and the skeleton data modality provide the 3D structure information that implies the 3D direction information of the body part motions. However, such 3D information is not provided when using the RGB data.</p><p>We also observe that many actions that contain finegrained hand gestures and finger motions, such as "make ok sign", "make victory sign", and "thumb up", are easily misclassified when using the skeleton data only, as depicted in TABLE 6. The performance limitation of the skeleton data in handing these actions is possibly due to that only three joints are provided for each hand in the skeleton data, and besides, the skeleton data provided by Kinect's tracker algorithm is not perfect and can be noisy sometimes. This indicates that the skeleton data has difficulties in representing the very detailed hand and finger motions. Therefore, the actions with fine-grained hand/finger motions could be easily misclassified when using the skeleton data only.</p><p>In TABLE 6, we also find that there are several tough action pairs that are easily confused for all the data modalities, such as "take off a shoe" and "put on a shoe". A possible explantation is that the human motion and object appearance information in these actions are both very similar, and thus they are difficult to be accurately distinguished.</p><p>In summary, the 2D appearance information (e.g., color and texture) and the 3D shape and structure information provided by different data modalities can all affect the recognition performance of different types of actions.</p><p>In TABLE 6, beside presenting the top 10 accurate actions and the top 10 confused action pairs for different data modalities, we also show the results of fusing the three modalities. We observe that when fusing these modalities which provide complementary appearance and geometric information, the recognition performance is improved. For example, when using RGB, depth, or skeleton data only, the recognition accuracies of "walk apart from each other" are 94%, 93%, and 92%, respectively. When fusing the three modalities, the accuracy reaches 99%. We also find that the confusion rates of the tough action pairs can also drop when fusing the three data modalities. For the very tough action pair: "take off a shoe" and "put on a shoe", the confusion rates for different single modalities are 52%, 65%, and 39%, respectively, and the confusion rate decreases to 32% with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Detailed analysis according to methods</head><p>We also analyze the experimental results of different 3D action recognition methods on our dataset in detail. We take five state-of-the-art methods as examples for analysis, namely Internal Feature Fusion <ref type="bibr" target="#b95">[94]</ref>, GCA-LSTM <ref type="bibr" target="#b31">[30]</ref>, Multi-Task Learning Network <ref type="bibr" target="#b50">[49]</ref>, FSNet <ref type="bibr" target="#b104">[103]</ref>, and Multi-Task CNN with RotClips <ref type="bibr">[106]</ref>.</p><p>We first plot the confusion matrices of these methods. The confusion matrix of the method, Internal Feature Fusion, is shown in <ref type="figure" target="#fig_8">Fig. 8</ref> as an example.</p><p>We then perform action-wise analysis for these methods. Specifically, we perform detailed analysis for the top 10 accurate actions and the top 10 confused action pairs of each method (see <ref type="bibr">TABLE 7)</ref>, considering the large number of action classes.</p><p>Among these methods, the method Internal Feature Fusion performs action recognition by fusing the 3D skeletonbased geometric features and the RGB-based appearance features, and the other four approaches all use 3D skeleton data as input for 3D action recognition.</p><p>In TABLE 7, we observe that the top 10 confused action pairs of GCA-LSTM, Multi-Task Learning Network, FSNet, and Multi-Task CNN with RotClips all contain many objectrelated actions (such as "put on jacket" and "play magic cube") and fine-grained hand/finger motion-based actions (such as "make victory sign" and "thumb up"). This is possibly owing to that all of these four approaches perform action recognition based on the 3D skeleton data that is not able to represent the object information and the finegrained finger motions well. Therefore, these approaches have difficulties in dealing with the object-related and finegrained hand/finger motion-based activities. In contrast, we observe that there are many object-related actions (such as "put on jacket", "bounce ball", and "carry things with other person") in the top 10 accurate actions of the method Internal Feature Fusion, which uses both 3D skeleton data and RGB data as input for action recognition.</p><p>In this table, there are also some actions (e.g., "take off a shoe" and "put on a shoe") that are very similar in motions and appearances, and are difficult to be distinguished well by all the five methods.</p><p>We also observe that the actions "walk apart from each other" and "walk towards each other" that have significant motions and discriminative posture patterns are in the top 10 accurate actions of all the methods. In TABLE 7, we observe the top 10 confused action pairs of FSNet contain some two-person mutual action pairs without object involved, such as "hit other person with body" with "support other person with hand", and "kicking other person" with "step on foot of other person", while these actions can be classified relatively reliably by the other approaches. A possible explanation of the performance limitation of FSNet in handling these mutual actions is that in FSNet, the features of the two persons are extracted separately, and these features are then simply averaged for action recognition. This indicates the interaction patterns between the two persons are not well represented, and thus the mutual actions may be easily misclassified by FSNet.</p><p>We also observe that the action "kick backward" is easily confused with "hopping" by the method GCA-LSTM, while "kick backward" is in the top 10 accurate actions of all the other four methods. A possible reason is that GCA-LSTM normalizes the skeleton data for the single-person actions by rotating the skeleton to the frontal view and translating the body center to the origin in each frame at the pre-processing stage. After such normalization, "hopping" could be similar to "kick backward", since the vertical movements of the body center in "hopping" are ignored by the method GCA-LSTM.</p><p>In TABLE 7, "grab other person's stuff" is in the top 10 accurate actions of the method FSNet, while "grab other person's stuff" can be easily confused with "touch other person's pocket (steal)" by Internal Feature Fusion and GCA-LSTM. A possible explanation of the performance disparity is that FSNet may be able to learn the motion speed information better than the other two methods. The human postures of these two actions are quite similar and their main difference is the motion speed, i.e., the motions in "touch other person's pocket (steal)" are very slow, while the motions in "grab other person's stuff" are much faster. Both Internal Feature Fusion and GCA-LSTM use recurrent models to learn the temporal dynamics of the actions based on the sampled 20 frames from each action sequence. This implies the speed information of the actions may be ignored by them. In contrast, FSNet uses a temporal convolutional model to learn the temporal context information over all the frames of each action sample, and thus is able to better learn the motion speed information, which is an important cue to distinguish "grab other person's stuff" from "touch other person's pocket (steal)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Evaluations of One-Shot Recognition</head><p>We evaluate the one-shot recognition performance on our dataset. In our experiments, the auxiliary set for feature generation network training contains 100 action classes, and the evaluation set for one-shot recognition evaluation contains the remaining 20 classes. We compare the following methods for one-shot 3D action recognition:</p><p>(1) Average Pooling. This method is similar to the approach in <ref type="bibr" target="#b95">[94]</ref>. To adapt <ref type="bibr" target="#b95">[94]</ref> for one-shot recognition, we use ST-LSTM <ref type="bibr" target="#b95">[94]</ref> as the feature generation network to learn the features of the body parts at each frame, and during evaluation, the features of all body parts at all frames are aggregated with average pooling. The distance between the testing sample and each exemplar is calculated using the average pooling representation of each video.</p><p>(2) Fully Connected. In this method, the feature generation network is constructed by adding a fully connected layer above the ST-LSTM model. Concretely, the outputs from all spatio-temporal units of the ST-LSTM are concatenated and fed to a fully connected layer to generate a global representation for the input video <ref type="bibr" target="#b31">[30]</ref>. During training, the ST-LSTM and the fully connected layer is trained in an endto-end fashion. During evaluation, the distance is calculated using the global representation of each video.</p><p>(3) Attention Network. This method is similar to the above "Fully Connected" method, except that an attention mechanism <ref type="bibr" target="#b31">[30]</ref> is added to the feature generation network, i.e., the attention scores of different joints are automatically learned in this method.</p><p>(4) APSR. This is the proposed Action-Part Semantic Relevance-aware (APSR) framework, which assigns different scores to different joints (i.e., weighted pooling), by considering the sematic relevance between the novel action's name and body part's name for one-shot recognition.</p><p>The comparison results of these approaches are shown in TABLE 8. The proposed APSR framework achieves the best results, which indicates the generalization capability of the proposed method on the novel action categories. We also observe that the performance of "Attention Network" <ref type="bibr" target="#b31">[30]</ref>, which learns to assign weights to different joints with the attention mechanism, is even weaker than that of "Average Pooling" <ref type="bibr" target="#b95">[94]</ref>. A possible explanation is that the attention ability is trained on the auxiliary set that does not contain the novel actions, thus when handling the novel actions, its performance is even worse than directly performing average pooling. This further demonstrates the superiority of the introduced APSR framework.</p><p>In the aforementioned experiments, the feature generation network of our APSR framework is trained on a large auxiliary set containing about 100 thousand videos. We also try downsizing the auxiliary training set, and evaluate the one-shot 3D action recognition performance on the same evaluation set. The results in TABLE 9 show that the oneshot recognition accuracy drops and the generalization capability to novel classes is weakened, when using fewer classes and samples for learning the feature generation network. This also implies the demand for a large dataset, and it is in line with our motivation for proposing the NTU RGB+D 120 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussions</head><p>The introduction of this very large-scale and challenging dataset with high variability in different aspects (e.g., subjects, environments, camera views, and action categories) will facilitate the users to apply, adapt, develop, and evaluate various learning-based techniques for the future research on human activity analysis. Below we discuss some of the potential research problems and techniques that could be investigated by taking advantage of our dataset:</p><p>(1) Activity analysis with different data modalities. Four different data modalities are provided by our dataset, namely, depth videos, 3D skeleton data, RGB videos, and infrared sequences. Different modalities have different structures of the data, and have different application advantages. Therefore, users can utilize our dataset to investigate the algorithms for depth-based, skeleton-based, RGB-based, or infrared-based action recognition.</p><p>(2) Heterogeneous feature fusion analysis. The provided data modalities contain complementary appearance and 3D geometrical information for human activity analysis. Thus users can take advantage of our dataset to identify the strengths of respective modalities, and further investigate various fusion techniques for the heterogeneous features <ref type="bibr" target="#b44">[43]</ref> extracted from different data modalities.</p><p>(3) Deep network pre-training. Most of the existing datasets for RGB+D action recognition are relative small, thus the deep models evaluated on them often suffer from overfitting issues. Since the proposed dataset has a large number of samples with diversity in various factors, it can also be employed for network pre-training. By initializing the network parameters on our proposed large-scale dataset, the deep models are expected to be able to generalize better on other relative small datasets for 3D activity analysis, as analyzed in <ref type="bibr" target="#b70">[69]</ref>.</p><p>(4) Cross-subject activity analysis. In the proposed dataset, the 106 human subjects are in a wide range of age and height distribution, and are from different cultural backgrounds. These factors bring realistic variation to the quality of actions, and make it possible to have more sensible crosssubject evaluations for the 3D activity analysis methods. This also encourages the community to develop action recognition algorithms that are robust for different subjects.</p><p>(5) Cross-environment activity analysis. Our dataset is collected under different environmental conditions that use 96 different backgrounds with significant illumination variation. This enables the users to perform cross-environment activity analysis. The different collection environments can also facilitate the analysis of the algorithms' robustness against the variation in backgrounds and illuminations.</p><p>(6) Cross-view activity analysis. The proposed dataset is collected with 155 camera views, which facilitates the crossview activity analysis and encourages the users to develop action recognition approaches that are robust against view variation for the practical applications.</p><p>(7) Cross-modal transfer learning. Learning representations from a large labeled modality for transfer learning for the smaller-scale new modalities has attracted a lot of research attention and has been applied to different tasks recently <ref type="bibr" target="#b113">[112]</ref>. The proposed large dataset that provides different data modalities could be utilized for the research on crossmodal transfer learning.</p><p>(8) Mutual activity analysis. Human-human interaction analysis is also an important branch of human activity analysis. The proposed dataset contains 25 thousand twoperson mutual action videos that correspond to 26 different two-person interaction classes. This facilitates the users to investigate and develop various approaches for handling the task of mutual action recognition.</p><p>(9) Real-time skeleton-based early action recognition. 3D skeleton data has shown its advantages in real-time early action recognition due to its succinct and high level representation, as analyzed in <ref type="bibr" target="#b65">[64]</ref>. This indicates our large dataset can also be used for the research on real-time early action recognition.</p><p>(10) One-shot 3D activity analysis. Our large-scale dataset can also be used to learn a discriminative representation model for one-shot 3D activity analysis for novel action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>A large-scale RGB+D action recognition dataset is introduced in this paper. Our dataset includes 114,480 video samples collected from 120 action classes in highly variant camera settings. Compared to the current datasets for this task, our dataset is larger in orders and contains much more variety in different aspects. The large scale of the collected data facilitates us to apply data-driven learning methods to this problem and achieve promising performance. We also propose an APSR framework for one-shot 3D action recognition. The provided experimental results show the availability of large-scale data enables the data-driven learning frameworks to achieve promising results. <ref type="figure">Fig. 9</ref>: Sample frames of the NTU RGB+D 120 dataset. The first four rows show the variety in human subjects, camera views, and environmental conditions. The fifth row depicts the intra-class variation of the performances. The last row illustrates the RGB, RGB+joints, depth, depth+joints, and IR modalities of a sample frame.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of the configuration of 25 body joints in our dataset. The labels of these joints are: (1) base of spine, (2) middle of spine, (3) neck, (4) head, (5) left shoulder, (6) left elbow, (7) left wrist, (8) left hand, (9) right shoulder, (10) right elbow, (11) right wrist, (12) right hand, (13) left hip, (14) left knee, (15) left ankle, (16) left foot, (17) right hip, (18) right knee, (19) right ankle, (20) right foot, (21) spine, (22) tip of left hand, (23) left thumb, (24) tip of right hand, (25) right thumb.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the body part feature generation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Evaluation of using different sizes of training set for action recognition with different methods. In this figure, ratio 1.0 means the full training set is used for network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Evaluation of using different sizes of training set for action recognition with different data modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>Top 10 accurate actions denote the actions that have the top 10 recognition accuracies. (2) Top 10 confused (misclassified) action pairs denote the action pairs that have the top 10 confusion rates (misclassification percentages). (3) A→B denotes a confused action pair, where some samples of class A are misclassified to class B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Confusion matrix of the RGB data modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Confusion matrix of Internal Feature Fusion<ref type="bibr" target="#b95">[94]</ref>. modality fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>The cameras' height and distance to the subjects in each collection setup.</figDesc><table><row><cell cols="6">Setup Height Distance Setup Height Distance</cell></row><row><cell>No.</cell><cell>(m)</cell><cell>(m)</cell><cell>No.</cell><cell>(m)</cell><cell>(m)</cell></row><row><cell>(01)</cell><cell>1.7</cell><cell>3.5</cell><cell>(02)</cell><cell>1.7</cell><cell>2.5</cell></row><row><cell>(03)</cell><cell>1.4</cell><cell>2.5</cell><cell>(04)</cell><cell>1.2</cell><cell>3.0</cell></row><row><cell>(05)</cell><cell>1.2</cell><cell>3.0</cell><cell>(06)</cell><cell>0.8</cell><cell>3.5</cell></row><row><cell>(07)</cell><cell>0.5</cell><cell>4.5</cell><cell>(08)</cell><cell>1.4</cell><cell>3.5</cell></row><row><cell>(09)</cell><cell>0.8</cell><cell>2.0</cell><cell>(10)</cell><cell>1.8</cell><cell>3.0</cell></row><row><cell>(11)</cell><cell>1.9</cell><cell>3.0</cell><cell>(12)</cell><cell>2.0</cell><cell>3.0</cell></row><row><cell>(13)</cell><cell>2.1</cell><cell>3.0</cell><cell>(14)</cell><cell>2.2</cell><cell>3.0</cell></row><row><cell>(15)</cell><cell>2.3</cell><cell>3.5</cell><cell>(16)</cell><cell>2.7</cell><cell>3.5</cell></row><row><cell>(17)</cell><cell>2.5</cell><cell>3.0</cell><cell>(18)</cell><cell>1.8</cell><cell>3.3</cell></row><row><cell>(19)</cell><cell>1.6</cell><cell>3.5</cell><cell>(20)</cell><cell>1.4</cell><cell>4.0</cell></row><row><cell>(21)</cell><cell>1.7</cell><cell>3.2</cell><cell>(22)</cell><cell>1.9</cell><cell>3.4</cell></row><row><cell>(23)</cell><cell>2.0</cell><cell>3.2</cell><cell>(24)</cell><cell>2.4</cell><cell>3.3</cell></row><row><cell>(25)</cell><cell>2.5</cell><cell>3.3</cell><cell>(26)</cell><cell>1.5</cell><cell>2.7</cell></row><row><cell>(27)</cell><cell>1.3</cell><cell>3.5</cell><cell>(28)</cell><cell>1.1</cell><cell>2.9</cell></row><row><cell>(29)</cell><cell>2.5</cell><cell>2.8</cell><cell>(30)</cell><cell>2.4</cell><cell>2.7</cell></row><row><cell>(31)</cell><cell>1.6</cell><cell>3.0</cell><cell>(32)</cell><cell>2.3</cell><cell>3.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Evaluation of using different data modalities (RGB, depth, and 3D skeleton data) for action recognition on the NTU RGB+D 120 dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Data Modality</cell><cell>Cross-Subject Accuracy Cross-Setup Accuracy</cell></row><row><cell></cell><cell></cell><cell cols="3">RGB Video</cell><cell></cell><cell>58.5%</cell><cell>54.8%</cell></row><row><cell></cell><cell></cell><cell cols="3">Depth Video</cell><cell></cell><cell>48.7%</cell><cell>40.1%</cell></row><row><cell></cell><cell></cell><cell cols="5">3D Skeleton Sequence</cell><cell>55.7%</cell><cell>57.9%</cell></row><row><cell></cell><cell></cell><cell cols="5">RGB Video + Depth Video</cell><cell>61.9%</cell><cell>59.2%</cell></row><row><cell></cell><cell></cell><cell cols="5">RGB Video + 3D Skeleton Sequence</cell><cell>61.2%</cell><cell>63.1%</cell></row><row><cell></cell><cell></cell><cell cols="5">Depth Video + 3D Skeleton Sequence</cell><cell>59.2%</cell><cell>61.2%</cell></row><row><cell></cell><cell></cell><cell cols="5">RGB Video + Depth Video + 3D Skeleton Sequence</cell><cell>64.0%</cell><cell>66.1%</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Part-Aware LSTM</cell></row><row><cell>Accuracy</cell><cell>0.4 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spatio-Temporal LSTM Internal Feature Fusion GCA-LSTM FSNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Two-Stream Attention LSTM</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell cols="4">Ratio of used training set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Action recognition results of different data modalities on the NTU RGB+D 120 dataset.</figDesc><table><row><cell>Data Modality</cell><cell>Top 10 accurate actions</cell><cell>Top 10 confused (misclassified) action pairs</cell></row><row><cell></cell><cell>1. walk apart from each other</cell><cell>1. take off a shoe→put on a shoe</cell></row><row><cell></cell><cell>2. walk towards each other</cell><cell>2. kick backward→side kick</cell></row><row><cell></cell><cell>3. hopping</cell><cell>3. rub two hands together→clapping</cell></row><row><cell></cell><cell>4. carry things with other person</cell><cell>4. reading→writing</cell></row><row><cell>RGB</cell><cell>5. arm swings</cell><cell>5. clapping→rub two hands together</cell></row><row><cell></cell><cell>6. staggering</cell><cell>6. vomiting condition→bow</cell></row><row><cell></cell><cell>7. pick up things</cell><cell>7. ball up paper→fold paper</cell></row><row><cell></cell><cell>8. put on jacket</cell><cell>8. open a box→fold paper</cell></row><row><cell></cell><cell>9. hugging other person</cell><cell>9. both hands up→cheer up</cell></row><row><cell></cell><cell>10. move heavy objects</cell><cell>10. yawn→blow nose</cell></row><row><cell></cell><cell>1. carry things with other person</cell><cell>1. take off a shoe→put on a shoe</cell></row><row><cell></cell><cell>2. walk apart from each other</cell><cell>2. reading→writing</cell></row><row><cell></cell><cell>3. move heavy objects</cell><cell>3. playing with tablet→writing</cell></row><row><cell></cell><cell>4. hugging other person</cell><cell>4. bow→vomiting condition</cell></row><row><cell>Depth</cell><cell>5. kick backward</cell><cell>5. both hands up→stretch oneself</cell></row><row><cell></cell><cell>6. walk towards each other</cell><cell>6. put on jacket→put on bag/backpack</cell></row><row><cell></cell><cell>7. hopping</cell><cell>7. vomiting condition→bow</cell></row><row><cell></cell><cell>8. arm swings</cell><cell>8. cheer up→both hands up</cell></row><row><cell></cell><cell>9. staggering</cell><cell>9. rub two hands together→clapping</cell></row><row><cell></cell><cell>10. open a box</cell><cell>10. take off bag/backpack→take off jacket</cell></row><row><cell></cell><cell>1. walk apart from each other</cell><cell>1. put on a shoe→take off a shoe</cell></row><row><cell></cell><cell>2. standing up</cell><cell>2. hit other person with object→wield knife towards other person</cell></row><row><cell></cell><cell>3. walk towards each other</cell><cell>3. make ok sign→make victory sign</cell></row><row><cell></cell><cell>4. hugging other person</cell><cell>4. thumb up→make victory sign</cell></row><row><cell>Skeleton</cell><cell>5. arm swings</cell><cell>5. put on jacket→put on bag/backpack</cell></row><row><cell></cell><cell>6. squat down</cell><cell>6. touch other person's pocket (steal)→grab other person's stuff</cell></row><row><cell></cell><cell>7. sitting down</cell><cell>7. make victory sign→make ok sign</cell></row><row><cell></cell><cell>8. pushing other person</cell><cell>8. play magic cube→counting money</cell></row><row><cell></cell><cell>9. arm circles</cell><cell>9. take a photo of other person→shoot at other person with a gun</cell></row><row><cell></cell><cell>10. kick backward</cell><cell>10. handshaking→giving something to other person</cell></row><row><cell></cell><cell>1. walk apart from each other</cell><cell>1. take off a shoe→put on a shoe</cell></row><row><cell></cell><cell>2. carry things with other person</cell><cell>2. vomiting condition→bow</cell></row><row><cell></cell><cell>3. hugging other person</cell><cell>3. both hands up→stretch oneself</cell></row><row><cell>RGB</cell><cell>4. walk towards each other</cell><cell>4. clapping→rub two hands together</cell></row><row><cell>+ Depth</cell><cell>5. standing up</cell><cell>5. yawn→blow nose</cell></row><row><cell>+ Skeleton</cell><cell>6. hopping</cell><cell>6. put on a shoe→take off a shoe</cell></row><row><cell></cell><cell>7. squat down</cell><cell>7. hush (say quite)→blow nose</cell></row><row><cell></cell><cell>8. move heavy objects</cell><cell>8. rub two hands together→clapping</cell></row><row><cell></cell><cell>9. arm swings</cell><cell>9. reading→writing</cell></row><row><cell></cell><cell>10. put on jacket</cell><cell>10. stretch oneself→both hands up</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Action recognition results of different methods on the NTU RGB+D 120 dataset.</figDesc><table><row><cell>Method</cell><cell>Top 10 accurate actions</cell><cell>Top 10 confused (misclassified) action pairs</cell></row><row><cell></cell><cell>1. walk apart from each other</cell><cell>1. put on a shoe→take off a shoe</cell></row><row><cell></cell><cell>2. walk towards each other</cell><cell>2. blow nose→hush (say quite)</cell></row><row><cell></cell><cell>3. carry things with other person</cell><cell>3. clapping→rub two hands together</cell></row><row><cell>Internal</cell><cell>4. hugging other person</cell><cell>4. take off a shoe→put on a shoe</cell></row><row><cell>Feature</cell><cell>5. standing up</cell><cell>5. rub two hands together→clapping</cell></row><row><cell>Fusion</cell><cell>6. kick backward</cell><cell>6. stretch oneself→both hands up</cell></row><row><cell>[94]</cell><cell>7. take off jacket</cell><cell>7. yawn→blow nose</cell></row><row><cell></cell><cell>8. arm swings</cell><cell>8. bow→vomiting condition</cell></row><row><cell></cell><cell>9. put on jacket</cell><cell>9. grab other person's stuff→touch other person's pocket (steal)</cell></row><row><cell></cell><cell>10. bounce ball</cell><cell>10. vomiting condition→sneeze/cough</cell></row><row><cell></cell><cell>1. walk apart from each other</cell><cell>1. put on a shoe→take off a shoe</cell></row><row><cell></cell><cell>2. standing up</cell><cell>2. play magic cube→counting money</cell></row><row><cell></cell><cell>3. walk towards each other</cell><cell>3. make victory sign→make ok sign</cell></row><row><cell></cell><cell>4. hugging other person</cell><cell>4. hit other person with object→wield knife towards other person</cell></row><row><cell>GCA-LSTM</cell><cell>5. high five</cell><cell>5. take something out of a bag/backpack→put something into a bag/backpack</cell></row><row><cell>[30]</cell><cell>6. handshaking</cell><cell>6. kick backward→hopping</cell></row><row><cell></cell><cell>7. arm swings</cell><cell>7. staggering→kick backward</cell></row><row><cell></cell><cell>8. sitting down</cell><cell>8. put on jacket→put on bag/backpack</cell></row><row><cell></cell><cell>9. arm circles</cell><cell>9. giving something to other person→exchange things with other person</cell></row><row><cell></cell><cell>10. squat down</cell><cell>10. grab other person's stuff→touch other person's pocket (steal)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>The results of one-shot 3D action recognition on the NTU RGB+D 120 dataset.</figDesc><table><row><cell>Method</cell><cell>Evaluation Accuracy</cell></row><row><cell>Average Pooling [94]</cell><cell>42.9%</cell></row><row><cell>Fully Connected [30]</cell><cell>42.1%</cell></row><row><cell>Attention Network [30]</cell><cell>41.0%</cell></row><row><cell>APSR</cell><cell>45.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>The results of using different sizes of auxiliary training set to learn the feature generation network, for oneshot recognition on the novel action classes.</figDesc><table><row><cell cols="2">Auxiliary Training Set</cell><cell>Evaluation</cell></row><row><cell cols="2">#Training Samples #Training Classes</cell><cell>Accuracy</cell></row><row><cell>19, 000</cell><cell>20</cell><cell>29.1%</cell></row><row><cell>38, 000</cell><cell>40</cell><cell>34.8%</cell></row><row><cell>57, 000</cell><cell>60</cell><cell>39.2%</cell></row><row><cell>76, 000</cell><cell>80</cell><cell>42.8%</cell></row><row><cell>95, 000</cell><cell>100</cell><cell>45.3%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. We emphasize the difference between RGBD and RGB+D terms. We suggest using RGBD when the two modalities are aligned pixelwise, and RGB+D when the resolutions of the two are different and frames are not aligned.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">point finger at other person→shoot at other person with a gun 3. walk towards each other 3. slapping other person→hit other person with object Multi-Task 4. arm swings 4. rub two hands together→apply cream on hand back Learning 5. kick backward 5. cutting paper using scissors→staple book Network 6. cheers and drink 6. yawn→hush (say quite) on back of other person→hit other person with object 9. running on the spot 9. kicking other person→step on foot of other person 10. walk towards each other 10. hit other person with body→support other person with hand towards each other 5</title>
		<imprint/>
	</monogr>
	<note>staple book→cutting paper using scissors RotClips 6. squat down 6. yawn→hush (say quite</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">swings 7. giving something to other person→exchange things with other person</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">3d object recognition in cluttered scenes with local surface features: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR Letters</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THMS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human activity detection from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human daily action analysis with multi-view and color-depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">IJRR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling 4d humanobject interactions for event and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hopc: Histogram of oriented principal components of 3d pointclouds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<title level="m">3d human activity recognition with reconfigurable convolutional neural networks,&quot; in ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-modal &amp; multi-view &amp; interactive benchmark dataset for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2186" to="2200" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relevance-based word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on human motion analysis from depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time-of-flight and depth imaging. sensors, algorithms, and applications</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rgb-dbased action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rgb-dbased human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of human motion analysis using depth imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR Letters</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Space-time representation of people based on 3d skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A survey of applications and human motion recognition with microsoft kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">IJPRAI</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coding kendall&apos;s shape trajectories for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tanfous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Range-sample depth feature for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep multimodal feature analysis for action recognition in rgb+d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilinear heterogeneous information machine for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">G3d: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Feature boosting network for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog 2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCCSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Real time action recognition using histograms of depth gradients and random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of long-term motion dynamics for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Global regularizer and temporal-aware crossentropy for skeleton-based early action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning action recognition model from depth and skeleton videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">When kernel methods meet feature learning: Log-covariance network for action recognition from skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Pose-conditioned spatio-temporal attention for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep bilinear learning for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Fusing geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A real-time and hardware-efficient processor for skeleton-based action recognition with lightweight convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS-II</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Computer vision for human-machine interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision for Assistive Healthcare</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">3d action recognition using data visualization and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning and refining of privileged information-based rnns for action recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Scene flow to action map: A new representation for rgb-d based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Skeletonnet: Mining deep part features for 3-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">One-shot action localization by learning sequence matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multi-attention network for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">One-shot learning for real-time action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<editor>ICPRIA</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Explore efficient local features from rgb-d data for one-shot learning gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">One-shot-learning gesture recognition using hog-hof features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">A novel hierarchical framework for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Skeletonbased action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ASRU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Representation learning for very short texts using weighted word embedding aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Canneyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhoedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">PRL</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Leveraging weak semantic relevance for complex video event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global contextaware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Ssnet: Scale selection network for online 3d action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
