<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
							<email>bichen@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
							<email>xdai@princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
							<email>yanghan@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<email>feisun@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
							<email>wyiming@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
							<email>yuandong@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
							<email>vajdap@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too resource demanding for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize Con-vNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets (Facebook-Berkeley-Nets), a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3[17] with similar accuracy. Despite higher accuracy and lower latency than MnasNet[20], we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPUhours. Searched for different resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher accuracy than Mo-bileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-Xoptimized model achieves a 1.4x speedup on an iPhone X. FBNet models are open-sourced at https://github. com/facebookresearch/mobile-vision. * Work done while interning at Facebook. … … Stochastic super net Distribution Operators Probability Training super net Proxy dataset Sampling Operator Latency LUT Deploy Target device Benchmark … Search space … … Neural Architectures Figure 1. Differentiable neural architecture search (DNAS) for ConvNet design. DNAS explores a layer-wise space that each layer of a ConvNet can choose a different block. The search space is represented by a stochastic super net. The search process trains the stochastic super net using SGD to optimize the architecture distribution. Optimal architectures are sampled from the trained distribution. The latency of each operator is measured on target devices and used to compute the loss for the super net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ConvNets are the de facto method for computer vision. In many computer vision tasks, a better ConvNet design usually leads to significant accuracy improvement. In previous works, accuracy improvement comes at the cost of higher computational complexity, making it more challenging to deploy ConvNets to mobile devices, on which computing capacity is limited. Instead of solely focusing on accuracy, recent work also aims to optimize for efficiency, especially latency. However, designing efficient and accurate ConvNets is difficult due to the challenges below.</p><p>Intractable design space: The design space of a Con-vNet is combinatorial. Using VGG16 <ref type="bibr" target="#b17">[18]</ref> as a motivating example: VGG16 contains 16 layers. Assume for each layer of the network, we can choose a different kernel size from {1, 3, 5} and a different filter number from {32, 64, 128, 256, 512}. Even with such simplified design choices and shallow layers, the design space contains (3 × 5) <ref type="bibr" target="#b15">16</ref>   training a ConvNet is very time-consuming, typically taking days or even weeks. As a result, previous ConvNet design rarely explores the design space. A typical flow of manual ConvNet design is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(a). Designers propose initial architectures and train them on the target dataset. Based on the performance, designers evolve the architectures accordingly. Limited by the time cost of training ConvNets, the design flow has to stop after a few iterations, which is far too few to sufficiently explore the design space.</p><p>Starting from <ref type="bibr" target="#b29">[30]</ref>, recent works adopt neural architecture search (NAS) to explore the design space automatically. Many previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> use reinforcement learning (RL) to guide the search and a typical flow is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b). A controller samples architectures from the search space to be trained. To reduce the training cost, sampled architectures are trained on a smaller proxy dataset such as CIFAR-10 or trained for fewer epochs on ImageNet. The performance of the trained networks is then used to train and improve the controller. Previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> has demonstrated the effectiveness of such methods in finding accurate and efficient ConvNet models. However, training each architecture is still timeconsuming, and it usually takes thousands of architectures to train the controller. As a result, the computational cost of such methods is prohibitively high.</p><p>Nontransferable optimality: the optimality of ConvNet architectures is conditioned on many factors such as input resolutions and target devices. Once these factors change, the optimal architecture is likely to be different. A common practice to reduce the FLOP count of a network is to shrink the input resolution. A smaller input resolution may require a smaller receptive field of the network and therefore shallower layers. On a different device, the same operator can have different latency, so we need to adjust the ConvNet architecture to achieve the best accuracy-efficiency trade-off. Ideally, we should design different ConvNet architectures case-by-case. In practice, however, limited by the computational cost of previous manual and automated approaches, we can only realistically design one ConvNet and use it for all conditions. Inconsistent efficiency metrics: Most of the efficiency metrics we care about are dependent on not only the Con-vNet architecture but also the hardware and software configurations on the target device. Such metrics include latency, power, energy, and in this paper, we mainly focus on latency. To simplify the problem, most of the previous works adopt hardware-agnostic metrics such as FLOPs (more strictly, number of multiply-add operations) to evaluate a ConvNet's efficiency. However, a ConvNet with lower FLOP count is not necessarily faster. For example, NasNet-A <ref type="bibr" target="#b30">[31]</ref> has a similar FLOP count as MobileNetV1 <ref type="bibr" target="#b5">[6]</ref>, but its complicated and fragmented cell-level structure is not hardware friendly, so the actual latency is slower <ref type="bibr" target="#b16">[17]</ref>. The inconsistency between hardware agnostic metrics and actual efficiency makes the ConvNet design more difficult.</p><p>To address the above problems, we propose to use differentiable neural architecture search (DNAS) to discover hardware-aware efficient ConvNets. The flow of our algorithm is illustrated in <ref type="figure">Figure 1</ref>. DNAS allows us to explore a layer-wise search space where we can choose a different block for each layer of the network. Following <ref type="bibr" target="#b20">[21]</ref>, DNAS represents the search space by a super net whose operators execute stochastically. We relax the problem of finding the optimal architecture to find a distribution that yields the optimal architecture. By using the Gumbel Softmax technique <ref type="bibr" target="#b8">[9]</ref>, we can directly train the architecture distribution using gradient-based optimization such as SGD. The search process is extremely fast compared with previous reinforcement learning (RL) based method. The loss used to train the stochastic super net consists of both the cross-entropy loss that leads to better accuracy and the latency loss that penalizes the network's latency on a target device. To estimate the latency of an architecture, we measure the latency of each operator in the search space and use a lookup table model to compute the overall latency by adding up the latency of each operator. Using this model allows us to quickly estimate the latency of architectures in this enormous search space. More importantly, it makes the latency differentiable with respect to layer-wise block choices.</p><p>We name the models discovered by DNAS as FBNets (Facebook-Berkeley-Nets). FBNets surpass the state-ofthe-art efficient ConvNets designed manually and automatically. FBNet-B achieves 74.1% top-1 accuracy with 295M FLOPs and 23.1 ms latency on an Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3. Being better than MnasNet, FBNet-B's search cost is 216 GPU-hours, 421x lower than the cost for MnasNet estimated based on <ref type="bibr" target="#b19">[20]</ref>. Such low search cost enables us to re-design ConvNets case-by-case. For different resolution and channel scaling, FBNets achieve 1.5% to 6.4% absolute gain in top-1 accuracy compared with MobileNetV2 models. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) with a batch size of 1 on Samsung S8. Using DNAS to search for device-specific ConvNet, an iPhone-x-optimized model achieves 1.4x speedup on an iPhone X compared with a Samsung-optimized model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Efficient ConvNet models: Designing efficient Con-vNet has attracted many research attention in recent years. SqueezeNet <ref type="bibr" target="#b7">[8]</ref> is one of the early works focusing on reducing the parameter size of ConvNet models. It is originally designed for classification, but later extended to object detection <ref type="bibr" target="#b21">[22]</ref> and LiDAR point-cloud segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. Following SqueezeNet, SqueezeNext <ref type="bibr" target="#b2">[3]</ref> and ShiftNet <ref type="bibr" target="#b22">[23]</ref> achieve further parameter size reduction. Recent works change the focus from parameter size to FLOPs. Mo-bileNetV1 and MobileNetV2 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> use depthwise convolutions to replace the more expensive spatial convolutions. ShuffleNet <ref type="bibr" target="#b28">[29]</ref> uses group convolution and shuffle operations to reduce the FLOP count further. More recent works realize that FLOP count does not always reflect the actual hardware efficiency. To improve actual latency, ShuffleNetV2 <ref type="bibr" target="#b12">[13]</ref> proposes a series of practical guidelines for efficient ConvNet design. Synetgy <ref type="bibr" target="#b27">[28]</ref> combines ideas from ShuffleNetV2 and ShiftNet to co-design hardware friendly ConvNets and FPGA accelerators.</p><p>Neural Architecture Search: <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> first proposes to use reinforcement learning (RL) to search for neural architectures to achieve competitive accuracy with low FLOPs. Early NAS methods are computationally expensive. Recent works try to reduce the computational cost by weight sharing <ref type="bibr" target="#b15">[16]</ref> or using gradient-based optimization <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1]</ref> further develop the idea of differentiable neural architecture search combining Gumbel Softmax <ref type="bibr" target="#b8">[9]</ref>. Early works of NAS <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12]</ref> focus on the cell level architecture search, and the same cell structure is repeated in all layers of a network. However, such fragmented and complicated celllevel structures are not hardware friendly, and the actual efficiency is low. Most recently, <ref type="bibr" target="#b19">[20]</ref> explores a stage-level hierarchical search space, allowing different blocks for different stages of a network, while blocks inside a stage are still the same. Instead of focusing on FLOPs, <ref type="bibr" target="#b19">[20]</ref> aims to optimize the latency on target devices. Besides searching for new architectures, works such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref> focus on adapting existing models to improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this paper, we use differentiable neural architecture search (DNAS) to solve the problem of ConvNet design. We formulate the neural architecture search problem as min a∈A min wa L(a, w a ).</p><p>(1)</p><p>Given an architecture space A, we seek to find an optimal architecture a ∈ A such that after training its weights w a , it can achieve the minimal loss L(a, w a ). In our work, we focus on three factors of the problem: a) the search space A. b) The loss function L(a, w a ) that considers actual latency. c) An efficient search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Search Space</head><p>Previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> focus on cell level architecture search. Once a cell structure is searched, it is used in all the layers across the network. However, many searched cell structures are very complicated and fragmented and are therefore slow when deployed to mobile CPUs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref>. Besides, at different layers, the same cell structure can have a different impact on the accuracy and latency of the overall network. As shown in <ref type="bibr" target="#b19">[20]</ref> and in our experiments, allowing different layers to choose different blocks leads to better accuracy and efficiency.</p><p>In this work, we construct a layer-wise search space with a fixed macro-architecture, and each layer can choose a different block. The macro-architecture is described in <ref type="table">Table  1</ref>. The macro architecture defines the number of layers and the input/output dimensions of each layer. The first and the last three layers of the network have fixed operators. For the rest of the layers, their block type needs to be searched. The filter numbers for each layer are hand-picked empirically. We use relatively small channel sizes for early layers, since the input resolution at early layers is large, and the computational cost (FLOP count) is quadratic to input size.</p><p>Each searchable layer in the network can choose a different block from the layer-wise search space. The block structure is inspired by MobileNetV2 <ref type="bibr" target="#b16">[17]</ref> and ShiftNet <ref type="bibr" target="#b22">[23]</ref>, and is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. It contains a point-wise (1x1) convolution, a K-by-K depthwise convolution where K denotes the kernel size, and another 1x1 convolution. "ReLU" activation functions follow the first 1x1 convolution and the depthwise convolution, but there are no activation functions following the last 1x1 convolution. If the output dimension stays the same as the input dimension, we use a skip connection to add the input to the output. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>, we use a hyperparameter, the expansion ratio e, to control  <ref type="table">Table 1</ref>. Macro-architecture of the search space. Column-"Block" denotes the block type. "TBS" means layer type needs to be searched. Column-f denotes the filter number of a block. Columnn denotes the number of blocks. Column-s denotes the stride of the first block in a stage. The filter size of the last 1x1 conv is 1504 for FBNet-A and 1984 for FBNet-{B, C}. the block. It determines how much do we expand the output channel size of the first 1x1 convolution compared with its input channel size. Following <ref type="bibr" target="#b19">[20]</ref>, we also allow choosing a kernel size of 3 or 5 for the depthwise convolution.</p><formula xml:id="formula_0">1x1 (group) Conv, ReLU K x K DWConv, ReLU H x W x C in H x W x (e x C in ) (H/s) x (W/s) x (e x C in ) (H/s) x (W/s) x C out + 1x1 (group) Conv</formula><p>In addition, we can choose to use group convolution for the first and the last 1x1 convolution to reduce the computation complexity. When we use group convolution, we follow <ref type="bibr" target="#b28">[29]</ref> to add a channel shuffle operation to mix the information between channel groups.</p><p>In our experiments, our layer-wise search space contains 9 candidate blocks, with their configurations listed in <ref type="table">Table  2</ref>. Note we also have a block called "skip", which directly feed the input feature map to the output without actual computations. This candidate block essentially allows us to reduce the depth of the network.</p><p>In summary, our overall search space contains 22 layers and each layer can choose from 9 candidate blocks from Block type expansion <ref type="table">Kernel Group  k3 e1  1  3  1  k3 e1 g2  1  3  2  k3 e3  3  3  1  k3 e6  6  3  1  k5 e1  1  5  1  k5 e1 g2  1  5  2  k5 e3  3  5  1  k5 e6  6  5  1  skip</ref> --- <ref type="table">Table 2</ref>. Configurations of candidate blocks in the search space. <ref type="table">Table 2</ref>, so it contains 9 22 ≈ 10 21 possible architectures.</p><p>Finding the optimal layer-wise block assignment from such enormous search space is a non-trivial task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latency-Aware Loss Function</head><p>The loss function used in (1) has to reflect not only the accuracy of a given architecture but also the latency on the target hardware. To achieve this goal, we define the following loss function:</p><formula xml:id="formula_1">L(a, w a ) = CE(a, w a ) · α log(LAT(a)) β .<label>(2)</label></formula><p>The first term CE(a, w a ) denotes the cross-entropy loss of architecture a with parameter w a . The second term LAT(a) denotes the latency of the architecture on the target hardware measured in micro-second. The coefficient α controls the overall magnitude of the loss function. The exponent coefficient β modulates the magnitude of the latency term. The cross-entropy term can be easily computed. However, the latency term is more difficult, since we need to measure the actual runtime of an architecture on a target device. To cover the entire search space, we need to measure about 10 21 architectures, which is an impossible task.</p><p>To solve this problem, we use a latency lookup table model to estimate the overall latency of a network based on the runtime of each operator. More formally, we assume</p><formula xml:id="formula_2">LAT(a) = l LAT(b (a) l ),<label>(3)</label></formula><p>where b</p><p>(a) l denotes the block at layer-l from architecture a. This assumes that on the target processor, the runtime of each operator is independent of other operators. The assumption is valid for many mobile CPUs and DSPs, where operators are computed sequentially one by one. This way, by benchmarking the latency of a few hundred operators used in the search space, we can easily estimate the actual runtime of the 10 21 architectures in the entire search space. More importantly, as will be explained in section 3.3, using the lookup table model makes the latency term in the loss function (2) differentiable with respect to layer-wise block choices, and this allows us to use gradient-based optimization to solve problem (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Search Algorithm</head><p>Solving the problem (1) through brute-force enumeration of the search space is very infeasible. The inner problem of optimizing w a involves training a neural network. For ImageNet classification, training a ConvNet typically takes several days or even weeks. The outer problem of optimizing a ∈ A has a combinatorially large search space.</p><p>Most of the early works on NAS <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> follow the paradigm above. To reduce the computational cost, the inner problem is replaced by training candidate architectures on an easier proxy dataset. For example, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> trains the architecture on the CIFAR10 dataset, and <ref type="bibr" target="#b19">[20]</ref> trains on Im-ageNet but only for 5 epochs. The learned architectures are then transferred to the target dataset. To avoid exhaustively iterating through the search space, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> use reinforcement learning to guide the exploration. Despite these improvements, solving problem (1) is still prohibitively expensive -training a network on the proxy dataset is still time-consuming, and thousands of architectures need to be trained before reaching the optimal solution.</p><p>We adopt a different paradigm of solving problem (1). We first represent the search space by a stochastic super net. The super net has the same macro-architecture as described in <ref type="table">Table 1</ref>, and each layer contains 9 parallel blocks as described in <ref type="table">Table 2</ref>. During the inference of the super net, only one candidate block is sampled and executed with the sampling probability of</p><formula xml:id="formula_3">P θ l (b l = b l,i ) = softmax(θ l,i ; θ l ) = exp(θ l,i ) i exp(θ l,i ) .<label>(4)</label></formula><p>θ l contains parameters that determine the sampling probability of each block at layer-l. Equivalently, the output of layer-l can be expressed as</p><formula xml:id="formula_4">x l+1 = i m l,i · b l,i (x l ),<label>(5)</label></formula><p>where m l,i is a random variable in {0, 1} and is evaluated to 1 if block b l,i is sampled. The sampling probability is determined by equation <ref type="formula" target="#formula_3">(4)</ref>. b l,i (x l ) denotes the output of block-i at layer l given the input feature map x l . We let each layer sample independently, therefore, the probability of sampling an architecture a can be described as</p><formula xml:id="formula_5">P θ (a) = l P θ l (b l = b (a) l,i ),<label>(6)</label></formula><p>where θ denotes the a vector consists of all the θ l,i for each block-i at layer-l. b l,i denotes that in the sampled architecture a, block-i is chosen at layer-l.</p><p>Instead of solving for the optimal architecture a ∈ A, which has a discrete search space, we relax the problem to optimize the probability P θ of the stochastic super net to achieve the minimum expected loss. Formally, we re-write the discrete optimization problem (1) as min θ min wa E a∼P θ {L(a, w a )}.</p><p>It is obvious the loss function in <ref type="formula" target="#formula_6">(7)</ref> is differentiable with respect to the architecture weights w a and therefore can be optimized by stochastic gradient descent (SGD). However, the loss is not directly differentiable to the sampling parameter θ, since we cannot pass the gradient through the discrete random variable m l,i to θ l,i . To sidestep this, we relax the discrete mask variable m l,i to be a continuous random variable computed by the Gumbel Softmax function <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> m l,i = GumbelSoftmax(θ l,i |θ l )</p><formula xml:id="formula_7">= exp[(θ l,i + g l,i )/τ ] i exp[(θ l,i + g l,i )/τ ] ,<label>(8)</label></formula><p>where g l,i ∼ Gumbel(0, 1) is a random noise following the Gumbel distribution. The Gumbel Softmax function is controlled by a temperature parameter τ . As τ approaches 0, it approximates the discrete categorical sampling following the distribution in <ref type="bibr" target="#b5">(6)</ref>. As τ becomes larger, m l,i becomes a continuous random variable. Regardless of the value of τ , the mask m l,i is directly differentiable with respect to the parameter θ l,i . The technique of using Gumbel Softmax for neural architecture search is also proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1]</ref>. As a result, it is clear that the cross-entropy term from the loss function <ref type="formula" target="#formula_1">(2)</ref> is differentiable with respect to the mask m l,i and therefore θ l,i . For the latency term, since we use the lookup table based model for efficiency estimation, equation <ref type="formula" target="#formula_2">(3)</ref> can be written as</p><formula xml:id="formula_8">LAT(a) = l i m l,i · LAT(b l,i ).<label>(9)</label></formula><p>The latency of each operator LAT(b l,i ) is a constant coefficient, so the overall latency of architecture-a is differentiable with respect to the mask m l,i , therefore θ l,i . As a result, the loss function <ref type="formula" target="#formula_1">(2)</ref> is fully differentiable with respect to both weights w a and the architecture distribution parameter θ. This allows us to use SGD to efficiently solve problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>Our search process is now equivalent to training the stochastic super net. During the training, we compute ∂L/∂w a to train each operator's weight in the super net. This is no different from training an ordinary ConvNet. After operators get trained, different operators can have a different contribution to the accuracy and the efficiency of the overall network. Therefore, we compute ∂L/∂θ to update the sampling probability P θ for each operator. This step selects operators with better accuracy and lower latency and suppresses the opposite ones. After the super net training finishes, we can then obtain the optimal architectures by sampling from the architecture distribution P θ .</p><p>As will be shown in the experiment section, the proposed DNAS algorithm is orders of magnitude faster than previous RL based NAS while generating better architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>To demonstrate the efficacy of our proposed method, we use DNAS to search for ConvNet models on ImageNet 2012 classification dataset <ref type="bibr" target="#b1">[2]</ref>, and we name the discovered models FBNets. We aim to discover models with high accuracy and low latency on target devices. In our first experiment, we target Samsung Galaxy S8 with a Qualcomm Snapdragon 835 platform. The model is deployed with Caffe2 with int8 inference engine for mobile devices.</p><p>Before the search starts, we first build a latency lookup table described in section 3.2 on the target device. Next, we train a stochastic super net with search space described in section 3.3. We set the input resolution of the network to 224-by-224. To reduce the training time, we randomly choose 100 classes from the original 1000 classes to train the stochastic super net. We train the stochastic super net for 90 epochs. In each epoch, we first train the operator weights w a and then the architecture probability parameter θ. w a is trained on 80% of ImageNet training set using SGD with momentum. The architecture distribution parameter θ is trained on the rest 20% of ImageNet training set with Adam optimizer <ref type="bibr" target="#b9">[10]</ref>. To control the temperature of the Gumbel Softmax from equation <ref type="formula" target="#formula_7">(8)</ref>, we use an exponentially decaying temperature. After the search finishes, we sample several architectures from the trained distribution P θ , and train them from scratch. Our architecture search framework is implemented in pytorch <ref type="bibr" target="#b14">[15]</ref> and searched models are trained in Caffe2. More training details will be provided in the supplementary materials.</p><p>Our experiment results are summarized in <ref type="table">Table 3</ref>. We compare our searched models with state-of-the-art efficient models both designed automatically and manually. The primary metrics we care about are top-1 accuracy on the Im-ageNet validation set and the latency. If the latency is not available, we use FLOP as the secondary efficiency metric. For baseline models, we directly cite the parameter size, FLOP count, and top-1 accuracy from the original paper. Since our network is deployed with caffe2 with highly efficient in8 implementation, we have an unfair latency advantage against other baselines. Therefore, we implement the baseline models ourselves and measure their latency under the same environment for a fair comparison. For automatically designed models, we also compare the search method, search space, and search cost. <ref type="table">Table 3</ref> divides the models into three categories according to their accuracy level. In the first group, FBNet-A achieves 73.0% accuracy, better than 1.0-MobileNetV2 (+1.0%), 1.5-ShuffleNet V2 (+0.4%), and CondenseNet (+2%), and are on par with DARTS and MnasNet-65. Regarding latency, FBNet-A is 1.9 ms (relative 9.6%), 2.2 ms (relative 11%), and 8.6 ms (relative 43%) better than the MobileNetV2, ShuffleNetV2, and CondenseNet counterparts. Although we did not optimize for FLOP count directly, FBNet-A's FLOP count is only 249M, 50M smaller (relative 20%) than MobileNetV2 and ShuffleNetV2, 20M (relative 8%) smaller than MnasNet, and 2.4X smaller than DARTS. In the second group, FBNet-B achieves comparable accuracy with 1.3-MobileNetV2, but the latency is 1.46x lower, and the FLOP count is 1.73x smaller, even smaller than 1.0-MobileNetV2 and 1.5-ShuffleNet V2. Compared with MnasNet, FBNet-B's accuracy is 0.1% higher, latency is 0.6ms lower, and FLOP count is 22M (relative 7%) smaller. We do not have the latency of NASNet-A and PNASNet, but the accuracy is comparable, and the FLOP count is 1.9x and 2.0x smaller. In the third group, FBNet-C achieves 74.9% accuracy, same as 2.0-ShuffleNetV2 and better than all others. The latency is 28.1 ms, 1.33x and 1.19x faster than MobileNet and Shuf-fleNet. The FLOP count is 1.56x, 1.58x, and 1.03x smaller than MobileNet, ShuffleNet, and MnasNet-92.</p><p>Among all the automatically searched models, FBNet's performance is much stronger than DARTS, PNAS, and NAS, and better than MnasNet. However, the search cost is orders of magnitude lower. MnasNet <ref type="bibr" target="#b19">[20]</ref> does not disclose the exact search cost (in terms of GPU-hours). However, it mentions that the controller samples 8,000 models during the search and each model is trained for five epochs. According to our experiments, training of MNas-Net for one epoch takes 17 minutes using 8 GPUs. So the estimated cost for training 8,000 models for 5 epochs is about 17/60 × 5 × 8 × 8, 000 ≈ 91 × 10 3 GPU hours. In comparison, the FBNet search takes 8 GPUs for only 27 hours, so the computational cost is only 216 GPU hours, or 421x faster than MnasNet, 222x faster than NAS, 27.8x faster than PNAS, and 1.33x faster than DARTS.</p><p>We visualize some of our searched FBNets, Mo-bileNetV2, and MnasNet in <ref type="figure">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Different Resolution and Channel Size Scaling</head><p>A common technique to reduce the computational cost of a ConvNet is to reduce the input resolution or channel size without changing the ConvNet structure. This approach is likely to be sub-optimal. We hypothesize that with a different input resolution and channel size scaling, the optimal ConvNet structure will be different. To test this, we use DNAS to search for several different combinations of input resolution and channel size scaling. Thanks to the superior efficiency of DNAS, we can finish the search very quickly. The result is summarized in  <ref type="table">Table 3</ref>. ImageNet classification performance compared with baselines. For baseline models, we directly cite the parameter size, FLOP count and top-1 accuracy on the ImageNet validation set from their original papers. For CPU latency, we deploy and benchmark the models on the same Samsung Galaxy S8 phone with Caffe2 int8 implementation. The details of MnasNet-{64, 92} are not disclosed from <ref type="bibr" target="#b19">[20]</ref> so we cannot measure the latency. *The search cost for MnasNet is estimated according to the description in <ref type="bibr" target="#b19">[20]</ref>. † The search cost is estimated based on the claim from <ref type="bibr" target="#b10">[11]</ref> that PNAS <ref type="bibr" target="#b10">[11]</ref> is 8x lower than NAS <ref type="bibr" target="#b30">[31]</ref>. ‡ The inference engine is faster than other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FBNet-A</head><p>FBNet-96-0.35-1</p><p>FBNet-s8</p><formula xml:id="formula_9">FBNet-iPhoneX MobileNetV2 MnasNet Skip K=3 E=6 K=3 E=3 K=3 E=1 K=3 E=1 G=2 K=5 E=6 K=5 E=3 K=5 E=1 K=5 E=1 G=2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FBNet-B</head><p>FBNet-C <ref type="figure">Figure 4</ref>. Visualization of some of the searched architectures. We use rectangle boxes to denote blocks for each layer. We use different colors to denote the kernel size of the depthwise convolution, blue for kernel size of 3, green for kernel size of 5, and empty for skipping. We use height to denote the expansion rate of the block: 6, 3, 1, and 1 with group-2 convolution.</p><p>ing, our searched models achieve 1.5% to 6.4% better accuracy with similar latency. Especially the FBNet-96-0.35-1 model achieves 50.2% (+4.7%) accuracy and 2.9 ms latency (345 frames per second) on a Samsung Galaxy S8.</p><p>We visualize the architecture of FBNet-96-0.35-1 in <ref type="figure">Figure 4</ref>, we can see that many layers are skipped, and the network is much shallower than FBNet-{A, B, C}, whose input size is 224. We conjecture that this is because with smaller input size, the receptive field needed to parse the image also becomes smaller, so having more layers will not effectively increase the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Different Target Devices</head><p>In previous ConvNet design practices, the same ConvNet model is deployed to many different devices. However, this is sub-optimal since different computing platforms and software implementation can have different characteristics. To validate this, we conduct search targeting two mobile devices: Samsung Galaxy S8 with Qualcomm Snapdragon 835 platforms, and iPhone X with A11 Bionic processors. We use the same architecture search space, but different latency lookup tables collected from two target devices. All the architecture search and training protocols are the same. After we searched and trained two models, we deploy them to both Samsung Galaxy S8 and iPhone X to benchmark the overall latency. The result is summarized in <ref type="table">Table.</ref> 5.</p><p>As we can see, the two models reach similar accuracy (73.20% vs. 73.27%). FBNet-iphoneX model's latency is 19.84 ms on its target device, but when deployed to a Samsung S8, its latency increases to 23.33 ms. On the other hand, FBNet-S8 reaches a latency of 22.12 ms on a Samsung S8, but when deployed to an iPhone X, the latency hikes to 27.53 ms, 7.69 ms (relatively 39%) higher than FBNet-iPhone X. This demonstrates the necessity of re-designing ConvNets for different target devices.</p><p>Two models are visualized in <ref type="figure">Figure 4</ref>. Note that FBNet- Ops adopted by FBNet-iPhoneX Ops adopted in FBNet-S8 <ref type="figure">Figure 5</ref>. Comparison of operator runtime on two devices. Runtime is in micro-second (us). Orange bar denotes the runtime on iPhone X and blue bar denotes the runtime on Samsung S8. The upper three operators are faster on iPhone X, therefore they are automatically adopted in FBNet-iPhoneX. The lower three operators are faster on Samsung S8, and they are also automatically adopted in FBNet-S8.</p><p>S8 uses many blocks with 5x5 depthwise convolution while FBNet-iPhoneX only uses them in the last two stages. We examine the depthwise convolution operators used in the two models and compare their runtime on both devices. As shown in <ref type="figure">Figure 5</ref>, the upper three operators are faster on iPhone X, therefore they are automatically adopted in FBNet-iPhoneX. The lower three operators are significantly faster on Samsung S8, and they are also automatically adopted in FBNet-S8. Notice the drastic runtime differ-ences of the lower three operators on two target devices. It explains why the Samsung-S8-optimized model performs poorly on an iPhone X. This shows DNAS can automatically optimize the operator adoptions and generate different ConvNets optimized for different devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present DNAS, a differentiable neural architecture search framework. It optimizes over a layer-wise search space and represents the search space by a stochastic super net. The actual target device latency of blocks is used to compute the loss for super net training. FBNets, a family of models discovered by DNAS surpass state-of-the-art models, both manually and automatically designed: FBNet-B achieves 74.1% top-1 accuracy with 295M FLOPs and 23.1 ms latency, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with the same accuracy. It also achieves better accuracy and lower latency than MnasNet, the state-of-the-art efficient model designed automatically; we estimate the search cost of DNAS is 420x smaller. Such efficiency allows us to conduct searches for different input resolutions and channel scaling. Discovered models achieve 1.5% to 6.4% accuracy gains. The smallest FBNet achieves 50.2% accuracy with a latency of 2.9 ms (345 frames/sec) with batch size 1. Over the Samsung-optimized FBNet, the improved FBNet achieves 1.4x speed up on an iPhone X, showing DNAS is able to adapt to different target devices automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment details</head><p>We describe more experiment details in this appendix to facilitate other researchers to reproduce our work. Our architecture search is divided into two stages. In the first stage, we train the stochastic super net to find an optimal architecture distribution. In the second stage, we sample architectures from the distribution and train them from scratch.</p><p>To train the stochastic super net, we randomly sample 100 classes from the original 1,000 classes of ImageNet. Training the super net on this smaller proxy dataset is much faster. We train the stochastic super net for 90 epochs with a batch size of 192. In each epoch, we first train the operator parameters w a on 80% of the training set using stochastic gradient descent with momentum. The initial learning rate is 0.1, and decay following a cosine decaying schedule. The momentum is 0.9, and weight decay is 10 −4 . Next, we train the architecture distribution parameter θ on the rest 20% of the training set with Adam optimizer <ref type="bibr" target="#b9">[10]</ref> with a learning rate of 10 −2 and weight decay of 5 × 10 −4 . The split of weight and architecture parameter training ensure the architecture generalize to the validation dataset. To control the Gumbel Softmax in (8), we use an initial temperature of 5.0 and exponentially anneal it by exp(−0.045) ≈ 0.956 every epoch. For the loss function in (2), we set α to 0.2 and β to 0.6. We use the standard ResNet data augmentation <ref type="bibr" target="#b3">[4]</ref> to process the input images. We found that at the beginning of the training, operators are usually not sufficiently trained, so their contributions to the accuracy are not clear. However, their computational costs are always significantly different from each other. As a consequence, the super net may always pick low computational cost operators at the beginning of the training. To prevent this, we postpone the training of the architecture parameter θ by 10 epochs to allow operator weights to be sufficiently trained first. At the end of the super net training, we sample 6 architectures from the final distribution to be trained from scratch.</p><p>To train the sampled architectures, the training protocols are different for different models. Here we describe the training protocol for FBNet-{A, B, C}. These models have an input resolution of 224, channel size scaling of 1.0. We train the models with a batch size of 256 on 8 GPUs for 360 epochs. We set the initial learning rate to be 0.1, and decay 10x at 90, 180, and 270 epochs. The momentum is 0.9, weight decay is 4 × 10 −5 . We use dropout at the last convolution layer of the network, and the dropout ratio is 0.2. We use the standard GoogleNet data augmentation <ref type="bibr" target="#b18">[19]</ref> to randomly resize the image during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>≈ 6 ×</head><label>6</label><figDesc>10 18 possible architectures. However, arXiv:1812.03443v3 [cs.CV] 24 May 2019 A typical flow of reinforcement learning based neural architecture search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of manual ConvNet design and reinforcement learning based neural architecture search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The block structure of the micro-architecture search space. Each candidate block in the search space can choose a different expansion rate, kernel size, and number of groups for group convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Compared with Mo-bileNetV2 under the same input size and channel size scal-</figDesc><table><row><cell>Model</cell><cell>Search method</cell><cell>Search space</cell><cell>Search cost (GPU hours / relative)</cell><cell cols="2">#Params #FLOPs</cell><cell>CPU Latency</cell><cell>Top-1 acc (%)</cell></row><row><cell>1.0-MobileNetV2 [17]</cell><cell>manual</cell><cell>-</cell><cell>-</cell><cell>3.4M</cell><cell>300M</cell><cell>21.7 ms</cell><cell>72.0</cell></row><row><cell>1.5-ShuffleNetV2 [13]</cell><cell>manual</cell><cell>-</cell><cell>-</cell><cell>3.5M</cell><cell>299M</cell><cell>22.0 ms</cell><cell>72.6</cell></row><row><cell cols="2">CondenseNet (G=C=8) [7] manual</cell><cell>-</cell><cell>-</cell><cell>2.9M</cell><cell>274M</cell><cell>28.4  ‡ ms</cell><cell>71.0</cell></row><row><cell>MnasNet-65 [13]</cell><cell>RL</cell><cell>stage-wise</cell><cell>91K  *  / 421x</cell><cell>3.6M</cell><cell>270M</cell><cell>-</cell><cell>73.0</cell></row><row><cell>DARTS [12]</cell><cell>gradient</cell><cell>cell</cell><cell>288 / 1.33x</cell><cell>4.9M</cell><cell>595M</cell><cell>-</cell><cell>73.1</cell></row><row><cell>FBNet-A (ours)</cell><cell cols="2">gradient layer-wise</cell><cell>216 / 1.0x</cell><cell>4.3M</cell><cell>249M</cell><cell>19.8 ms</cell><cell>73.0</cell></row><row><cell>1.3-MobileNetV2 [17]</cell><cell>manual</cell><cell>-</cell><cell>-</cell><cell>5.3M</cell><cell>509M</cell><cell>33.8 ms</cell><cell>74.4</cell></row><row><cell cols="2">CondenseNet (G=C=4) [7] manual</cell><cell>-</cell><cell>-</cell><cell>4.8M</cell><cell>529M</cell><cell>28.7  ‡ ms</cell><cell>73.8</cell></row><row><cell>MnasNet [20]</cell><cell>RL</cell><cell>stage-wise</cell><cell>91K  *  / 421x</cell><cell>4.2M</cell><cell>317M</cell><cell>23.7 ms</cell><cell>74.0</cell></row><row><cell>NASNet-A [31]</cell><cell>RL</cell><cell>cell</cell><cell>48K / 222x</cell><cell>5.3M</cell><cell>564M</cell><cell>-</cell><cell>74.0</cell></row><row><cell>PNASNet [11]</cell><cell>SMBO</cell><cell>cell</cell><cell>6K  † / 27.8x</cell><cell>5.1M</cell><cell>588M</cell><cell>-</cell><cell>74.2</cell></row><row><cell>FBNet-B (ours)</cell><cell cols="2">gradient layer-wise</cell><cell>216 / 1.0x</cell><cell>4.5M</cell><cell>295M</cell><cell>23.1 ms</cell><cell>74.1</cell></row><row><cell>1.4-MobileNetV2 [17]</cell><cell>manual</cell><cell>-</cell><cell>-</cell><cell>6.9M</cell><cell>585M</cell><cell>37.4 ms</cell><cell>74.7</cell></row><row><cell>2.0-ShuffleNetV2 [13]</cell><cell>manual</cell><cell>-</cell><cell>-</cell><cell>7.4M</cell><cell>591M</cell><cell>33.3 ms</cell><cell>74.9</cell></row><row><cell>MnasNet-92 [20]</cell><cell>RL</cell><cell>stage-wise</cell><cell>91K  *  / 421x</cell><cell>4.4M</cell><cell>388M</cell><cell>-</cell><cell>74.8</cell></row><row><cell>FBNet-C (ours)</cell><cell cols="2">gradient layer-wise</cell><cell>216 / 1.0x</cell><cell>5.5M</cell><cell>375M</cell><cell>28.1 ms</cell><cell>74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>FBNets searched for different input resolution and channel scaling. MnasNet-scale is the MnasNet model with input and channel size scaling. MnasNet-search-192-0.5 is a model searched with an input size of 192 and channel scaling of 0.5. Details of it are not disclosed in<ref type="bibr" target="#b19">[20]</ref>, so we only cite the accuracy. FBNets searched for different devices.</figDesc><table><row><cell cols="2">Input size &amp; Channel Scaling</cell><cell></cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell>#Parameters #FLOPs CPU Latency Top-1 acc (%)</cell></row><row><cell></cell><cell></cell><cell cols="5">MobileNetV2-224-0.35</cell><cell>1.7M</cell><cell>59M</cell><cell>9.3 ms</cell><cell>60.3</cell></row><row><cell cols="2">(224, 0.35)</cell><cell cols="5">MNasNet-scale-224-0.35</cell><cell>1.9M</cell><cell>76M</cell><cell>10.7 ms</cell><cell>62.4 (+2.1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">FBNet-224-0.35</cell><cell></cell><cell>2.0M</cell><cell>72M</cell><cell>10.7 ms</cell><cell>65.3 (+5.0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MobileNetV2</cell><cell></cell><cell>2.0M</cell><cell>71M</cell><cell>8.4 ms</cell><cell>63.9</cell></row><row><cell cols="2">(192, 0.50)</cell><cell cols="5">MnasNet-search-192-0.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.6 (+1.7)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">FBNet-192-0.5 (ours)</cell><cell></cell><cell>2.6M</cell><cell>73M</cell><cell>9.9 ms</cell><cell>65.9 (+2.0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MobileNetV2</cell><cell></cell><cell>3.5M</cell><cell>99M</cell><cell>8.4 ms</cell><cell>65.3</cell></row><row><cell cols="2">(128, 1.0)</cell><cell cols="5">MnasNet-scale-128-1.0</cell><cell>4.2M</cell><cell>103M</cell><cell>9.2 ms</cell><cell>67.3 (+2.0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">FBNet-128-1.0 (ours)</cell><cell></cell><cell>4.2M</cell><cell>92M</cell><cell>9.0 ms</cell><cell>67.0 (+1.7)</cell></row><row><cell cols="2">(128, 0.50)</cell><cell></cell><cell cols="3">MobileNetV2 FBNet-128-0.5 (ours)</cell><cell></cell><cell>2.0M 2.4M</cell><cell>32M 32M</cell><cell>4.8 ms 5.1 ms</cell><cell>57.7 60.0 (+2.3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MobileNetV2</cell><cell></cell><cell>1.7M</cell><cell>11M</cell><cell>3.8 ms</cell><cell>45.5</cell></row><row><cell cols="2">(96, 0.35)</cell><cell cols="5">FBNet-96-0.35-1 (ours)</cell><cell>1.8M</cell><cell>12.9M</cell><cell>2.9 ms</cell><cell>50.2 (+4.7)</cell></row><row><cell></cell><cell></cell><cell cols="5">FBNet-96-0.35-2 (ours)</cell><cell>1.9M</cell><cell>13.7M</cell><cell>3.6 ms</cell><cell>51.9 (+6.4)</cell></row><row><cell cols="2">Model</cell><cell></cell><cell cols="4">#Parameters #FLOPs</cell><cell>Latency on iPhone X</cell><cell>Latency on Samsung S8</cell><cell>Top-1 acc (%)</cell></row><row><cell cols="2">FBNet-iPhoneX</cell><cell></cell><cell>4.47M</cell><cell></cell><cell cols="2">322M</cell><cell>19.84 ms (target)</cell><cell>23.33 ms</cell><cell>73.20</cell></row><row><cell cols="2">FBNet-S8</cell><cell></cell><cell>4.43M</cell><cell></cell><cell cols="2">293M</cell><cell>27.53 ms</cell><cell>22.12 ms (target)</cell><cell>73.27</cell></row><row><cell cols="5">Comparison of Operator Runtime (us)</cell><cell></cell><cell></cell></row><row><cell>hw14_c384_k3_s1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hw14_c336_k3_s1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hw28_c192_k3_s1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hw14_c336_k5_s1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hw14_c192_k5_s1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hw28_c192_k5_s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell>1200</cell></row><row><cell></cell><cell cols="2">iPhone X</cell><cell>Samsung S8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Anonymous. under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10615</idno>
		<title level="m">Squeezenext: Hardware-aware neural network design</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">group</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11164</idno>
		<title level="m">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning time/memory-efficient deep architectures with budgeted super networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00046</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time roadobject segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mixed precision quantization of convnets via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00090</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squeeze-segv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lavagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08634</idno>
		<title level="m">Algorithm-hardware co-design for convnet accelerators on embedded fpgas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
