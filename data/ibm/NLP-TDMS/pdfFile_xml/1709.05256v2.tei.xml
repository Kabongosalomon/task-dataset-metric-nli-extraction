<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Faces Using Region-based Fully Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
							<email>yitongwang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<email>hawelwang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Faces Using Region-based Fully Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face detection has achieved great success using the region-based methods. In this report, we propose a region-based face detector applying deep networks in a fully convolutional fashion, named Face R-FCN. Based on Region-based Fully Convolutional Networks (R-FCN), our face detector is more accurate and computationally efficient compared with the previous R-CNN based face detectors. In our approach, we adopt the fully convolutional Residual Network (ResNet) as the backbone network. Particularly, we exploit several new techniques including position-sensitive average pooling, multi-scale training and testing and on-line hard example mining strategy to improve the detection accuracy. Over two most popular and challenging face detection benchmarks, FDDB and WIDER FACE, Face R-FCN achieves superior performance over state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face detection plays an important role in the modern face-relevant applications. Despite the great progress made in recent years, the technical challenging of face detection still exists out of the complex variations of real-world face images. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the visual faces vary a lot as the result of the affecting factors including occlusion on the facial part, different scales, illumination conditions, various poses of person, rich expressions, etc. Recently, remarkable advances of objection detection have been driven by the success of region-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Among recent novel algorithms, Fast/Faster R-CNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> are representative R-CNN based methods that perform region-wise detections on the regions of interest (RoIs). However, directly applying the strategy of region-specific operation to fully convolutional networks, such as Residual Nets (ResNets) <ref type="bibr" target="#b4">[5]</ref>, results in inferior detection performance owing to the overwhelming classification accuracy. In contrast, R-FCN <ref type="bibr" target="#b5">[6]</ref> is proposed to address the problem in the fully convolutional manner. The ConvNet of R-FCN is built with the computations shared on the entire image, which leads to the improvement of training and testing efficiency. Comparing with R-CNN based methods, R-FCN proposes much fewer region-wise layers to balance the learning of classification and detection for naturally combining fully convolutional network with region-based module.</p><p>As a specific area of generic object detection, face detection has achieved superior performance thanks to the appearance of region-based methods. Previous works primarily focus on the R-CNN based methods and achieve promising results. In this report, we develop a face detector on the top of R-FCN with elaborate design of the details, which achieves more decent performance than the R-CNN face detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. According to the size of the general face, we carefully design size of anchors and RoIs. Since the contribution of facial parts may be different for detection, we introduce a position-sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part. Furthermore, we also apply the multi-scale training and testing strategy in this work. The on-line hard example mining (OHEM) technique <ref type="bibr" target="#b8">[9]</ref> is integrated into our network as well for boosting the learning on hard examples.</p><p>Our key contributions are summarized below:</p><p>(1) We develop a face detection framework that takes the special properties of face into account by integrating several innovative and effective techniques. The proposed approach is based on R-FCN and is well suited for face detection, thus we call it Face R-FCN.</p><p>(2) We introduce a novel position-sensitive average pooling to re-weight embedding responses on score maps and eliminate the effect of non-uniformed contribution in each facial part.</p><p>(3) By far, the proposed algorithm is benchmarked on WIDER FACE dataset <ref type="bibr" target="#b9">[10]</ref> and FDDB dataset <ref type="bibr" target="#b10">[11]</ref>. Our Face R-FCN has reached the first-rate performance over the state-of-the-arts on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the past decades, face detection has been extensively studied. The pioneering work of Viola and Jones <ref type="bibr" target="#b11">[12]</ref> invents a cascaded AdaBoost face detector using Haar-like features. After that, numerous of works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> have focused on developing more advanced features and more powerful classifiers. Besides the boosted cascade methods, several studies apply deformable part models (DPM) for face detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. The DPM methods detect faces by modeling the relationship of deformable facial parts.</p><p>Recent progress in face detection mainly benefits from the powerful deep learning approaches. The CNN-based detectors have achieved the highest performance. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> construct cascaded CNNs to learn face detectors with a coarse-to-fine strategy. MTCNN <ref type="bibr" target="#b20">[21]</ref> develops a multi-task training framework to jointly learn the face detection and alignment. UnitBox <ref type="bibr" target="#b21">[22]</ref> propose the intersectionover-union (IoU) loss function, to directly minimize the IoUs of the predictions and the groundtruths. Recently, several methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> use the Faster R-CNN framework to improve the face detection performance. <ref type="bibr" target="#b25">[26]</ref> explores the contextual information for face detection and proposes a framework achieving high performance, especially improving the accuracy of tiny faces. Most recently, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> propose to use single stage framework for face detection, with carefully designed strategies and achieve the state-of-the-art performance.</p><p>Similar to face detection, general object detection is advancing rapidly thanks to the deep learning approaches. Typical work including R-CNN <ref type="bibr" target="#b0">[1]</ref>, Fast R-CNN <ref type="bibr" target="#b2">[3]</ref>, Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> and their extensions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Among these studies, R-FCN makes the detection in a nearly fully convolutional manner, which greatly enhances the efficiency of training and testing. The methods of hard example mining further help deep learning based object detection to improve the performance. In <ref type="bibr" target="#b8">[9]</ref>, the authors proposed an on-line hard example mining (OHEM) algorithm to improve the object detection performance. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21]</ref> also use hard example mining algorithms to boost the performance of face detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, the proposed Face R-FCN (See <ref type="figure" target="#fig_1">Figure 2</ref>) is described in detail. Since our framework is based on the R-FCN, we refer the reader to <ref type="bibr" target="#b5">[6]</ref> for more technical details.</p><p>We improve the R-FCN framework for targeting face detection in three aspects. First, we introduce additional smaller anchors and modify the position sensitive RoI pooling to a smaller size for suiting the detection of the tiny faces. Second, we propose to use position-sensitive average pooling instead of normal average pooling for the last feature voting in R-FCN, which leads to an improved embedding. Third, multi-scale training strategy and on-line Hard Example Mining (OHEM) strategy <ref type="bibr" target="#b8">[9]</ref> are adopted for training. In the testing phase, we also ensemble the multi-scale detection results to improve the performance. The details of the proposed approach are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">R-FCN Based</head><p>Architecture</p><formula xml:id="formula_0">R-FCN [6]</formula><p>is a region-based fully convolutional network initially proposed for object detection. Unlike other region-based detectors such as Faster RCNN <ref type="bibr" target="#b2">[3]</ref>, R-FCN constructs a deeper fully convolutional network without increasing the speed overhead by shared computation on the entire image. R-FCN builds upon 101-layer ResNet <ref type="bibr" target="#b4">[5]</ref>, consists of a region proposal network (RPN) and a R-FCN module in contrast to R-CNN module that presented in Faster R-CNN <ref type="bibr" target="#b2">[3]</ref>.</p><p>ResNet architecture in R-FCN plays the role of feature extractor. It is common knowledge that ResNet construct a very deep network which is able to extract highly representative image features. These features hold much larger receptive field where tiny face detection can be benefited from the context information. From the feature maps that output by the fundamental ResNet, RPN generates a batch of the region of interests (RoIs) according to the anchors. These RoIs further are fed into two sibling position sensitive RoI pooling layer in R-FCN module to produce class score maps and bounding box prediction maps. In the end of R-FCN, two global average pooling are applied on both class score maps and bounding box prediction maps respectively for aggregating the class scores and bounding box predictions.</p><p>There are two major advantages that we adopt R-FCN over R-CNN. Firstly, the position sensitive RoI pooling ingeniously encodes position information into each RoI by pooling group of feature maps to a certain location of the output score maps; Secondly, without unnaturally injecting fully connected layers into ResNet architecture, the feature maps of R-FCN are trained more expressive and easier for the network to learn the class score and bounding box of faces.</p><p>Based on R-FCN, We make several effective modifications for improving detection performance. For better describing tiny faces, we introduce more anchors with smaller scales (say, from 1 to 64). These smaller anchors are very helpful for sufficiently capturing the extremely tiny faces. Besides, we set smaller pooling size for position sensitive RoI pooling to reduce redundant information, and refine the following voting scheme (average pooling) to be position sensitive average pooling, which will be described in the following section. Finally, we apply atrous convolution in the last stage of ResNet to keep the scale of feature maps without losing the contextual information in larger receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Position-Sensitive Average Pooling</head><p>In the original R-FCN work, global average pooling is adopted to aggregate the features after position-sensitive RoI pooling into a single dimension. This operation leads to the uniform contribution of each position of the face. However, the contribution of each part of the face may be non-uniformed for detection. For example, in terms of face recognition, eyes usually are paid more attentions than mouth which has been verified by experiments in <ref type="bibr" target="#b30">[31]</ref>. Intuitively, We believe such assumption that distinct regions on the face have different importance should also hold in face detection. Hence, we propose to perform weighted average for each area of the output of position sensitive RoI pooling in order to re-weight the region, which is called position-sensitive average pooling.</p><p>Formally, letX = {X i |i = 1, 2, ..., M } denote the output M feature maps of a position-sensitive RoI pooling layer, and X i = {x i,j |j = 1, 2, ..., N 2 } denote the i th feature map, where N denotes the size of the pooled feature map. Position-sensitive average pooling calculates the weighted average value of the feature responses to get the pooling feature Y = {y i |i = 1, 2, ..., M } fromX, where y i is denoted as:</p><formula xml:id="formula_1">y i = 1 N 2 N 2 j=1 w j x i,j ,<label>(1)</label></formula><p>where w j denotes the weight for the j-th position. Note that position-sensitive average pooling can be thought as performing feature embedding on every location of responses followed by average pooling. Hence, it is very convenient to implement position-sensitive average pooling on most of the popular deep neural network frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Scale Training and Testing</head><p>Inspired by <ref type="bibr" target="#b7">[8]</ref>, we perform multi-scale training and testing strategy to improve performance. In the training phase, we resize the shortest side of the input to 1024 or 1200 pixels. This training strategy keeps our model being robust on detecting the target at the different scale, especially on tiny faces. On-line Hard Example Mining (OHEM) <ref type="bibr" target="#b8">[9]</ref> is a simple yet effective technique for bootstrapping.</p><p>During training, we also apply OHEM on negative samples and set the positive and negative samples ratio to 1:3 in each mini-batch. In the testing phase, we build an image pyramid for each test image. Each scale in the pyramid is independently tested. The results from various scales are eventually merged together as the final result of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform evaluation on two public-domain face detection benchmarks: the WIDER FACE dataset <ref type="bibr" target="#b9">[10]</ref> and the FDDB dataset <ref type="bibr" target="#b10">[11]</ref>. The WIDER FACE dataset has a total collection of 393,703 labeled face in 32,203 images, of which 40% are used for training, 10% for validation and 50% for testing. Specifically, the validation set and the test set are divided into three subsets (Easy, Medium, and Hard) for evaluation based on different level of difficulties, as defined in <ref type="bibr" target="#b9">[10]</ref>. The FDDB dataset contains 5,171 labeled faces in 2,845 images. Example images of WIDER FACE and FDDB are shown in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our training hyper-parameters are similar to Face R-CNN <ref type="bibr" target="#b7">[8]</ref>. Different from Face R-CNN, we initialize our network with the pre-trained weights of 101-layer ResNet trained on ImageNet. Specifically, we freeze the general kernels (weights of few layers at the beginning) of the pre-trained model throughout the entire training process in order to keep the essential feature extractor trained on ImageNet.</p><p>In terms of the RPN stage, Face R-FCN enumerates multiple configurations of the anchor in order to accurately search for faces. We combine a range of multiple scales and aspect ratios together to construct multi-scale anchors. These anchors then map to the original image to calculate the IoU scores with the ground truth for further picking up with following rules: First, the anchors with highest IoU score are strictly kept as positive; Second, the anchors with IoU score above 0.7 are assigned as positive; Third, If the anchors have IoU score that is lower than 0.3, they are marked as negative. The R-FCN is then trained on the processed anchors (proposals) where the positive samples and negative samples are defined as IoU greater than 0.5 and between 0.1 and 0.5 respectively. The RPN and R-FCN are both learned jointly with the softmax loss and the smooth L1 loss.</p><p>Non-maximum suppression (NMS) is adopted for regularizing the anchors with certain IoU scores. The proposals are processed by OHEM to train with hard examples. We set the 256 for the size of RPN mini-batch and 128 for R-FCN respectively. Approximate joint training strategy is applied for training in the end-to-end fashion.</p><p>We utilize multi-scale training where the input image is resized with bilinear interpolation to various scales (say, 1024 or 1200). In the testing stage, multi-scale testing is performed by scale image into an image pyramid for better detecting on both tiny and general faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison on Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">WIDER FACE</head><p>We train our model on the training set of WIDER FACE and perform evaluation on the validation set and test set following the Scenario-Int criterion <ref type="bibr" target="#b9">[10]</ref>. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, our proposed approach consistently wins the 1st place across the three subsets on both the validation set and test set of WIDER FACE and significantly outperforms the existing results <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, on WIDER FACE hard subset, our approach is superior to the prior best-performing one <ref type="bibr" target="#b27">[28]</ref> by a clear margin, which demonstrates the robustness of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">FDDB</head><p>There are two evaluation protocols for evaluating the FDDB dataset: one is 10-fold cross-validation and the other is unrestricted training (using the data outside FDDB for training). Our experiments strictly follow the protocol for unrestricted training.</p><p>We use the training set of the WIDER FACE dataset to train our model (denoted as Model-A in <ref type="figure">Figure 4</ref>) and compare against the recently published top approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> on FDDB. All of these approaches use the protocol for unrestricted training defined in <ref type="bibr" target="#b10">[11]</ref>. The discrete ROC curves and continuous ROC curves of these approaches are plotted in <ref type="figure">Figure 4</ref>. From <ref type="figure">Figure  4</ref>, it is clearly that Face R-FCN consistently achieves the impressive performance in terms of both the discrete ROC curve and continuous ROC curve. Our discrete ROC curve is superior to the prior best-performing method <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref>. We also obtain the best true positive rate of the discrete ROC curve at 1000/2000 false positives (98.49%/99.07%). For the reason that we do not optimize our method to regress the elliptical ground truth in FDDB dataset, our continuous ROC curve is lower than the first place <ref type="bibr" target="#b27">[28]</ref> and slightly lower than <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>. Additionally, one of the factors that may affect the performance of Face R-FCN demonstrated in the last row of 5(b): the false positive bounding boxes in images exactly contain faces from human perspective where these faces have not been annotated as ground truth. This factor partly leads to the lower performance comparing with <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28]</ref>. But the competitive result we achieved is still noticeable. All of these methods use the same Scenario-Int criterion <ref type="bibr" target="#b9">[10]</ref>. Face R-FCN shows the superior performance over the prior methods across the three subsets (easy, medium and hard) in both validation and test sets. Best viewed in color. Furthermore, We expand the training dataset by augmenting with a privately collected dataset and use the enlarged dataset to train a more discriminative face detector (denoted as Model-B). The discrete and continuous ROC curves of Model-B are also plotted in <ref type="figure">Figure 4</ref>. As expected, the performance of Face R-FCN is further improved. Finally, we obtain the true positive rate 98.99% of the discrete ROC curve at 1000 false positives and 99.42% at 2000 false positives, which are new state-of-the-art among all the published methods on FDDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Face detection is a fundamental problem in vision task. In this technical report, we propose a powerful face detection approach named Face R-FCN by integrating R-FCN and several sophisticated techniques for better detecting faces and boosting overall performance. By reasoning the drawbacks of R-CNN and R-FCN, we explore the details and invent new designs to improve the popular detection framework specifically for face detection. The proposed approach is evaluated on the challenging WIDER FACE dataset and FDDB dataset. Our experimental results demonstrate the superiority of our approach over the state-of-the-arts. These innovations are inspired from past experience and we expect our innovations will be easy to generally applied to the future face detection architectures as past experience. The green frames in the image represent the face detection results while the red frames or ellipses represent the ground-truth annotations. Note that in the last row of (b), some of human faces detected by Face R-FCN have not been annotated as ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example image which has extreme variability in the face regions. Green frames stand for the detection results of the proposed face detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of our R-FCN based framework. Note that position-sensitive average pooling is used to replace global average pooling for the final feature voting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Precision-Recall curves on WIDER FACE's validation set and test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Examples of our detected results on the (a) WIDER FACE validation set and (b) FDDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Evaluation of our results on the FDDB published methods. We show the ROC curves on the (a) Discrete ROC curve and (b) Continuous ROC curve. Model-A and Model-B is trained by WIDER FACE's training set and a augmented private dataset respectively. We show the true positive rate at 2000 false positives for each model. Best viewed in color.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True positive rate</cell><cell>0.7 0.8 0.9</cell><cell>0</cell><cell>500</cell><cell>1000 Model-A (Face R-FCN) 0.9907 1500 Model-B (Face R-FCN) 0.9942 Face R-CNN[8] 0.9874 SFD[28] 0.9843 HR[26] 0.9762 Conv3D[34] 0.9118 MXNet[35] 0.9727 MTCNN[21] 0.9505 LDCF+[33] 0.9337</cell><cell>2000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False positive</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True positive rate</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell>0</cell><cell>500</cell><cell>1000 Model-A (Face R-FCN) 0.7572 1500 Model-B (Face R-FCN) 0.7595 Face R-CNN[8] 0.7597 SFD[28] 0.8572 HR[26] 0.6931 Conv3D[34] 0.7665 MXNet[35] 0.7265 MTCNN[21] 0.7137 LDCF+[33] 0.6905</cell><cell>2000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False positive</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P N</forename><surname>Fotheringham-Smythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gamow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face detection with the Faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1606.03473</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Face R-CNN. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training Region-based Object Detectors with Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UMCS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast polygonal integration and its application in extending haarlike features to improve object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint training of cascaded cnn for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bootstrapping face detection with hard negative examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Face Detection using Deep Learning: An Improved Faster RCNN Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08289</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CMS-RCNN: Contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1606.05413</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding Tiny Faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSH: Single Stage Headless Face Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single Shot Scale-invariant Face Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class. Transactions on Pattern analysis and machine intelligence (TPAMI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martnez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="748" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From Facial Parts Responses to Face Detection: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Open source code and models</title>
		<ptr target="https://github.com/tornadomeet/mxnet-face#face-detection/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
