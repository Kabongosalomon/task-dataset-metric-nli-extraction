<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformation Flow Based Two-Stream Network for Lip Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deformation Flow Based Two-Stream Network for Lip Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip reading is the task of recognizing the speech content by analyzing movements in the lip region when people are speaking. Observing on the continuity in adjacent frames in the speaking process, and the consistency of the motion patterns among different speakers when they pronounce the same phoneme, we model the lip movements in the speaking process as a sequence of apparent deformations in the lip region. Specifically, we introduce a Deformation Flow Network (DFN) to learn the deformation flow between adjacent frames, which directly captures the motion information within the lip region. The learned deformation flow is then combined with the original grayscale frames with a two-stream network to perform lip reading. Different from previous two-stream networks, we make the two streams learn from each other in the learning process by introducing a bidirectional knowledge distillation loss to train the two branches jointly. Owing to the complementary cues provided by different branches, the two-stream network shows a substantial improvement over using either single branch. A thorough experimental evaluation on two large-scale lip reading benchmarks is presented with detailed analysis. The results accord with our motivation, and show that our method achieves state-of-the-art or comparable performance on these two challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual speech recognition, also known as lip reading, is the task of decoding speech content based on the visual cues of a speaker's lip motion. Lip reading is a developing topic that has received growing attention in recent years. It has broad application prospects in hearing aids, special education for hearing impaired people, complementing acoustic speech recognition in noisy environments, new human-machine interaction methods, among many other potential applications.</p><p>The field of video understanding has progressed significantly in recent years. However, lip reading, a special task of video understanding, remains a challenging task. Different from coarse-grained video analysis tasks, such as action detection and action recognition, lip reading is a fine-grained video analysis task, and requires subtle spatial information in the lip region as well as continuous and discriminative temporal information of lip motion. While humans outperform machines in action recognition, machines have already exceeded humans in lip reading. This is partly because the visual details and lip motions are too subtle for humans to capture and analyze, while machines have an innate advantage in this respect.</p><p>Recent lip reading methods are based on deep learning and often conducted in end-to-end fashion. Although promising This work was done by Jingyun Xiao during his internship at the Institute of Computing Technology, Chinese Academy of Sciences. performance has been achieved by these methods, there are still several issues that demand more consideration. First, most existing lip reading methods extract frame-wise features and then model the temporal relationships with RNNs, with less consideration of the innate spatiotemporal correlation of adjacent frames. Second, one main difference between lip reading and other video analysis tasks is that the input video is focused on the face, and usually a crop of the lip region. It sets higher demands on the discriminative power of subtle facial information in the videos.</p><p>In this paper, we propose a Deformation Flow Network (DFN) to generate the deformation flow of the face in a video. It is trained in a completely self-supervised manner, with no need for labeled data. The deformation flow is a sequence of deformation fields. A deformation field is a mapping of the correlated pixels from the source frame to the target frame, which directly represents the motion information from the source frame to the target frame. By computing the deformation field between each pair of adjacent frames, we can capture and represent the motion of the face in the video.</p><p>For effective lip reading, we use both the computed deformation flow and the raw videos as the input to a twostream network. The two branches predict the probabilities of each word class independently. To make the two branches exchange information during training, we adopt knowledge distillation, and utilize a bidirectional knowledge distillation loss to help the two branches learn from each other's predictions during training. At test time, we fuse predictions from both branches to make the final prediction. We observe that a simple average of the predictions produces more accurate predictions, compared with results of using either single branch. It suggests that the two sources of input, the raw video and the deformation flows, provide complementary cues for the lip reading task.</p><p>Our contributions are threefold: (a) we propose a Deformation Flow Network (DFN) to generate deformation flows that can capture the motion information of the faces, which is trained in a self-supervised manner; (b) we use the deformation flows and the raw videos as the inputs to a twostream network, which provide complementary cues for lip reading, and utilize a bidirectional knowledge distillation loss to train the two branches jointly; (c) we conduct extensive experiments on LRW <ref type="bibr" target="#b3">[4]</ref> and LRW-1000 <ref type="bibr" target="#b16">[17]</ref>, demonstrating the effectiveness of our methods. II. RELATED WORKS In this section, we briefly review previous works on deep learning methods for lip reading, as well as self-supervised methods for facial deformation modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning Methods for Lip Reading</head><p>With the rapid development of deep learning in recent years, some works have begun to apply deep learning methods to lip reading and obtained considerable improvements over traditional methods using hand-engineered features. Noda <ref type="bibr" target="#b6">[7]</ref> was the first to employ a convolutional neural network (CNN) to extract the features for lip reading for the first time. Wand et al. <ref type="bibr" target="#b11">[12]</ref> used Long Short-Term Memory (LSTM) to replace the traditional classifier for lip reading, and achieved considerable improvement. In 2016, Chung et al. <ref type="bibr" target="#b3">[4]</ref> proposed an end-to-end lip reading model and compared several strategies of processing the frames for word classification, which has founded a solid base for the subsequent progress for lip reading. Since then, more recent lip reading approaches have followed an end-to-end paradigm. Concurrently, Assael et al. <ref type="bibr" target="#b0">[1]</ref> proposed LipNet, which is the first end-to-end sentence-level lip reading model.</p><p>In 2017, Stafylakis et.al. <ref type="bibr" target="#b9">[10]</ref> proposed a new word-level lip reading model that attains 83.0% classification accuracy on the LRW dataset, which is a significant improvement over prior art. It uses a combination of a single 3D convolution layer, ResNet <ref type="bibr" target="#b4">[5]</ref>, and bidirectional LSTM networks <ref type="bibr" target="#b5">[6]</ref>. The proposed architecture shows a strong spatiotemporal modeling power, successfully copes with many in-the-wild variations that LRW presents. Inspired by the success of deep spatiotemporal convolutional networks and two-stream architectures in action recognition, Weng et al. <ref type="bibr" target="#b13">[14]</ref> introduced deep spatiotemporal convolutional networks to lip reading. They also employ optical flow and two-stream networks. However, the optical flow is hard to obtain and it costs considerable time and storage. Moreover, most existing optical flow methods are unable to capture the fine-grained motion information of the lip region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised Facial Deformation</head><p>Recently, there have been a series of works using the deformation field and warping methods for face manipulation, facial attributes learning and other face-related tasks.</p><p>The Deforming Autoencoder (DAE) <ref type="bibr" target="#b8">[9]</ref> presents an unsupervised method to disentangle shape (in the form of a deformation field) and appearance (texture information disregarding the pose variations) of a face. The learned features are demonstrated to be effective for face manipulation, landmarks localization and emotion estimation.</p><p>X2Face <ref type="bibr" target="#b15">[16]</ref> is a network that can generate face images with a target pose and expression. In the evaluation stage, given a source face and a driving face, the network is able to generate a new face that preserves the identity, appearance, hairstyle and other attributes of the source face, while possessing the pose and expression of the driving face. In the training stage, it uses a pixel-wise L1 loss between the generated frame and the driving frame to supervise the training process. In this way, the training process of the network does not need any annotations.</p><p>FAb-Net <ref type="bibr" target="#b14">[15]</ref> has a similar architecture to X2Face. However, it aims to learn the facial attributes in a self-supervised manner. The learned facial attributes are demonstrated to achieve results comparable to and even surpassing the features learned by supervised methods in several tasks.</p><p>Inspired by these works, we propose the Deformation Flow Network (DFN) in our work to model the lip motion in the Given a source frame and a target frame, the encoder encodes them into two feature vectors, v s and v t . The decoder takes the concatenation of v s and v t as input, and generates a deformation field. The source frame is warped by the deformation field, and generates an output frame. A pixel-wise L1 loss between the output frame and target frame can supervise the network effectively. The DFN is trained in a completely self-supervised manner.</p><p>speaking process for lip reading, which is also trained in a self-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this section, we introduce our Deformation Flow Network (DFN) for generating the deformation flow, Deformation Flow Based Two-stream Network (DFTN) for wordlevel lip reading, and the bidirectional knowledge distillation loss for training the two-stream network jointly.</p><p>An overview of the pipeline is shown in <ref type="figure">Fig. 1</ref>. Given an input video (i.e., cropped grayscale image sequence of the lip region), we first feed it to the Deformation Flow Network to generate a series of deformation fields, one for each pair of adjacent frames. This resulting deformation field sequence is the deformation flow of the original video. Next, the grayscale video and the deformation flow are fed into the two branches separately for recognition. The two branches are optimized with individual classification losses, and a bidirectional knowledge distillation loss, which helps the two branches learn from each other. At test time, we fuse the results of each branch to make the final prediction for the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deformation Flow Network</head><p>The architecture of the Deformation Flow Network (DFN) is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The input to the DFN is a pair of frames (i.e., a source frame and a target frame). The output of the DFN is a deformation field, which is a 2-channel map with the same size as the input frames. The DFN consists of an encoder and a decoder. The encoder encodes the source frame s and target frame t into a source vector v s , and a target vector v t . The decoder takes the concatenation of v s and v t as input, and generates a deformation field d, which predicts the relative offsets (δ x, δ y) for each pixel location (x, y) in the target frame relative to the source frame. An output frame o is generated by sampling from the source frame s with the offsets (δ x, δ y) of the deformation field d:</p><formula xml:id="formula_0">o(x, y) = s(x + δ x, y + δ y)<label>(1)</label></formula><p>The output frame o = D(s,t), is expected to be identical to the target frame t, which can be supervised by a pixel-wise L1 loss between the output frame and target frame:  Examples of the source frames, target frames, output frames, deformation flow generated by DFN, and optical flow generated by PWC-Net <ref type="bibr" target="#b10">[11]</ref>. The color variations indicate that the deformation flow captures more details of the face than the optical flow. </p><formula xml:id="formula_1">L 1 = 1 n ∑ (x,y) |o(x, y) − t(x, y)|<label>(2)</label></formula><p>Given the above optimization target, the DFN can be trained in a completely self-supervised manner, with no need for any extra manual annotations. Examples of the source frames, target frames and output frames are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It is worth noting that since the deformation field is estimated at the pixel level, it can capture very subtle variations of faces and directly represent the motion information, which means it also has great potentials in other face-related tasks beyond lip reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deformation Flow Based Two-stream Network</head><p>In this subsection, we introduce the two branches (i.e., the grayscale branch and the deformation flow branch) of the Deformation Flow Based Two-stream Network, as well as the fusion strategy of the two branches in detail.</p><p>Firstly, we introduce the baseline model in this paper. The grayscale branch adopts the widely used architecture proposed by <ref type="bibr" target="#b9">[10]</ref>, which is a combination of CNN and RNN, except that we use Gated Recurrent Units (GRU) <ref type="bibr" target="#b1">[2]</ref> instead of LSTMs. Specifically, it consists of a front-end (i.e., a single layer of 3D CNN followed by ResNet-18 <ref type="bibr" target="#b4">[5]</ref> and a back-end (i.e., a 2-layer bidirectional RNN with GRUs).  The front-end extracts the visual features for each frame, and outputs a sequence of feature vectors. The back-end decodes the feature sequences, and predicts the probability of each word class. The deformation flow branch mostly mirrors the structure of the grayscale branch. The only difference is that the first layer of this branch is a 2D convolution layer, while it is a 3D convolution layer in the grayscale branch. The detailed architecture is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>Massive amount of works on two-stream networks have explored methods to fuse the two branches. In this work, we experimented with different fusion strategies, and the results with different fusion strategies are presented in IV-C. Among all the strategies, we find that fusing the output probabilities of the two branches gives the best performance.</p><p>However, the problem with fusing the predicted probabilities from individual branches is that the two branches are optimized separately, and lack interaction in the training stage. We wish to design a method that can help the two branches exchange the knowledge they learned during the training process. Therefore, we propose the bidirectional knowledge distillation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bidirectional Knowledge Distillation Loss</head><p>In this subsection, we introduce the bidirectional knowledge distillation loss as an additional supervision for training the two branches jointly.</p><p>Fusion strategies for the two-stream architecture have been widely explored in the field of action recognition. Here, we adopt the method of knowledge distillation. The two branches are able to make word-level classification as two independent models respectively. The outputs of the fully-connected (FC) layers of the grayscale branch and the deformation branch are denoted as z g and z d respectively. We then obtain the predicted probability distribution over all classes, q g and q d as:</p><formula xml:id="formula_2">q (i) = exp(z (i) /T ) ∑ j exp(z ( j) /T ) ,<label>(3)</label></formula><p>where T is a parameter known as temperature. T is usually set to 1 for classification tasks, and the equation becomes the softmax function. In knowledge distillation, a large T makes the probability distribution q "softer", which is easier for a student network to learn than a one hot vector corresponding to the ground truth. In our work, we set T to 20. The knowledge distillation loss is defined as:</p><formula xml:id="formula_3">L KD (q t , q s ) = − N ∑ i=1 q (i) t logq (i) s ,<label>(4)</label></formula><p>where q t and q s denotes the soft probability distributions of the teacher network and student network, respectively, and N denotes the number of classes.</p><p>Since we expect the two branches to learn from each other, we adopt a bidirectional knowledge distillation loss:</p><formula xml:id="formula_4">L BiKD (q g , q d ) = L KD (q g , q d ) + L KD (q d , q g )<label>(5)</label></formula><p>Therefore the final objective function of the two-stream network is:</p><formula xml:id="formula_5">L = L CE (z g , y) + L CE (z d , y) + λ L BiKD (q g , q d ),<label>(6)</label></formula><p>where L CE represents the standard cross-entropy loss for classification tasks, y is the one hot vector indicating the word class label of the video, and λ is a hyper-parameter indicating the weight of L BiKD .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The proposed methods are evaluated on two large-scale public lip reading datasets, LRW <ref type="bibr" target="#b3">[4]</ref> and LRW-1000 <ref type="bibr" target="#b16">[17]</ref>. Here we give a brief overview of the two datasets.</p><p>LRW. LRW <ref type="bibr" target="#b3">[4]</ref> is a large and challenging word-level lip reading dataset. Each sample of LRW is a video snippet of 29 frames captured from BBC programs. The label is the corresponding word class of the video snippet. The dataset has 500 word classes and each class has around 1000 training samples, 50 validation samples and 50 testing samples. The total duration of LRW is approximately 173 hours. The main challenges of LRW are: (a) the variability of appearance and pose of the speakers, (b) similar word classes such as "benefit" and "benefits", "allow" and "allowed", which demands strong discriminative power of models, and (c) the target words do not exist independently in the videos; rather, they are presented with surrounding context, which requires the model to focus on the correct keyframes.</p><p>LRW-1000. LRW-1000 <ref type="bibr" target="#b16">[17]</ref> is the first public large-scale Mandarin lip reading dataset. It is a naturally-distributed large-scale benchmark for lip reading in the wild which contains 1, 000 word classes with more than 700, 000 samples from more than 2, 000 individual speakers. Each class corresponds to the syllables of a Mandarin word composed of one or several Chinese characters. It is a challenging dataset, marked by the following properties: (a) it contains significant image quality variations such as lighting conditions and scale, as well as speakers' attribute variations in pose, speech rate, age, make-up and so on, (b) the frequency of each word class is imbalanced, which is consistent with the natural case that some words occur more frequently than others in the everyday life, and (c) the samples of the same word are not limited to a constant length range to allow for modeling of different speech rates. These factors make LRW-1000 a challenging lip reading benchmark with a large lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Data preprocessing. For both LRW and LRW-1000, we resize the cropped images of lip region to 96 × 96 as input. For LRW, we randomly crop the input to 88 × 88 during training and apply random horizontal flipping. For LRW-1000, we take a central 88 × 88 crop, and do not apply random flipping.</p><p>Network architecture. For DFN, we employ a ResNet-18 <ref type="bibr" target="#b4">[5]</ref> as the encoder, and 7 cascaded pairs of deconvolutional layers and bilinear upsampling layers as the decoder. The encoder yields a 256-dimensional vector for each frame. The decoder takes the concatenation of a source vector v s and a target vector v t as input, which is 512-dimensional, and then generates a 2-channel deformation field with the same size as the input frames. The two channels of the deformation field denote the offsets along the x and y axis at each pixel location.</p><p>For the lip reading model, as mentioned earlier, we employ ResNet-18 as the front-end and GRU as the back-end. More specifically, for the grayscale branch, the front-end is a single 3D convolution layer followed by a powerful ResNet-18 network which yields a 512-dimensional vector for each frame. For the deformation branch, we use a single 2D convolution layer on top of the ResNet-18 network. As for the back-end, we use a 2-layer bidirectional Gated Recurrent Unit (Bi-GRU) RNN with 1024 hidden units to process the sequence of the 512-dimensional vectors, each vector extracted from a frame.</p><p>Training strategies. We use the three-stage training strategy proposed in <ref type="bibr" target="#b9">[10]</ref>. We use the Adam optimizer with default hyperparameters. For LRW, the learning rate is initialized to 0.0001 and reduced by half every time when the validation loss stagnates, until the model reaches convergence. For LRW-1000, the learning rate is initialized to 0.001. In all of our experiments, when the validation loss stagnates for the first time, we reduce the learning rate of the back-end to 10% of the learning rate of the front-end. This policy works well in alleviating the overfitting problem. As for the weight of bidirectional knowledge distillation loss, we initialize it to be 100, and reduce it by half every time when the validation loss stagnates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of DFN</head><p>We performed a thorough evaluation of DFN over several aspects on LRW <ref type="bibr" target="#b3">[4]</ref>.</p><p>Firstly, the source frames, target frames, output frames, and generated deformation fields are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, the output frame matches the target frame quite well. Visualizations of the deformation field shows clear discrimination of the lip region, which carries the motion information we wish to capture, from neighboring regions. This indicates that DFN can generate precise deformation fields, which meets our expectation of directly capturing motion in the speakers' faces, especially in the lip region.</p><p>Secondly, we also studied the reconstruction quality of the output frames qualitatively and quantitatively. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, DFN is able to reconstruct faces of varying poses by warping the source frames. We randomly chose 2000 pairs of target frames and output frames to evaluate the peak signalto-noise ratio (PSNR) and structural similarity (SSIM) index. The average PSNR is 26.86 and the SSIM index is 0.82, which also proves the effectiveness of our method.</p><p>Inspired by the observations in <ref type="bibr" target="#b7">[8]</ref>, we further experiment with replacing the L1 loss with classification loss to supervise the DFN. This should help the DFN learn to generate task-specific deformation flows which better suits the lip reading task. Specifically, we freeze the decoder and unfreeze the encoder of DFN when training the deformation flow branch with classification loss, after pretraining in the self-supervised manner. As shown in <ref type="figure">Fig. 6</ref>, the action of mouth opening or closing is slightly amplified in the output frames compared with the motion in the target frames. The classification accuracy is also improved, as shown in <ref type="table" target="#tab_1">Table  I</ref>.</p><p>Finally, we compared DFN with a state-of-the-art optical flow method, PWC-Net <ref type="bibr" target="#b10">[11]</ref>, on the task of lip reading qualitatively and quantitatively in the following aspects.</p><p>(1) We utilize the pretrained model in <ref type="bibr" target="#b10">[11]</ref> to generate the optical flow of the adjacent frames in the video, and use the optical flow for lip reading. The generated optical flow and deformation flow are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It shows that the deformation flow reflects more fine-grained details.</p><p>(2) We use the deformation flow generated by DFN and optical flow generated by PWC-Net as inputs to evaluate their lip reading performance on LRW respectively. The results of are presented in <ref type="table" target="#tab_1">Table I</ref>. It indicates that our task-specific deformation flow is more effective for the lip reading task.</p><p>(3) We also compared the network complexity (i.e., floating point operations (FLOPs) and the number of params) of DFN and PWC-Net, which is shown in <ref type="table" target="#tab_1">Table II</ref>. The result shows that the computational complexity of DFN is much lower than PWC-Net, which is one of our motivations to propose DFN. The greatly reduced complexity makes it possible to use DFN in real-time applications.  <ref type="figure">6</ref>. The output frames and deformation fields generated by DFN, where the encoder is optimized with the classification loss instead of the L1 loss. The output frames have slight differences from the target frames.</p><p>According to the views in <ref type="bibr" target="#b7">[8]</ref>, optical flow learned for action recognition in a task-specific manner differs from traditional optical flow and improves the performance of action recognition. This is also the case with the deformation flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation of DFTN</head><p>In this subsection, we present the ablation studies of DFTN on LRW and LRW-1000.</p><p>Evaluation of each single branch. We pretrained the two branches (i.e., the grayscale branch and the deformation flow branch) of the two-stream networks independently. The inputs of the two branches are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. The grayscale branch alone is also the baseline model in this paper. The results in terms of recognition accuracy on LRW and LRW-1000 are shown in <ref type="table" target="#tab_1">Table III</ref>.</p><p>Evaluation of the two-stream network. We fused the probabilities predicted by the two branches to make the final classification of the testing samples. The results are shown in <ref type="table" target="#tab_1">Table III</ref>. Empirically, we found using multiplicative fusion, i.e. taking the product of the probabilities results in higher   recognition accuracy than additive fusion, i.e. taking the average of the probabilities of the two branches.</p><p>Evaluation of the bidirectional knowledge distillation loss. To make the two branches exchange the learned knowledge and further improve the performance of DFTN, we trained the two-stream network with the bidirectional knowledge distillation loss as an additional supervision. The results are presented in <ref type="table" target="#tab_1">Table III</ref>. It is shown that the bidirectional knowledge distillation not only improves the accuracy of the joint prediction, but also improves the prediction accuracy of each branch when they work independently.</p><p>Evaluation of different fusion strategies and distillation strategies. To further validate the effectiveness of the bidirectional knowledge distillation loss, we conducted experiments to compare the performance of different fusion strategies and distillation strategies. We experimented with two fusion methods that fuse the intermediate features of the two branches rather than the probabilities: 1) Average the outputs of FC layers of the two branches, feed the vector to a softmax layer to get the probability distribution, and then compute the cross-entropy loss; 2) Adopt the fusion method in <ref type="bibr" target="#b13">[14]</ref>, i.e. sum the outputs of the last layers of ResNet of the two branches, feed the resulting vector to the back-end to get the probability distribution, and then compute the cross-entropy loss. Besides the above fusion strategies, we also experimented with two unidirectional knowledge distillation strategies to compare with the bidirectional knowledge distilling strategy: 1) Distill knowledge from the grayscale branch to the deformation flow branch. 2) Distill knowledge from the deformation flow branch to the grayscale branch. The results are presented in <ref type="table" target="#tab_1">Table IV</ref>. It indicates that the fusion of the output probabilities performs better than the fusion of the intermediate features of the two branches (mid-fusion). Also, the bidirectional knowledge distillation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) Chung16 <ref type="bibr" target="#b3">[4]</ref> 61.10 Chung17 <ref type="bibr" target="#b2">[3]</ref> 76.20 Stafylakis17 <ref type="bibr" target="#b9">[10]</ref> 83.00 Stafylakis17 <ref type="bibr" target="#b9">[10]</ref> (reproduced) 77.80 Weng19 <ref type="bibr" target="#b13">[14]</ref> 84.07 DFTN 84.13 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) Yang19 <ref type="bibr" target="#b16">[17]</ref> 38.19 Wang19 <ref type="bibr" target="#b12">[13]</ref> 36.91 DFTN 41.93 outperforms unidirectional knowledge distillation with an obvious improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with State-of-the-Art</head><p>Comparison with other methods on LRW. We compared our method with other word-level lip reading methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref> on LRW. The results are presented in <ref type="table" target="#tab_5">Table V</ref>. The model in <ref type="bibr" target="#b13">[14]</ref> employs deep 3D CNNs and optical flow based two-stream networks, which achieved the existing state-of-the-art performance. Our method outperforms it, and establishes the new state-of-the-art performance.</p><p>Comparison with other methods on LRW-1000. We compared our method with other word-level lip reading methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b12">[13]</ref> on LRW-1000. The results are presented in <ref type="table" target="#tab_1">Table VI</ref>. Our method shows a considerable improvement over all previous methods on LRW-1000 and achieves stateof-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a Deformation Flow Network (DFN) to generate the deformation flow, a way to model the lip movements in the speaking process as a sequence of deformations over the lip region. Notably, the network is lightweight and trained in a self-supervised manner. To take advantages of the complementary cues provided by the deformation flow and the raw videos, we propose a Deformation Flow Based Two-stream Network (DFTN) for word-level lip reading. Different from previous methods that fuse the features of the two branches, we employ the bidirectional knowledge distillation loss to help the two branches interact with each other, and exchange knowledge during training. Finally, we compare our method with other word-level lip reading methods, and show that our method achieves state-of-the-art performance. Our work makes a first attempt to introduce facial deformation to generate a new modality. It provides potential applications and possibilities for not only lip reading, but also other face analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of DFN. It consists of an encoder and a decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of the source frames, target frames, output frames, deformation flow generated by DFN, and optical flow generated by PWC-Net [11]. The color variations indicate that the deformation flow captures more details of the face than the optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of the difference images of the output frames and target frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of the two-stream network for lip reading. The learning propcess is guided by both the classification loss and the bidirectional knowledge distillation loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of the inputs of the grayscale branch and the deformation flow branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EVALUATION</head><label>I</label><figDesc>OF DIFFERENT INPUTS ON LRW.</figDesc><table><row><cell>Input</cell><cell>Accuracy (%)</cell></row><row><cell>Grayscale</cell><cell>81.91</cell></row><row><cell>Deformaion Flow</cell><cell>77.24</cell></row><row><cell>Deformaion Flow (optimized by classification loss)</cell><cell>79.43</cell></row><row><cell>Optical Flow</cell><cell>67.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPUTATION</head><label>II</label><figDesc>EXPENSE OF DIFFERENT NETWORKS.</figDesc><table><row><cell>Network</cell><cell cols="2">GFLOPS # Params</cell></row><row><cell>DFN</cell><cell>14.5</cell><cell>7.95M</cell></row><row><cell>PWC-Net</cell><cell>635</cell><cell>9.37M</cell></row><row><cell>Lip Reading Model</cell><cell>18.4</cell><cell>40.5M</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EVALUATION</head><label>III</label><figDesc>OF DFTN ON LRW AND LRW-1000.</figDesc><table><row><cell>Method</cell><cell cols="2">LRW (%) LRW-1000 (%)</cell></row><row><cell>Grayscale branch (baseline)</cell><cell>81.91</cell><cell>38.56</cell></row><row><cell>Deformaion flow branch</cell><cell>79.43</cell><cell>36.44</cell></row><row><cell>Two-stream</cell><cell>83.03</cell><cell>41.46</cell></row><row><cell>Grayscale branch (with L BiKD )</cell><cell>82.93</cell><cell>38.76</cell></row><row><cell>Deformation flow branch (with L BiKD )</cell><cell>80.85</cell><cell>37.47</cell></row><row><cell>Two-stream(with L BiKD )</cell><cell>84.13</cell><cell>41.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc>OF DIFFERENT STRATEGIES ON LRW.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Grayscale branch</cell><cell>81.91</cell></row><row><cell>Deformation flow branch</cell><cell>79.43</cell></row><row><cell>Avg (FC)</cell><cell>82.13</cell></row><row><cell>Add (Res4)</cell><cell>82.52</cell></row><row><cell>Mul (probabilities)</cell><cell>83.03</cell></row><row><cell>Mul (probabilities) (with L KD(d−&gt;g) )</cell><cell>82.14</cell></row><row><cell>Mul (probabilities) (with L KD(g−&gt;d) )</cell><cell>82.92</cell></row><row><cell>Mul (probabilities) (with L BiKD )</cell><cell>84.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>WITH OTHER METHODS ON LRW.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>WITH OTHER METHODS ON LRW-1000.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: End-to-end sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lipreading with long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-grained spatio-temporal modeling for lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11618</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal features with twostream deep 3d cnns for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06882</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Sophia</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lrw-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
