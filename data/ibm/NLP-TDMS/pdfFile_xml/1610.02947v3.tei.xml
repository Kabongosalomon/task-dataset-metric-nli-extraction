<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
							<email>gunhee@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes a video as input and generates a list of concept words as useful semantic priors for language generation models. The proposed word detector has two important properties. First, it does not require any external knowledge sources for training. Second, the proposed word detector is trainable in an end-to-end manner jointly with any video-to-language models. To effectively exploit the detected words, we also develop a semantic attention mechanism that selectively focuses on the detected concept words and fuse them with the word encoding and decoding in the language model. In order to demonstrate that the proposed approach indeed improves the performance of multiple video-to-language tasks, we participate in all the four tasks of LSMDC 2016 <ref type="bibr" target="#b21">[22]</ref>. Our approach has won three of them, including fill-in-theblank, multiple-choice test, and movie retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video-to-language tasks, including video captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> and video question answering (QA) <ref type="bibr" target="#b26">[27]</ref>, are recent emerging challenges in computer vision research. This set of problems is interesting as one of frontiers in artificial intelligence; beyond that, it can also potentiate multiple practical applications, such as retrieving video content by users' free-form queries or helping visually impaired people understand the visual content. Recently, a number of large-scale datasets have been introduced as a common ground for researchers to promote the progress of video-tolanguage research (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>).</p><p>The objective of this work is to propose a concept word detector, as shown in <ref type="figure">Fig.1</ref>, which takes a training set of videos and associated sentences as input, and generates a list of high-level concept words per video as useful semantic priors for a variety of video-to-language tasks, including video captioning, retrieval, and question answering. We  <ref type="figure">Figure 1</ref>. The intuition of the proposed concept word detector. Given a video clip, a set of tracing LSTMs extract multiple concept words that consistently appear across frame regions. We then employ semantic attention to combine the detected concepts with text encoding/decoding for several video-to-language tasks of LSMDC 2016, such as captioning, retrieval, and question answering. design our word detector to have the following two characteristics, to be easily integrated with any video-to-language models. First, it does not require any external knowledge sources for training. Instead, our detector learns the correlation between words in the captions and video regions from the whole training data. To this end, we use a continuous soft attention mechanism that traces consistent visual information across frames and associates them with concept words from captions. Second, the word detector is trainable in an end-to-end manner jointly with any video-to-language models. The loss function for learning the word detector can be plugged as an auxiliary term into the model's overall cost function; as a result, we can reduce efforts to separately arXiv:1610.02947v3 [cs.CV] 25 Jul 2017</p><p>collect training examples and learn both models.</p><p>We also develop language model components to to effectively exploit the detected words. Inspired by semantic attention in image captioning research <ref type="bibr" target="#b37">[38]</ref>, we develop an attention mechanism that selectively focuses on the detected concept words and fuse them with word encoding and decoding in the language model. That is, the detected concept words are combined with input words to better represent the hidden states of encoders, and with output words to generate more accurate word prediction.</p><p>In order to demonstrate that the proposed word detector and attention mechanism indeed improve the performance of multiple video-to-language tasks, we participate in four tasks of LSMDC 2016 (Large Scale Movie Description Challenge) <ref type="bibr" target="#b21">[22]</ref>, which is one of the most active and successful benchmarks that advance the progress of video-tolanguage research. The challenges include movie description and multiple-choice test as video captioning, fill-in-theblank as video question answering, and movie retrieval as video retrieval. Following the public evaluation protocol of LSMDC 2016, our approach achieves the best accuracies in the three tasks (fill-in-the-blank, multiple-choice test, and movie retrieval), and comparable performance in the other task (movie description).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Our work can be uniquely positioned in the context of two recent research directions in image/video captioning.</p><p>Image/Video Captioning with Word Detection. Image and video captioning has been actively studied in recent vision and language research, including <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, to name a few. Among them, there have been several attempts to detect a set of concept words or attributes from visual input to boost up the captioning performance. In image captioning research, Fang et al. <ref type="bibr" target="#b6">[7]</ref> exploit a multiple instance learning (MIL) approach to train visual detectors that identify a set of words with bounding boxed regions of the image. Based on the detected words, they retrieve and rerank the best caption sentence for the image. Wu et al. <ref type="bibr" target="#b32">[33]</ref> use a CNN to learn a mapping between an image and semantic attributes. They then exploit the mapping as an input to the captioning decoder. They also extend the framework to explicitly leverage external knowledge base such as DBpedia for question answering tasks. Venugopalan et al. <ref type="bibr" target="#b29">[30]</ref> generate description with novel words beyond the ones in the training set, by leveraging external sources, including object recognition datasets like ImageNet and external text corpus like Wikipedia. You et al. <ref type="bibr" target="#b37">[38]</ref> also exploit weak labels and tags on Internet images to train additional parametric visual classifiers for image captioning.</p><p>In the video domain, it is more ambiguous to learn the relation between descriptive words and visual patterns. There have been only few work in video captioning. Rohrbach et al. <ref type="bibr" target="#b20">[21]</ref> propose a two-step approach for video captioning on the LSMDC dataset. They first extract verbs, objects, and places from movie description, and separately train SVM-based classifiers for each group. They then learn the LSTM decoder that generates text description based on the responses of these visual classifiers.</p><p>While almost all previous captioning methods exploit external classifiers for concept or attribute detection, the novelty of our work lies in that we use only captioning training data with no external sources to learn the word detector, and propose an end-to-end design for learning both word detection and caption generation simultaneously. Moreover, compared to video captioning work of <ref type="bibr" target="#b20">[21]</ref> where only movie description of LSMDC is addressed, this work is more comprehensive in that we validate the usefulness of our method for all the four tasks of LSMDC.</p><p>Attention for Captioning. Attention mechanism has been successfully applied to caption generation. One of the earliest works is <ref type="bibr" target="#b34">[35]</ref> that dynamically focuses on different image regions to produce an output word sequence. Later this soft attention has been extended as temporal attention over video frames <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref> for video captioning.</p><p>Beyond the attention on spatial or temporal structure of visual input, recently You et al. <ref type="bibr" target="#b37">[38]</ref> propose an attention on attribute words for image captioning. That is, the method enumerates a set of important object labels in the image, and then dynamically switch attention among these concept labels. Although our approach also exploits the idea of semantic attention, it bears two key differences. First, we extend the semantic attention to video domains for the first time, not only for video captioning but also for retrieval and question answering tasks. Second, the approach of <ref type="bibr" target="#b37">[38]</ref> relies on the classifiers that are separately learned from external datasets, whereas our approach is learnable end-toend with only training data of captioning. It significantly reduces efforts to prepare for additional multi-label classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>We summarize the contributions of this work as follows.</p><p>(1) We propose a novel end-to-end learning approach for detecting a list of concept words and attend on them to enhance the performance of multiple video-to-language tasks. The proposed concept word detection and attention model can be plugged into any models of video captioning, retrieval, and question answering. Our technical novelties can be seen from two recent trends of image/video captioning research. First, our work is a first end-to-end trainable model not only for concept word detection but also for language generation. Second, our work is a first semantic attention model for video-to-language tasks.</p><p>(2) To validate the applicability of the proposed approach, we participate in all the four tasks of LSMDC 2016.</p><p>Our models have won three of them, including fill-in-theblank, multiple-choice test, and movie retrieval. We also attain comparable performance for movie description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Detection of Concept Words from Videos</head><p>We first explain the pre-processing steps for representation of words and video frames. Then, we explain how we detect concept words for a given video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>Dictionary and Word Embedding. We define a vocabulary dictionary V by collecting the words that occur more than three times in the dataset. The dictionary size is |V| = 12 486, from which our models sequentially select words as output. We train the word2vec skip-gram embedding <ref type="bibr" target="#b17">[18]</ref> to obtain the word embedding matrix E ∈ R d×|V| where d is the word embedding dimension and V is the dictionary size. We set d = 300 in our implementation.</p><p>Video Representation. We first equidistantly sample one per ten frames from a video, to reduce the frame redundancy while minimizing loss of information. We denote the number of video frames by N . We limit the maximum number of frames to be N max = 40; if a video is too long, we use a wider interval for uniform sampling.</p><p>We employ a convolutional neural network (CNN) to encode video input. Specifically, we extract the feature map of each frame from the res5c layer (i.e. R 7×7×2,048 ) of ResNet <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet dataset <ref type="bibr" target="#b23">[24]</ref>, and then apply a 2 × 2 max-pooling followed by a 3 × 3 convolution to reduce dimension to R 4×4×500 . Reducing the number of spatial grid regions to 4 × 4 helps the concept word detector get trained much faster, while not hurting detection performance significantly. We denote resulting visual features of frames by {v n } N n=1 . Throughout this paper, we use n for denoting video frame index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">An Attention Model for Concept Detection</head><p>Concept Words and Traces. We propose the concept word detector using LSTM networks with soft attention mechanism. Its structure is shown in the red box of <ref type="figure" target="#fig_1">Fig.2</ref>. Its goal is, for a given video, to discover a list of concept words that consistently appear across frame regions. The detected concept words are used as additional references for video captioning models (section 3.1), which generates output sentence by selectively attending on those words.</p><p>We first define a set of candidate words with a size of V from all training captions. Among them, we discover K concept words per video. We set V = 2, 000 and K = 10. We first apply the automatic POS tagging of NLTK <ref type="bibr" target="#b2">[3]</ref>, to extract nouns, verbs and adjectives from all training caption sentences <ref type="bibr" target="#b6">[7]</ref>. We then compute the frequencies of those words in a training set, and select the V most common words as concept word candidates.</p><p>Since we do not have groundtruth bounding boxes for concept words in the videos, we cannot train individual concept detectors in a standard supervised setting. Our idea is to adopt a soft attention mechanism to infer words by tracking regions that are spatially consistent. To this end, we employ a set of tracing LSTMs, each of which takes care of a single spatially-consistent meaning being tracked over time, what we call trace. That is, we keep track of spatial attention over video frames using an LSTM, so that spatial attentions in adjacent frames resemble the spatial consistency of a single concept (e.g. a moving object, or an action in video clips; see <ref type="figure">Fig.1</ref>). We use a total of L tracing LSTMs to capture out L traces (or concepts), where L is the number of spatial regions in the visual feature (i.e. L = 4 × 4 = 16 for v ∈ R 4×4×D ). Fusing these L concepts together, we finally discover K concept words, as will be described next.</p><p>Computation of Spatial Attention. For each trace l, we maintain spatial attention weights α (l) n ∈ R 4×4 , indicating where to attend on (4 × 4) spatial grid locations of v n , through video frames n = 1 . . . N . The initial attention weight α (l) 0 at n = 0 is initialized with an one-hot matrix, for each of L grid locations. We compute the hidden states h (l) n ∈ R 500 of the LSTM through n = 1 . . . N by:</p><formula xml:id="formula_0">c (l) n = α (l) n ⊗ v n (1) h (l) n = LSTM(c (l) n , h (l) n−1 ).<label>(2)</label></formula><p>where A ⊗ B = j,k A (j,k) · B (j,k,:) . The input to LSTMs is the context vector c (l) n ∈ R 500 , which is obtained by applying spatial attention α (l) n to the visual feature v n . Note that the parameters of L LSTMs are shared.</p><p>The attention weight vector α (l) n ∈ R 4×4 at time step n is updated as follows:</p><formula xml:id="formula_1">e (l) n (j, k) = v n (j, k) h (l) n−1 ,<label>(3)</label></formula><formula xml:id="formula_2">α (l) n = softmax Conv(e (l) n ) ,<label>(4)</label></formula><p>where is elementwise product, and Conv(·) denotes two convolution operations before the softmax layer in <ref type="figure" target="#fig_1">Fig.2</ref>. Note that α n measures how each spatial grid location of visual features is related to the concept being tracked through tracing LSTMs. By repeating these two steps of Eq.(1)-(3) from n = 1 to N , our model can continuously find important and temporally consistent meanings over time, that are closely related to a part of video, rather than focusing on each video frame individually.</p><p>Finally, we predict the concept confidence vector p:  that is, we first concatenate the hidden states {h Training and Inference. For training, we obtain a reference concept confidence vector p * ∈ R V whose element p * i is 1 if the corresponding word exists in the groundtruth caption; otherwise, 0. We minimize the following sigmoid cross-entropy cost L con , which is often used for multi-label classification <ref type="bibr" target="#b33">[34]</ref> where each class is independent and not mutually exclusive:</p><formula xml:id="formula_3">p = σ W p h<label>(1)</label></formula><formula xml:id="formula_4">N ; · · · ; h (L) N + b p ∈ R V ,<label>(5)</label></formula><formula xml:id="formula_5">L con = − 1 V V i=1 [p * i log(p i ) + (1 − p * i ) log(1 − p i )] . (6)</formula><p>Strictly speaking, since we apply an end-to-end learning approach, the cost of Eq. <ref type="formula">(6)</ref> is used as an auxiliary term for the overall cost function, which will be discussed in section 3. For inference, we compute p for a given query video, and find top K words from the score p (i.e. argmax 1:K p). Finally, we represent these K concept words by their word embedding</p><formula xml:id="formula_6">{a i } K i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video-to-Language Models</head><p>We design a different base model for each of LSMDC tasks, while they share the concept word detector and the semantic attention mechanism. That is, we aim to validate that the proposed concept word detection is useful to a wide range of video-to-language models. For base models, we take advantage of state-of-the-art techniques, for which we do not argue as our contribution. We refer to our video-tolanguage models leveraging the concept word detector as CT-SAN (Concept-Tracing Semantic Attention Network).</p><p>For better understanding of our models, we outline the four LSMDC tasks as follows: (i) Movie description: generating a single descriptive sentence for a given movie clip, (ii) Fill-in-the-blank: given a video and a sentence with a single blank, finding a suitable word for the blank from the whole vocabulary set, (iii) Multiple-choice test: given a video query and five descriptive sentences, choosing the correct one out of them, and (iv) Movie retrieval: ranking 1,000 movie clips for a given natural language query. We defer more model details to the supplementary file. Especially, we skip the description of multiple-choice and movie retrieval models in <ref type="figure" target="#fig_4">Figure 3</ref>(b)-(c), which can be found in the supplementary file. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Model for Description</head><formula xml:id="formula_7">{a i } K i=1</formula><p>as input, and produces a word sequence as output {y t } T t=1 . The model comprises video encoding and caption decoding LSTMs, and two semantic attention models. The two LSTM networks have two layers in depth, with layer normalization <ref type="bibr" target="#b0">[1]</ref> and dropout <ref type="bibr" target="#b25">[26]</ref> with a rate of 0.2.</p><p>Video Encoder. The video encoding LSTM encodes a video into a sequence of hidden states {s n } N n=1 ∈ R D .</p><formula xml:id="formula_8">s n = LSTM(v n , s n−1 )<label>(7)</label></formula><p>where v n ∈ R D is obtained by (4, 4)-average-pooling v n . Caption Decoder. The caption decoding LSTM is a normal LSTM network as follows:</p><formula xml:id="formula_9">h t = LSTM(x t , h t−1 ),<label>(8)</label></formula><p>where the input x t is an intermediate representation of tth word input with semantic attention applied, as will be described below. We initialize the hidden state at t = 0 by the last hidden state of the video encoder:</p><formula xml:id="formula_10">h 0 = s N ∈ R D .</formula><p>Semantic Attention. Based on <ref type="bibr" target="#b37">[38]</ref>, our model in <ref type="figure" target="#fig_1">Fig.2</ref> uses the semantic attention in two different parts, which are called as input and output semantic attention, respectively.</p><p>The input semantic attention φ computes an attention weight γ t,i , which is assigned to each predicted concept word a i . It helps the caption decoding LSTM focus on different concept words dynamically at each step t.</p><p>The attention weight γ t,i ∈ R K and input vector x t ∈ R D to the LSTM are obtained by</p><formula xml:id="formula_11">γ t,i ∝ exp((Ey t−1 ) W γ a i ), (9) x t = φ(y t−1 , {a i }) = W x (Ey t−1 + diag(w x,a ) i γ t,i a i ).<label>(10)</label></formula><p>We multiply a previous word y t−1 ∈ R |V| by the word embedding matrix E to be d-dimensional. The parameters to</p><formula xml:id="formula_12">learn include W γ ∈ R d×d , W x ∈ R D×d and w x,a ∈ R d .</formula><p>The output semantic attention ϕ guides how to dynamically weight the concept words {a i } when generating an output word y t at each step. We use h t , the hidden state of decoding LSTM at t as an input to the output attention function ϕ. We then compute p t ∈ R D by attending the concept words set {a i } with the weight β t,i :</p><formula xml:id="formula_13">β t,i ∝ exp(h t W β σ(a i )),<label>(11)</label></formula><formula xml:id="formula_14">p t = ϕ(h t , {a i }) = h t + diag(w h,a ) i β t,i W β σ(a i ),<label>(12)</label></formula><p>where σ is the hyperbolic tangent, and parameters include w h,a ∈ R D and W β ∈ R D×d . Finally, the probability of output word is obtained as</p><formula xml:id="formula_15">p(y t | y 1:t−1 ) = softmax(W y p t + b y ),<label>(13)</label></formula><p>where W y ∈ R |V|×D and b y ∈ R |V| . This procedure loops until y t corresponds to the &lt;EOS&gt; token.</p><p>Training. To learn the parameters of the model, we define a loss function as the total negative log-likelihood of all the words, with regularization terms on attention weights {α t,i }, {β t,i }, and {γ t,i } <ref type="bibr" target="#b37">[38]</ref>, as well as the loss L con for concept discovery (Eq.6):</p><formula xml:id="formula_16">L = − t log p(y t ) + λ 1 (g(β) + g(γ)) + λ 2 L con (14)</formula><p>where λ 1 , λ 2 are hyperparameters and g is a regularization function with setting to p = 2, q = 0.5 as</p><formula xml:id="formula_17">g(α) = α 1,p + α 1,q (15) = i t α t,i p 1/p + t i α t,i q 1/q .</formula><p>For the rest of models, we transfer the parameters of the concept word detector trained with the description model, and allow the parameters being fine-tuned. <ref type="figure" target="#fig_4">Fig.3</ref>(a) illustrates the proposed model for the fill-in-theblank task. It is based on a bidirectional LSTM network (BLSTM) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref>, which is useful in predicting a blank word from an imperfect sentence, since it considers the sequence in both forward and backward directions. Our key idea is to employ the semantic attention mechanism on both input and output of the BLSTM, to strengthen the meaning of input and output words with the detected concept words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Model for Fill-in-the-Blank</head><p>The model takes word representation {c t } T t=1 and concept words {a i } K i=1 as input. Each c t ∈ R d is obtained by multiplying the one-hot word vector by an embedding matrix E. Suppose that the t-th text input is a blank for which we use a special token &lt;blank&gt;. We add the word prediction module only on top of the t-th step of the BLSTM.</p><p>BLSTM. The input video is represented by the video encoding LSTM in <ref type="figure" target="#fig_1">Figure 2</ref>. The hidden state of the final video frame s N is used to initialize the hidden states of the BLSTM:</p><formula xml:id="formula_18">h b T +1 = h f 0 = s N , where {h f t } T t=1 and {h b t } T t=1</formula><p>are the forward and backward hidden states of the BLSTM, respectively:</p><formula xml:id="formula_19">h f t = LSTM(x t , h f t−1 ),<label>(16)</label></formula><formula xml:id="formula_20">h b t = LSTM(x t , h b t+1 ).<label>(17)</label></formula><p>We also use the layer normalization <ref type="bibr" target="#b0">[1]</ref>. Semantic Attention. The input and output semantic attention of this model is almost identical to those of the captioning model in section 3.1, only except that the word representation c t ∈ R d is used as input at each time step, instead of previous word vector y t−1 . Then the attention weighted word vector {x t } T t=1 is fed into the BLSTM. The output semantic attention is also similar to that of the captioning model in section 3.1, only except that we apply the attention only once at t-th step where the &lt;blank&gt; token is taken as input. We feed the output of the BLSTM  Finally, the output word probability y given {c t } T t=1 is obtained via softmax on p as</p><formula xml:id="formula_21">p(y | {c t } T t=1 ) = softmax(W y p + b y ),<label>(19)</label></formula><p>where parameters include W y ∈ R |V|×D and b y ∈ R |V| .</p><p>Training. During training, we minimize the loss L as</p><formula xml:id="formula_22">L = − log p(y) + λ 1 (g(β) + g(γ)) + λ 2 L con ,<label>(20)</label></formula><p>where λ 1 , λ 2 are hyperparameters, and g is the same regularization function of Eq. <ref type="bibr" target="#b14">(15)</ref>. Again, L con is the cost of the concept word detector in Eq.(6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We report the experimental results of the proposed models for the four tasks of LSMDC 2016. More experimental results and implementation details can be found in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The LSMDC Dataset and Tasks</head><p>The LSMDC 2016 comprises four video-to-language tasks on the LSMDC dataset, which contains a parallel corpus of 118,114 sentences and 118,081 video clips sampled from 202 movies. We strictly follow the evaluation protocols of the challenge. We defer more details of the dataset and challenge rules to <ref type="bibr" target="#b21">[22]</ref> and the challenge homepage 1 . Movie Description. This task is related to video captioning; given a short video clip, its goal is to generate a single descriptive sentence. The challenge provides a subset of LSMDC dataset named LSMDC16. It is divided into training, validation, public test, and blind test set, whose sizes are 91,941, 6,542, 10,053, and 9,578, respectively. The official performance metrics include BLEU-1,2,3,4 <ref type="bibr" target="#b18">[19]</ref>, ME-TEOR <ref type="bibr" target="#b1">[2]</ref>, ROUGE-L <ref type="bibr" target="#b15">[16]</ref> and CIDEr <ref type="bibr" target="#b28">[29]</ref>.</p><p>Multiple-Choice Test. Given a video query and five candidate captions, from which its goal is to find the best option. The correct answer is the GT caption of the query video, and four other distractors are randomly chosen from the other captions that have different activity-phrase labels from the correct answer. The evaluation metric is the percentage of correctly answered test questions out of 10,053 public-test data.</p><p>Movie Retrieval. The objective is, given a short query sentence, to search for its corresponding video out of 1,000 candidate videos, sampled from the LSMDC16 public-test data. The evaluation metrics include Recall@1/5/10, and Median Rank (MedR). The Recall@k means the percentage of the GT video included in the first k retrieved videos, and the MedR indicates the median rank of the GT. Each algorithm predicts 1, 000 × 1, 000 pairwise rank scores between phrases and videos, from which all the evaluation metrics are calculated.</p><p>Movie Fill-in-the-Blank. This task is related to visual question answering; given a video clip and a sentence with a blank in it, its goal is to predict a single correct word to fill in the blank. The test set includes 30,000 examples from 10,000 clips (i.e. about 3 examples per sentence). The evaluation metric is the prediction accuracy, which is the percentage of predicted words that match with GTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>We compare with the results on the public dataset in the official evaluation server of LSMDC 2016 as of the submission deadline (i.e. November 15th, 2016 UTC 23:59). Except award winners, the LSMDC participants have no obligation to disclose their identities or used technique. Below we use the IDs in the leaderboard to denote participants.</p><p>Movie description. <ref type="table" target="#tab_3">Table 1</ref> compares the performance of movie description between different algorithms. Among comparable models, our approach ranks (5, 4, 1, 1)-th in the B1 B2 B3 B4 M R Cr EITanque <ref type="bibr" target="#b13">[14]</ref> 0.144 <ref type="bibr" target="#b3">(4)</ref> 0.042 <ref type="bibr" target="#b4">(5)</ref> 0.016 <ref type="bibr" target="#b2">(3)</ref> 0.007 <ref type="bibr" target="#b1">(2)</ref> 0.056 <ref type="bibr" target="#b6">(7)</ref> 0.130 <ref type="bibr" target="#b6">(7)</ref> 0.098 <ref type="bibr" target="#b1">(2)</ref> S2VT <ref type="bibr" target="#b30">[31]</ref> 0.162 <ref type="bibr" target="#b0">(1)</ref> 0.051 <ref type="bibr" target="#b0">(1)</ref> 0.017 <ref type="bibr" target="#b0">(1)</ref> 0.007 <ref type="bibr" target="#b1">(2)</ref> 0.070 <ref type="bibr" target="#b3">(4)</ref> 0.149 <ref type="bibr" target="#b3">(4)</ref> 0.082 (4) SNUVL 0.157 <ref type="bibr" target="#b1">(2)</ref> 0.049 <ref type="bibr" target="#b1">(2)</ref> 0.014 <ref type="bibr" target="#b3">(4)</ref> 0.004 <ref type="bibr" target="#b5">(6)</ref> 0.071 <ref type="bibr" target="#b1">(2)</ref> 0.147 <ref type="bibr" target="#b4">(5)</ref> 0.070 (6) sophieag 0.151 <ref type="bibr" target="#b2">(3)</ref> 0.047 <ref type="bibr" target="#b2">(3)</ref> 0.013 <ref type="bibr" target="#b4">(5)</ref> 0.005 <ref type="bibr" target="#b3">(4)</ref> 0.075 <ref type="bibr" target="#b0">(1)</ref> 0.152 <ref type="bibr" target="#b1">(2)</ref> 0.072 (5) ayush11011995 0.116 <ref type="bibr" target="#b7">(8)</ref> 0.032 <ref type="bibr" target="#b6">(7)</ref> 0.011 <ref type="bibr" target="#b6">(7)</ref> 0.004 <ref type="bibr" target="#b5">(6)</ref> 0.070 <ref type="bibr" target="#b3">(4)</ref> 0.138 <ref type="bibr" target="#b5">(6)</ref> 0.042 <ref type="bibr" target="#b7">(8)</ref> rakshithShetty 0.119 <ref type="bibr" target="#b6">(7)</ref> 0.024 <ref type="bibr" target="#b7">(8)</ref> 0.007 <ref type="bibr" target="#b7">(8)</ref> 0.003 <ref type="bibr" target="#b7">(8)</ref> 0.046 <ref type="bibr" target="#b7">(8)</ref> 0.108 <ref type="bibr" target="#b7">(8)</ref> 0.044 <ref type="bibr" target="#b6">(7)</ref> Aalto 0.070 <ref type="bibr" target="#b8">(9)</ref> 0.017 <ref type="bibr" target="#b8">(9)</ref> 0.005 <ref type="bibr" target="#b8">(9)</ref> 0.002 <ref type="bibr" target="#b8">(9)</ref> 0.033 <ref type="bibr" target="#b8">(9)</ref> 0.069 <ref type="bibr" target="#b8">(9)</ref> 0.037 <ref type="bibr" target="#b8">(9)</ref> Base-SAN 0.123 <ref type="bibr" target="#b5">(6)</ref> 0.038 <ref type="bibr" target="#b5">(6)</ref> 0.013 <ref type="bibr" target="#b4">(5)</ref> 0.005 <ref type="bibr" target="#b3">(4)</ref> 0.066 <ref type="bibr" target="#b5">(6)</ref> 0.150 <ref type="bibr" target="#b2">(3)</ref> 0.090 (3) CT-SAN 0.135 <ref type="bibr" target="#b4">(5)</ref> 0.044 <ref type="bibr" target="#b3">(4)</ref> 0.017 <ref type="bibr" target="#b0">(1)</ref> 0.008 <ref type="bibr" target="#b0">(1)</ref> 0.071 <ref type="bibr" target="#b1">(2)</ref> 0.159 <ref type="bibr" target="#b0">(1)</ref> 0.100 <ref type="bibr" target="#b0">(1)</ref> Fill-in-the-Blank BLEU language metrics, and (2, 1, 1)-th in the other language metrics. That is, our approach ranks first in four metrics, which means that our approach is comparable to the state-of-the-art methods. In order to quantify the improvement by the proposed concept word detection and semantic attention, we implement a variant (Base-SAN), which is our model of <ref type="figure" target="#fig_1">Fig.2</ref> without those two components. As shown in <ref type="table" target="#tab_3">Table 1</ref>, the performance gaps between (CT-SAN) and (Base-SAN) are significant.</p><p>Movie Fill-in-the-Blank. <ref type="table" target="#tab_3">Table 1</ref> also shows the results of the fill-in-the-blank task. We test an ensemble of our models, denoted by (CT-SAN) (Ensemble); the answer word is obtained by averaging the output word probabilities of three identical models trained independently. Our approach outperforms all the participants with large margins. We also compare our model with a couple of baselines: (CT-SAN) outperforms the simple single-layer LSTM/BLSTM variants with the scoring layer on top of the blank location, and (Base-SAN), which is the base model of (CT-SAN) without the concept detector and semantic attention.</p><p>Movie Multiple-Choice Test. For the multiple-choice test, our approach also ranks first as shown in <ref type="table">Table 2</ref>. As in the fill-in-the-blank, the multiple-choice task also bene-fits from the concept detector and semantic attention. Moreover, an ensemble of six models trained independently further improves the accuracy from 63.8% to 67.0%.</p><p>Movie Retrieval. <ref type="table">Table 2</ref> compares Recall@k (R@k) and Median Rank (MedR) metrics between different methods. We also achieve the best retrieval performance with significant margins from baselines. Our (CT-SAN) (Ensemble) obtains the video-sentence similarity matrix with an ensemble of two different models. First, we train six retrieval models with different parameter initializations. Second, we obtain the similarity matrix using the multiple-choice version of (CT-SAN), because it can also generate a similarity score for a video-sentence pair. Finally, we average the seven similarity matrices into the final similarity matrix. <ref type="figure" target="#fig_5">Fig.4</ref> illustrates qualitative results of our algorithm with correct or wrong examples for each task. In each set, we show sampled frames of a query video, groundtruth (GT), our prediction (Ours), and the detected concept words. We provide more examples in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>Movie Description. <ref type="figure" target="#fig_5">Fig.4</ref>(a)-(b) illustrates examples of our movie description. The predicted sentences are often related to the content of clips closely, but the words themselves are not always identical to the GTs. For instance, the generated sentence for <ref type="figure" target="#fig_5">Fig.4(b)</ref> reads the clock shows a minute, which is relevant to the video clip although its GT sentence much focuses on awards on a shelf. Nonetheless, the concept words relevant to the GT sentence are well detected such as office or clock.</p><p>Movie Fill-in-the-Blank. <ref type="figure" target="#fig_5">Fig.4(c)</ref> shows that the detected concept words are well matched with the content of the clip, and possibly help predict the correct answer. <ref type="figure" target="#fig_5">Fig.4(d)</ref> is a near-miss case where our model also predict a plausible answer (e.g. run instead of hurry).</p><p>Movie Multiple-Choice Test. <ref type="figure" target="#fig_5">Fig.4</ref>(e) shows that our concept detection successfully guides the model to select the correct answer. <ref type="figure" target="#fig_5">Fig.4(f)</ref> is an example of failure to understand the situation; the fifth candidate is chosen because GT : The sun sets behind the watery horizon as the foursome continues along the shore toward a distant resort. Ours : The sun shines as the sun sets to the horizon.  it is overlapped with much of detected words such as hall, walk, go, although the correct answer is the second. Movie Retrieval. Interestingly, the concept words of <ref type="figure" target="#fig_5">Fig.4(g)</ref> capture the abstract relation between swimming, water, and pool. Thus, the first to fifth retrieved clips include water. <ref type="figure" target="#fig_5">Fig.4(h)</ref> is a near-miss example in which our method fails to catch rare word like twitch and cocks. The first to fourth retrieved clips contain a woman's head and mouth, yet miss to catch subtle movement of mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an end-to-end trainable approach for detecting a list of concept words that can be used as semantic priors for multiple-video-to-language models. We also developed a semantic attention mechanism that effectively exploits the discovered concept words. We implemented our approach into multiple video-to-language models to participate in four tasks of LSMDC 2016. We demonstrated that our method indeed improved the performance of video captioning, retrieval, and question answering, and finally won three tasks in LSMDC 2016, including fill-in-the-blank, multiple-choice test, and movie retrieval. <ref type="figure" target="#fig_6">Figure 5(b)</ref> illustrates the proposed model for the multiple-choice test. It takes a video and five choice sentences among which only one is the correct answer. Hence, our model computes the compatibility scores between the query video and five sentences, and selects the one with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. A Model for Multiple-Choice Test</head><p>The multiple-choice model shares much resemblance to the model for fill-in-the-blank in <ref type="figure" target="#fig_6">Figure 5(a)</ref>. First, it is based on the LSTM network, although it is not bidirectional. Second, it inputs the query video into the video encoding LSTM, and use its last hidden state s N to initialize the following LSTM. Third, it uses the same word representation {c t } T t=1 for each candidate sentence. Finally, it exploits the same input semantic attention of Eq.(9)-(10), although it does not apply the output semantic attention because output is not a word but a score in this task.</p><p>We obtain a joint embedding of a pair of a single video and a sentence using the LSTM network:</p><formula xml:id="formula_23">h t = LSTM(x t , h t−1 )<label>(21)</label></formula><p>where x t = φ(c t , {a i }) ∈ R D is obtained via the input semantic attention φ of Eq.(9)-(10), from the input sentence representation {c t } T t=1 . We also initialize the hidden state h 0 = s N by the final hidden state of video representation. Once the sentence is fed into the LSTM, we obtain a multimodal embedding of a video-sentence pair as the final hidden state h T of the LSTM.</p><p>Alignment Objective. The objective of the multiplechoice model is to assign high scores for the correctly matched video-sentence pairs but low scores for incorrect pairs. Therefore, we predict a similarity score S kl between a movie clip k and a sentence l as follows:</p><formula xml:id="formula_24">S kl = (W s ) ReLU(W a h T + b a ),<label>(22)</label></formula><p>where W a ∈ R D×D , b a ∈ R D and W s ∈ R D are parameters. We train the model using a max-margin structured loss objective:</p><formula xml:id="formula_25">L = k 5 l=1 max(0, S k,l − S k,l * + ∆) + λ 1 · g(γ) + λ 2 L con<label>(23)</label></formula><p>where l * denotes the answer sentence among the five candidates. This objective encourages a positive video-sentence pair to have a higher score than a misaligned negative pair by a margin ∆. We use ∆ = 1 in our experiments. At test, for a query video k, we compute five scores {S k,l } 5 l=1 of the candidate sentences, and select the one with maximum score S k,l as the answer. <ref type="figure" target="#fig_6">Figure 5</ref>(c) illustrates our model for movie retrieval. The basic idea is to compute a score for a query text and video pair, by learning a joint representation between two modalities (i.e. query text and video) using the CBP (Compact Bilinear Pooling) layer <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. A Model for Retrieval</head><p>For the video encoding, we use the final hidden state s N of the video encoding LSTM as done in other models. We also obtain a query representation via input semantic attention like as in section A.1, through the LSTM network:</p><formula xml:id="formula_26">h t = LSTM(x t , h t−1 )<label>(24)</label></formula><p>Similarly, x t = φ(c t , {a i }) ∈ R D is obtained via the input semantic attention of Eq.(9)-(10), from the input query sentence representation {c t } T t=1 . Then, we use the final hidden state h T of query encoding LSTM as query representation.</p><p>To measure a similarity score S k,l between a movie k and a sentence l as follows (see <ref type="figure" target="#fig_6">Figure 5</ref>(c)):</p><formula xml:id="formula_27">S k,l = (W s ) maxout(W p ν(s N , h T ))<label>(25)</label></formula><p>where ν(·) denotes the CBP (Compact Bilinear Pooling) layer <ref type="bibr" target="#b7">[8]</ref>, which captures the interactions between different modalities better than simple concatenation. That is, we learn the multimodal space for common features between video encoding LSTM and query encoding LSTM. The joint representation extracted from the MCB layer is multiplied by W p ∈ R 8,000×1,500 , and further processed by a consequent maxout layer <ref type="bibr" target="#b9">[10]</ref>, which yields non-sparse activations while mitigating overfitting. Finally, we obtain the score S k,l by multiplying the output by W s ∈ R 1500×1 . We use the same max-margin structured loss objective with the multiple-choice model:</p><formula xml:id="formula_28">L = k l max(0, S k,l − S k,l * + ∆) + λ 1 · g(γ) + λ 2 L con<label>(26)</label></formula><p>which encourages a positive video-sentence pair to have a higher score than a misaligned pair by a margin ∆ (e.g. ∆ =  . Each of the models take advantage of the concept word detector described illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, and semantic attention for the sake of its objective.  <ref type="table">Table 3</ref>. Performance comparison of more baselines, for the movie description task and for the fill-in-the-blank.  <ref type="table">Table 4</ref>. Performance comparison of more baselines on multiplechoice, and movie retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Implementation Details</head><p>Optimization. We train all of our models using the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> to minimize the loss, with an initial learning rate in the range of 10 −4 to 10 −5 . We adopt the data augmentation of image mirroring. We also use batch shuffling in every training epoch. We use Xavier initialization <ref type="bibr" target="#b8">[9]</ref> for initializing the weight variables. For all models, the LSTM (BLSTM) networks are two-layered in depth, and we apply layer normalization <ref type="bibr" target="#b0">[1]</ref> and dropout <ref type="bibr" target="#b25">[26]</ref> with a rate of 0.2 to reduce overfitting.</p><p>During training of fill-in-the-blank, multiple-choice, and retrieval models, we initialize the parameters in the concept word detector component with a pre-trained model of the movie description task. The new parameters (e.g. W s , W a and the LSTM parameters for multi-choice test) are initialized randomly, and then the whole model is trained end-to-end using the provided training set.</p><p>Movie Description. The split of LSMDC16 dataset is provided by the challenge organizers: (training, validation, test, blind test set) = (101079, 9578, 10053, 7409) videosentence pairs respectively. We train our model using the training set of this split, and the Para-Phrase AD sentences additionally provided by the challenge organizers.</p><p>Fill-in-the-blank. The LSMDC16 dataset for the fillin-the-blank is splitted into (training, validation, test set) = (296961, 98483, 30350). We also train our model using the officially provided training set only. To improve prediction accuracy, we use an ensemble of models; the answer word is obtained by averaging the output word probabilities of three copies of models trained with different initializations.</p><p>Multiple-choice test. The training/validation/test split of LSMDC16 dataset is same as in the movie description task. Although it is possible to include more negative sentences other than the provided four distractors (we also find that it leads to a better accuracy), we experiment the models trained using the four distractors only. we simply average the score matrix S k,l of individual models, to obtain the ensembled score matrix. In our experiments, an ensemble of six copies of model trained independently, denoted by CT-SAN (Ensemble), shows a considerable improvement of accuracy.</p><p>Movie Retrieval. Our video encoding LSTM and query encoding LSTM use the same parameter setting with the LSTM networks for movie description. We use the  <ref type="table">Table 6</ref>. Performance comparison for ablation study of our model (CT-SAN) in two tasks. We apply the semantic attention to (i) only input, and (ii) only output.</p><p>dropout <ref type="bibr" target="#b25">[26]</ref> before the maxout layer with the rate of 0.5. The video-sentence similarity matrix M ∈ R 1,000×1,000 is obtained with an ensemble of identical models and multiple-choice model. First, we train six retrieval models and one multiple-choice model with different parameter setting. Second, we obtain the similarity matrix of alignment score from all possible pair between 1,000 natural language sentences and 1,000 movie clip. To build an ensemble model, we average the multiple similarity matrices into the final similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Experimental Results</head><p>In this section, we provide additional experimental results to support the validity of the proposed concept word detector and semantic attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. On the Quality of Concept Words</head><p>To study the effect of quality of concept words, we present and experiment more baselines: (rand-SAN), (no-ATT-SAN), and (NN-SAN).</p><p>Random Concept Words. A baseline (rand-SAN) is a variant of the same structure as (CT-SAN), except that it uses random concept words instead of the ones detected by the concept word detector. We uniformly sample K = 10 words for concept words, from the V candidates.</p><p>Without Attention. We also study an effect of spatial attention in the proposed concept word detector (section 2). With a simple baseline model denoted by (no-ATT-SAN), the spatial attention component in the concept word detector is replaced by a single two-layered LSTM. Specifically, we compute the LSTM states {h n } N n=1 (a single LSTM instead of L ones) by feeding the average-pooled visual features v n ∈ R D , and then the concept confidence vector p using the last hidden state:</p><formula xml:id="formula_29">h n = LSTM(v n , h n−1 ) (n = 1 . . . N ),<label>(27)</label></formula><formula xml:id="formula_30">p = σ(W p h N + b p ) ∈ R V ,<label>(28)</label></formula><p>which replaces Eq.(2) and Eq.(5), respectively. This baseline model simply transforms the video representation into concept words, but does not involve any spatial attention.</p><p>Nearest Neighbor. We also study a simpler baseline which use a nearest-neighbor method instead of concept word detector. This simple baseline is denoted by (NN-SAN). In this method, we simply take the concept words of the closest training video, in terms of ResNet video features averaged over time.</p><p>Quantitative Result. As shown in <ref type="table">Table 3</ref> and 4, the performance of (no-ATT-SAN) is better than (Base-SAN) and (NN-SAN), but poorer than the full model (CT-SAN), in all of the four tasks. This implies that the spatial attention helps detect concept words that are useful for video captioning. Especially, (CT-SAN) outperforms (no-ATT-SAN) in the fill-in-the-blank and the multi-choice tasks with a large margin. Nevertheless, using semantic attention turns out to be more helpful than not using it, as one can observe that (no-ATT-SAN) shows a better performance than (Base-SAN).</p><p>The performance of (rand-SAN) with semantic attention but with poor concept words, is much inferior to (Base-SAN), which even lacks semantic attention. As such, we find that the quality of concept words is crucial for performance enhancement. Besides, retrieved words from (NN-SAN) are not so helpful in training semantic attention network. (Decoupled CT-SAN) also shows worse performance than (CT-SAN) which is trained with an end-to-end manner. These suggest that joint learning the concept word detector and the task-specific network is effective in achieving a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Ablation Study</head><p>We conduct an additional ablation experiment on the semantic attention, and present the results of movie description and FITB (Fill-in-the-Blank) tasks in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. On the Number of Concept Words</head><p>We also conduct another simple experiment on the number of concept words. We compute the performance of (CT-SAN), with changing the number of detected concept words, K ∈ {5, 10, 20}. As shown in <ref type="table" target="#tab_8">Table 5</ref>, we observe only a marginal performance difference. However, as the number of concept words increases, the time required to train the whole model increases, and an overfitting is more prone to occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Examples and Qualitative Results</head><p>We visualize some examples of the spatial attention computed in the concept word detector in <ref type="figure" target="#fig_7">Figure 6-7</ref>. The spatial attentions roughly captures high-level concepts in the video (e.g. a blue car moving left to right, in <ref type="figure" target="#fig_7">Fig.6(a)</ref>). <ref type="figure">Figure 8-9</ref> show some examples of generated movie description with the concept words detected by several baselines and our approach.</p><p>In the following, we present more examples of movie description results in <ref type="figure" target="#fig_9">Figure 10</ref>. Additional examples of the fill-in-the-blank task follows in <ref type="figure" target="#fig_10">Figure 11</ref>, and more examples of the multi-choice test are given in <ref type="figure" target="#fig_1">Figure 12</ref>. Finally, we present examples of the movie retrieval task in <ref type="figure" target="#fig_4">Figure  13</ref>-14. We also show each model's output and the detected concept words correspondingly.     <ref type="figure">Figure 8</ref>. Examples of our method and baselines in movie description. We show the generated description and the detected concept words of (CT-SAN) and (no-ATT-SAN). We also compare other movie description baselines, including S2VT <ref type="bibr" target="#b30">[31]</ref> and Temporal Attention <ref type="bibr" target="#b36">[37]</ref> (we referenced their public code).       Concepts: crowd, stage, run, dancer, people, down, audience, dance, back, leap <ref type="figure" target="#fig_4">Figure 13</ref>. Positive examples of movie retrieval. From left to right, we show the 1st-5th retrieved movie clips from natural language sentence. The groundtruth movie clip is shown in the green box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) (a)</head><p>Q : SOMEONEs face contorts in anguish as she gazes at the comatose woman.  <ref type="figure" target="#fig_5">Figure 14</ref>. Negative examples of movie retrieval. The first 4 columns represent the 1st-4th retrieved movie clips, and the last one is the groundtruth movie clip (in the red box). We also show the retrieved rank of the groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the concept word detection in a top red box (section 2.2), and our video description model in bottom, which uses semantic attention on the detected concept words (section 3.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>last time step of all tracing LSTMs, apply a linear transform parameterized by W p ∈ R V ×(500L) and b p ∈ R V , and apply the elementwise sigmoid activation σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>shows the proposed video captioning model. It takes video features {v n } N n=1 and the detected concept words</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The model architectures for (a) fill-in-the-blank (section 3.2), (b) multiple-choice, and (c) movie retrieval task. The description of models for (b)-(c) can be found in the supplementary file. Each model takes advantage of the concept word detector inFig.2, and semantic attention for the sake of its objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative examples of the four vision-to-language tasks: (a)-(b) movie description, (c)-(d) fill-in-the-blank, (e)-(f) multiplechoice, and (g)-(h) movie retrieval. The left column shows correct examples while the right column shows wrong examples. In (h), we also show our retrieval ranks of the GT clips (the red box), 24th. We present more, clearer, and larger examples in the supplementary file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>(Repeat of Figure 3 in the main paper) The model architectures for (a) fill-in-the-blank (section 3.2), (b) multi-choice (section A.1), and (c) movie retrieval task (section A.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Smiling and chatting, they speed down a narrow sun-dappled road in the woods. Ours : SOMEONE drives through the trees. Concepts : road, drive, car, tree, house, park, down, back, speed, pull GT : He slows down in front of one house with a triple garage and box tree on the front lawn and pulls up onto the driveway. Ours : A car pulls up onto the driveway. Concepts : drive, down, pull, car, outside, front, house, street, get, road Visualization of spatial attentions in the movie description model. In the first row, we show five sampled keyframes from the input movie. Below, we select three tracing-LSTMs among L = 16 ones and show their spatial attention maps α (l) t (see section 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of spatial attentions in the movie description model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Examples of movie descriptions. (a)-(d) are positive examples, and (e)-(f) are near-miss or wrong examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Examples of fill-in-the-blank task. (a)-(d) are positive examples, and (e)-(f) are near-miss or wrong examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Examples of multiple-choice test. The groundtruth answer is in bold, and the output of our model is marked with a red checkbox. (a)-(c) are positive examples, and (d) is a near-miss or wrong example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Q</head><label></label><figDesc>: SOMEONE meets her daughters gaze. Concepts: smile, down, back, gaze, stare, woman, close, room, kiss, lip (a) Q : SOMEONE bows, then exits the stage with the other dancers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Left: Performance comparison for the movie description task on the LSMDC 2016 public test dataset. For language metrics, we use BLEU (B), METEOR (M), ROUGE (R), and CIDEr (Cr). We also show the ranking in parentheses. Right: Accuracy comparison (in percentage) for the movie fill-in-the-blank task.</figDesc><table><row><cell></cell><cell>Accuracy</cell></row><row><cell>Simple-LSTM</cell><cell>30.9</cell></row><row><cell>Simple-BLSTM</cell><cell>31.6</cell></row><row><cell>Base-SAN (Single)</cell><cell>34.5</cell></row><row><cell>Merging-LSTM [17]</cell><cell>34.2</cell></row><row><cell>Base-SAN (Ensemble)</cell><cell>36.9</cell></row><row><cell>SNUVL (Single)</cell><cell>38.0</cell></row><row><cell>SNUVL (Ensemble)</cell><cell>40.7</cell></row><row><cell>CT-SAN (Single)</cell><cell>41.9</cell></row><row><cell>CT-SAN (Ensemble)</cell><cell>42.7</cell></row></table><note>Table 2. Performance comparison for the multiple-choice test (ac- curacy in percentage) and movie retrieval task: Recall@k (R@k, higher is better) and Median Rank (MedR, lower is better).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>the car speeds down an empty road lined with tall evergreens that</head><label></label><figDesc>We can see awards line a shelf in his office. Ours : The clock shows a minute, then the screen shows a map of the mothership. Concepts : room, hall, back, walk, down, stand, go, step, smile, see SOMEONE sets hers down and smiles. ④ Now she lies on top of him. ⑤ As SOMEONE gazes after them, SOMEONE approaches.</figDesc><table><row><cell></cell><cell>Correct</cell><cell></cell><cell>Wrong</cell></row><row><cell cols="2">: cloud, sky, sun, horizon, vast, shore, distance, light,</cell><cell cols="2">GT : Concepts : read, screen, office, clock, row, red, show, name,</cell></row><row><cell>boat, white</cell><cell>(a)</cell><cell>map, down</cell><cell>(b)</cell></row><row><cell cols="2">Blank Sentence : He slows down in front of one _____ with a triple</cell><cell cols="2">Blank Sentence : People _____ down the path and hide behind</cell></row><row><cell cols="2">garage and box tree on the front lawn and pulls up onto the driveway.</cell><cell>the pile of pumpkins.</cell><cell></cell></row><row><cell>Answer : house</cell><cell>Our result : house</cell><cell>Answer : hurry</cell><cell>Our result : run</cell></row><row><cell cols="2">Concepts : drive, car, pull, down, front, outside, house, street, get,</cell><cell cols="3">Concepts : tree, down, towards, run, walk, people, stone, house,</cell></row><row><cell>road</cell><cell>(c)</cell><cell>forest, river</cell><cell>(d)</cell></row><row><cell>Candidate Sentences</cell><cell></cell><cell>Candidate Sentences</cell><cell></cell></row><row><cell cols="2">① SOMEONE slams SOMEONEs head against the trunk.</cell><cell></cell><cell></cell></row><row><cell cols="2">② Now, Concepts : car, drive, road, pull, down, street, house, get, speed,</cell><cell></cell><cell></cell></row><row><cell>front</cell><cell>(e)</cell><cell></cell><cell>(f)</cell></row><row><cell></cell><cell></cell><cell cols="2">Concepts : smile, down, back, gaze, stare, woman, blonde,</cell><cell>24th</cell></row><row><cell></cell><cell>(g)</cell><cell>head, watch, lip</cell><cell>(h)</cell></row></table><note>① SOMEONE glares at SOMEONE, his lips curved into a frown. ② SOMEONE follows, looking dazed. (GT Answer) ③ The kid walks into the garage and sees him.④ He comes towards her and pulls up a chair. ⑤ He walks down the hall past an open doorway and starts to go upstairs.just into the pale blue sky. (GT Answer) ③Q : They notice SOMEONE swimming.Concepts: water, pool, back, watch, down, stare, arm, smile, gaze, boy Q : SOMEONE cocks her head, her mouth twitching.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison of our model (CT-SAN) in three tasks, varying the number of detected concept words K.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Movie Description</cell><cell>Fill-in-the-Blank</cell><cell>Multi-Choice</cell></row><row><cell></cell><cell>B1</cell><cell></cell><cell>B2</cell><cell></cell><cell>B3</cell><cell></cell><cell>B4</cell><cell></cell><cell>M</cell><cell>R</cell><cell>Cr</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>CT-SAN (K = 5)</cell><cell cols="2">0.133</cell><cell cols="2">0.043</cell><cell cols="2">0.015</cell><cell cols="2">0.007</cell><cell cols="2">0.066</cell><cell>0.156</cell><cell>0.100</cell><cell>41.5</cell><cell>63.0</cell></row><row><cell>CT-SAN (K = 10)</cell><cell cols="2">0.135</cell><cell cols="2">0.044</cell><cell cols="2">0.017</cell><cell cols="2">0.008</cell><cell cols="2">0.071</cell><cell>0.159</cell><cell>0.100</cell><cell>41.9</cell><cell>63.8</cell></row><row><cell>CT-SAN (K = 20)</cell><cell cols="2">0.136</cell><cell cols="2">0.044</cell><cell cols="2">0.016</cell><cell cols="2">0.008</cell><cell cols="2">0.068</cell><cell>0.156</cell><cell>0.106</cell><cell>41.9</cell><cell>63.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Movie Description</cell><cell>Fill-in-the-Blank</cell></row><row><cell></cell><cell></cell><cell>B1</cell><cell></cell><cell>B2</cell><cell></cell><cell>B3</cell><cell></cell><cell>B4</cell><cell></cell><cell>M</cell><cell>R</cell><cell>Cr</cell><cell>Accuracy</cell></row><row><cell>only input</cell><cell></cell><cell cols="2">0.128</cell><cell cols="2">0.041</cell><cell cols="2">0.012</cell><cell cols="2">0.006</cell><cell>0.066</cell><cell>0.151</cell><cell>0.078</cell><cell>37.7</cell></row><row><cell cols="2">only output</cell><cell cols="2">0.130</cell><cell cols="2">0.043</cell><cell cols="2">0.014</cell><cell cols="2">0.005</cell><cell>0.067</cell><cell>0.148</cell><cell>0.097</cell><cell>39.1</cell></row><row><cell cols="2">input&amp;output</cell><cell cols="2">0.135</cell><cell cols="2">0.044</cell><cell cols="2">0.017</cell><cell cols="2">0.008</cell><cell>0.071</cell><cell>0.159</cell><cell>0.100</cell><cell>41.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>People and the girls sit around plates of vegetables. Ours : SOMEONE sits at a table with smile. Concepts : smile, down, table, chair, mother, watch, breakfast, walk, sits, plate</figDesc><table><row><cell>(a)</cell></row><row><cell>GT : The car is parked outside a grand mansion with a pillared entranceway.</cell></row><row><cell>Ours : The car parked outside the house.</cell></row><row><cell>Concepts : house, street, window, building, down, people, outside, front, car, camera</cell></row><row><cell>(b)</cell></row></table><note>GT :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>GroundTruth : Reaching underneath her dress once again, she shimmies them up to her waist. CT-SAN : She walks over to the couch and puts on dress. Now, at night, our view glides over a highway, its lanes glittering from the lights of traffic below. He watches them get in their Mercedes, then spots SOMEONE in a parked car. CT-SAN : SOMEONEs car pulls up outside the house.</figDesc><table><row><cell>CT-SAN-concepts : room, couch, cat, down, sits, dress, bed, back, walk, table</cell></row><row><cell>no-ATT-SAN : She puts on a shoe and puts her head down on the bed</cell></row><row><cell>no-ATT-SAN-concepts : smile, sits, down, step, bed, bedroom, people, room, put, turn</cell></row><row><cell>S2VT : SOMEONE is wearing a white gown and a woman in a white dress</cell></row><row><cell>Temporal Attention : She looks at SOMEONE</cell></row><row><cell>(a)</cell></row><row><cell>(b)</cell></row><row><cell>(c)</cell></row><row><cell>CT-SAN-concepts : car, street, get, drive, down, outside, run, front, pull, back</cell></row><row><cell>no-ATT-SAN : SOMEONE and SOMEONE walk out of the car.</cell></row><row><cell>no-ATT-SAN-concepts : car, street, tree, kiss, run, drive, back, down, black, house</cell></row><row><cell>S2VT : SOMEONE gets out of the car and run away.</cell></row><row><cell>Temporal Attention : A car pulls up the street.</cell></row><row><cell>(d)</cell></row></table><note>GroundTruth : They head along a winding road through mountains, across a steel girder bridge, and through rolling countryside. CT-SAN : The sun sets down on the river and the car pass through the bridge. CT-SAN-concepts : drive, city, road, bridge, river, sky, down, building, sun, car no-ATT-SAN : The hogwarts express is visible through the mist. no-ATT-SAN-concepts : car, walk, field, hogwarts, track, down, wall, street, dark, past S2VT : The sun is rising on the dark road. Temporal Attention : The car pulls up.GroundTruth :CT-SAN : The city lights are on the city CT-SAN-concepts : city, sky, skyscraper, light, night, crowd, view, building, sun, york no-ATT-SAN : The expo lights twinkle from the night no-ATT-SAN-concepts : crowd, couple, red, back, down, form, light, street, man, open S2VT : The sun is floating in the sky. Temporal Attention : We see lights.GroundTruth :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>GroundTruth : SOMEONE feels his way to an empty seat. CT-SAN : SOMEONE walks over to the desk in office. CT-SAN-concepts : desk, room, down, walk, back, table, sits, enters, office, woman no-ATT-SAN : SOMEONE walks over to the counter and picks up a bottle of tea.GroundTruth : SOMEONE makes a phone call on his cell. CT-SAN : The boat is a large boat, and the boat is floating on water. CT-SAN-concepts : lifeboat, water, wave, surfer, board, boat, paddle, back, down, raft no-ATT-SAN : As the fish tips over, the boats boat rocks and the water rises. More examples of our method and baselines in movie description. He sees the face of SOMEONE.Ours : SOMEONEs eyes widen as he stares at the glowing surface of the sphere.Concepts : light, glowing, sphere, screen, surface, watch, cloud, image, yellow, room GT : A while later, SOMEONE sits alone in front of two uneaten salads on the table. Ours : SOMEONE sets a plate on a table and sets it on the table Concepts : plate, table, breakfast, food, counter, kitchen, pick, egg, set, napkin We glimpse a black eagle emblem amid the return address. Ours : SOMEONE opens the envelope and finds a note written on the page. Concepts : page, note, card, envelope, book, name, find, read, paper, letter GT : SOMEONE approaches a balding man. Ours : SOMEONE sits at a desk in a meeting room. Concepts : desk, sits, table, office, down, room, man, people, phone, woman (d) GT : A bright white light is held aloft by a statuesque woman wearing a long, white robe with a blue sash draped around her. Ours : The sun shines brightly in the sky Concepts : light, cloud, sky, sphere, white, fiery, smoke, towards, energy, back GT : SOMEONE reaches out a hand and grabs SOMEONEs breast. Ours : She is wearing a pink dress. Concepts : dress, woman, pink, girl, apartment, down, walk, back, black, room</figDesc><table><row><cell>no-ATT-SAN-concepts : table, back, down, car, man, open, stand, people, one, smile S2VT : SOMEONE is wearing a white shirt. Temporal Attention : He takes out his jacket. (a) (b) no-ATT-SAN-concepts : boat, knee, back, down, man, open, stand, people, one, fish S2VT : SOMEONE is in the boat. Temporal Attention : SOMEONE is in the water. (c) (d) (b) (c) GT : (e) Figure 9. (a) (f)</cell></row></table><note>GroundTruth : Nearby a man holds up a boom box while another starts break dancing. CT-SAN : SOMEONE and SOMEONE walk through the crowd. CT-SAN-concepts : crowd, people, man, woman, dance, down, back, walk, arm, stand no-ATT-SAN : SOMEONE and SOMEONE step forward. no-ATT-SAN-concepts : walk, back, down, other, man, open, stand, people, one, smile S2VT : SOMEONE is walking towards the crowd. Temporal Attention : People walk up the street.GroundTruth : Now the SOMEONE and SOMEONE movie plays on SOMEONEs TV. CT-SAN : SOMEONE walk around SOMEONE in the tv. CT-SAN-concepts : window, room, sits, back, down, man, room, tv, walk, men no-ATT-SAN : SOMEONE watches SOMEONE and leaves the room. no-ATT-SAN-concepts : room, step, go, small, back, down, bedroom, make, get, building S2VT : SOMEONE is sitting in the room. Temporal Attention : He looks around.GT :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Blank Sentence :</head><label>Sentence</label><figDesc>They down their drinks and set the glasses back down on the _____. Answer/Our result : (counter / counter) Concepts : back, down, table, drink, smile, arm, counter, sits, beer, bar Blank Sentence : He slows down in front of one _____ with a triple garage and box tree on the front lawn and pulls up onto the driveway. Answer/Our result : (house / house) Concepts : drive, car, pull, down, front, outside, house, street, get, road Blank Sentence : As they walk down a _____, SOMEONE takes notes. Answer/Our result : (hall / corridor) Concepts : man, room, corridor, walk, step, pocket, people, down, stand, men Blank Sentence : People _____ down the steep steps cut into the grassy slope towards SOMEONEs cottage. Answer/Our result : (run / walk) Concepts : tree, towards, down, field, walk, people, horse, run, hill, river Blank Sentence : In the girls _____, SOMEONE sits in bed wearing glasses and writing. Answer/Our result : (bedroom / bedroom) Concepts : bed, sits, back, room, down, bedroom, table, arm, open, put Blank Sentence : he nervously shifts his _____ back and forth under the tipped down brim of his fedora.</figDesc><table /><note>Answer/Our result : (eyes / eyes) Concepts : gaze, stare, car, glance, back, smile, window, down, watch, hat</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Now, SOMEONE lies on the bed using his tablet as SOMEONE enters. Concepts⑤ People walk down the street.</head><label></label><figDesc>Now her hair in pigtails, SOMEONE sings on a sound stage. ② Scientists and military men look at the wreckage of something big. ③ Now, in bathing suits, the couple jumps into a pool. ④ She sets down her belongings and sits heavily on a love seat.' ⑤ : bed, sits, bedside, room, lie, back, bedroom, gaze, stare, down He takes the mic. ② SOMEONEs boss, SOMEONE, switches off his monitors. ③ then moves the woman down the hall. The stern-faced G-Man turns gravely toward SOMEONE and nods. Concepts : necklace, dance, arm, dress, woman, back, hug, down, hair, back ① As they drive away, SOMEONE peers through the back window seating between two agents. ② SOMEONEs at home composing. ③ SOMEONE gives SOMEONE a dubious look. ④ and climbs the stairs after the detective. : walk, house, down, porch, apartment, back, garden, step, outside, front ① He heads over. ② His eyes catch site of a companion set by the fireplace. ③ Looking away, she shrugs again. ④ He shows her a web video of a woman dunking her breasts in cake batter.</figDesc><table><row><cell>①</cell></row><row><cell>(a)</cell></row><row><cell>(c) ① (b)</cell></row></table><note>④ He furtively puts them back in place. ⑤⑤ He hurries up the front walkway to his house and enters.ConceptsConcepts : walk, step, people, back, down, boy, stand, woman, run, man</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>: smile, gaze, nod, back, stare, give, tear, glance, woman, down 43 th Q : Setting her own drink down, she faces the stage and takes his hand. Concepts: woman, smile, drink, bar, people, table, back, sits, watch, glass 11 th</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">o t = tanh(W o [h f t ; h b t ] + b o ),(18)where W o ∈ R D×2D and b o ∈ R D , into the output attention function ϕ, which generates p ∈ R D as in Eq.(12) of the description model, p = ϕ(o t , {a i }).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/ describingmovies/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">in our experiments). At test, for a query sentence k, we compute scores {S k,l } l for all videos l in the test set. From the score matrix, we can rank the videos for the query. As mentioned in section 4.2, an ensemble of multiple score matrices is used in our final model, which yields much better retreival performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is partially supported by Convergence Research Center through National Research Foundation of Korea (2015R1A5A7037676). Gunhee Kim is the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Details of Video-to-Language Models</head><p>In this section, we describe the further details of videoto-language models (section 3).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer Normalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>O&apos;Reilly Media Inc</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-shot Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporal tessellation for video annotation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06950</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WAS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04062</idno>
		<title level="m">Video fill in the blank with merging lstms</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coherent Multi-Sentence Video Description with Variable Level of Detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Long-Short Story of Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Translating Video Content to Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TSP</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<title level="m">Learning Language-Visual Embedding for Movie Understanding with Natural-Language</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based Image Description Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07770</idno>
		<title level="m">Captioning Images with Diverse Objects</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to Sequence -Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02814</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Describing Videos by Exploiting Temporal Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
