<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Video and Text via Large-Scale Discriminative Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><forename type="middle">Normale</forename><surname>Supérieure</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 3 CIIRC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Video and Text via Large-Scale Discriminative Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discriminative clustering has been successfully applied to a number of weakly-supervised learning tasks. Such applications include person and action recognition, text-to-video alignment, object co-segmentation and colocalization in videos and images. One drawback of discriminative clustering, however, is its limited scalability. We address this issue and propose an online optimization algorithm based on the Block-Coordinate Frank-Wolfe algorithm. We apply the proposed method to the problem of weakly-supervised learning of actions and actors from movies together with corresponding movie scripts. The scaling up of the learning problem to 66 featurelength movies enables us to significantly improve weaklysupervised action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition has been significantly improved in recent years. Most existing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> rely on supervised learning and, therefore, require large-scale, diverse and representative action datasets for training. Collecting such datasets, however, is a difficult task given the high costs of manual search and annotation of the video. Notably, the largest action datasets today are still orders of magnitude smaller (UCF101 <ref type="bibr" target="#b35">[36]</ref>, ActivityNet <ref type="bibr" target="#b6">[7]</ref>) compared to large image datasets, they often contain label noise and target specific domains such as sports (Sports1M <ref type="bibr" target="#b19">[20]</ref>).</p><p>Weakly supervised learning aims to bypass the need of manually-annotated datasets using readily-available, but possibly noisy and incomplete supervision. Examples of such methods include learning of person names from image captions or video scripts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. Learning actions from movies and movie scripts has been approached in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. Most of the work on weakly supervised person and action learning, however, has been limited to one or a few movies. Therefore the power of leveraging large-scale weakly annotated video data has not been fully explored.</p><p>In this work we aim to scale weakly supervised learning of actions. In particular, we follow the work of <ref type="bibr" target="#b3">[4]</ref> and learn actor names together with their actions from movies and movie scripts. While actors are learned separately for each movie, differently from <ref type="bibr" target="#b3">[4]</ref>, our method simultaneously learns actions from all movies and movie scripts available for training. Such an approach, however, requires solving a large-scale optimization problem. We address this issue and propose to scale weakly supervised learning by adapting the Block-Coordinate Frank-Wolfe approach <ref type="bibr">[21]</ref>. Our optimization procedure enables action learning from tens of movies and thousands of action samples, readily available from our subset of movies or other recent datasets with movie descriptions <ref type="bibr" target="#b29">[30]</ref>. This, in turn, results in large improvements in action recognition.</p><p>Besides the optimization, our work introduces a new model for background class in the form of a constraint. It enables better and automatic modeling of the background class (i.e. unknown actors and actions). We evaluate our method on 66 movies and demonstrate significant improvements for both actor and action recognition. Example results are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>This section reviews related work on discriminative clustering, Frank-Wolfe optimization and its applications to the weakly supervised learning of people and actions in video.</p><p>Discriminative clustering and Frank-Wolfe. The Frank-Wolfe algorithm <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">15]</ref> allows to minimize large convex problems over convex sets by solving a sequence of linear problems. In computer vision, it has been used in combination with discriminative clustering <ref type="bibr" target="#b1">[2]</ref> for action localization <ref type="bibr" target="#b4">[5]</ref>, text-to-video alignment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, object co-localization in videos and images <ref type="bibr" target="#b17">[18]</ref> or instance-level segmentation <ref type="bibr" target="#b30">[31]</ref>. A variant of Frank-Wolfe with randomized block coordinate descent was proposed in <ref type="bibr">[21]</ref>. This extension leads to lower complexity in terms of time and memory requirements while preserving the convergence rate. In this work we build on [21] and adapt it for the problem of large-scale weakly supervised learning of actions from movies.</p><p>Weakly supervised action recognition. Movie scripts are used as a source of weak supervision for temporal action localization in <ref type="bibr" target="#b8">[9]</ref>. An extension of this work <ref type="bibr" target="#b4">[5]</ref> exploits the temporal order of actions as a learning constraint. Other <ref type="bibr" target="#b21">[22]</ref> target spatio-temporal action localization and recognition in video using a latent SVM. A weakly supervised extension of this method <ref type="bibr" target="#b32">[33]</ref> localizes actions without location supervision at the training time. Another recent work [40] proposes a multi-fold Multiple-Instance Learning (MIL) SVM to localize actions given video-level supervision at training time. Closer to us is the work of <ref type="bibr" target="#b3">[4]</ref> that improves weakly supervised action recognition by joint action-actor constraints derived from scripts. While the approach in <ref type="bibr" target="#b3">[4]</ref> is limited to a few action classes and movies, we propose here a scalable solution and demonstrate significant improvements in action recognition when applied to the large-scale weakly supervised learning of actions from many movies.</p><p>Weakly supervised person recognition. Person recognition in TV series has been studied in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref> where the authors propose a solution to the problem of associating speaker names in scripts and faces in videos. Speakers in videos are identified by detecting face tracks with lip motion. The method in <ref type="bibr" target="#b7">[8]</ref> presents an alternative solution by formulating the association problem using a convex surrogate loss. Parkhi et al. <ref type="bibr" target="#b26">[27]</ref> present a method for person recognition combining a MIL SVM with a model for the background class. Most similar to our model is the one presented in <ref type="bibr" target="#b3">[4]</ref>. The authors propose a discriminative clustering cost under linear constraints derived from scripts to recover the identities and actions of people in movies. Apart from scaling-up the approach of <ref type="bibr" target="#b3">[4]</ref> to much larger datasets, our model extends and improves <ref type="bibr" target="#b3">[4]</ref> with a new background constraint.</p><p>Contributions. In this paper we make the following contributions: (i) We propose an optimization algorithm based on Block-Coordinate Frank-Wolfe that allows scaling up discriminative clustering models <ref type="bibr" target="#b1">[2]</ref> to much larger datasets. (ii) We extend the joint weakly-supervised Person-Action model of <ref type="bibr" target="#b3">[4]</ref>, with a simple yet efficient model of the background class. (iii) We apply the proposed optimization algorithm to scale-up discriminative clustering to an order of magnitude larger dataset, resulting in significantly improved action recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discriminative Clustering for Weak Supervision</head><p>We want to assign labels (e.g. names or action classes) to samples (e.g. person tracks in the video). As opposed to the standard supervised learning setup, the exact labels of samples are not known at training time. Instead, we are given only partial information that some samples in a subset (or bag) may belong to some of the labels. This ambiguous setup, also known as multiple instance learning, is common, for example, when learning human actions from videos and associated text descriptions.</p><p>To address this challenge of ambiguous and partial labels, we build on the discriminative clustering criterion based on a linear classifier and a quadratic loss (DIFFRAC <ref type="bibr" target="#b1">[2]</ref>). This framework has shown promising results in weakly supervised and unsupervised computer vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b30">31]</ref>. In particular, we use this approach to group samples into linearly separable clusters. Suppose we have N samples to group into K classes. We are given d-dimensional features X ∈ R N ×d , one for each of the N samples, and our goal is to find a binary matrix Y ∈ {0, 1} N ×K assigning each of the N samples to one of the labels, where Y nk = 1 if and only if the sample n (e.g. a person track in a movie) is assigned to the label k (e.g. action class running).</p><p>First, suppose the assignment matrix Y is given. In this case finding a linear classifier W can be formulated as a ridge regression problem</p><formula xml:id="formula_0">min W ∈R d×K 1 2N Y − XW 2 F + λ 2 W 2 F ,<label>(1)</label></formula><p>where X is a matrix of input features, Y is the given labels assignment matrix, . F is the matrix norm (or Frobenius norm) induced by the matrix inner product ., . F (or Frobenius inner product) and λ is a regularization hyperparameter set to a fixed constant. The key observation is that we can resolve the classifier W * in closed form as</p><formula xml:id="formula_1">W * (Y ) = (X X + N λI) −1 X Y.<label>(2)</label></formula><p>In our weakly supervised setting, however, Y is unknown. Therefore, we treat Y as a latent variable and optimize (1) w.r.t. W and Y . In details, plugging the optimal solution W * (2) in the cost (1) removes the dependency on W and the cost can be written as a quadratic function of Y , i.e. C(Y ) = Y, A(X, λ)Y F , where A(X, λ) is a matrix that only depends on the data X and a regularization parameter λ. Finding the best assignment matrix Y can then be written as the minimization of the cost C(Y ):</p><formula xml:id="formula_2">min Y ∈{0,1} N ×K Y, A(X, λ)Y F .<label>(3)</label></formula><p>Solving the above problem, however, can lead to degenerate solutions <ref type="bibr" target="#b1">[2]</ref> unless additional constraints on Y are provided. In section 3, we incorporate weak supervision in the form of constraints on the latent assignment matrices Y . The constraints on Y used for weak supervision generally decompose into small independent blocks. This block structure is the key for our optimization approach that we will present next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Large-Scale optimization</head><p>The Frank-Wolfe (FW) algorithm has been shown effective for optimizing convex relaxation of (3) in a number of vision problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>. It only requires solving linear programs on a set of constraints. Therefore, it avoids costly projections and allows the use of complicated constraints such as temporal ordering <ref type="bibr" target="#b4">[5]</ref>. However, the standard FW algorithm is not well suited to solve (3) for a large number of samples N .</p><p>First, storing the N × N matrix A(X, λ) in memory becomes prohibitive (e.g. the size of A becomes ≥ 100GB for N ≥ 200000). Second, each update of the FW algorithm requires a full pass over the data resulting in a space and time complexity of order N for each FW step.</p><p>Weakly supervised learning is, however, largely motivated by the desire of using large-scale data with "cheap" and readily-available but incomplete and noisy annotation. Scaling up weakly supervised learning to a large number of samples is, therefore, essential for its success. We address this issue and develop an efficient version of the FW algorithm. Our solution builds on the Block-Coordinate Frank-Wolfe (BCFW) [21] algorithm and extends it with a smart block-dependent update procedure as described next. The proposed update procedure is one of the key contribution of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Block-coordinate Frank-Wolfe (BCFW)</head><p>The Block-Coordinate version of the Frank-Wolfe algorithm [21] is useful when the convex feasible set Y can be written as a Cartesian product of n smaller sets of constraints: Y = Y (1) × . . . × Y (n) . Inspired by coordinate descent techniques, BCFW consists of updating one variable block Y (i) at a time with a reduced Frank-Wolfe step. This method has potentially n times cheaper iterates both in space and time. We will see that most of the weakly supervised problems exhibit such a block structure on latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">BCFW for discriminative clustering</head><p>To benefit from BCFW, we have to ensure that the time and space complexity of the block update does not depend on the total number of samples N (e.g. person tracks in all movies) but only depends on the size N i of smaller blocks of samples i (e.g. person tracks within one movie). After a block is sampled, the update consists of two steps. First, the gradient with respect to the block is computed. Then the linear oracle is called to obtain the next iterate. As we show below, the difficult part in our case is to efficiently compute the gradient with respect to the block.</p><formula xml:id="formula_3">Block gradient: a naive approach. Let's denote N i the size of block i. The objective function f of problem (3) is f (Y ) = Y, A(X, λ)Y F , where (see [2]) A(X, λ) = 1 2N (I N − X(X X + N λI d ) −1 X ). (4) To avoid storing matrix A(X, λ) of size N ×N , one can pre- compute the matrix P = (X X + N λI d ) −1 X ∈ R d×N .</formula><p>We can write the block gradient with respect to a subset of samples i as follows:</p><formula xml:id="formula_4">∇ (i) f (Y ) = 1 N (Y (i) − X (i) P Y ),<label>(5)</label></formula><p>where Y (i) ∈ R Ni×K and X (i) ∈ R Ni×d are the label assignment variable and the feature matrix for block i (e.g. person tracks in movie i), respectively. Because of the P Y matrix multiplication, naively computing this formula has the complexity O(N dK), where N is the total number of samples, d is the dimensionality of the feature space and K is the number of classes. As this depends linearly on N , we aim to find a more efficient way to compute block gradients, as described next.</p><p>Block gradient: a smart update. We now propose an update procedure that avoids re-computation of block gradients and whose time and space complexity at each iteration depends on N i instead of N . The main intuition is that we need to find a way to store information about all the blocks in a compact form. A natural way of doing so is to maintain the weights of the linear regression parameters W ∈ R d×K . From (2) we have W = P Y . If we are able to maintain the variable W at each iteration with the desired complexity O(N i dK), then the block gradient computation (5) can be reduced from O(N dK) to O(N i dK). We now explain how to effectively achieve that.</p><p>At each iteration t of the algorithm, we only update a block i of Y while keeping all other blocks fixed. We denote the direction of the update by D t ∈ R N ×K and the step size by γ t . With this notation the update becomes</p><formula xml:id="formula_5">Y t+1 = Y t + γ t D t .<label>(6)</label></formula><p>The update rule for the weight variable W t can now be written as follows:</p><formula xml:id="formula_6">W t+1 = P (Y t + γ t D t ) W t+1 = W t + γ t P D t ,<label>(7)</label></formula><p>Recall that at iteration t, BCFW only updates block i, therefore D t has non zero value only at block i. In block notation we can therefore write the matrix product P D t as:</p><formula xml:id="formula_7">P (1) , · · · , P (i) , · · · , P (n) ×   0 D (i) t 0   = P (i) D (i) t ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">P (i) ∈ R d×Ni and D (i)</formula><p>t ∈ R Ni×K are the i-th blocks of matrices P and D t , respectively. The outcome is an update of the following form</p><formula xml:id="formula_9">W t+1 = W t + γ t P (i) D (i) t ,<label>(9)</label></formula><p>where the computational complexity for updating W has been reduced to O(N i dK) compared to O(N dK) in the standard update.</p><p>We have designed a Block-Coordinate Frank-Wolfe update with time and space complexity depending only on the size of the blocks and not the entire dataset. This allows to scale discriminative clustering to problems with a very large number of samples. The pseudo-code for the algorithm is summarized in Algorithm 1. Next, we describe an application of this large-scale discriminative clustering algorithm to weakly supervised person and action recognition in movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly supervised Person-Action model</head><p>We now describe an application of our large-scale discriminative clustering algorithm with weak-supervision. The goal is to assign to each person track a name and an action. Both names and actions are mined from movie scripts. For a given movie i, we assume to have N i automatically extracted person tracks as well as the parsing of a movie script into person names and action classes. We also assume that scripts and movies have been roughly aligned in time. In such a setup we can assign labels (e.g. a name or an action) from a script section to a subset of tracks N from the corresponding time interval of a movie (see <ref type="figure" target="#fig_2">Figure 2</ref> for example). In the following, we explain how to convert such form of weak supervision into a set of constraints on latent Algorithm 1 BCFW for Discriminative Clustering <ref type="bibr" target="#b1">[2]</ref> Initiate Y 0 , P := (X X + N λI d ) −1 X , W 0 = P Y 0 , g i = +∞, ∀i. for t = 1 . . . N iter do i ← sample from distribution proportional to g <ref type="bibr" target="#b25">[26]</ref> ∇</p><formula xml:id="formula_10">(i) f (Y t ) ← 1 N (Y (i) t − X (i) W t ) # Block gradient Y min ← arg min x∈Y ( i) ∇ (i) f (Y t ), x F # Linear oracle D ← Y min − Y (i) g i ← − D, ∇ (i) f (Y t ) F # Block gap γ ← min(1, gi 1 N D,D−X (i) P (i) D F ) # Line-search W t+1 ← W t + γP (i) D # W update Y (i) t+1 ← Y (i) t + γD # Block update end for</formula><p>variables corresponding to the names and actions of people. We will also show how these constraints easily decompose into blocks. We denote Z the latent variable assignment matrix for person names and T for actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weak-supervision as constraints</head><p>We use linear constraints to incorporate weak supervision from movie scripts. In detail, we define constraints on subsets of person tracks that we call "bags". In the following we explain the procedure for construction of bags together with the definition of the appropriate constraints.</p><p>'At least one' constraint. Suppose a script reveals the presence of a person p in some time interval of the movie. We construct a set N with all person tracks in this interval. As first proposed by <ref type="bibr" target="#b3">[4]</ref>, we model that at least one track in N is assigned to person p by the following constraint n∈N Z np ≥ 1.</p><p>We can apply the same type of constraint when solving for action assignment T .</p><p>Person-Action constraint. Scripts can also provide information that a person p is performing an action a in a scene.</p><p>In such cases we can formulate stricter and more informative constraints as follows. We construct a set N containing all person tracks appearing in this scene. Following <ref type="bibr" target="#b3">[4]</ref>, we formulate a joint constraint on presence of a person performing a specific action as</p><formula xml:id="formula_12">n∈N Z np T na ≥ 1.<label>(11)</label></formula><p>Mutual exclusion constraint. We also model that each person track can only be assigned to exactly one label. This  restriction can be formalized by the mutual exclusion constraint</p><formula xml:id="formula_13">Z1 P = 1 N ,<label>(12)</label></formula><p>for Z (i.e. rows sum up to 1). Same constraint holds for T .</p><p>Background class constraint. One of our contributions is a novel way of coping with the background class. As opposed to previous work <ref type="bibr" target="#b3">[4]</ref>, our approach allows us to have background model that does not require any external data. Also it does not require a specific background class classifier as in <ref type="bibr" target="#b26">[27]</ref>. Our background class constraint can be seen as a way to supervise people and actions that are not mentioned in scripts. We observe that tracks that are not subject to constraints from Eq. (10) and tracks that belong to crowded shots are likely to belong to the background class. Let us denote by B the set of such tracks. We impose that at least a certain fraction α ∈ [0, 1] of tracks in B must belong to the background class. Assuming that person label p = 1 corresponds to the background, we obtain the following linear constraint (similar constraint can be defined for actions on T ):</p><formula xml:id="formula_14">n∈B Z n1 ≥ α | B | .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person-Action model formulation</head><p>Here we summarize the complete formulation of the person and action recognition problems.</p><p>Solving for names. We formulate the person recognition problem as discriminative clustering, where X 1 are face descriptors: min Z∈{0,1} N ×P Z, A(X 1 , λ)Z F , (Discriminative cost) <ref type="bibr" target="#b13">(14)</ref> such that</p><formula xml:id="formula_15">     n∈N Z np ≥ 1, (At least one) n∈B Z n1 ≥ α | B |, (Background) Z1 P = 1 N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Mutual exclusion)</head><p>Solving for actions. After solving the previous problem for names separately for each movie, we vertically concatenate all person name assignment matrices Z. We also define a single action assignment variable T in {0, 1} M ×A , where M is the total number of tracks across all movies and X 2 are action descriptors (details given later). We formulate our action recognition problem as a large QP:</p><formula xml:id="formula_16">min T ∈{0,1} M ×A T, A(X 2 , µ)T F , (Discriminative cost) (15) such that          n∈N T na ≥ 1, (At least one) n∈N Z np T na ≥ 1, (Person-Action) n∈B T n1 ≥ β | B |, (Background) T 1 A = 1 M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Mutual exclusion)</head><p>Block-Separable constraints. The set of linear constraints on the action assignment matrix T is block separable since each movie has it own set of constraints, i.e. there are no constraints spanning multiple movies. Therefore, we can fully demonstrate here the power of our large-scale discriminative clustering optimization (Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Our dataset is composed of 66 Hollywood feature-length movies (see the list in Appendix) that we obtained from either BluRay or DVD. For all movies, we downloaded their scripts (on www.dailyscript.com) that we temporally aligned with the videos and movie subtitles using the method described in <ref type="bibr" target="#b22">[23]</ref>. The total number of frames in all 66 movies is 11,320,252. The number of body tracks detected across all movies (see 4.3 for more details) is M = 201874.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Text pre-processing</head><p>To provide weak supervision for our method we process movie scripts to extract occurrences of the 13 most frequent action classes: Stand Up, Eat, Sit Down, Sit Up, Hand Shake, Fight, Get Out of Car, Kiss, Hug, Answer Phone, Run, Open Door and Drive. To do so, we collect a corpus of movie scripts different from the set of our 66 movies and train simple text-based action classifiers using linear SVM and a TF-IDF representation of words composed of uni-grams and bi-grams. After retrieving actions in our target movie scripts, we also need to identify who is performing the action. We used spaCy <ref type="bibr" target="#b13">[14]</ref> to parse every sentence classified as describing one of the 13 actions and get every subject for each action verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Person detection and Features</head><p>Face tracks. To obtain tracks of faces in the video, we run the multi-view face detector <ref type="bibr" target="#b24">[25]</ref> based on the DPM model <ref type="bibr" target="#b11">[12]</ref>. We then extract face tracks using the same method as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>. For each detected face, we compute facial landmarks <ref type="bibr" target="#b34">[35]</ref> followed by the face alignment and resizing of face images to 224x224 pixel. We use pretrained vgg-face features <ref type="bibr" target="#b27">[28]</ref> to extract descriptors for each face. We kept the features of dimension 4096 computed by the network at the last fully-connected layer that we L 2 normalized. For each face track, we choose the top K (in practice, we choose K=5) faces that have the best facial landmark confidence. Then we represent each track by averaging the features of the top K faces.</p><p>Body tracks. To get the person body tracks, we run the Faster-RCNN network with VGG-16 architecture finetuned on VOC 07 <ref type="bibr" target="#b31">[32]</ref>. Then we track bounding boxes using the same tracker as used to obtain face tracks. To get person identity for body tracks, we greedily link each body track to one face track by maximizing a spatio-temporal bounding box overlap measure. However if a body track does not have an associated face track as the actor's face may look away from the camera, we cannot obtain its identity. Such tracks can be originating from any actor in the movie. To capture motion features of each body track, we compute bag-of-visual-words representation of dense trajectory descriptors <ref type="bibr" target="#b37">[38]</ref> inside the bounding boxes defined by the body track. We use 4000 cluster centers for each of the HOF, MBHx and MBHy channels. In order to capture appearance of each body track we extract ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet. For each body bounding box, we compute the average RoI-pooled <ref type="bibr" target="#b31">[32]</ref> feature map of the last convolutional layer within the bounding box, which yields a feature vector of dimension 2048 for each box. We extract a feature vector every 10th frame, average extracted feature vectors over the duration of the track and L 2 normalize. Finally, we concatenate the dense trajectory descriptor and the appearance descriptor resulting in a 14028-dimensional descriptor for each body track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Multi-Class AP Background AP   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation of person recognition</head><p>We compare our person recognition method to several other methods on the Casablanca benchmark from <ref type="bibr" target="#b3">[4]</ref> and the Buffy benchmark from <ref type="bibr" target="#b34">[35]</ref>. All methods are evaluated on the same inputs: same face tracks, scripts and characters. <ref type="table">Table 1</ref> shows the Accuracy (Acc.) and Average Precision (AP) of our approach compared to other methods on the Casablanca benchmark <ref type="bibr" target="#b3">[4]</ref>. In particular we compare to Parkhi et al. <ref type="bibr" target="#b26">[27]</ref> which is a strong baseline using the same CNN face descriptors as in our method. We also show the AP of classifying the background character class (Background AP). We compare in <ref type="table" target="#tab_1">Table 2</ref> our approach to other methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref> reporting results on season 5 of the TV series "Buffy the Vampire Slayer". Both of these methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref> use speaker detection to mine additional strong (but possibly incorrect) labels from the script, which we also incorporate (as additional bags) to make the comparison fair. Our method demonstrates significant improvement over the previous results. It also outperforms other methods on the task of classifying background characters. Finally, <ref type="table" target="#tab_2">Table 3</ref> shows the sensitivity to hyper-parameter α from the background constraint (13) on the Casablanca benchmark. Note that in contrast to other methods, our background model does not require supervision for the background class. This clearly demonstrates the advantage of our proposed background model. For all experiments the hyper-parameter α of the background constraint (13) was set to 30%. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates our qualitative results for character recognition in different movies.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of action recognition</head><p>First, we compare our method to Bojanowski et al. 2013 <ref type="bibr" target="#b3">[4]</ref>. Their evaluation uses different body tracks than ours, we design here an algorithm-independent evaluation setup. We compare our model using the Casablanca movie and the Sit Down action. For the purpose of evaluation, we have manually annotated all person tracks in the movie and then manually labeled whether or not they contain the Sit Down action. Given this ground truth, we assess the two models in a similar way as typically done in object detection. <ref type="figure" target="#fig_4">Figure 3</ref> shows a precision-recall curve evaluating recognition of the Sit Down action. We show our method trained on Casablanca only (as done in <ref type="bibr" target="#b3">[4]</ref>) and then on all 66 movies. Our method trained on Casablanca is already better than <ref type="bibr" target="#b3">[4]</ref>. The improvement becomes even more evident when training our method on all 66 movies.</p><p>To evaluate our method on all 13 action classes, we use five movies (American Beauty, Casablanca, Double Indemnity, Forrest Gump and Fight Club). For each of these movies we have manually annotated all person tracks produced by our tracker according to 13 target action classes and the background action class. We assume that each track corresponds to at most one target action. In rare cases where  this assumption is violated, we annotate the track by one of the correct action classes.</p><p>In <ref type="table">Table 4</ref> we compare results of our model to different baselines. The first baseline (a) corresponds to the random assignment of action classes. The second baseline (b) Script only uses information extracted from the scripts: each time an action appears in a bag, all person tracks in this bag are then simply annotated with this action. Baseline (c) is using our action descriptors but trained in a fully supervised setup on a small subset of annotated movies. To demonstrate the strength of this baseline we have used the same action descriptors on the LSMDC2016 1 movie clip retrieval challenge. This is the largest public benchmark <ref type="bibr" target="#b29">[30]</ref> related to our work that considers movie data (but without person localization as we do in our work). <ref type="table" target="#tab_6">Table 5</ref> shows our features employed in simple CCA method as done in <ref type="bibr" target="#b23">[24]</ref> achieving state-of-the-art on this benchmark. The fourth baseline (d) is our method train only using the five evaluated movies. The fifth baseline (e) is our model without the joint personaction constraint <ref type="bibr" target="#b10">(11)</ref>, but still trained on all 66 movies. Finally, the last result (f) is from our model using all the 66 training movies and person-action constraints <ref type="bibr" target="#b10">(11)</ref>. Results demonstrate that optimizing our model on more movies brings the most significant improvement to the final results. We confirm the idea from <ref type="bibr" target="#b3">[4]</ref> that adding the information of who is performing the action in general helps identifying actions. However we also notice it is not always true for actions with interacting people such as: Fight, Hand Shake, Hug or Kiss. Knowing who is doing the action does not seems to help for these actions. <ref type="figure" target="#fig_3">Figure 4</ref> shows improvements in action recognition when gradually increasing the number of training movies. <ref type="figure" target="#fig_7">Figure 6</ref> shows qualitative results of our model on different movies. The statistics about the ground truth and constraints together with additional results are provided in Appendix.  : Qualitative results for action recognition. P stands for for the name of the character and A for the action performed by P. Last row (in red) shows mislabeled tracks with high confidence (e.g. hugging labeled as kissing, sitting in a car labeled as driving).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed an efficient online optimization method based on the Block-Coordinate Frank-Wolfe algorithm. We use this new algorithm to scale-up discriminative clustering model in the context of weakly-supervised person and action recognition in feature-length movies. Moreover, we have proposed a novel way of handling the background class, which does not require collecting background class data as required by the previous approaches, and leads to better performance for person recognition. In summary, the proposed model significantly improves action recognition results on 66 feature-length movies. The significance of the technical contribution goes beyond the problem of personaction recognition as the proposed optimization algorithm can scale-up other problems recently tackled by discriminative clustering. Examples include: unsupervised learning from narrated instruction videos <ref type="bibr" target="#b0">[1]</ref>, text-to-video alignment <ref type="bibr" target="#b5">[6]</ref>, co-segmentation <ref type="bibr" target="#b15">[16]</ref>, co-localization in videos and images <ref type="bibr" target="#b17">[18]</ref> or instance-level segmentation <ref type="bibr" target="#b30">[31]</ref>, which can be now scaled-up to an order of magnitude larger datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This appendix contains details and supplementary results to the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Slack variables</head><p>To account for imprecise information in movie scripts, we add slack variables to our constraints. We penalyze the values of slack variables with the L 2 penalty. The slackaugmented constraints are defined as:</p><formula xml:id="formula_17">n∈N Z np ≥ 1 − ξ,<label>(16)</label></formula><formula xml:id="formula_18">n∈N T na ≥ 1 − ξ,<label>(17)</label></formula><formula xml:id="formula_19">n∈N Z np T na ≥ 1 − ξ,<label>(18)</label></formula><p>where ξ is the slack variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Lower bound</head><p>In practice, we noticed that modifying the value of the lower bound in constraints <ref type="bibr" target="#b15">(16)</ref>, <ref type="bibr" target="#b16">(17)</ref>, (18) from 1 to a higher value can significantly improve the performance of the algorithm. The constraints we use become:</p><formula xml:id="formula_20">n∈N Z np ≥ α 1 − ξ,<label>(19)</label></formula><formula xml:id="formula_21">n∈N T na ≥ α 2 − ξ,<label>(20)</label></formula><formula xml:id="formula_22">n∈N Z np T na ≥ α 2 − ξ,<label>(21)</label></formula><p>where α 1 , α 2 ∈ R + are hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Combining face and body tracks</head><p>Let's denote a 1 , a 2 , ..., a n , n faces tracks in the current shot and b 1 , b 2 , ..., b m the m body tracks in this same shot (we assume m ≥ n). We want to model that each face track is associated to at most one body track but a body track does not necessary have a face track, as the face of a person may not always be visible. Let's also define the following overlap measure O between a face track a and a body track b. If A is a set of all frames of the track a and a(t), b(t) are bounding boxes of tracks a and b at frame t, we have:</p><formula xml:id="formula_23">O(a, b) = t∈A Area(a(t) ∩ b(t))</formula><p>Area(a(t)) .</p><p>We compute the overlap for all possible pairs O(a i , b j ), where i ∈ [1, n] and j ∈ [1, m]. Then we associate each face track a i with the body track b j that maximizes O(a i , b j ). Finally, for each body track b j we either do not have any associated face track (then the body track won't have a match) or have multiple face tracks a i associated to it. In the latter case, we match the body track b j with the face track a i that maximizes O(a i , b j ).</p><p>A.4. Sensitivity to the background constraint hyperparameter for action recognition <ref type="table">Table 6</ref> shows the low sensitivity of the action recognition results to the β (15) hyper-parameter on the action recognition results. <ref type="table">Table 7</ref> provides the number of action constraints we extracted from 66 movie scripts. It also shows the number of ground truth intervals for each action we obtained by an exhaustive manual annotation of human actions in five testing movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Additional dataset statistics</head><p>Our dataset contains the following 66 movies: American Beauty, As Good As It Gets, </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We automatically recognize actors and their actions in a of dataset of 66 movies with scripts as weak supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>:</head><label></label><figDesc>Person assignment matrix of movie i : # of known characters in movie i : # of person tracks in movie i : Subset of tracks with constraints : Action assignment matrix : # of action classes in the model : # of person tracks in ALL movies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the Person-Action weakly supervised model, see text for detailed explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :</head><label>4</label><figDesc>Average Precision of actions evaluated on 5 movies. (St.U: Stand Up, E.: Eat, S.D: Sit Down, Si.U.: Sit Up, H.S: Hand Shake, F.: Fight, G.C.: Get out of Car, K.: Kiss, H.: Hug, A.: Answer Phone, R.: Run, O.D.: Open Door, D.: Drive)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>PR curves of action SitDown from Casablanca.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Action recognition mAP with increasing number of training movies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results for face recognition. Green bounding boxes are face tracks correctly classified as background characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6</head><label>6</label><figDesc>Figure 6: Qualitative results for action recognition. P stands for for the name of the character and A for the action performed by P. Last row (in red) shows mislabeled tracks with high confidence (e.g. hugging labeled as kissing, sitting in a car labeled as driving).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Being John Malkovich, Big Fish, Bringing Out the Dead, Bruce the Almighty, Casablanca, Charade, Chasing Amy, Clerks, Crash, Dead Poets Society, Double Indemnity, Erin Brockovich, Fantastic Four, Fargo, Fear and Loathing in Las Vegas, Fight Club, Five Easy Pieces, Forrest Gump, Gandhi, Gang Related, Get Shorty, Hudsucker Proxy, I Am Sam, Independence Day, Indiana Jones and the Last Crusade, It Happened One Night, Jackie Brown, Jay and Silent Bob Strike Back, LA Confidential, Legally Blonde, Light Sleeper, Little Miss Sunshine, Living in Oblivion, Lone Star, Lost Highway, Men In Black, Midnight Run, Misery, Mission to Mars, Moonstruck, Mumford, Ninotchka, O Brother, Pirates of the Caribbean Dead Mans Chest, Psycho, Pulp Fiction, Quills, Raising Arizona, Rear Window, Reservoir Dogs, The Big Lebowski, The Butterfly Effect, The Cider House Rules, The Crying Game, The Godfather, The Graduate, The Grapes of Wrath, The Hustler, The Lord of the Rings The Fellowship of the Ring, The Lost Weekend, The Night of the Hunter, The Pianist, The Princess Bride, Truman Capote.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the Buffy benchmark<ref type="bibr" target="#b34">[35]</ref> using AP.</figDesc><table><row><cell>α</cell><cell>0 0.1 0.2 0.3 0.4 0.5 0.75 1.0</cell></row><row><cell cols="2">Accuracy 58 58 70 82 84 83 76 55</cell></row><row><cell>AP</cell><cell>86 87 90 94 94 93 85 58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Sensitivity to hyper-parameter α (13) on Casablanca.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>METHOD # movies Joint-Model St.U. E. S.D. Si.U. H.S.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F.</cell><cell>G.C.</cell><cell>K.</cell><cell>H. A.</cell><cell>R.</cell><cell>O.D.</cell><cell>D.</cell><cell>mAP</cell></row><row><cell>(a) Random</cell><cell>∅</cell><cell>No</cell><cell>0.9</cell><cell>0.1 0.7</cell><cell>0.1</cell><cell>0.1</cell><cell>0.6</cell><cell>0.2</cell><cell cols="3">0.3 0.5 0.2 1.8</cell><cell>0.8</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>(b) Script only</cell><cell>∅</cell><cell>No</cell><cell>3.0</cell><cell>4.3 5.5</cell><cell>2.8</cell><cell>4.7</cell><cell>2.5</cell><cell>1.6</cell><cell cols="3">11.3 4.2 1.4 13.7</cell><cell>3.1</cell><cell>3.0</cell><cell>4.7</cell></row><row><cell>(c) Fully-supervised</cell><cell>4</cell><cell>No</cell><cell cols="2">21.2 0.2 22.2</cell><cell>0.9</cell><cell>0.6</cell><cell>7.3</cell><cell>1.4</cell><cell cols="4">1.9 4.5 2.0 33.2 18.5</cell><cell>6.3</cell><cell>9.3</cell></row><row><cell>(d) Few training movies</cell><cell>5</cell><cell>Yes</cell><cell cols="2">22.6 9.6 15.6</cell><cell>8.1</cell><cell>9.7</cell><cell>6.1</cell><cell>1.0</cell><cell cols="5">6.0 2.1 4.2 44.0 16.2 15.9 12.4</cell></row><row><cell>(e) No Joint Model</cell><cell>66</cell><cell>No</cell><cell cols="2">10.7 7.0 17.1</cell><cell>7.3</cell><cell cols="2">18.0 12.6</cell><cell>2.0</cell><cell cols="5">14.9 3.6 5.8 24.4 14.2 24.9 12.5</cell></row><row><cell>(f) Full setup</cell><cell>66</cell><cell>Yes</cell><cell cols="2">27.0 9.8 28.2</cell><cell>6.7</cell><cell>7.8</cell><cell>5.9</cell><cell>1.0</cell><cell cols="5">12.9 1.7 5.7 56.3 21.3 29.7 16.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Baseline comparison against winners of the LSMDC2016 movie clip retrieval challenge</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>[40] P.Weinzaepfel, X. Martin, and C. Schmid. Towards weakly-supervised action localization. arXiv preprint arXiv:1605.05197, 2016. 2 [41] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and retrieval models with semantic attention. In ECCV LSMDC2016 Workshop, 2016. 7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>mAP 15.0 15.7 15.9 15.8 16.6 16.1 16.2 16.0 15.5Table 6: Influence of the hyper-parameter β (13) for action recognition.</figDesc><table><row><cell>β</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6 0.75 0.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/describingmovies/lsmdc-2016</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments. This work has been supported by ERC grants</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffrac: a discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">848</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding Actors and Actions in Movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from ambiguously labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Buffy&quot; -Automatic Naming of Characters in TV Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Hello! My name is</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>release 5. 6</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative Clustering for Image Co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with frank-wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Block-coordinate frank-wolfe optimization for structural SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video Description by Combining Strong Representation and a Simple Nearest Neighbor Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV LSMDC2016 Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minding the gaps for block frank-wolfe optimization of structured svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lukasewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">It&apos;s in the bag: Stronger supervision for automated face labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference. In BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linking people in videos with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instance-level video segmentation from object tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Similarity constrained latent support vector machine: An application to weakly supervised action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shapovalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Who are you?&quot; -Learning person specific classifiers from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knock! Knock! Who is it?&quot; probabilistic person identification in tv-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Action recognition ground truth and constraint statistics</title>
		<editor>St.U: Stand Up, E.: Eat, S.D: Sit Down, Si.U.: Sit Up, H.S: Hand Shake, F.: Fight, G.C.</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Get out of Car, K.: Kiss, H.: Hug, A.: Answer Phone, R.: Run, O.D.: Open Door, D.: Drive</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
