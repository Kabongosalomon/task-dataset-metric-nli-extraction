<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graphical Abstract A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-12">12 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<postCode>510631</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<postCode>528225</postCode>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<postCode>528225</postCode>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<postCode>510631</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graphical Abstract A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-12">12 Feb 2020</date>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>aspect term extraction aspect polarity classification Chinese sentiment analysis multi-task learning multilingual ABSA domain-adaption BERT</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proposing a model for the joint task of aspect term extraction and aspect polarity classification.</p><p>• The model proposed is Chinese language-oriented and applicable to the English language, with the ability to handle both Chinese and English reviews.</p><p>• The model also integrates the domain-adapted BERT model for enhancement.</p><p>• The model achieves state-of-the-art performance on seven ABSA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Aspect-based sentiment analysis (ABSA) task is a multi-grained task of natural language processing and consists of two subtasks: aspect term extraction (ATE) and aspect polarity classification (APC). Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction. Besides, the existing researches do not pay attention to the research of the Chinese-oriented ABSA task. Based on the local context focus (LCF) mechanism, this paper firstly proposes a multi-task learning model for Chineseoriented aspect-based sentiment analysis, namely LCF-ATEPC. Compared with existing models, this model equips the capability of extracting aspect term and inferring aspect term polarity synchronously, moreover, this model is effective to analyze both Chinese and English comments simultaneously and the experiment on a multilingual mixed dataset proves its availability. By integrating the domain-adapted BERT model, the LCF-ATEPC model achieved the state-ofthe-art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets. Besides, the experimental results on the most commonly used SemEval-2014 task4 Restaurant and Laptop datasets outperform the state-of-the-art performance on the ATE and APC subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aspect-based sentiment analysis <ref type="bibr" target="#b20">Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos and Manandhar (2014)</ref>; <ref type="bibr" target="#b19">Pontiki, Galanis, Papageorgiou, Manandhar and Androutsopoulos (2015)</ref>; Pontiki, Galanis, Papageorgiou, Androutsopoulos, Manandhar, AL-Smadi, Al-Ayyoub, <ref type="bibr" target="#b18">Zhao, Qin, De Clercq, Hoste, Apidianaki, Tannier, Loukachevitch, Kotelnikov, Bel, Jiménez-Zafra and Eryigit (2016)</ref> (ABSA) is a fine-grained task compared with traditional sentiment analysis, which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects. For example, given a restaurant review: "The dessert at this restaurant is delicious but the service is poor," the full-designed model for ABSA needs to extract the aspects "dessert" and "service" and correctly reason about their polarity. In this review, the consumers' opinions on "dessert" and "service" are not consistent, with positive and negative sentiment polarity respectively.</p><p>Generally, aspects and their polarity need to be manually labeled before running the aspect polarity classification procedure in the supervised deep learning models. However, most of the proposed models for aspect-based sentiment analysis tasks only focus on improving the classification accuracy of aspect polarity and ignore the research of aspect term extraction. Therefore, when conducting transfer learning on aspect-based sentiment analysis, those proposed models often fall into the dilemma of lacking aspect extraction method on targeted tasks because there is not enough research support.</p><p>The APC task is a kind of classification problem. The researches concerning APC tasks is more abundant than the ATE task, and a large number of deep learning-based models have been proposed to solve APC problems, such as the models <ref type="bibr" target="#b28">Vo and Zhang (2015)</ref>; <ref type="bibr" target="#b29">Wagner, Arora, Cortes, Barman, Bogdanova, Foster and Tounsi (2014)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b26">Tang, Qin, Feng and Liu (2016)</ref>; <ref type="bibr" target="#b31">Wang, Huang, Zhu and Zhao (2016)</ref>; <ref type="bibr" target="#b14">Ma, Li, Zhang and Wang (2017)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b7">Fan, Feng and Zhao (2018)</ref> based on long short-term memory (LSTM) and the methodologies <ref type="bibr" target="#b25">Song, Wang, Jiang, Liu and Rao (2019)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b34">Zeng, Yang, Xu, Zhou and Han (2019a)</ref> based on transformer <ref type="bibr" target="#b27">Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin (2017)</ref>. The purpose of the APC task is to predict the exact sentiment polarity of different aspects in their context, rather than to fuzzily analyze the overall sentiment polarity on the sentence-level or document-level. In the APC task, the polarities are most usually classified into three categories: positive, negative, and neutral. It is obvious that the sentiment polarity classified based on aspects can better mine the fine-grained emotional tendency in reviews or tweets, thus providing a more accurate reference for decision-makers.</p><p>Similar to the named entity recognition <ref type="bibr" target="#b24">Sang and De Meulder (2003)</ref> (NER) task, the ATE task is a sequence labeling task, which aims to extract aspects from the reviews or tweet. In most researches <ref type="bibr" target="#b3">Chen, Sun, Bing and Yang (2017a)</ref>; <ref type="bibr" target="#b33">Xue and Li (2018)</ref>; <ref type="bibr" target="#b4">Chen, Xu, He and Wang (2017b)</ref>, the ATE task is studied independently, away from the APC task. The ATE task first segments a review into separate tokens and then infers whether the tokens belong to any aspect. The tokens may be labeled in different forms in different studies, but most of the studies have adopted the IOB 2 label to annotate tokens. Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously, this paper proposes a multi-task learning model for aspect-based sentiment analysis. Multilingual processing is an important research orientation of natural language processing. The LCF-ATEPC 3 model proposed in this paper is a novel multilingual and multi-task-oriented model. Apart from achieving state-of-the-art performance in commonly used SemEval-2014 task4 datasets, the experimental results in four Chinese review datasets also validate that this model has a strong ability to expand and adapt to the needs of multilingual task. The proposed model is based on multi-head self-attention (MHSA) and integrates the pre-trained <ref type="bibr">BERT Devlin, Chang, Lee and Toutanova (2019)</ref> and the local context focus mechanism, namely LCF-ATEPC. By training on a small amount of annotated data of aspect and their polarity, the model can be adapted to a large-scale dataset, automatically extracting the aspects and predicting the sentiment polarities. In this way, the model can discover the unknown aspects and avoids the tedious and huge cost of manually annotating all aspects and polarities. It is of great significance for the field-specific aspect-based sentiment analysis.</p><p>The main contributions of this article are as follows:</p><p>1. For the first time, this paper studies the multi-task model of APC subtask and ATE subtask for multilingual reviews, which provides a new idea for the research of Chinese aspect extraction. 2. This paper firstly applies self-attention and local context focus techniques to aspect word extraction task, and fully explore their potential in aspect term extraction task. 3. The LCF-ATEPC model proposed in this paper integrates the pre-trained BERT model, significantly improves both the performance of ATE task and APC subtask, and achieves new state-of-the-art performance especially the F1 score of ATE task. Besides, we adopted the domain-adapted BERT model trained on the domain-related <ref type="bibr">2</ref> The labels adopted in this paper are: , , <ref type="bibr">3</ref> The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC corpus to the ABSA joint-task learning model. The experimental results show that the domain-adapted BERT model significantly promotes the performance of APC tasks on the three datasets, especially the Restaurant dataset. 4. We designed and applied dual labels for the input sequence applicable for the SemEval-2014 and Chinese review datasets of ABSA joint-task, the aspect term label, and the sentiment polarity label, respectively. The dual label improves the learning efficiency of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Most ABSA-oriented methodologies regard the ATE and the APC as independent tasks and major in one of them. Accordingly, this section will introduce the related works of ATE and APC in two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Aspect Term Extraction</head><p>The approaches to ATE tasks are classified into two categories: the early dictionary-based or rule-based approaches, and methodologies based on machine-learning or deep learning. <ref type="bibr" target="#b22">Poria, Cambria, Ku, Gui and Gelbukh (2014)</ref> proposed a new rule-based approach to extracting aspects from product reviews using common sense and sentence dependency trees to detect explicit and implicit aspects. <ref type="bibr" target="#b13">Liu, Gao, Liu and Zhang (2015)</ref> adopts an unsupervised and domain-independent aspect extraction method that relies on syntactic dependency rules and can selects rules automatically.</p><p>Compared with manually annotating all aspects in the dataset, the models for ATE can learn the features of aspects and automatically extract aspects in the text, which greatly saves labor and time. <ref type="bibr" target="#b15">Mukherjee and Liu (2012)</ref> proposed a model that can extract and cluster aspects simultaneously according to the seed words provided by users for several aspect categories. By classification, synonymous aspects can be grouped into the same category. <ref type="bibr" target="#b21">Poria, Cambria and Gelbukh (2016)</ref> proposed the first aspect-oriented deep learning model in opinion mining, which deploys a 7layer deep convolutional neural network to mark each word in the sentences with opinions as an aspect or non-aspect word. <ref type="bibr" target="#b8">He, Lee, Ng and Dahlmeier (2017)</ref> proposed a new method for aspect term extraction, which utilizes word embedding to explore the co-occurrence distribution of words and applies the attention mechanism to weaken the irrelevant words and further improves the coherence of all aspects. <ref type="bibr" target="#b30">Wang, Pan, Dahlmeier and Xiao (2017)</ref> proposed a deep neural network-based model namely coupled multilevel attention, which does not require any parser or other linguistic resources to be pre-processed and provides an end-to-end solution. Besides, the proposed model is a multilayer attention network, where each layer deploys a pair of attentions. This model allows the aspect terms and opinion terms learned interactively and dual propagate during the training process.</p><p>For the Chinese-oriented ATE task, a multi-aspect bootstrapping (MAB) method <ref type="bibr" target="#b39">Zhu, Wang, Zhu, Tsou and Ma (2011)</ref> is proposed to extract the aspects of Chinese restaurant reviews. <ref type="bibr" target="#b37">Zhao, Dong and Yang (2015a)</ref> introduced machine learning methods to explore and extract aspect terms from Chinese hotel reviews. they chose the optimal feature-dimension, feature representation, and maximum entropy (ME) classifier according to the empirical results, and studied the integral effect of aspect extraction.</p><p>Up to now, the MHSA and pre-trained model has not been applied in the ATE task. This paper explores the potential of the new techniques of deep learning and new network architecture in the ATE task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Aspect Polarity Classification</head><p>Aspect polarity classification is another important subtask of ABSA. The approaches designed for the APC task can be categorized into traditional machine learning and recent deep learning methods.The APC task has been comprehensively turned to the the deep neural networks. Therefore, this section mainly introduces approaches based on deep learning techniques.</p><p>The most commonly applied deep neural network architectures for APC task are recurrent neural networks <ref type="bibr" target="#b26">Tang et al. (2016)</ref>; <ref type="bibr" target="#b31">Wang et al. (2016)</ref>; <ref type="bibr" target="#b14">Ma et al. (2017)</ref>; <ref type="bibr" target="#b11">Li, Bing, Lam and Shi (2018)</ref>; <ref type="bibr" target="#b10">Huang, Ou and Carley (2018)</ref> (RNNs) and convolutional neural networks (CNNs) <ref type="bibr" target="#b33">Xue and Li (2018)</ref>; <ref type="bibr" target="#b4">Chen et al. (2017b)</ref>; <ref type="bibr" target="#b36">Zhang, Zou and Gan (2018)</ref>. TD-LSTM <ref type="bibr" target="#b26">Tang et al. (2016)</ref> first divides the context of aspects into the left and right parts and modeling for them independently. Attention mechanism <ref type="bibr" target="#b0">Bahdanau, Cho and Bengio (2014)</ref> has been adapted to APC task in the last few years. ATAE-LSTM takes the feature representation of aspects and context words as the input of the model and applies an attention mechanism to dynamically calculate the attention weight according to the relationship between aspects and context words, and finally predicts the polarity of aspects according to the weighted context features. Another LSTM-based model <ref type="bibr">IAN Ma et al. (2017)</ref> deployed with attention mechanism equips two independent LSTM networks to capture the features of the context and aspect, with interactively integrating and learning the inner correlation of the features of context and targeted aspects. The <ref type="bibr">RAM Chen et al. (2017a)</ref> is a bi-directional LSTM-based architecture deploys a multi-layer deep neural network with dedicated memory layers. The multi-layer network utilizes the token features learned based on the attention mechanism and GRUs to finally obtain the global semantic features of the text to predict the sentiment polarities of targeted aspects. In order to retard the loss of context features during the training process, TNet <ref type="bibr" target="#b11">Li et al. (2018)</ref> introduced a conventional transformation architecture based on context-preserving transformation (CPT) units. TNet integrates the bidirectional LSTM network and convolutional neural network and significantly improves the accuracy of sentiment polarity prediction. Multi-grained attention network <ref type="bibr" target="#b7">Fan et al. (2018)</ref>  <ref type="figure">(MGAN)</ref> is a new deep neural network model, which equips with a variety of finegrained attention mechanisms, and applies the fine-grained attention mechanisms to interactively learn the token-level features between aspects and context, making great use of the inherent semantic correlation of aspects and context. <ref type="bibr" target="#b17">Peng, Ma, Li and Cambria (2018)</ref> proposed the methods for the Chinese language APC task, which conducted the APC task at the aspect level via three granularities. Two fusion methods for the granularities in the Chinese APC task are introduced and applied. Empirical results show that the proposed methods achieved promising performance on the most commonly used ABSA datasets and four Chinese review datasets. Meanwhile, a joint framework aimed to aspect sentiment classification subtask and aspect-opinion pair identification subtask is proposedby <ref type="bibr" target="#b2">Chen and Huang (2019)</ref>, in which the external knowledge are considered and put into the network to alleviate the problem of insufficient train data. The gated alternate neural network (GANN) <ref type="bibr" target="#b12">Liu and Shen (2019)</ref> proposed for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the gate truncation RNN (GTR) to learn the aspectdependent sentiment clue representations. <ref type="bibr" target="#b35">Zeng, Ma, Chen and Li (2019b)</ref> proposed an end-to-end neural network model for the ABSA task based on joint learning, and the experimental results on a Chinese review show that the proposed model works fine while conducting ATE and APC subtask simultaneously.</p><p>BERT-SPC is the BERT text pair classification model, it is a variation model of BERT and is adapted to solve the ABSA task in <ref type="bibr" target="#b25">Song et al. (2019)</ref> and achieve high performance. LCF- <ref type="bibr">BERT Zeng et al. (2019a)</ref> proposed a featurelevel local context focus mechanism based on self-attention, which can be applied to aspect level emotion analysis and many other fine-grained natural language processing tasks. BERT-ADA <ref type="bibr" target="#b23">Rietzler, Stabinger, Opitz and Engl (2019)</ref> shows that although the pre-trained model based on a large universal corpus, and is easy to be applied to most tasks and improve performance. Still, it is not task-specific. For specific tasks, if the pre-trained BERT is adapted to specific tasks through the fine-tuning process on a task-related corpus, the task performance can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Aspect-based sentiment analysis relies on the targeted aspects, and most existing studies focus on the classification of aspect polarity, leaving the problem of aspect term extraction. To propose an effective aspect-based sentiment analysis model based on multi-task learning, we adopted domain-adapted BERT model from BERT-ADA and integrated the local context focus mechanism into the proposed model. This section introduces the architecture and methodology of LCF-ATEPC.</p><p>This section introduces the methodology of the APC module and the ATE module, respectively. and the contents are organized by order of the network layer hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Aspect Term Extraction</head><p>Similar to name entity recognition (NER) task, the ATE task is a kind of sequence labeling task, and prepare the input based on IOB labels. We design the IOB labels as , , , and the labels indicate the beginning, inside and outside of the aspect terms, respectively. For ATE task, the input of the example review "The price is reasonable although the service is poor." will be prepared as = { 1 , 2 ⋯ }, and stands for a token after tokenization, = 10 is the total number of tokens. The example will be labeled in = { , , , , , , , , , }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Aspect Polarity Classification</head><p>Aspect polarity classification is a multi-grained sub-task of sentiment analysis, aiming at predicting the aspect polarity for targeted aspects. Suppose that "The price is reasonable although the service is poor . " is the input for APC task, consistently with ATE task, = { 1 , 2 ⋯ } stands for all the token of the review, and = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Architecture</head><p>Aiming at the problem of insufficient research on aspect term extraction task, a joint deep learning model is designed in this section. This model combines aspect polarity classification task and aspect term extraction task, and two independent BERT layers are adopted to model the global context and the local context respectively. For conducting multi-task training at the same time, the input sequences are tokenized into different tokens and the each token is assigned two kinds of label. The first label indicates whether the token belongs to an aspect; the second label marks the polarity of the tokens belongs to the aspect. <ref type="figure" target="#fig_1">Fig 2 is</ref> the network architecture of LCF-ATEPC. Local context feature generator (LCFG) unit is on the left and a global context feature generator (GCFG) unit is on the right. Both context feature generator units contain an independent pre-trained BERT layer, and respectively. The LCFG unit extracts the features of the local context by a local context focus layer and a MHSA encoder. The GCFG unit deploys only one MHSA encoder to learn the global context feature. The feature interactive learning (FIL) layer combines the learning of the interaction between local context features and global context features and predicts the sentiment polarity of aspects. The extraction of aspects based on the features of the global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">BERT-Shared Layer</head><p>The pre-trained BERT model is designed to improve performance for most NLP tasks, and The LCF-ATEPC model deploys two independent BERT-Shared layers that are aimed to extract local and global context features. For pre-trained BERT, the fine-tuning learning process is indispensable. Both BERT-Shared layers are regarded as embed-ded layers, and the fine-tuning process is conducted independently according to the joint loss function of multi-task learning.</p><p>and are used to represent the tokenized inputs of LCFG and GCFG respectively, and we can obtain the preliminary outputs of local and global context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>(1)</p><formula xml:id="formula_0">= ( )<label>(2)</label></formula><p>and are the output features of the LCFG and the GCFG, respectively. and are the corresponding BERT-shared layer embedded in the LCFG and the GCFG respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Head Self-Attention</head><p>Multi-head self-attention is based on multiple scale-dot attention (SDA), which can be utilized to extract deep semantic features in the context, and the features are represented in self-attention score. The MHSA can avoids the negative influence caused by the long distance dependence of the context when learning the features. Suppose is the input features learned by the LCFG. The scale-dot attention is calculate as follows:</p><formula xml:id="formula_1">SDA(X SDA ) = ⋅ √ ⋅ (3) , , = ( ) (4) ( ) = ⎧ ⎪ ⎨ ⎪ ⎩ = ⋅ = ⋅ = ⋅<label>(5)</label></formula><p>, and are the abstract matrices packed from the input features of SDA by three weight matrices</p><formula xml:id="formula_2">∈ ℝ ℎ × , ∈ ℝ ℎ × , ∈ ℝ ℎ × .</formula><p>The MHSA performs multiple scaled-dot attention in parallel and concatenate the output features, then transform the features by multiplying a vector . ℎ represents the number of the attention heads and equal to 12.</p><formula xml:id="formula_3">( ) = tanh 1 ; … ; ℎ ⋅<label>(6)</label></formula><p>The ";" means feature concatenation of each head. ∈ ℝ ℎ × ℎ is the parameter matrices for projection . Additionally, we apply a tanh activation function for the MHSA learning process, which significantly enhanced featurecapture capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Local Context Focus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Semantic-Relative Distance</head><p>The determination of local context depends on semantic-relative distance (SRD), which is proposed to determine whether the context word belongs to the local context of a targeted aspect to help the model capture the local context. Local context is a new concept that can be adapted to most fine-grained NLP tasks. In the ABSA field, existing models generally segment input sequences into aspect sequences and context sequences, treat aspects and context as independent segments and model their characteristics separately. Instead of leaving the aspect alone as part of the input, this paper mines the aspect and its local context, because the empirical result shows the local context of the target aspect contains more important information.</p><p>SRD is a concept based on token-aspect pairs, describing how far a token is from the aspect. It counts the number of tokens between each specific token towards a targeted aspect as the SRD of all token-aspect pairs. The SRD is calculated as:</p><formula xml:id="formula_4">= | − | − ⌊ 2 ⌋<label>(7)</label></formula><p>Figure 3: The simulation of the context-feature dynamic mask (CDM) mechanism. The arrows mean the contribution of the token in the computation of the self-attention score to arrowed positions (POS). And the features of the output position that the dotted arrow points to will be masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>The simulation of the context-feature dynamic weighting (CDW) mechanism. The features of the output position (POS) that the dotted arrow points to will be weighted decay.</p><p>where (1 &lt; &lt; ) is the position of the specific token, is the central position of aspect. is the length of targeted aspect, and represents for the SRD between the -th token and the targeted aspect. <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref> are two implementations of the local context focus mechanism, the context-feature dynamic mask (CDM) layer and context-feature dynamic weighting (CDW) layer, respectively. The bottom and top of the figures represent the feature input and output positions (POS) corresponding to each token. The self-attention mechanism treats all tokens equally, so that each token can generate the self-attention score with other tokens through parallel matrix operation. According to the definition of MHSA, the features of the output position corresponding to each token are more closely related to itself. After calculating the output of all tokens by MHSA encoder, the output features of each output position will be masked or attenuated, except that the local context will be retained intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Context-features Dynamic Mask</head><p>Apart from to the features of the local context, the CDM layer will mask non-local context's features learned by the layer. Although it is easy to directly mask the non-local context words in the input sequence, it is inevitable to discard the features of non-local context words. As the CDM layer is deployed, only a relatively small amount of the semantic context itself will be masked at the corresponding output position. The relative representation of context words and aspects with relatively few semantics is preserved in the corresponding output position.</p><p>According to the CDM implementation, the features on all the positions of non-local context words will be set to zero vectors. In order to avoid the unbalanced distribution of features after the CDM operation, an MHSA encoder is utilized to learn and rebalance the masked local context features. Suppose that the is the preliminary output features of , then we get the local context feature output as follows,</p><formula xml:id="formula_5">= ≤ &gt; (8) = 1 , 2 , … (9) = ⋅<label>(10)</label></formula><p>To mask the features of non-local context, we defines a feature masking matrix , and is the mask vectors for each token in the input sequence. is the SRD threshold and is the length of input sequence including aspect. Tokens whose SRD regarding to the targeted aspect is less than the threshold are the local contexts. The ∈ ℝ ℎ represents the ones vector and ∈ ℝ ℎ is the zeros vectors. "." denotes the dot-product operation of the vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>(11) Finally the local context features learned by the CDM layer are delivered as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Context-features Dynamic Weighting</head><p>Although empirical results show that the CDM has achieved excellent performance compared with existing models, we design the CDW to explore the potential of LCF mechanism. The CDW is another implementation of the LCF mechanism, takes a more modest strategy compared to the CDM layer, which simply drops the features of the non-local context completely. While the features of local context retained intact, the features of the non-local context words will be weighted decay according to their SRD concerning a targeted aspect.</p><formula xml:id="formula_6">= ≤ −( − ) ⋅ &gt; (12) = 1 , 2 , …<label>(13)</label></formula><formula xml:id="formula_7">= ⋅<label>(14)</label></formula><p>where is the constructed weight matrix and is the weight vector for each non-local context words. Consistently with CDM, is the SRD between the i-th context token and a targeted aspect. is the length of the input sequence. is the SRD threshold. "." denotes the vector dot-product operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>( <ref type="formula" target="#formula_1">15)</ref> is the output of CDW layer. The CDM and CDW layers are independent, which mean they are alternative. Both the output features of CDM and CDW layers are denoted as . Besides, we tried to concatenate the learned features of CDM and CDW layers and take linear transformation as the features of local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= [ ; ]</head><p>(16)</p><formula xml:id="formula_8">= ⋅ + (17) =<label>(18)</label></formula><p>, and are weight matrix and bias vector, respectively. The model can choose one of the three approaches to learn the local context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Feature Interactive Learning</head><p>LCF-ATEPC does not only rely on local context features for sentiment polarity classification, but combines and learns the local context features and the global context features to conduct polarity classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>;</p><formula xml:id="formula_9">(19) = ⋅ + (20) =<label>(21)</label></formula><p>and are the local context features and global context features, respectively. ∈ ℝ ℎ ×2 ℎ and ∈ ℝ ℎ are the weights and bias vectors, respectively. To learn the features of the concatenated vectors, an MHSA encoding process is performed on the .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Aspect Polarity Classifier</head><p>Aspect polarity classifier performs a head-pooling on the learned concatenated context features. Head-pooling is to extract the hidden states on the corresponding position of the first token in the input sequence. then a Softmax operation is applied to predict the sentiment polarity.</p><formula xml:id="formula_10">= (22) = exp( ) ∑ =1 exp( )<label>(23)</label></formula><p>where is the number of sentiment categories, and represents the polarity predicted by aspect polarity classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Aspect Term Extractor</head><p>Aspect term extractor first performs the token-level classification for each token, suppose is the features on the corresponding position of token ,</p><formula xml:id="formula_11">= exp( ) ∑ =1 exp( )<label>(24)</label></formula><p>where is the number of token categories, and represents the token category inferred by aspect polarity classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Training Details</head><p>The LCFG and the GCFG are based on the BERT-BASE models. However, the BERT-SPC <ref type="bibr" target="#b25">Song et al. (2019)</ref> model significantly improved the APC tasks and can be adapted to enhance the APC performance of LCF-ATEPC model. The BERT-SPC only refactored the input sequence form compared with BERT-BASE model. The input sequence of BERT-BASE is formed in "[CLS]" + sequence + "[SEP]", while it is formed in "[CLS]" + sequence + "[SEP]" + aspect + "[SEP]" for BERT-SPC.</p><p>Since LCF-ATEPC is a multi-task learning model, we redesigned the form of data input and adopted dual labels of sentiment polarity and token category. The <ref type="figure" target="#fig_2">Figure 5</ref> are the input samples of BERT-BASE and BERT-SPC model, respectively. The cross-entropy loss is adopted for APC and ATE subtask and the 2 regularization is applied in LCF-ATEPC, here is the loss function for APC task,</p><formula xml:id="formula_12"> = ∑ 1̂ log + ∑ ∈Θ 2 (25)</formula><p>where is the number of polarity categories, is the 2 regularization parameter, and Θ is the parameter-set of the LCF-ATEPC. The loss function for ATE task is</p><formula xml:id="formula_13"> = ∑ 1 ∑ 1̂ log + ∑ ∈Θ 2 (26)</formula><p>where is the number of token classes and is the sum of the tokens in each input sequence. Accordingly, the loss function of LCF-ATEPC is as follows: <ref type="bibr">27)</ref> 4. Experiments</p><formula xml:id="formula_14"> =  + <label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Hyperparameters Setting</head><p>To comprehensive evaluate the performance of the proposed model, the experiments were conducted in three most commonly used ABSA datasets, the Laptops and Restaurant datasets of SemEval-2014 Task4 subtask2 <ref type="bibr" target="#b20">Pontiki et al. (2014)</ref> and an ACL Twitter social dataset <ref type="bibr" target="#b6">Dong, Wei, Tan, Tang, Zhou and Xu (2014)</ref>. To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets <ref type="bibr" target="#b1">Che, Zhao, Guo, Su and Liu (2015)</ref>; Zhao, Pan, Du and Zheng (2015b); <ref type="bibr" target="#b17">Peng et al. (2018)</ref> (Car, Phone, Notebook, Camera). We preprocessed the seven datasets. We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks, respectively. The polarity of each aspect on the Laptops, Restaurants and datasets may be positive, neutral, and negative, and the conflicting labels of polarity are not considered. The reviews in the four Chinese datasets have been purged, with each aspect may be positive or negative binary polarity. To verify the effectiveness and performance of LCF-ATEPC models on multilingual datasets, we built a multilingual dataset by mixing the 7 datasets. We adopt this dataset to conduct multilingual-oriented ATE and APC experiments.</p><p>The table demonstrates the details of these datasets 4 . The samples distribution of those datasets is not balanced. For example, most samples in the restaurant dataset are positive, while the neutral samples in the Twitter dataset account for the majority. Apart from some hyperparameters setting referred to previous researches, we also conducted the controlled trials and analyzed the experimental results to optimize the hyperparameters setting. The superior hyperparameters are listed in <ref type="table" target="#tab_1">Table 2</ref>. The default SRD setting for all experiments is 5, with additional instructions for experiments with different SRD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compared Methods</head><p>We compare the LCF-ATEPC model to current state-of-the-art methods. Experimental results show that the proposed model achieves state-of-the-art performance both in the ATE and APC tasks. ATAE-LSTM <ref type="bibr" target="#b31">Wang et al. (2016)</ref> is a classical LSTM-based network for the APC task, which applies the attention mechanism to focus on the important words in the context. Besides, ATAE-LSTM appends aspect embedding and the learned features to make full use of the aspect features. The ATAE-LSTM can be adapted to the Chinese review datasets. ATSM-S Peng et al. <ref type="formula" target="#formula_0">(2018)</ref> is a baseline model of the ATSM variations for Chinese language-oriented ABSA task. This model learns the sentence and aspect terms at three perspectives of granularity. GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs.</p><p>The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets. AEN- <ref type="bibr">BERT Song et al. (2019)</ref> is an attentional encoder network based on the pretrained BERT model, which aims to solve the aspect polarity classification. BERT- <ref type="bibr">PT Xu, Liu, Shu and Yu (2019)</ref> is a BERT-adapted model for Review Reading Comprehension (RRC) task, a task inspired by machine reading comprehension (MRC), it could be adapted to aspect-level sentiment classification task. BERT-BASE <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> is the basic pretrained BERT model. We adapt it to ABSA multi-task learning, which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF-ATEPC model. <ref type="bibr">BERT-SPC Song et al. (2019)</ref> is a pretrained BERT model designed for the sentence-pair classification task and improves the APC subtask of LCF-ATEPC model. BERT-ADA <ref type="bibr" target="#b23">Rietzler et al. (2019)</ref> is a domain-adapted BERT-based model proposed for the APC task, which finetuned the BERT-BASE model on task-related corpus. This model obtained state-of-the-art accuracy on the Laptops dataset. LCF-ATEPC 5 is the multi-task learning model for the ATE and APC tasks, which is based on the the BERT-SPC model and local context focus mechanism. LCF-ATE are the variations of the LCF-ATEPC model which only optimize for the ATE task. LCF-APC are the variations of LCF-ATEPC and it only optimize for the APC task during training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results Analysis</head><p>The experiments are conducted in several segments. First, the baseline performance of LCF-ATEPC on all Chinese and English data sets was tested, and then the effectiveness of multi-task learning was demonstrated. Finally, the assistance of domain-adapted BERT model in improving performance was evaluated and the sensitivity of different datasets to SRD was studied. <ref type="table">Table 3</ref> are the experimental results of LCF-ATEPC models on four Chinese review datasets. <ref type="table">Table 3</ref> The experimental results (%) of LCF-ATEPC models on four Chinese datasets. "-" represents the result is not unreported. The optimal performance is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Performance on Chinese Review Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Car Phone Notebook Camera  <ref type="table" target="#tab_3">Table 4</ref> lists the main experimental results of LCF-ATEPC models to compare the performance with other ABSAoriented models.</p><p>The LCF-ATEPC models are multilingual-oriented. To demonstrate its ability to simultaneously input and analyze reviews in multiple languages, we constructed and experimented with a multilingual dataset fore-mentioned. And result on the multilingual mixed dataset illustrates the effectiveness of the LCF-ATEPC models. 1 , and 1 are the macro-F1 score of ATE subtask, accuracy and macro-F1 score of the APC subtask. The unreported experimental results are denoted by "-". The " †" means the F1 score of the ATE task is not available for BERT-SPC input format and the optimal performance is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Laptop Restaurant Twitter Multilingual Mixed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Overall Performance Analysis</head><p>Many models for ABSA tasks do not take into account the ATE subtask, but there are still some joint models <ref type="bibr" target="#b16">Nguyen and Shirai (2018)</ref> based on the traditional neural network architecture to conduct the APC and ATE tasks simultaneously. Benefit from the joint training process, the two ABSA subtasks of APC and ATE can promote each other and improve the performance.</p><p>The ATEPC-Fusion mechanism works better on most datasets, especially English review datasets. Surprisingly, for the Laptop and Restaurant datasets, guests occasionally have a unified "global" view in a specific review. That is, if the customer is not satisfied with one aspect, it is likely to criticize the other. Things will be the same if a customer prefers a restaurant he would be tolerant of some small disamenity, so the ATEPC-CDW mechanism performs better because it does not completely mask the local context of the other aspect. In the multi-task learning process, the convergence rate of APC and ATE tasks is different, so the model does not achieve the optimal effect at the same time.</p><p>We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on. Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification. In addition, for the first time,LCF-ATEPC has increased the F1 score of ATE subtask on three English datasets up to 82%, 89%, 96%, respectively.</p><p>ATEPC-Fusion is a supplementary scheme of LCF mechanism, and it adopts a moderate approach to generate local context features. The experimental results show that its performance is also better than the existing BERT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Effectiveness of Multi-task Learning</head><p>Keeping the main architecture of the LCF-ATEPC model unchanged, we tried to only optimize parameters for a single task in the multi-task model to explore the difference between the optimal performance of a single task and the multi-task learning model 6 .</p><p>The <ref type="figure" target="#fig_2">Figure 5</ref> depicts the performance of the LCF-ATEPC model when performing an single APC or ATE task. Experimental results show that on some datasets the LCF-ATEPC model performs better concerning APC or ATE single task than conducting ABSA multi-task on some datasets. In general, the proposed model LCF-ATEPC proposed in this paper is still superior to other ABSA-oriented multi-task models and even the single-task models aim to APC or ATE. When optimizing the model parameters for through back-propagation of multiple tasks, the multi-task learning model needs to take into account multiple loss functions of the different subtasks. So sometimes the multi-task learning cannot achieve as the best effect as single-task learning does, which is also the compromise of the multi-task learning model when dealing with multiple tasks.  The BERT-BASE model is trained on a large-scale general corpus, so the fine-tuning during process during training process is significant and inevitable for BERT-based models. Meanwhile, the ABSA datasets commonly benchmarked are generally small with the domain-specific characteristic, the effect of BERT-BASE model on the most ABSA datasets can be further improved through domain-adaption. Domain adaption is a effective technique while integrating the pre-trained BERT-BASE model. By further training the BERT-BASE model in a domain-related corpus similar to or homologous to the target ABSA dataset, then domain-related pretrained BERT model can be obtained. We adopted the method proposed in <ref type="bibr" target="#b23">Rietzler et al. (2019)</ref> to obtain the domain-adapted pre-trained BERT model based on the corpus of Yelp Dataset Challenge reviews 7 and the amazon Laptops review datasetHe and <ref type="bibr" target="#b9">McAuley (2016)</ref>. <ref type="table" target="#tab_6">Table  6</ref> shows that the performance of APC task significantly improved by domain-adapted BERT model. The accuracy benchmark in the classical Restaurant achieving more than 90%, which means that the LCF-ATEPC is the first ABSAoriented model obtained up to 90% accuracy on the Restaurant dataset. In addition, experimental result on the Laptop dataset also prove the effectiveness of domain-adaption in multi-task learning. Besides, the experimental results on the laptop dataset also validate the effectiveness of domain-adapted BERT model for ABSA multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">SRD Sensitivity on Different Datasets</head><p>We tested the sensitivity of SRD threshold on the typical Chinese and English ABSA datasets: the Phone dataset and The Restaurant dataset, respectively. Besides, for the evaluation of the restaurant dataset, we adopted the domainadapted BERT model as the underlying architecture of the LCF-ATEPC model. The experimental result of <ref type="figure" target="#fig_3">Figure 6</ref>, 7 are evaluated in multi-task learning process.</p><p>For the Chinese Phone dataset, the LCF-ATEPC-CDM model can achieve the best APC accuracy and F1 score when the SRD thresholds are 3, 6, while the best ATE task performance reaches the highest when the SRD thresholds are 1, 4. The LCF-ATEPC-CDW model obtains the best APC performance on the Phone dataset when the SRD thresholds are 4 and 6, while the best ATE F1 score is approximately obtained when the SRD threshold is 7. For the Restaurant dataset, the optimal APC accuracy and F1 score achieved by LCF-ATEPC-CDM while the SRD threshold is approximately between 2 and 4. While the SRD threshold for the LCF-ATEPC-CDW is between 5 and 7, the model achieves the optimal aspect classification accuracy and F1 score. However, the F1 score of the ATE task is less sensitive to the SRD threshold, indicating that aspect polarity classification task has less assistance on it during the joint learning process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The ATE and APC subtasks were treated as independent tasks in previous studies. Moreover, the multi-task learning model for ATE and APC subtasks has not attracted enough attention from researchers. Besides, the researches concerning the Chinese language-oriented ABSA task are not sufficient and urgent to be proposed and developed. To address the above problems, this paper proposes a multi-task learning model LCF-ATEPC for aspect-based sentiment analysis based on the MHSA and the LCF mechanisms and applies the pre-trained BERT to the ATE sub-tasks for the first time. Not only for the Chinese language, but the models proposed in this paper are multilingual and applicable to the classic English review sentiment analysis task, such as the SemEval-2014 task4. The proposed model can automatically extract aspects from reviews and infer aspects' polarity. Empirical results on 3 commonly English datasets and four Chinese review datasets for ABSA tasks show that, compared with all models based on basic BERT, the LCF-ATEPC model achieves state-of-the-art performance on ATE and APC tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Several samples of the seven ATEPC datasets. All the datasets are domainspecific.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of LCF-ATEPC { , +1 ⋯ }(1 &lt;= &lt; &lt;= ) is the aspect sequence within , and are the beginning and end positions in respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>5(a) is he input sequence formatted for the BERT-BASE model. The first line is composed of the aspect term labels; The second and third lines are the input sequence after tokenization and polarity label, respectively. 5(b) represent the sample input sequence formatted for BERT-SPC model and the aspects in two position are labeled simultaneously. 5(c) depicts a sample of Chinese sequence input format for BERT-SPC model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The 6(a), 6(b) are the performance visualization of LCF-ATEPC-CDM and LCF-ATEPC-CDW on the Chinese Phone dataset under different SRDs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The 7(a), 7(b) are the performance visualization of LCF-ATEPC-CDM and LCF-ATEPC-CDW on the Restaurant dataset under different SRDs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The ABSA datasets for ATE and APC subtasks, including three English datasets and four Chinese datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Positive</cell><cell cols="2">Negative</cell><cell cols="2">Neural</cell></row><row><cell></cell><cell cols="6">Train Test Train Test Train Test</cell></row><row><cell>Laptop</cell><cell>994</cell><cell>341</cell><cell>870</cell><cell>128</cell><cell>464</cell><cell>169</cell></row><row><cell>Restaurant</cell><cell>2164</cell><cell>728</cell><cell>807</cell><cell>196</cell><cell>637</cell><cell>196</cell></row><row><cell>Twitter</cell><cell>1561</cell><cell>173</cell><cell>1560</cell><cell>173</cell><cell>3127</cell><cell>346</cell></row><row><cell>Car</cell><cell>708</cell><cell>164</cell><cell>213</cell><cell>66</cell><cell>-</cell><cell>-</cell></row><row><cell>Phone</cell><cell>1319</cell><cell>341</cell><cell>668</cell><cell>156</cell><cell>-</cell><cell>-</cell></row><row><cell>Notebook</cell><cell>328</cell><cell>88</cell><cell>168</cell><cell>35</cell><cell>-</cell><cell>-</cell></row><row><cell>Camera</cell><cell>1197</cell><cell>322</cell><cell>546</cell><cell>113</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Multilingual Mixed 8271 2157 4340</cell><cell>867</cell><cell>4228</cell><cell>711</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Global hyperparameters settings for the LCF-ATEPC model, BERT-BASE and BERT-SPC models in the experiments.</figDesc><table><row><cell>Hyperparameters</cell><cell>Setting</cell></row><row><cell>learning rate</cell><cell>3 × 10 −5</cell></row><row><cell>batch size</cell><cell>16</cell></row><row><cell>training epochs</cell><cell>5</cell></row><row><cell>max sequence length</cell><cell>80</cell></row><row><cell>SRD</cell><cell>5</cell></row><row><cell>optimizer</cell><cell>AdamW</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Experimental results (%) of the LCF-ATEPC model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Experimental results of LCF-ATEPC on Laptop and Restaurant data sets using domainadapted pretrained BERT.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Laptop</cell><cell></cell><cell>Restaurant</cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell></row><row><cell>BERT-BASE</cell><cell cols="5">84.64 79.72 75.18 89.76 85.97 77.68</cell></row><row><cell>BERT-SPC</cell><cell>†</cell><cell>82.08 78.34</cell><cell>†</cell><cell cols="2">88.83 83.52</cell></row><row><cell>BERT-ADA</cell><cell>-</cell><cell>80.23 75.77</cell><cell>-</cell><cell cols="2">87.14 80.09</cell></row><row><cell>LCF-ATEPC-CDM</cell><cell cols="5">85.29 83.02 79.84 89.78 90.18 85.88</cell></row><row><cell>LCF-ATEPC-CDW</cell><cell cols="4">85.24 81.76 78.06 89.99 88.65</cell><cell>83.7</cell></row><row><cell cols="6">LCF-ATEPC-Fusion 85.06 81.76 78.65 89.94 89.45 84.76</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Email Address: yangjianhao@m.scnu.edu.cn (Jianhao Yang), songyouwei@baidu.com (Youwei Song) cs_xuruyang@m.scnu.edu.cn (Ruyang Xu)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The dataset processed for joint ATE and APC task are available at https://github.com/yangheng95/LCF-ATEPC</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We implement our model based on pytorch-transformers: https://github.com/huggingface/pytorch-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The loss function of the LCF-ATEPC is set to  and  while optimizing for APC and ATE task, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This corpus are available at https://www.yelp.com/dataset/challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Funding</head><p>Thanks to the anonymous reviewers and the scholars who helped us. This research is supported by the Innovation Project of Graduate School of South China Normal University and funded by National Natural Science Foundation of China, Multi-modal Brain-Computer Interface and Its Application in Patients with Consciousness Disorder, Project approval number: 61876067.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 5</ref> <p>The empirical performance comparison between multi-task and single-task learning. The "-" indicates that the statistics are not important during single-task learning optimization and not listed in the table. The optimal performance is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Laptop Restaurant Twitter</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<ptr target="https://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentence compression for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2015.2443982</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2111" to="2124" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced neural networks for sentiment analysis of chinese reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.08.054</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2019.08.054" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving sentiment analysis via sentence type classification using bilstm-crf and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.10.065</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p14-2009</idno>
		<idno>doi:10.3115/ v1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd annual meeting of the association for computational linguistics</title>
		<meeting>the 52nd annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="14" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-grained attention network for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3433" to="3442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
		<idno>doi:10.1145/2872427.2883037</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee</title>
		<meeting>the 25th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with attention-over-attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyder</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-93372-6_22</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-319-93372-6_22" />
	</analytic>
	<monogr>
		<title level="m">Social, Cultural, and Behavioral Modeling</title>
		<editor>Bisgin, H.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1087</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Aspect-based sentiment analysis with gated alternate neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2019.105010</idno>
		<idno>105010doi:10.1016/ j.knosys.2019.105010</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated rule selection for aspect extraction in opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2832415.2832429" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1291" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/568</idno>
		<idno>doi:10.24963/ijcai.2017/568</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/568" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2390524.2390572" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A joint model of term extraction and polarity classification for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
		<idno type="DOI">10.1109/kse.2018.8573340</idno>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Knowledge and Systems Engineering (KSE), IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multi-grained aspect target sequence for chinese sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.02.034</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jiménez-Zafra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1002</idno>
		<idno>doi:10. 18653/v1/S16-1002</idno>
		<ptr target="https://www.aclweb.org/anthology/S16-1002" />
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>SemEval-2016 task 5: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2082</idno>
		<ptr target="https://www.aclweb.org/anthology/S15-2082" />
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 12: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
		<ptr target="https://www.aclweb.org/anthology/S14-2004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Aspect extraction for opinion mining with a deep convolutional neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2016.06.00</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A rule-based approach to aspect extraction from product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/w14-5905</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second workshop on natural language processing for social media (SocialNLP)</title>
		<meeting>the second workshop on natural language processing for social media (SocialNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Engl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11860</idno>
		<ptr target="https://arxiv.org/abs/1908.11860" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.10.065</idno>
		<idno>142-147doi:10. 1016/j.eswa.2016.10.065</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09314</idno>
		<ptr target="https://arxiv.org/abs/1902.09314" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C16-1311" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3295222.3295349" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc., USA</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2832415.2832437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1347" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dcu: Aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tounsi</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/s14-2036</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3298023.3298050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1242</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1242" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1234</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1234" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lcf: A local context focus mechanism for aspect-based sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9163389</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3389</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint learning for aspect category detection and sentiment analysis in chinese reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-31624-2_9</idno>
	</analytic>
	<monogr>
		<title level="m">China Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="108" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Textual sentiment analysis via three different attention convolutional neural networks and cross-modality consistent regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2017.09.080</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2017.09.080" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1407" to="1415" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effect research of aspects extraction for chinese hotel reviews based on machine learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.14257/ijsh.2015.9.3.03</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Smart Home</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Principal curvature for infrared small target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.infrared.2014.12.014</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1350449514002825" />
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aspect-based opinion polling from customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.32657/10356/61830</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
