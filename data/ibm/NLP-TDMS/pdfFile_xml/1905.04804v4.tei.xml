<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<email>linjie.yang@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Yuchen Fan UIUC</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called Mask-Track R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Segmentation in images and videos is one of the fundamental problems in computer vision. In the image domain, the task of instance segmentation, i.e. simultaneous detection and segmentation of object instances in images, was first proposed by Hariharan et al. <ref type="bibr" target="#b10">[11]</ref> and since then has attracted tremendous amount of attention in computer vision due to its importance. In this paper, we extend the instance segmentation problem in the image domain to the video domain. Different from image instance segmentation, the new problem aims at simultaneous detection, segmentation and tracking of object instances in videos. <ref type="figure">Figure 1</ref> illustrates a sample video with ground truth annotations for this problem. Naturally, we name the new task video instance segmentation. The new task opens up pos- * This work is partially done when Linjie is with Snap Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video frames</head><p>Video instance annotations Video instance predictions <ref type="figure">Figure 1</ref>. A illustration of video instance segmentation. The three rows show image frames in a video, video instance annotations, and video instance predictions by our algorithm respectively. Masks in same color belong to the same object instance. Ground truth and predicted object categories are given on top of each bounding box. sibilities for applications which requires video-level object masks such as video editing, autonomous driving and augmented reality. To our best knowledge, this is the first work to address video instance segmentation problem.</p><p>Video instance segmentation is more challenging than image instance segmentation in that it not only requires instance segmentation on individual frames, but also the tracking of instances across frames. On the other hand, video content contains richer information than a single image such as motion pattern of different objects and temporal consistency, and thus provides more cues for object recognition and segmentation. Video instance segmentation is also related to several existing tasks. For example, video object segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> aims at segmenting and tracking objects in videos, but does not require recognition of object categories. Video object detection aims at detecting and tracking objects, but does not deal with object segmentation.</p><p>One potential reason that video instance segmentation is seldomly studied is the lack of a large-scale dataset. Despite the existence of video segmentation datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> for other tasks, none of them is directly applicable to video instance segmentation. Given a video, our task requires both the masks of all instances of a predefined category set and the instance identities across frames to be labeled. Existing video segmentation datasets either do not have exhaustive labeling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref>, or do not have the object identities <ref type="bibr" target="#b6">[7]</ref>. Therefore, in this paper, we present the first largescale dataset, named YouTube-VIS, for video instance segmentation. The new dataset contains 2, 883 high-resolution YouTube videos, a 40-category label set including common objects such as person, animals and vehicles, 4, 883 unique video instances and 131k high-quality masks. Our new dataset can be served as a benchmark for not only the video instance segmentation task, but also related tasks such as video semantic segmentation and video object detection.</p><p>In addition, we propose a novel algorithm called Mask-Track R-CNN for video instance segmentation. Based upon Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> which is a state-of-the-art method for image instance segmentation, a new branch is added to the framework for tracking instances across video frames. Predicted instances are stored to an external memory and matched with objects in later frames. Moreover, we also propose several baselines by adapting topperforming methods from related tasks to our task, and compare their performance with our new method. Experimental results clearly demonstrate the advantage of our new algorithm and reveal insights for future improvement. Our dataset has been released at https:// youtube-vos.org/dataset/vis. The code of our algorithm has been released at https://github.com/ youtubevos/MaskTrackRCNN.</p><p>We conclude the contribution of this paper as follows.</p><p>• To our best knowledge, it is the first time that video instance segmentation is formally defined and explored.</p><p>• We create the first large-scale video instance segmentation dataset which contains 2.9k videos and 40 object categories.</p><p>• We propose a novel algorithm for video instance segmentation and compare it with several baselines on our new dataset.</p><p>The rest of our paper is organized as follows. In Section 2 we briefly state the difference between related tasks and our new task. In Section 3 we formally introduce the video instance segmentation problem and evaluation metrics. Our new dataset and algorithm is elaborated in Section 4 and 5 respectively. Finally, experimental results are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Although video instance segmentation has been largely neglected in the literature, several related tasks have been well studied such as image instance segmentation, video object tracking, video object detection, video semantic segmentation and video object segmentation. Image Instance Segmentation Instance segmentation not only group pixels into different semantic classes, but also group them into different object instances <ref type="bibr" target="#b10">[11]</ref>. A two-stage paradigm is usually adopted, which first generate object proposals using a Region Proposal Network (RPN) <ref type="bibr" target="#b23">[24]</ref>, and then predict object bounding boxes and masks using aggregated RoI features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref>. The proposed video instance segmentation not only requires segmenting object instances in each frame, but also determining the correspondence of objects across frames. Video Object Tracking Video object tracking has two different settings. One is the detection-based tracking which simultaneously detect and track video objects. Methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref> under this setting usually take the "trackingby-detection" strategy. The other setting is the detectionfree tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>, which targets at tracking objects given their initial bounding boxes in the first frame. Among the two settings, DBT is more similar to our problem as it also requires a detector. However, DBT only requires to produce bounding boxes, which is different from our task. Recently, a multi-object tracking and segmentation dataset <ref type="bibr" target="#b31">[32]</ref> is proposed to evaluate multi-object tracking along with instance segmentation. Their dataset is similar to our YouTube-VIS with respect to the exhaustive video instance annotations, but is greatly inferior to ours on the data scale and object categories. Video Object Detection Video object detection aims at detecting objects in videos, which is first proposed as part of ImageNet visual challenge <ref type="bibr" target="#b24">[25]</ref>. While object identity information are often utilized for improve robustness of detection algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref>, the evaluation metric is limited to per-frame detection and does not require joint object detection and tracking. Video Semantic Segmentation Video semantic segmentation is a direct extension of semantic segmentation to videos, where image pixels are predicted as different semantic classes. Temporal information such as optical flow is adopted to improve either accuracy <ref type="bibr" target="#b36">[37]</ref> or efficiency <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> of semantic segmentation models. Video semantic segmentation does not require explicit matching of object instances across frames. Video Object Segmentation Video object segmentation has gained substantial attention in recent years, which has two scenarios: semi-supervised and unsupervised. Semisupervised video object segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> targets at tracking and segment a given object with a mask. Visual similarity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>, motion cues <ref type="bibr" target="#b5">[6]</ref>, and temporal con-sistency <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref> are extracted to identity the same object across the video. In unsupervised scenario, a single foreground object is segmented <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. In both settings, algorithms consider the target objects as general objects and does not care about the semantic categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Instance Segmentation</head><p>Problem Definition. In video instance segmentation, we have a predefined category label set C = {1, ..., K} where K is the number of categories. Given a video sequence with T frames, suppose there are N objects belonging to the category set C in the video. For each object i, let c i ∈ C denote its category label, and let m i p...q denote its binary segmentation masks across the video where p ∈ [1, T ] and q ∈ [p, T ] denote its starting and ending time. Suppose a video instance segmentation algorithm produces H instance hypotheses. For each hypothese j, it needs to have a predicted category labelc j ∈ C, a confidence score s j ∈ [0, 1] and a sequence of predicted binary masksm j p...q . The confidence score is used for our evaluation metrics which will be explained shortly.</p><p>The goal of our task is minimizing the difference between the ground truth and the hypotheses. In other words, a good video instance segmentation method should be able to have a good detection rate of all instances, track all the instances reliably and localize the instance boundaries accurately. It should be noted that there is some minor difference between our task and the multi-object tracking problem <ref type="bibr" target="#b17">[18]</ref> in that a still object instance is treated as a ground truth, and if an object is occluded or out of scene for several frames then reappears in the following frames, the instance label should be consistent. Evaluation Metrics. We borrow the standard evaluation metrics in image instance segmentation with modification adapted to our new task. Specifically, the metrics are average precision (AP) and average recall (AR). AP is defined as the area under the precision-recall curve. The confidence score is used to plot the curve. AP is averaged over multiple intersection-over-union (IoU) thresholds. We follow the COCO evaluation to use 10 IoU thresholds from 50% to 95% at step 5%. AR is defined as the maximum recall given some fixed number of segmented instances per video. Both of the two metrics are first evaluated per category and then averaged over the category set.</p><p>Our IoU computation is different from image instance segmentation because each instance contains a sequence of masks. To compute the IoU between a ground truth instance m i p...q and a hypothese instancem j p...q , we first extend p andp to 1, q andq to T by padding empty masks. Then,</p><formula xml:id="formula_0">IoU(i, j) = Σ T t=1 |m i t ∩m j t | Σ T t=1 |m i t ∪m j t |<label>(1)</label></formula><p>The proposed IoU computes the spatial-temporal consistency of predicted and ground truth segmentations. If the algorithm detects object masks successfully, but fails to track the objects across frames, it will get a low IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">YouTube-VIS</head><p>Since none of the existing video segmentation datasets matches the requirement for our video instance segmentation task, we need to collect a new benchmark dataset for the development and evaluation of the proposed methods. There are several criteria that the new benchmark need to satisfy. First, it should contain common instance categories, just like recent image instance segmentation benchmarks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>. Second, it should contain video instances with various challenging cases such as occlusion, appearance change, heavy camera motion etc. Last but not the least, the annotation quality should also be high, which is a common problem in some of the existing segmentation datasets with polygon-based annotations.</p><p>With the above criteria in mind, we create a new largescale benchmark called YouTube-VIS. Instead of building our benchmark from scratch, we take advantage of an existing dataset called YouTube-VOS <ref type="bibr" target="#b34">[35]</ref>. YouTube-VOS is a large-scale video object segmentation dataset which is comprised of 4453 high-resolution YouTube videos and 94 common object categories. In each video, several objects are labeled by tracing the object boundaries manually at every 5 frames in a 30fps frame rate. The length of each video is around 3 to 6 seconds. Even though object masks are not exhaustively labeled in YouTube-VOS, it still serves as a very good resource to build our own dataset. Specifically, we first select 40 common category labels from the 94 category labels as our category set. Then we sample around 2.9k videos with objects from the 40 categories in YouTube-VOS. We then ask human annotators to carefully label the other objects belonging to the category set exhaustively in these videos. As a result, our dataset is annotated with 4, 883 unique objects and approximately 131k object masks. A comparison of some high level statistics of YouTube-VIS and related datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. The distribution of the unique objects per category in our dataset is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Our new dataset YouTube-VIS is not only the first largescale benchmark for video instance segmentation, but also a useful benchmark for other vision tasks such as video object detection and video semantic segmentation. It also complements the original YouTubeVOS dataset with more objects. We believe our new dataset will serve as a useful benchmark for various pixel-level video understanding tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MaskTrack R-CNN</head><p>Our new algorithm for video instance segmentation is built based on Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>. In addition to its original three branches for object classification, bounding box regression, and mask generation, we add the forth branch together with an external memory to track object instances across frames. The tracking branch mainly leverages the cue of appearance similarity. In addition, we propose a simple yet effective method to combine it with the other cues such as semantic consistency and spatial correlation to improve the tracking accuracy substantially. The overall framework of our algorithm is illustrated in <ref type="figure">Figure 3</ref>. For inference, our method processes video frames sequentially in an online fashion. Next we first briefly review Mask R-CNN and then describe our new components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Mask R-CNN</head><p>Mask R-CNN is a high-performing method for image instance segmentation. It consists of two stages. In the first stage, a RPN <ref type="bibr" target="#b23">[24]</ref> takes an image as input and proposes a set of candidate object bounding boxes. In the second stage, features are extracted by the RoIAlign operation from each candidate box and further used to perform classification, bounding box regression and binary segmentation in parallel by three dedicated branches. Please refer to <ref type="bibr" target="#b11">[12]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">New Tracking Branch</head><p>Our network adopts the same two-stage procedure, with an identical first stage which proposes a set of object bounding boxes at each frame. In the second stage, in parallel to the three branches (i.e. classification, bounding box regression, binary segmentation), we add the forth branch to assign an instance label to each candidate box. Suppose there are already N instances identified by our algorithm from previous frames. Then a new candidate box can only be assigned to one of the N identities if it is one of the previous instances or a new identity if it is a new instance. We formulate the problem as multiclass classification. There are N + 1 classification digits which represent the N already identified instance and a new unseen instance which is denoted by digit 0. The probability of assigning label n to a candidate box i is defined as</p><formula xml:id="formula_1">p i (n) =      e f i fn 1+Σ N j=1 e f i f j n ∈ [1, N ] 1 1+Σ N j=1 e f i f j n = 0<label>(2)</label></formula><p>where f i and f j , j ∈ [1, N ] denote the new features extracted by our tracking branch from the candidate box and the N identified instances. Our tracking branch has two fully connected layers which project the feature maps extracted by RoIAlign into new features. Since the features of previously identified instances have already been computed, we use an external memory to store them for efficiency. The cross entropy loss is used for out tracking branch, i.e. L track = −Σ i log(p i (y i )) where y i is the ground truth instance label. We dynamically update our external memory when a new candidate box is assigned with an instance label. If the candidate box belongs to an existing instance, we update the instance feature stored in the memory with the new candidate feature, which represent the latest state of the instance. If the candidate object is assigned with label 0, we insert feature of the candidate object to the memory and add 1 to the number of identified instances.</p><p>We need a sequence of frames to train the new tracking branch. In our implementation we use a pair of frames which are randomly sampled from a training video. One of the frames is randomly picked as the reference frame while the other one is picked as the query frame. On the reference frame, we do not generate any candidate boxes but only extract features from its ground truth instance regions and save them into our external memory. On the query frame, candidate boxes are generated in the first stage and only positive candidate boxes are then matched to the instance labels in the memory and contribute to the tracking loss. A positive candidate box is the one with at least 70% IoU overlapping with any ground truth object boxes. Our whole network is trained end-to-end with losses combined from the four <ref type="figure">Figure 3</ref>. An overview of our approach. A tracking head is embedded in the MaskRCNN framework to facilitate identity tracking of object instances through interaction with a memory queue. The memory queue is used to maintain all the existing object instances in the video. branches together L = L cls + L box + L mask + L track .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Combining Other Cues</head><p>Our tracking branch computes the probability of assigning an instance label to a candidate box based on the appearance similarity. However, there are also other information such as semantic consistency, spatial correlation and detection confidence which could be leveraged to determine the instance labels. We propose a simple yet effective way to combine all these cues together to improve the tracking accuracy in a post-processing way.</p><p>Specifically, for a new candidate box i, let b i , c i and s i denote its bounding box prediction, category label and detection score, which are obtained from the bounding box branch and the classification branch of our network. Similarly, for an identified instance with label n, let b n and c n denote its bounding box prediction and category label associated with the saved features in the memory. Then a score for assigning the label n to candidate box i is computed as</p><formula xml:id="formula_2">v i (n) = log p i (n) + α log s i + βIoU(b i , b n ) + γδ(c i , c n )<label>(3)</label></formula><p>where p i (n) is obtained by Equation 2, IoU(b i , b n ) computes the IoU between b i and b n , and δ(c i , c n ) is a Kronecker delta function which equals 1 when c i and c n are equivalent and 0 otherwise. α, β and γ are hyperparameters to balance the effect of different cues. Empirically we find that the score is not sensitive to different values of α and β. Note that Equation 3 is only used in the testing stage and does not contribute to the training of our network. There are also other possible ways to integrate these cues, for example, take all the cues as inputs and train an end-to-end network, which will be left as an interesting future study for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Inference</head><p>Given a new testing video, our external memory is set empty and the number of identified instances is set 0. Our method processes each frame sequentially in an online fashion. At each frame, our network first generates a set of instance hypotheses. Non-Max Suppression (NMS) (50% overlapping threshold) is applied to reduce the hypotheses. Then the remaining hypotheses are matched to identified instances from previous frames by Equation <ref type="bibr" target="#b2">3</ref>. Note that we do not match hypotheses within a single frame to avoid conflicts. All instance hypotheses of the first frame are directly regarded as new instances and saved into the external memory. It is possible for our method to match multiple hypotheses from a single frame to one instance label, which contradicts the common sense. We handle this case by keeping only one hypothese which has the largest score v among the conflicting hypotheses while discarding the others.</p><p>After processing all frames, our method produces a set of instance hypotheses, each of which contains an unique instance label, and a sequence of binary segmentation, category labels and detection confidence. We use the averaged detection confidence as the confidence score for the whole sequence and use the majority votes of category labels as the final category label for the instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we compare our MaskTrack R-CNN with several baselines on our new dataset YouTube-VIS. We first present the information of the dataset splits and implemen-tation details of our method. Dataset. We randomly split the YouTube-VIS dataset into 2, 238 training videos, 302 validation videos and 343 test videos. Each of the validation and test set is guaranteed to have more than 4 instances per category. All the methods are trained on the training set and all hyperparameters are cross validated on the validation set. We present results on both the validation set and test set in the results section. Implementation. The backbone of our network is based on the network structure of ResNet-50-FPN in <ref type="bibr" target="#b11">[12]</ref> and we use a public implementation <ref type="bibr" target="#b3">[4]</ref> which is pretrained on MS COCO <ref type="bibr" target="#b16">[17]</ref>. The structure of our new tracking branch is two fully connected layers. The first fully connected layer transforms the 7 × 7 × 256 input feature maps to 1-D 1024 dimensions. The second fully connected layer also maps its input to 1-D 1024 dimensions. Our full model is trained end-to-end in 12 epochs. The initial learning rate is set to 0.05 and decays with a factor of 10 at epoch 8 and 11. In testing, our model runs at 20 FPS with a NVIDIA 1080Ti GPU. The hyperparameters α, β and γ in Equation 3 are cross validated and chosen to be 1, 2 and 10 to produce our final results. We downsample original frame sizes to 640 × 360 for all the methods in both training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Baselines</head><p>To our best knowledge, there is no prior work directly applicable to our new task. Therefore we combine ideas from related tasks to propose several new baselines. We incorporate two types of algorithms for the baselines. The first type uses the object masks detected in the first frame of the video as initial guidance and applies video object segmentation algorithms to propagate the masks. We evaluate two recent video object segmentation algorithms OSMN <ref type="bibr" target="#b35">[36]</ref> and FEELVOS <ref type="bibr" target="#b30">[31]</ref>. The second type follows the "tracking-bydetection" idea which is very popular in the multi-object tracking task. The basic idea of this type of works is using image detection methods on each frame independently and then linking the detection across frames by various tracking methods. In our experiment, all the baselines are given the same per-frame instance segmentation results which are produced by a Mask R-CNN. The Mask R-CNN has the same structure as our network except the tracking branch. To make the evaluation fair, The Mask R-CNN is pretrained on MS COCO and is then finetuned on YouTube-VIS with 12 training epochs. Next we describe different track-bydetect methods in our experiment. IoUTracker+. This method computes a score between a new candidate box with each identified instance by using a similar equation as Equation 3 except without using the first term, i.e. the appearance similarity. Therefore the matching does not leverage any visual information. The candidate box is assigned to the instance label with the largest score, with a minimum IoU threshold (30%). Otherwise it is as-signed with a new label. The matching process is similar to IoUTracker <ref type="bibr" target="#b1">[2]</ref>. The difference is that a similar memory as our method is equipped with the baseline to save the information of identified instances. OSMN <ref type="bibr" target="#b35">[36]</ref>. Given an identified instance mask, OSMN estimates a new mask of the instance at a new frame. The new mask is then used to compute IoU with candidate boxes at the same frame. This is better than IoU directly computed via consecutive frames especially when an instance is occluded or has large motion. The rest of the matching process is the same as IoUTracker+.</p><p>DeepSORT <ref type="bibr" target="#b32">[33]</ref>. DeepSORT is a top-performing tracking method. it uses Kalman filter to predict bounding box location to avoid directly computing IoU of consecutive frames. In addition, it use a deep network to measure the appearance similarity between bounding boxes. Finally the IoU score and the visual appearance score are combined to match tracks by the Hungarian algorithm. SeqTracker. This is an offline algorithm following Seq-NMS <ref type="bibr" target="#b9">[10]</ref>. Given a video and a set of instance segmentation results of every frame, SeqTracker searches all possible tracks to find the one with the largest score, which is computed similarly as IoUTracker+. Then the instance segmentation of the track will be removed from the set and the search process repeats. The method halts until the length of a retrieved track is less than a threshold, which is set to 8 in our experiment. <ref type="table" target="#tab_1">Table 2</ref> presents the comparison results. Notably, our method MaskTrack R-CNN achieves the best results under all evaluation metrics and on both the validation and test sets. The main difference between our method and the other track-by-detect baselines is the new tracking branch which is trained end-to-end with the other branches, so that useful information can be shared among multiple tasks. The key for the joint training of tracking with other tasks is that we formulate the instance matching process as a differentiable component, which enables the matching loss to be properly back propagated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Main Results</head><p>Next we analyze the performance of the baselines. For mask propagation algorithms, they suffer from a natural disadvantage, that they cannot handle objects appear in the intermediate frames. Also the flawed detections in the frist frames directly degenerate their performance. Even the state-of-the-art video object segmentation algorithm FEELVOS only gains 26.9 AP on validation set. For trackby-detect algorithm, IoUTracker+ does not leverage any visual information which is not surprising to gain weak performance. OSMN predicts the possible location of previously identified instances at new frames, and use the prediction to match instances, which is useful to handle occlusion and fast motion. DeepSORT improves IoUTracker+   on both the IoU matching and usage of visual similarity, achieving better results. SeqTracker does not depend on any visual information and achieves better performance than the other baselines. However, it is an offline method which requires instance segmentation results to be precomputed for all frames. The other methods including MaskTrack R-CNN are online methods, which produce instance tracks sequentially. <ref type="figure" target="#fig_2">Figure 4</ref> shows six sample videos with our predictions. The first four rows ((a),(b), (c) and (d)) are successful pre-dictions and the last two rows are failure cases. In video (a), the frame-level prediction gives incorrect results in the first two frames, where the bear is predicted as "deer" and "earless seal". The video-level prediction corrects these mistakes by majority voting of all frames. In video (c), the surfboard is occluded by the wave in multiple frames, our algorithm is able to track the surfboard after it disappears and reoccurs. The memory queue in MaskTrack R-CNN is able to keep track of all previous objects even they are disappeared in intermediate frames. In video (d), we show   <ref type="table">Table 3</ref>. Ablation study of our method on the YouTube-VIS validation set. "Det", "IoU", and "Cat" denote the detection confidence, the bounding box IoU, and the category consistency in <ref type="bibr">Equation 3</ref> respectively. Numbers in brackets shows the difference compared to the complete score.</p><p>Det IoU Cat AP AP50 AP75 21.1(-9.2) 37.7(-13.4) 23.6(-9.0) 23.4(-6.9) 42. ) and (f) shows two challenging cases. In video (e), the dear has quite different appearance in different poses, and our algorithm fail to recognize the same object and consider them as two different objects. In video (f), multiple similar fishes move around the aquarium and occlude each other. Our algorithm groups two fishes as one in the second and third frame, and gets confused with the object identities later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>We study the importance of three cues used in Equation 3 to our method. They are the detection score, the bouding box IoU and category consistency. We evaluate our method on the validation set by turning these cues on and off. The results are presented in <ref type="table">Table 3</ref>. We find that the bounding box IoU and the category consistency are most important to the performance of our method. Without any of them, AP will drop around 5%. While the detection confidence score only improves our method slightly. Intuitively, the bounding box IoU correlates to the spatial relationship be- tween instances, which is a strong prior in many cases. The category consistency also provides a very strong constraint because the category label of an instance should not change in a video. However, relying too much on the these factors can also cause problems due to the imperfect estimation. Therefore our method uses these cues as soft constraints. To visualize the effect of these three factors, we also generate predictions with the three factors added one by one on one specific sample, which is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Note that the first three variants cannot track the identity of the "green" motorbike well, while the variant with four different cues is able to track it through the whole video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Oracle Results</head><p>Additionally, we investigate the effectiveness of the two parts in our algorithm: image-level prediction and crossframe association. We evaluate effectiveness of videolevel association by applying ground truth image-level annotations to our algorithm. Specifically, given ground truth image-level predictions including bounding boxes, masks and categories, we compute matching score p i using RoIAlign features of ground truth bounding boxes and match objects across frames using combined score v i . The result is shown in <ref type="table" target="#tab_3">Table 4</ref> with "Image Oracle". We also evaluate image-level predictions with ground truth object identities. Towards this end, per-frame predictions are first matched to their closest ground truth image objects, and then video objects are aggregated using ground truth object identities. The result is shown in <ref type="table" target="#tab_3">Table 4</ref> with "Identity Oracle". It shows that Image Oracle achieves much better performance than Identity Oracle, which means image-level predictions is critical for better performance on video instance segmentation. Identity Oracle is only marginally better than MaskTrack RCNN, which indicates limited potential of improving over our current method by modifying object tracking method. Improving image-level detection performance by utilizing properly-designed spatial-temporal feature could be a promising direction. Meanwhile, even with image-level ground truth, it is still challenging to associate objects across frames due to object occlusions and fast motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We present a new task named video instance segmentation and an accompany dataset named YouTubeVIS in this work. The new tasks is a combination of object detection, segmentation, and tracking, which poses specific challenges given the rich and complex scenes. We also propose a new method combining single-frame instance segmentation and object tracking, which aims to provide some early explorations towards this task. There are a few interesting future directions: object proposal and detection with spatialtemporal features, end-to-end trainable matching criterion, and incorporating motion information for better recognition and identity association. We believe the new task and new algorithm will innovate the research community on new research ideas and directions for video understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Number of unique video objects for the 40 categories in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Sample results of MaskTrack R-CNN. Each row have five sampled frames from a video sequence. (a),(b),(c) and (d) show correct predictions while (e) and (f) are failure cases. Objects with same predicated identity have the same color. Object category is shown on top of each bounding box. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>A sample result with different matching cues used. With all four factors, the result is the best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>High level statistics of YouTubeVIS and previous video object segmentation datasets. YTO, YTVOS, and YTVIS stands for YouTubeObjects, YouTubeVOS, and YouTube-VIS respectively.</figDesc><table><row><cell></cell><cell>YTO [13]</cell><cell>FBMS [20]</cell><cell>DAVIS [22, 23]</cell><cell></cell><cell>YTVOS [35]</cell><cell>YTVIS</cell></row><row><cell>Videos</cell><cell>96</cell><cell>59</cell><cell>50</cell><cell>90</cell><cell>4,453</cell><cell>2,883</cell></row><row><cell>Categories</cell><cell>10</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>94</cell><cell>40</cell></row><row><cell>Objects</cell><cell>96</cell><cell>139</cell><cell>50</cell><cell>205</cell><cell>7,755</cell><cell>4,883</cell></row><row><cell>Masks</cell><cell>1.7k</cell><cell>1.5k</cell><cell>3.4k</cell><cell>13.5k</cell><cell>197k</cell><cell>131k</cell></row><row><cell>Exhaustive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation of the proposed algorithm and baselines on the YouTube-VIS validation and test set. The best results are highlighted in bold. AP AP 50 AP 75 AR 1 AR 10 AP AP 50 AP 75 AR 1 AR 10 Mask propagation OSMN [36] 23.4 36.5 25.7 28.9 31.1 27.3 44.4 28.0 28.8 34.0 FEELVOS [31] 26.9 42.0 29.7 29.9 33.4 29.6 45.4 30.7 33.4 36.8</figDesc><table><row><cell cols="2">Methods</cell><cell>validation set</cell><cell>test set</cell></row><row><cell></cell><cell>IoUTracker+</cell><cell cols="2">23.6 39.2 25.5 26.2 30.9 25.2 41.9 26.2 28.7 33.7</cell></row><row><cell></cell><cell>OSMN [36]</cell><cell cols="2">27.5 45.1 29.1 28.6 33.1 27.3 44.4 28.0 28.8 34.0</cell></row><row><cell>Track-by-detect</cell><cell>DeepSORT [33]</cell><cell cols="2">26.1 42.9 26.1 27.8 31.3 27.2 44.0 29.2 29.1 33.3</cell></row><row><cell></cell><cell>SeqTracker</cell><cell cols="2">27.5 45.7 28.7 29.7 32.5 29.5 48.1 31.2 32.0 34.5</cell></row><row><cell></cell><cell cols="3">MaskTrack R-CNN 30.3 51.1 32.6 31.0 35.5 32.3 53.6 34.2 33.6 37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>new object enters the video in the intermediate frames. Our algorithm is able to detect the deer in the second frame as new object and add it to the external memory.</figDesc><table><row><cell></cell><cell>5(-8.6)</cell><cell>24.4(-8.2)</cell></row><row><cell cols="3">22.7 (-7.6) 40.7 (-10.4) 25.2 (-7.4)</cell></row><row><cell cols="2">24.7 (-5.6) 44.3 (-6.8)</cell><cell>26.7 (-5.9)</cell></row><row><cell cols="2">27.9 (-2.4) 47.1 (-4.0)</cell><cell>30.5 (-2.1)</cell></row><row><cell cols="2">29.2 (-1.1) 49.2 (-1.9)</cell><cell>31.9 (-0.7)</cell></row><row><cell cols="2">29.5 (-0.8) 48.7 (-2.4)</cell><cell>32.2 (-0.4)</cell></row><row><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell></row><row><cell>a case that</cell><cell></cell><cell></cell></row></table><note>Video (e</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Oracle results in two settings on validation set. Image oracle is results with predicted object identity based on ground truth image-level annotations, identity oracle is results with ground truth object identities based on predicted image-level instances.</figDesc><table><row><cell></cell><cell>AP</cell><cell>AR 10</cell></row><row><cell>Image Oracle</cell><cell cols="2">78.7 83.7</cell></row><row><cell cols="3">Identity Oracle 31.5 34.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Highspeed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Seq-nms for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1602.08465</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollr, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervoxelconsistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yule</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1612.07217</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4481" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MOTS: multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai Lu Yuan Yichen Wei Xizhou Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
