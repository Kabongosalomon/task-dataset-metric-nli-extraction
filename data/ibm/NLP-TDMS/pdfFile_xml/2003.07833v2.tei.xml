<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mohamed Bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mohamed Bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generalized zero-shot classification · Feature synthesis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The stateof-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We first introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are then transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot object and action classification reveal the benefit of semantic consistency and iterative feedback, outperforming existing methods on six zero-shot learning benchmarks. Source code at https://github.com/akshitac8/tfvaegan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper strives for zero-shot learning, a challenging vision problem that involves classifying images or videos into new ("unseen") categories at test time, without having been provided any corresponding visual example during training. In the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42]</ref>, this is typically achieved by utilizing the labelled seen class instances and class-specific semantic embeddings (provided as a side information), which encode the inter-class relationships. Different from the zeroshot setting, the test samples can belong to the seen or unseen categories in generalized zero-shot learning <ref type="bibr" target="#b40">[41]</ref>. In this work, we investigate the problem of both zero-shot learning (ZSL) and generalized zero-shot learning (GZSL).</p><p>Most recent ZSL and GZSL recognition approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref> are based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[11]</ref>, which aim at directly optimizing the divergence between real and generated data. The work of <ref type="bibr" target="#b41">[42]</ref> learns a GAN using the seen class feature instances and the corresponding class-specific semantic embeddings, which are either manually annotated or word vector <ref type="bibr" target="#b26">[27]</ref> representations. Feature instances of the unseen categories, whose real features are unavailable during training, are then synthesized using the trained GAN and used together with the real feature instances from the seen categories to train zero-shot classifiers in a fully-supervised setting. A few works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref> additionally utilize auxiliary modules, such as a decoder, to enforce a cycle-consistency constraint on the reconstruction of semantic embeddings during training. Such an auxiliary decoder module aids the generator to synthesize semantically consistent features. Surprisingly, these modules are only employed during training and discarded during both the feature synthesis and ZSL classification stages. Since the auxiliary module aids the generator during training, it is also expected to help obtain discriminative features during feature synthesis and reduce the ambiguities among different classes during classification. In this work, we address the issues of enhanced feature synthesis and improved zero-shot classification.</p><p>Further, GANs are likely to encounter mode collapse issues <ref type="bibr" target="#b1">[2]</ref>, resulting in decreased diversity of generated features. While Variational Autoencoders (VAEs) <ref type="bibr" target="#b17">[18]</ref> achieve more stable feature generation, the approximate inference distribution is likely to be different from the true posterior <ref type="bibr" target="#b47">[48]</ref>. Recently, <ref type="bibr" target="#b42">[43]</ref> build on <ref type="bibr" target="#b41">[42]</ref> to combine the strengths of VAEs and GANs and introduce an f-VAEGAN ZSL framework by sharing the VAE decoder and GAN generator modules. To ensure that the generated features are semantically close to the distribution of real feature, a cycle-consistency loss <ref type="bibr" target="#b48">[49]</ref> is employed between generated and original features, during training. Here, we propose to additionally enforce a similar consistency loss on the semantic embeddings during training and further utilize the learned information during feature synthesis and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>We propose a novel method, which advocates the effective utilization of a semantic embedding decoder (SED) module at all stages of the ZSL framework: training, feature synthesis and classification. Our method is built on a VAE-GAN architecture. (i) We design a feedback module for (generalized) zero-shot learning that utilizes SED during both training and feature synthesis stages. The feedback module first transforms the latent embeddings of SED, which are then used to modulate the latent representations of the generator. To the best of our knowledge, we are the first to propose a feedback module, within a VAE-GAN architecture, for the problem of (generalized) zero-shot recognition. (ii) We introduce a discriminative feature transformation, during the classification stage, that utilizes the latent embeddings of SED along with their corresponding visual features for reducing ambiguities among object categories. In addition to object recognition, we show effectiveness of the proposed approach for (generalized) zero-shot action recognition in videos.</p><p>We validate our approach by performing comprehensive experiments on four commonly used ZSL object recognition datasets: CUB <ref type="bibr" target="#b39">[40]</ref>, FLO <ref type="bibr" target="#b28">[29]</ref>, SUN <ref type="bibr" target="#b29">[30]</ref> and AWA <ref type="bibr" target="#b40">[41]</ref>. Our experimental evaluation shows the benefits of utilizing SED at all stages of the ZSL/GZSL pipeline. In comparison to the baseline, the proposed approach obtains absolute gains of 4.6%, 7.1%, 1.7%, and 3.1% on CUB, FLO, SUN, and AWA, respectively for generalized zero-shot (GZSL) object recognition. In addition to object recognition, we evaluate our method on two (generalized) zero-shot action recognition in videos datasets: HMDB51 <ref type="bibr" target="#b19">[20]</ref> and UCF101 <ref type="bibr" target="#b37">[38]</ref>. Our approach outperforms existing methods on all six datasets. We also show the generalizability of our proposed contributions by integrating them into GAN-based (generalized) zero-shot recognition framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, the problem of object recognition under zero-shot learning (ZSL) settings has been well studied <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42]</ref>. Earlier ZSL image classification works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> learn semantic embedding classifiers for associating seen and unseen classes. Different from these methods, the works of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34</ref>] learn a compatibility function between the semantic embedding and visual feature spaces. Other than these inductive approaches that rely only on the labelled data from seen classes, the works of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> leverage additional unlabelled data from unseen classes through label propagation under a transductive zero-shot setting.</p><p>Recently, Generative Adversarial Networks <ref type="bibr" target="#b10">[11]</ref> (GANs) have been employed to synthesize unseen class features, which are then used in a fully supervised setting to train ZSL classifiers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>. A conditional Wasserstein GAN <ref type="bibr" target="#b2">[3]</ref> (WGAN) is used along with a seen category classifier to learn the generator for unseen class feature synthesis <ref type="bibr" target="#b41">[42]</ref>. This is achieved by using a WGAN loss and a classification loss. In <ref type="bibr" target="#b7">[8]</ref>, the seen category classifier is replaced by a decoder together with the integration of a cycle-consistency loss <ref type="bibr" target="#b48">[49]</ref>. The work of <ref type="bibr" target="#b34">[35]</ref> proposes an approach where cross and distribution alignment losses are introduced for aligning the visual features and corresponding embeddings in a shared latent space, using two Variational Autoencoders <ref type="bibr" target="#b17">[18]</ref> (VAEs). The work of <ref type="bibr" target="#b42">[43]</ref> introduces a f-VAEGAN framework which combines a VAE and a GAN by sharing the decoder of VAE and generator of GAN for feature synthesis. For training, the f-VAEGAN framework utilizes a cycle-consistency constraint between generated and original visual features. However, a similar constraint is not enforced on the semantic embeddings in their framework. Different from f-VAEGAN, other GAN-based ZSL classification methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref> investigate the utilization of auxiliary modules to enforce cycle-consistency on the embeddings. Nevertheless, these modules are utilized only during training and discarded during both feature synthesis and ZSL classification stages.</p><p>Previous works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref> have investigated leveraging feedback information to incrementally improve the performance of different applications, including classification, image-to-image translation and super-resolution. To the best of our knowledge, our approach is the first to incorporate a feedback loop for improved feature synthesis in the context of (generalized) zero-shot recognition (both image and video). We systematically design a feedback module, in a VAE-GAN framework, that iteratively refines the synthesized features for ZSL.</p><p>While zero-shot image classification has been extensively studied, zero-shot action recognition in videos received less attention. Several works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28]</ref> study the problem of zero-shot action recognition in videos under transductive setting. The use of image classifiers and object detectors for action recognition under ZSL setting are investigated in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>. Recently, GANs have been utilized to synthesize unseen class video features in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b24">25]</ref>. Here, we further investigate the effectiveness of our framework for zero-shot action recognition in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We present an approach, TF-VAEGAN, for (generalized) zero-shot recognition. As discussed earlier, the objective in ZSL is to classify images or videos into new classes, which are unknown during the training stage. Different from ZSL, test samples can belong to seen or unseen classes in the GZSL setting, thereby making it a harder problem due to the domain shift between the seen and unseen classes. Let x ∈ X denote the encoded feature instances of images (videos) and y ∈ Y s the corresponding labels from the set of M seen class labels Y s = {y 1 , . . . , y M }. Let Y u = {u 1 , . . . , u N } denote the set of N unseen classes, which is disjoint from the seen class set Y s . The seen and unseen classes are described by the category-specific semantic embeddings a(k) ∈ A, ∀k ∈ Y s ∪Y u , which encode the relationships among all the classes. While the unlabelled test features x t ∈ X are not used during training in the inductive setting, they are used during training in the transductive setting to reduce the bias towards seen classes. The tasks in ZSL and GZSL are to learn the classifiers f zsl : X → Y u and f gzsl : X → Y s ∪Y u , respectively. To this end, we first learn to synthesize the features using the seen class features x s and corresponding embeddings a(y). The learned model is then used to synthesize unseen class featuresx u using the unseen class embeddings a(u). The resulting synthesized featuresx u , along with the real seen class features x s , are further deployed to train the final classifiers f zsl and f gzsl .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: f-VAEGAN</head><p>We base our approach on the recently introduced f-VAEGAN <ref type="bibr" target="#b42">[43]</ref>, which combines the strengths of the VAE <ref type="bibr" target="#b17">[18]</ref> and GAN <ref type="bibr" target="#b10">[11]</ref> as discussed earlier, achieving impressive results for ZSL classification. Compared to GAN based models, e.g., f-CLSWGAN <ref type="bibr" target="#b41">[42]</ref>, the f-VAEGAN <ref type="bibr" target="#b42">[43]</ref> generates semantically consistent features by sharing the decoder and generator of the VAE and GAN. In f-VAEGAN, the feature generating VAE <ref type="bibr" target="#b17">[18]</ref> (f-VAE) comprises an encoder E(x, a), which encodes an input feature x to a latent code z, and a decoder G(z, a) (shared with f-WGAN, as a conditional generator) that reconstructs x from z. Both E and G are conditioned on the embedding a, optimizing,</p><formula xml:id="formula_0">L V = KL(E(x, a)||p(z|a)) − E E(x,a) [log G(z, a)],<label>(1)</label></formula><p>where KL is the Kullback-Leibler divergence, p(z|a) is a prior distribution, assumed to be N (0, 1) and log G(z, a) is the reconstruction loss. The feature generating network <ref type="bibr" target="#b41">[42]</ref> (f-WGAN) comprises a generator G(z, a) and a discriminator Here,x = G(z, a) is the synthesized feature, λ is the penalty coefficient andx is sampled randomly from the line connecting x andx. The f-VAEGAN is then optimized by:</p><formula xml:id="formula_1">L vaegan = L V + αL W ,<label>(2)</label></formula><p>where α is a hyper-parameter. For more details, we refer to <ref type="bibr" target="#b42">[43]</ref>. Limitations: The loss formulation for training f-VAEGAN, contains a constraint (second term in Eq. 1) that ensures the generated visual features are cyclicallyconsistent, at train time, with the original visual features. However, a similar cycle-consistency constraint is not enforced on the semantic embeddings. Alternatively, other GAN-based ZSL methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref> utilize auxiliary modules (apart from the generator) for achieving cyclic-consistency on embeddings. However, these modules are employed only during training and discarded at both feature synthesis and ZSL classification stages. In this work, we introduce a semantic embedding decoder (SED) that enforces cycle-consistency on semantic embeddings and utilize it at all stages: training, feature synthesis and ZSL classification. We argue that the generator and SED contain complementary information with respect to feature instances, since the two modules perform inverse transformations in relation to each other. The generator module transforms the semantic embed-dings to the feature instances whereas, SED transforms the feature instances to semantic embeddings. Our approach focuses on the utilization of this complementary information for improving feature synthesis and reducing ambiguities among classes (e.g., fine-grained classes) during ZSL classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>The overall architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The VAE-GAN consists of an encoder E, generator G and discriminator D. The input to E are the real features of seen classes x and the semantic embeddings a and the output of E are the parameters of a noise distribution. These parameters are matched to those of a zero-mean unit-variance Gaussian prior distribution using the KL divergence (L KL ). The noise z and embeddings a are input to G, which synthesizes the featuresx. The synthesized featuresx and original features x are compared using a binary cross-entropy loss L BCE . The discriminator D takes either x orx along with embeddings a as input, and computes a real number that determines whether the input is real or fake. The WGAN loss L W is applied at the output of D to learn to distinguish between the real and fake features. The focus of our design is the integration of an additional semantic embedding decoder (SED) Dec at both the feature synthesis and ZSL/GZSL classification stages. Additionally, we introduce a feedback module F , which is utilized during training and feature synthesis, along with Dec. Both the semantic embedding decoder Dec and feedback module F collectively address the objectives of enhanced feature synthesis and reduced ambiguities among categories during classification. The Dec takes either x orx and reconstructs the embeddingsâ. It is trained using a cycle-consistency loss L R . The learned Dec is subsequently used in the ZSL/GZSL classifiers. The feedback module F transforms the latent embedding of Dec and feeds it back to the latent representation of generator G in order to achieve improved feature synthesis. The SED Dec and feedback module F are described in detail in Sec. 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Embedding Decoder</head><p>Here, we introduce a semantic embedding decoder Dec : X → A, for reconstructing the semantic embeddings a from the generated featuresx. Enforcing a cycle-consistency on the reconstructed semantic embeddings ensures that the generated features are transformed to the same embeddings that generated them. As a result, semantically consistent features are obtained during feature synthesis. The cycle-consistency of the semantic embeddings is achieved using the 1 reconstruction loss as follows:</p><formula xml:id="formula_2">L R = E[||Dec(x) − a|| 1 ] + E[||Dec(x) − a|| 1 ].<label>(3)</label></formula><p>The loss formulation for training the proposed TF-VAEGAN is then given by, where β is a hyper-parameter for weighting the decoder reconstruction error. As discussed earlier, existing GAN-based ZSL approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref> employ a semantic embedding decoder (SED) only during training and discard it during both unseen class feature synthesis and ZSL classification stage. In our approach, SED is utilized at all three stages of VAE-GAN based ZSL pipeline: training, feature synthesis and classification. Next, we describe importance of SED during classification and later investigate its role during feature synthesis (Sec. 3.4). Discriminative feature transformation: Here, we describe the proposed discriminative feature transformation scheme to effectively utilize the auxiliary information in semantic embedding decoder (SED) at the ZSL classification stage. The generator G learns a per-class "single semantic embedding to many instances" mapping using only the seen class features and embeddings. Similar to the generator G, the SED is also trained using only the seen classes but learns a per-class "many instances to one embedding" inverse mapping. Thus, the generator G and SED Dec are likely to encode complementary information of the categories. Here, we propose to use the latent embedding from SED as a useful source of information at the classification stage (see <ref type="figure" target="#fig_1">Fig. 2a</ref>) for reducing ambiguities among features instances of different categories.</p><formula xml:id="formula_3">L total = L vaegan + βL R ,<label>(4)</label></formula><p>First, the training of feature generator G and semantic embedding decoder Dec is performed. Then, Dec is used to transform the features (real and synthesized) to the embedding space A. Afterwards, the latent embeddings from Dec are concatenated with the respective visual features. Let h s andĥ u ∈ H denote the hidden layer (latent) embedding from the Dec for inputs x s andx u , respectively. The transformed features are represented by: x s ⊕ h s andx u ⊕ĥ u , where ⊕ denotes concatenation. In our method, the transformed features are used to learn final ZSL and GZSL classifiers as,</p><formula xml:id="formula_4">f zsl : X ⊕ H → Y u and f gzsl : X ⊕ H → Y s ∪ Y u .<label>(5)</label></formula><p>As a result, the final classifiers learn to better distinguish categories using transformed features. Next, we describe integration of Dec during feature synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feedback Module</head><p>The baseline f-VAEGAN does not enforce cycle-consistency in the attribute space and directly synthesizes visual featuresx from the class-specific embeddings a via the generator (see <ref type="figure">Fig. 3a</ref>). This results in a semantic gap between the real and synthesized visual features. To address this issue, we introduce a feedback loop that iteratively refines the feature generation (see <ref type="figure">Fig. 3b</ref>) during both the training and synthesis stages. The feedback loop is introduced from the semantic embedding decoder Dec to the generator G, through our feedback module F (see <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure" target="#fig_1">Fig. 2b</ref>). The proposed module F enables the effective utilization of Dec during both training and feature synthesis stages. Let g l denote the l th layer output of G andx f denote the feedback component that additively modulates g l . The feedback modulation of output g l is given by,</p><formula xml:id="formula_5">g l ← g l + δx f ,<label>(6)</label></formula><p>wherex f = F (h), with h as the latent embedding of Dec and δ controls the feedback modulation. To the best of our knowledge, we are the first to design and incorporate a feedback loop for zero-shot recognition. Our feedback loop is based on <ref type="bibr" target="#b35">[36]</ref>, originally introduced for image super-resolution. However, we observe that it provides sub-optimal performance for zero-shot recognition due to its less reliable feedback during unseen class feature synthesis. Next, we describe an improved feedback loop with necessary modifications for zero-shot recognition. Feedback module input: The adversarial feedback employs a latent representation of an unconditional discriminator D as its input <ref type="bibr" target="#b35">[36]</ref>. However, in the ZSL problem, D is conditional and is trained with an objective to distinguish between the real and fake features of the seen categories. This restricts D from providing reliable feedback during unseen class feature synthesis. In order to overcome this limitation, we turn our attention to semantic embedding decoder Dec, whose aim is to reconstruct the class-specific semantic embeddings from features instances. Since Dec learns class-specific transformations from visual features to the semantic embeddings, it is better suited (than D) to provide feedback to generator G.</p><p>Training strategy: Originally, the feedback module F is trained in a two-stage fashion <ref type="bibr" target="#b35">[36]</ref>, where the generator G and discriminator D are first fully trained, as in the standard GAN training approach. Then, F is trained using a feedback from D and freezing G. Since, the output of G improves due to the feedback from F , the discriminator D is continued to be trained alongside F , in an adversarial manner. In this work, we argue that such a two-stage training strategy is sub-optimal for ZSL, since G is always fixed and not allowed to improve its feature synthesis. To further utilize the feedback for improved feature synthesis, G and F are trained alternately in our method. In our alternating training strategy, the generator training iteration is unchanged. However, during the training iterations of F , we perform two sub-iterations (see <ref type="figure" target="#fig_1">Fig. 2b</ref>). <ref type="figure">Fig. 3</ref>: Conceptual illustration between the baseline (a) and our feedback module designed for enhanced feature synthesis (b), using three classes ( , and ). The baseline learns to synthesize featureŝ x from class-specific semantic embeddings a via generator G, without enforcing cycleconsistency in the attribute space. As a consequence, a semantic gap is likely to exist between the synthesized and real x features. In our approach, cycle-consistency is enforced using SED. Further, the disparity between the reconstructed embeddingsâ and a is used as a feedback signal to reduce the semantic gap betweenx and x, resulting in enhanced synthesized featuresxe.</p><p>First sub-iteration: The noise z and semantic embeddings a are input to the generator G to yield an initial synthesized featurex[0] = G(z, a), which is then passed through to the semantic embedding decoder Dec. Second sub-iteration: The latent embeddingĥ from Dec is input to F , resulting in an outputx f [t] = F (ĥ), which is added to the latent representation (denoted as g l in Eq. 6) of G. The same z and a (used in the first sub-iteration) are used as input to G for the second sub-iteration, with the additional inputx f [t] added to the latent representation g l of generator G. The generator then outputs a synthesized featurex[t + 1], as,</p><formula xml:id="formula_6">x[t + 1] = G(z, a,x f [t]).<label>(7)</label></formula><p>The refined featurex[t + 1] is input to D and Dec, and corresponding losses are computed (Eq. 4) for training. In practice, the second sub-iteration is performed only once. The feedback module F allows generator G to view the latent embedding of Dec, corresponding to current generated features. This enables G to appropriately refine its output (feature generation) iteratively, leading to an enhanced feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">(Generalized) Zero-Shot Classification</head><p>In our TF-VAEGAN, unseen class features are synthesized by inputting respective embeddings a(u) and noise z to G, given byx u = G(z, a(u),x f [0]).</p><p>Here,x f [0] denotes feedback output of F , computed for the same a(u) and z. The synthesized unseen class featuresx u and real seen class features x s are further input to Dec to obtain their respective latent embeddings, which are concatenated with input features. In this way, we obtain transformed features x s ⊕ h s andx u ⊕ĥ u , which are used to train ZSL and GZSL classifiers, f zsl and f gzsl , respectively. At inference, test features x t are transformed in a similar manner, to obtain x t ⊕ h t . The transformed features are then input to classifiers for final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We evaluate our TF-VAEGAN framework on four standard zero-shot object recognition datasets: Caltech-UCSD-Birds <ref type="bibr" target="#b39">[40]</ref> (CUB), Oxford Flowers <ref type="bibr" target="#b28">[29]</ref> (FLO), SUN Attribute <ref type="bibr" target="#b29">[30]</ref> (SUN), and Animals with Attributes2 <ref type="bibr" target="#b40">[41]</ref> (AWA2) containing 200, 102, 717 and 50 categories, respectively. For fair comparison, we use the same splits, evaluation protocols and class embeddings as in <ref type="bibr" target="#b40">[41]</ref>. Visual features and embeddings: We extract the average-pooled feature instances of size 2048 from the ImageNet-1K <ref type="bibr" target="#b5">[6]</ref> pre-trained ResNet-101 <ref type="bibr" target="#b11">[12]</ref>. For semantic embeddings, we use the class-level attributes for CUB (312-d), SUN (102-d) and AWA2 (85-d). For FLO, fine-grained visual descriptions of image are used to extract 1024-d embeddings from a character-based CNN-RNN <ref type="bibr" target="#b31">[32]</ref>.    based methods for both ZSL and GZSL in fine-tuned inductive (FT-IN) and fine-tuned transductive (FT-TR) settings. For FT-IN ZSL, f-VAEGAN obtains classification scores of 72.9%, 70.4%, 65.6%, and 70.3% on CUB, FLO, SUN and AWA, respectively. Our TF-VAEGAN achieves consistent improvement over f-VAEGAN on all datasets, achieving classification scores of 74.3%, 74.7%, 66.7%, and 73.4% on CUB, FLO, SUN and AWA, respectively. Our approach also improves over f-VAEGAN for the FT-TR ZSL setting. In the case of FT-IN GZSL, our TF-VAEGAN achieves gains (in terms of H) of 1.8%, 4.3%, 3.2%, and 1.5% on CUB, FLO, SUN and AWA, respectively over f-VAEGAN. A similar trend is also observed for the FT-TR GZSL setting. In summary, our TF-VAEGAN achieves promising results for various settings and backbone feature combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Baseline comparison: We first compare our proposed TF-VAEGAN with the baseline f-VAEGAN <ref type="bibr" target="#b42">[43]</ref> on CUB for (generalized) zero-shot recognition in both inductive and transductive settings. The results are reported in Tab. 2 in terms of average top-1 classification accuracy for ZSL and harmonic mean of the classification accuracies of seen and unseen classes for GZSL. For the baseline, we present the results based on our re-implementation. In addition to our final TF-VAEGAN, we report results of our feedback module alone (denoted <ref type="table">Table 2</ref>: Baseline performance comparison on CUB <ref type="bibr" target="#b39">[40]</ref>. In both inductive and transductive settings, our Feedback and T-feature provide consistent improvements over the baseline for both ZSL and GZSL. Further, our final TF-VAEGAN framework, integrating both Feedback and T-feature, achieves further gains over the baseline in both inductive and transductive settings, for ZSL and GZSL. as Feedback in Tab. 2) without feature transformation utilized at classification stage. Moreover, the performance of discriminative feature transformation alone (denoted as T-feature), without utilizing the feedback is also presented. For the inductive setting, Baseline obtains a classification performance of 61.2% and 53.5% for ZSL and GZSL. Both our contributions, Feedback and T-feature, consistently improve the performance over the baseline. The best results are obtained by our TF-VAEGAN, with gains of 3.7% and 4.6% over the baseline, for ZSL and GZSL. Similar to the inductive (IN) setting, our proposed TF-VAEGAN also achieves favourable performance in transductive (TR) setting. <ref type="figure">Fig. 4</ref> shows a comparison between baseline and our TF-VAEGAN methods, using t-SNE visualizations <ref type="bibr" target="#b23">[24]</ref> of test instances from four example finegrained classes of CUB. While the baseline struggles to correctly classify these fine-grained class instances due to inter-class confusion, our TF-VAEGAN improves inter-class grouping leading to a favorable classification performance. Generalization capabilities: Here, we base our approach on a VAE-GAN architecture <ref type="bibr" target="#b42">[43]</ref>. However, our proposed contributions (a semantic embedding decoder at all stages of the ZSL pipeline and the feedback module) are generic and can also be utilized in other GAN-based ZSL frameworks. To this end, we perform an experiment by integrating our contributions in the f-CLSWGAN <ref type="bibr" target="#b41">[42]</ref> architecture. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the comparison between the baseline f-CLSWGAN and our TF-CLSWGAN for ZSL and GZSL tasks, on all four datasets. Our TF-CLSWGAN outperforms the vanilla f-CLSWGAN in all cases for both ZSL and GZSL tasks. Feature visualization: To qualitatively assess the feature synthesis stage, we train an upconvolutional network to invert the feature instances back to the image space by following a similar strategy as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43]</ref>. Corresponding implementation details are provided in the supplementary. The model is trained on all real feature-image pairs of the 102 classes of FLO <ref type="bibr" target="#b28">[29]</ref>. The comparison between Baseline and our Feedback synthesized features on four example flowers is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. For each flower class, a ground-truth (GT) image along with three images inverted from its GT feature, Baseline and Feedback synthesized features, respectively are shown. Generally, inverting the Feedback synthesized feature yields an image that is semantically closer to the GT image than inverting the Baseline synthesized feature. This suggests that our Feedback improves the feature synthesis stage over the Baseline, where no feedback is present. Additional quantitative and qualitative results are given in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDUCTIVE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">(Generalized) Zero-Shot Action Recognition</head><p>Finally, we validate our TF-VAEGAN for action recognition in videos under ZSL and GZSL. Here, we use the I3D features <ref type="bibr" target="#b4">[5]</ref>, as in the GAN-based zero-shot action classification method CEWGAN <ref type="bibr" target="#b24">[25]</ref>. While using improved video features is likely to improve the performance of a zero-shot action recognition framework, our goal is to show that our TF-VAEGAN generalizes to action classification and improves the performance using the same underlying video features. As in <ref type="bibr" target="#b24">[25]</ref>, we extract spatio-temporally pooled 4096-d I3D features from pre-trained RGB and Flow I3D networks and concatenate them to obtain 8192-d video features. Further, an out-of-distribution classifier is utilized at the classification stage, as in <ref type="bibr" target="#b24">[25]</ref>. For HMDB51, a skip-gram model <ref type="bibr" target="#b26">[27]</ref> is used to generate semantic embeddings of size 300, using action class names as input. For UCF101, we use semantic embeddings of size 115, provided with the dataset. Tab. 3 shows state-of-the-art comparison on HMDB51 <ref type="bibr" target="#b19">[20]</ref> and UCF101 <ref type="bibr" target="#b37">[38]</ref>. For a fair comparison, we use the same splits, embeddings and evaluation pro-  tocols as in <ref type="bibr" target="#b24">[25]</ref>. On HMDB51, f-VAEGAN obtains classification scores of 31.1% and 35.6% for ZSL and GZSL. The work of <ref type="bibr" target="#b49">[50]</ref> provides classification results of 24.4% and 17.5% for HMDB51 and UCF101, respectively for ZSL. Note that <ref type="bibr" target="#b49">[50]</ref> also reports results using cross-dataset training on large-scale ActivityNet <ref type="bibr" target="#b3">[4]</ref>. On HMDB51, CEWGAN [25] obtains 30.2% and 36.1% for ZSL and GZSL. Our TF-VAEGAN achieves 33.0% and 37.6% for ZSL and GZSL. Similarly, our approach performs favourably compared to existing methods on UCF101. Hence, our TF-VAEGAN generalizes to action recognition and achieves promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an approach that utilizes the semantic embedding decoder (SED) at all stages (training, feature synthesis and classification) of a VAE-GAN based ZSL framework. Since SED performs inverse transformations in relation to the generator, its deployment at all stages enables exploiting complementary information with respect to feature instances. To effectively utilize SED during both training and feature synthesis, we introduce a feedback module that transforms the latent embeddings of the SED and modulates the latent representations of the generator. We further introduce a discriminative feature transformation, during the classification stage, which utilizes the latent embeddings of SED along with respective features. Experiments on six datasets clearly suggest that our approach achieves favorable performance, compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Quantitative Results</head><p>In this section, we present the ablation studies with respect to the feedback design choices and the choice of latent embeddings. Feedback design choices: Here, we explore the effect of changing the input to the feedback module F and its associated training strategy on CUB. Originally, the input to F is taken from discriminator D and the training of F is performed in a two-stage strategy. This setup is denoted by TwoStage+D and obtains classification performance of 61.4% and 53.3% for ZSL and GZSL. Instead, in our approach, the input to F is taken from SED Dec. This setup is denoted by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Feature Visualization Comparison</head><p>Here, we present the implementation details and additional qualitative results for the visualization of synthesized features discussed in Sec. 4.2 of the paper. Implementation details: The image generator, which inverts the feature instances to images of size 64x64, consists of a fully-connected (FC) layer followed by five upconvolutional blocks. Each upconvolutional block contains an Upsampling layer, a 3x3 convolution, BatchNorm and ReLu non-linearity. An 1 loss between the ground truth and inverted images, along with a perceptual loss ( 2 loss between the corresponding feature vectors at conv5 of a pre-trained ResNet-101) and an adversarial loss are employed to construct good quality images. The discriminator, required for adversarial training, takes image and feature embedding as inputs. The input image is processed through four downsampling blocks to obtain an image embedding, while the feature embedding is passed through an FC layer and spatially replicated to match the spatial dimensions of the obtained image embedding. The resulting two embeddings are concatenated and passed through convolutional and sigmoid layers for predicting whether the input image is real or fake. The model is trained on all the real feature-image pairs of the 102 classes of FLO <ref type="bibr" target="#b28">[29]</ref>. Visualization: The comparison between Baseline and our Feedback synthesized features on eight example flowers is shown in <ref type="figure">Fig. 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Classification Performance Comparison</head><p>Here, we qualitatively illustrate the performance of our TF-VAEGAN framework, in comparison to the baseline f-VAEGAN <ref type="bibr" target="#b42">[43]</ref> method, on two fine-grained object recognition datasets: CUB and FLO. <ref type="figure">Fig. 8 and 9</ref> present the comparison on CUB and FLO, respectively. For each dataset, images from five most confusing categories (with respect to the baseline f-VAEGAN) are shown. The comparison is illustrated for five image instances in each category. The ground truth instances are shown in the top row for each category, followed by the classification results of the baseline and proposed frameworks in second and third rows, respectively. Correctly classified images are marked with a green border, while the incorrectly classified images are marked with a red border. For the misclassifications, the name of the incorrectly predicted class is denoted below the instance for the respective methods. CUB: The qualitative comparison between the baseline and the proposed approaches for the CUB <ref type="bibr" target="#b39">[40]</ref> dataset is shown in <ref type="figure">Fig. 8</ref>. Five categories of birds that are most confusing for the baseline approach are presented. The categories are Prairie warbler, Great crested flycatcher, Grovve billed ani, Herring gull and California gull. Generally, for all these categories, the baseline f-VAEGAN approach confuses with similar looking bird categories in the dataset. Our TF-VAEGAN reduces this confusion between similar looking classes and improves the classification performance. In <ref type="figure">Fig. 8</ref>, we observe that the baseline approach confuses Prairie warbler class with other similar looking warbler categories such as Blue winged warbler, Magnolia warbler and Orange crowned warbler. This confusion is reduced in the predictions of our TF-VAEGAN. Similarly, the confusion present, in the baseline method, between the Great crested flycatcher and other flycatcher categories is reduced for the proposed method. As a result, the overall classification performance improves for the proposed method over the baseline.</p><p>FLO: <ref type="figure">Fig. 9</ref> shows the qualitative comparison for five categories of flowers from the Oxford Flowers <ref type="bibr" target="#b28">[29]</ref> dataset that are most confusing for the baseline method. The categories are Dafodil, Pink primrose, Siam tulip, King Protea and Common dandelion. For all these categories, the proposed TF-VAEGAN reduces the confusion present between the similar looking classes in the baseline f-VAEGAN approach and improves the classification performance. In general, we observe that the instances are misclassified to other similar looking categories in the dataset. E.g., instances of Common dandelion are commonly misclassified as either Colt's foot or Yellow iris. All three categories have yellow flowers and share similar appearance. We observe that the baseline makes confused predictions with respect to these classes. However, the confusion is less in the predictions of the proposed TF-VAEGAN. This leads to a favourable improvement in the zeroshot classification performance for the proposed approach. Similar observations can also be made in the case of other categories. The baseline f-VAEGAN generally confuses Dafodil with Globe flower and Yellow iris due to the yellow colour, while Pink primrose is mostly confused with Petunia and Monkshood due to the pinkish petals in the flowers. The misclassifications are reduced when using the proposed TF-VAEGAN for classification, resulting in an improved performance. <ref type="figure">Fig. 8</ref>: Qualitative comparison between the baseline and our proposed approach on the CUB <ref type="bibr" target="#b39">[40]</ref> dataset. The comparison is based on the most confusing categories as per the baseline performance. For each category, while the top row denotes different variations of ground truth class instances, the second and third rows show the classification predictions by the baseline and proposed approaches, respectively. The green and red boxes denote correct and incorrect classification predictions, respectively. The class names under each red box show the corresponding incorrectly predicted label.</p><p>In general, we observe that the instances are misclassified to other similar looking categories in the dataset. For instance, Prairie warbler is confused with Blue winged warbler, while Groove billed ani is confused commonly with Common raven. For all these categories, the proposed TF-VAEGAN reduces the confusion among similar looking classes in the baseline f-VAEGAN and improves the classification performance over the baseline. See associated text for additional details. Best viewed in color and zoom. <ref type="figure">Fig. 9</ref>: Qualitative comparison between the baseline and our proposed approach on the Oxford Flowers <ref type="bibr" target="#b28">[29]</ref> dataset. The comparison is based on the most confusing categories as per the baseline performance. For each category, while the top row denotes different variations of ground truth class instances, the second and third rows show the classification predictions by the baseline and proposed approaches, respectively. The green and red boxes denote correct and incorrect classification predictions, respectively. The class names under each red box show the corresponding incorrectly predicted label.</p><p>In general, we observe that the instances are misclassified to other similar looking categories in the dataset. For instance, Common dandelion is confused with Colt's foot, while Pink primrose is confused with Petunia. For all these categories, the proposed TF-VAEGAN reduces the confusion among similar looking classes in the baseline f-VAEGAN and improves the classification performance over the baseline. See associated text for additional details. Best viewed in color and zoom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Proposed architecture (Sec 3.2). Given a seen class image, visual features x are extracted from the backbone network and input to the encoder E, along with the corresponding semantic embeddings a. The encoder E outputs a latent code z, which is then input together with embeddings a to the generator G that synthesizes featuresx. The discriminator D learns to distinguish between real and synthesized features x and x, respectively. Both E and G together constitute the VAE, which is trained using a binary cross-entropy loss (LBCE) and the KL divergence (LKL). Similarly, both G and D form the GAN trained using the WGAN loss (LW ). A semantic embedding decoder Dec is introduced (Sec. 3.3) to reconstruct the embeddingsâ using a cycle-consistency loss (LR). Further, a feedback module F (Sec. 3.4) is integrated to transform the latent embeddingĥ of Dec and feed it back to G, which iteratively refinesx. D(x, a). The generator G(z, a) synthesizes a featurex ∈ X from a random input noise z, whereas the discriminator D(x, a) takes an input feature x and outputs a real value indicating the degree of realness or fakeness of the input features. Both G and D are conditioned on the embedding a, optimizing the WGAN loss L W = E[D(x, a)] − E[D(x, a)] − λE[(||∇D(x, a)|| 2 − 1) 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Integration of semantic embedding decoder Dec at the ZSL/GZSL classification stage. A feature transformation is performed by concatenating (⊕) the input visual features x with the corresponding latent embedding h from SED. The transformed discriminative features are then used for ZSL/GZSL classification. (b) Feedback module overview. First sub-iteration: The generator G synthesizes initial featuresx[0] using the noise z and embeddings a. The initial features are passed through the Dec. Second sub-iteration: The module F transforms the latent embedding h from Dec tox f , which represents the feedback to G. The generator G synthesizes enhanced featuresx[1] using the same z and a along with the feedbackx f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>State-of-the-art comparison on four datasets. Both inductive (IN) and transductive (TR) results are shown. The results with fine-tuning the backbone network using the seen classes only (without violating ZSL), are reported under fine-tuned inductive (FT-IN) and transductive (FT-TR) settings. For ZSL, results are reported in terms of average top-1 classification accuracy (T1). For GZSL, results are reported in terms of top-1 accuracy of unseen (u) and seen (s) classes, together with their harmonic mean (H). Our TF-VAEGAN performs favorably in comparison to existing methods on all four datasets, in all settings (IN, TR, FT-IN and FT-TR), for both ZSL and GZSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Generalization capabilities. (a) ZSL and (b) GZSL performance comparison to validate the generalization capabilities of our contributions. Instead of a VAE-GAN architecture, we integrate our proposed contributions in the f-CLSWGAN framework. Our TF-CLSWGAN outperforms the vanilla f-CLSWGAN on all datasets. Best viewed in zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative comparison between inverted images of Baseline synthesized features and our Feedback synthesized features on four example classes of FLO<ref type="bibr" target="#b28">[29]</ref>. The ground-truth image and the reconstructed inversion of its real feature are also shown for each example. Our Feedback improves petal shapes (Sunflower ), shape of bud and petals (Blanket flower ), color (Pink primrose), black lining on petals (Balloon flower ) and achieves promising improvements over Baseline. Best viewed in zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>T1) accuracies of 71.1%, 89.1%, 70.1%, and 89.8% on the four datasets. Our TF-VAEGAN outperforms f-VAEGAN on all datasets, achieving classification accuracies of 74.7%, 92.6%, 70.9%, and 92.1% on CUB, FLO, SUN and AWA, respectively. Similarly, our TF-VAEGAN also performs favourably compared to existing methods on all datasets for both inductive and transductive GZSL settings. Utilizing unlabelled instances during training, to reduce the domain shift problem for unseen classes, in the transductive setting yields higher results compared to inductive setting.Some previous works, including f-VAEGAN<ref type="bibr" target="#b42">[43]</ref> have reported results with fine-tuning the backbone network only using the seen classes (without violating the ZSL condition). Similarly, we also evaluate our TF-VAEGAN by utilizing finetuned backbone features. Tab. 1 shows the comparison with existing fine-tuning</figDesc><table /><note>Implementation details: The discriminator D, encoder E and generator G are implemented as two-layer fully-connected (FC) networks with 4096 hidden units. The dimensions of z and a are set to be equal (R dz = R da ). The semantic embedding decoder Dec and feedback module F are also two-layer FC networks with 4096 hidden units. The input and output dimensions of F are set to 4096 to match the hidden units of Dec and G. For transductive setting, an uncondi- tional discriminator D2 is employed for utilizing the unlabelled feature instances during training, as in [43]. Since the corresponding semantic embeddings are not available for unlabelled instances, only the visual feature is input to D2. Leaky ReLU activation is used everywhere, except at the output of G, where a sigmoid activation is used for applying BCE loss. The network is trained using the Adam optimizer with 10 −4 learning rate. Final ZSL/GZSL classifiers are single layer FC networks with output units equal to number of test classes. Hyper-parameters α, β and δ are set to 10, 0.01 and 1, respectively. The gradient penalty coefficient λ is initialized to 10 and WGAN is trained, similar to [3].4.1 State-of-the-art Comparison Tab. 1 shows state-of-the-art comparison on four object recognition datasets. Re- sults for inductive (IN) and transductive (TR) settings are obtained without any fine-tuning of the backbone network. For inductive (IN) ZSL, the Cycle-WGAN [8] obtains classification scores of 58.6%, 70.3%, 59.9%, and 66.8% on CUB, FLO, SUN and AWA, respectively. The f-VAEGAN [43] reports classification accuracies of 61%, 67.7%, 64.7%, and 71.1% on the same datasets. Our TF-VAEGAN outper- forms f-VAEGAN on all datasets achieving classification scores of 64.9%, 70.8%, 66.0%, and 72.2% on CUB, FLO, SUN and AWA, respectively. In the transduc- tive (TR) ZSL setting, f-VAEGAN obtains top-1 classification (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Ours: TF-VAEGAN 64.9 70.8 66.0 72.2 52.8 64.7 58.1 62.5 84.1 71.7 45.6 40.7 43.0 59.8 75.1 66.6 70.4 65.6 70.3 63.2 75.6 68.9 63.3 92.4 75.1 50.1 37.8 43.1 57.1 76.1 65.2 Ours: TF-VAEGAN 74.3 74.7 66.7 73.4 63.8 79.3 70.7 69.5 92.5 79.4 41.8 51.9 46.3 55.5 83.6 66.7 Ours: TF-VAEGAN 85.1 96.0 73.8 93.0 78.4 83.5 80.9 96.1 97.6 96.8 44.3 66.9 53.3 89.2 90.0 89.6</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Zero-shot Learning</cell><cell></cell><cell></cell><cell cols="7">Generalized Zero-shot Learning</cell></row><row><cell></cell><cell></cell><cell cols="4">CUB FLO SUN AWA</cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell>FLO</cell><cell></cell><cell></cell><cell>SUN</cell><cell>AWA</cell></row><row><cell></cell><cell></cell><cell cols="3">T1 T1 T1</cell><cell>T1</cell><cell>u</cell><cell>s</cell><cell>H</cell><cell>u</cell><cell>s</cell><cell>H</cell><cell>u</cell><cell>s</cell><cell>H</cell><cell>u</cell><cell>s</cell><cell>H</cell></row><row><cell></cell><cell>f-CLSWGAN [42]</cell><cell cols="13">57.3 67.2 60.8 68.2 3.7 57.7 49.7 59.0 73.8 65.6 42.6 36.6 39.4 57.9 61.4 59.6</cell></row><row><cell></cell><cell>Cycle-WGAN [8]</cell><cell cols="13">58.6 70.3 59.9 66.8 47.9 59.3 53.0 61.6 69.2 65.2 47.2 33.8 39.4 59.6 63.4 59.8</cell></row><row><cell>IN</cell><cell>LisGAN [22] TCN [17]</cell><cell cols="13">58.8 69.6 61.7 70.6 46.5 57.9 51.6 57.7 83.8 68.3 42.9 37.8 40.2 52.6 76.3 62.3 59.5 -61.5 71.2 52.6 52.0 52.3 ---31.2 37.3 34.0 61.2 65.8 63.4</cell></row><row><cell></cell><cell>f-VAEGAN [43]</cell><cell cols="13">61.0 67.7 64.7 71.1 48.4 60.1 53.6 56.8 74.9 64.6 45.1 38.0 41.3 57.6 70.6 63.5</cell></row><row><cell></cell><cell>ALE-tran [41]</cell><cell cols="13">54.5 48.3 55.7 70.7 23.5 45.1 30.9 13.6 61.4 22.2 19.9 22.6 21.2 12.6 73.0 21.5</cell></row><row><cell></cell><cell>GFZSL [39]</cell><cell cols="13">50.0 85.4 64.0 78.6 24.9 45.8 32.2 21.8 75.0 33.8 0.0 41.6 0.0 31.7 67.2 43.1</cell></row><row><cell>TR</cell><cell>DSRL [45]</cell><cell cols="13">48.7 57.7 56.8 72.8 17.3 39.0 24.0 26.9 64.3 37.9 17.7 25.0 20.7 20.8 74.7 32.6</cell></row><row><cell></cell><cell>f-VAEGAN [43]</cell><cell cols="13">71.1 89.1 70.1 89.8 61.4 65.1 63.2 78.7 87.2 82.7 60.6 41.9 49.6 84.8 88.6 86.7</cell></row><row><cell></cell><cell cols="14">Ours: TF-VAEGAN 74.7 92.6 70.9 92.1 69.9 72.1 71.0 91.8 93.2 92.5 62.4 47.1 53.7 87.3 89.6 88.4</cell></row><row><cell></cell><cell>SBAR-I [31]</cell><cell>63.9</cell><cell>-</cell><cell cols="6">62.8 65.2 55.0 58.7 56.8 -</cell><cell>-</cell><cell cols="4">-50.7 35.1 41.5 30.3 93.9 46.9</cell></row><row><cell cols="15">FT-IN 72.9 FT-TR f-VAEGAN [43] SBAR-T [31] 74.0 UE-finetune [37] 72.1 f-VAEGAN [43] 82.6 95.4 72.6 89.3 73.8 81.4 77.3 91.0 97.4 94.1 54.2 41.8 47.2 86.3 88.7 87.5 -67.5 88.9 67.2 73.7 70.3 ---58.8 41.5 48.6 79.7 91.0 85.0 -58.3 79.7 74.9 71.5 73.2 ---33.6 54.8 41.7 93.1 66.2 77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>State-of-the-art ZSL and GZSL comparison for action recognition. Our TF-VAEGAN performs favorably against all existing methods, on both datasets.</figDesc><table><row><cell></cell><cell cols="7">GGM [28] CLSWGAN [42] CEWGAN [25] Obj2Act [15] ObjEmb [26] f-VAEGAN [43] TF-VAEGAN</cell></row><row><cell>HMDB51</cell><cell>ZSL 20.7 GZSL 20.1</cell><cell>29.1 32.7</cell><cell>30.2 36.1</cell><cell>24.5 -</cell><cell>--</cell><cell>31.1 35.6</cell><cell>33.0 37.6</cell></row><row><cell>UCF101</cell><cell>ZSL 20.3 GZSL 17.5</cell><cell>37.5 44.4</cell><cell>38.3 49.4</cell><cell>38.9 -</cell><cell>40.4 -</cell><cell>38.2 47.2</cell><cell>41.0 50.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>TwoStage+Dec and achieves performance of 62.0% and 53.8% for ZSL and GZSL. Further, we utilize an alternate training strategy combined with TwoStage+Dec to facilitate the generator training, thereby improving feature synthesis. This setup, denoted by Our Feedback, achieves improved performance of 62.8% and 54.8% for ZSL and GZSL. These results show that (i) TwoStage+Dec provides improved performance over original TwoStage+D and (ii) the best results are obtained by Our Feedback, demonstrating the impact of our modifications for improved zero-shot recognition.Choice of latent embeddings for T-feature: Here, we evaluate the impact of concatenating different embeddings from SED to the baseline features. We compare our proposed concatenation (T-feature) of baseline features with latent embeddings h of SED with both the original baseline features (OrigFeat) and the baseline features concatenated with the reconstructed attributes (ConcatFeat). On CUB, OrigFeat achieves 61.2% and 53.5% on ZSL and GZSL tasks, respectively. ConcatFeat achieves gains of 1.6% and 2.0% over OrigFeat. In case of ConcatFeat, the reconstructed attributes have single feature representations per-class with inter-class separability but no intra-class diversity. Different to reconstructed attributes, the latent embeddings h possess both intra-class diversity (multiple feature instances per class) and inter-class separability. Our T-feature exploits these properties of latent embeddings with improved results over both OrigFeat and ConcatFeat. Compared to OrigFeat, T-feature obtains gains of 2.8% and 3.4% on ZSL and GZSL tasks, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. For each flower class, a ground-truth (GT) image along with three images inverted from its GT feature, Baseline and Feedback synthesized features, respectively are shown. Generally, inverting the Feedback synthesized feature yields an image that is semantically closer to the GT image than inverting the Baseline synthesized feature. Inverting the feature instances from our Feedback improves the color of bud and shape of petals (Californian poppy, Globe flower and Osteospermum), structure of the flower (Hippeastrum), in comparison to the Baseline synthesized features. A considerable improvement for our Feedback over the Baseline is visible in these flowers (Californian poppy, Globe flower, Hippeastrum and Osteospermum). However, there are a few challenging cases (e.g., Globe thistle, Windflower, Sweet william, Moon orchid ), where a semantic gap still exists between the inversion of real features (denoted as Reconstructed) and inversion of Feedback synthesized features, even though there is a marginal improvement for our Feedback over the Baseline. These qualitative observations suggest that our Feedback improves the feature synthesis stage over the Baseline, where no feedback is present, resulting in improved zero-shot classification. Qualitative comparison between inverted images of Baseline synthesized features and our Feedback synthesized features on eight example classes of FLO<ref type="bibr" target="#b28">[29]</ref>. The ground-truth image (GT) and the reconstructed inversion (Reconstructed) of its real feature are also shown for each example. Inverting the feature instances from our Feedback improves the color of bud and shape of petals (Californian poppy, Globe flower and Osteospermum), structure of the flower (Hippeastrum), in comparison to the Baseline synthesized features. Semantic gap still exists between the inversion of real features (denoted as Reconstructed) and inversion of Feedback synthesized features for a few challenging cases (e.g., Globe thistle, Windflower, Sweet william, Moon orchid ), even though there is some improvement for our Feedback over the Baseline. These observations suggest that our Feedback improves the quality of synthesized features over the Baseline, where no feedback is present. Best viewed in color and zoom.</figDesc><table><row><cell>Fig. 7:</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-modal cycle-consistent generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Vijay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pougetabadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Warde-Farley</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative dual adversarial network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feedback adversarial learning: Spatial feedback for improving generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ob-jects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transferable contrastive network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attribute-based classification for zero-shot visual object categorization. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Leveraging the invariant side of generative zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devraj</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiva Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantically aligned bias reducing zero shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akanksha</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munjal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Alon Shoshan, and Lihi Zelnik-Manor. Adversarial feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Shama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transductive unbiased embedding for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple exponential family framework for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Zeynep Akata. f-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot classification with discriminative semantic representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te-Lin</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual data synthesis via gan for zero-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Infovae: Balancing learning and inference in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
