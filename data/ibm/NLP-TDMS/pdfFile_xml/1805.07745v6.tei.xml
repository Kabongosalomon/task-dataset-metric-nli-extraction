<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abstractive Text Classification Using Sequence-to-convolution Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
							<email>taehoonkim@sogang.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
								<orgName type="laboratory">Data Mining Research Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Yang</surname></persName>
							<email>yangjh@sogang.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
								<orgName type="laboratory">Data Mining Research Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Abstractive Text Classification Using Sequence-to-convolution Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new deep neural network model and its training scheme for text classification. Our model Sequence-to-convolution Neural Networks(Seq2CNN) consists of two blocks: Sequential Block that summarizes input texts and Convolution Block that receives summary of input and classifies it to a label. Seq2CNN is trained end-to-end to classify various-length texts without preprocessing inputs into fixed length. We also present Gradual Weight Shift(GWS) method that stabilize training. GWS is applied to our model's loss function. We compared our model with word-based TextCNN trained with different data preprocessing methods. We obtained significant improvement in classification accuracy over word-based TextCNN without any ensemble or data augmentation. Code is available at https://github.com/tgisaturday/Seq2CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ever since humans began to record information in the form of text, it was necessary to classify and manage information in a certain category to store and retrieve information efficiently. This need encouraged many researchers to develop a good text classification technique that can assign predefined categories to various kinds of text document such as emails, news articles, reviews, or patents.</p><p>In commercial world, text classification techniques such as Naïve Bayes classifier <ref type="bibr" target="#b4">[5]</ref>, TFIDF <ref type="bibr" target="#b36">[37]</ref>, Support Vector Machines(SVM) <ref type="bibr" target="#b14">[15]</ref> are already used in various fields including spam filtering, news categorization, and sentiment analysis. Recent development in deep neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7]</ref> are also achieving excellent results in extracting information from a text and classifying it into certain classes.</p><p>As Convolutional Neural Networks(CNNs) achieved remarkable results in computer vision <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, researchers also applied CNNs to text classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7]</ref> and showed excellent results. Training CNNs on top of pretrained word vectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> or character-level features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7]</ref> with hyperparameter tuning, they could get similar or outperforming results compared to other text classification models.</p><p>Although TextCNNs' performance in text classification is remarkable, they can only be applied to data whose input has fixed size. Since the number of parameters in TextCNN is determined by the length of input text, researchers had to crop or pad input texts into a certain length to train their TextCNN. This can result information loss when classifying longer texts and cause performance degradation.</p><p>In section 4.2, we show that performance of TextCNNs can be improved by training the model with summaries of input text. There are two ways to generate the summary of a text. One is extractive summarization, mere selection of a few existing sentences extracted from the source. The other is abstractive summarization, compressed paraphrasing of main contents of source, potentially using vocabulary unseen in the source. Both methods can change texts of various lengths into texts of fixed length still maintaining important features of source texts.</p><p>TextRank <ref type="bibr" target="#b20">[21]</ref> is a graph-based ranking model for extractive text summarization. TextRank gives a ranking over all sentences in a text allowing it to extract very short summaries without any training corpora. TextRank is widely used in summarizing structured text like news articles.</p><p>Many researchers worked with Sequence-to-sequence Recurrent Neural Networks (Seq2seq RNNs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24]</ref> to model abstractive text summarization. Using attention mechanism <ref type="bibr" target="#b5">[6]</ref> that allows neural networks to focus on different parts of their input, Seq2seq RNNs have been showing significant results in the task of abstractive summarization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper, we introduce Sequence-to-convolution Neural Networks(Seq2CNN) model that consists of two blocks: Sequence Block and Convolution Block. Sequence Block based on Attentional Encoder-Decoder Recurrent Neural Networks <ref type="bibr" target="#b23">[24]</ref> summarizes input texts and feeds them into Convolution Block. Convolution Block based on TextCNN <ref type="bibr" target="#b16">[17]</ref> classifies input texts into certain classes using the summaries provided by Sequential Block. Both blocks share non-static word embedding layer, encouraging them to collaborate for performance improvement.</p><p>Simply connecting two blocks and train them with single end-to-end procedure cannot guarantee optimal results because Sequential Block doesn't generate proper summaries in early stages of training. To solve this problem, we also propose a new training scheme that gradually shifts from finetuning for summarization task to fine-tuning for classification task as training progresses. Our model is implemented with Tensorflow <ref type="bibr" target="#b0">[1]</ref>. Code is available at https://github.com/tgisaturday/Seq2CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There was similar approach of text classification with summaries using Latent Semantic Analysis(LSA) as extractive summarization method <ref type="bibr" target="#b8">[9]</ref>. They proposed a hybrid model for unlabeled document classification using SVM classifier with classification rules are generated using summaries of the training documents. Although we cannot directly compare the performance because of the domain difference in training data, we discuss performance of TextCNN trained with extractive summaries generated with TextRank <ref type="bibr" target="#b20">[21]</ref>.</p><p>Mnih et al. <ref type="bibr" target="#b22">[23]</ref> came up with novel RNN models with visual attention that is capable of extracting information from an image or video by adaptively selecting a sequence of regions and this idea of using attention mechanism was successfully applied to machine translation by Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref>. We used Bahdanau Attention <ref type="bibr" target="#b1">[2]</ref> to improve the performance of our Sequential Block.</p><p>Our approach to use Attentional Encoder-Decoder RNNs for abstractive summarization is closely related to Nallapati et al. <ref type="bibr" target="#b23">[24]</ref> who were the first to use Seq2seq RNNs and Attention model for abstractive text summarization. Our Tensorflow <ref type="bibr" target="#b0">[1]</ref> implementation of Sequential Block is very similar to Pan and Liu <ref type="bibr" target="#b19">[20]</ref>. They used Annotated English Gigaword <ref type="bibr" target="#b24">[25]</ref> dataset to train their model and achieved state-of-the-art results (as of June 2016).</p><p>Seq2seq models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b23">24]</ref> require training set of input text and corresponding example summary. None of datasets that we used for evaluation has any example summary. We used TextRank <ref type="bibr" target="#b20">[21]</ref> to generate extractive summaries out of input texts and feed them to Sequential Block as example summary. We chose TextRank <ref type="bibr" target="#b20">[21]</ref> because it can generate practical summaries even with short texts and doesn't require any training corpora.</p><p>The architecture of our Convolution Block is based on TextCNN model proposed by <ref type="bibr">Kim[17]</ref>. Kim <ref type="bibr" target="#b16">[17]</ref> was the first to use CNN in text classification. Using this model as a baseline, we applied Batch Normalization after each convolution layer and changed hyperparameters for optimization.</p><p>Our training scheme is closely related to the one proposed in Faster R-CNN paper <ref type="bibr" target="#b27">[28]</ref>. Their scheme alternates between fine-tuning two different tasks. We also tried to train our model with training scheme that alternates between fine-tuning summarization task and classification task.However, this was not effective because summaries generated in early stages of training were filled with series of UNK (unknown word) tokens. Instead, our training scheme gradually changes focus of training from summary generation to text classification so that summaries generated in early stages of training don't lower the classification accuracy of Convolution Block.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Seq2CNN Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequential Block</head><p>Our baseline model of Sequential Block corresponds to the Attentional Encoder-Decoder RNN model used in Nallapati et al. <ref type="bibr" target="#b23">[24]</ref> which encodes a source sentence into a fixed-length vector from which decoder generates abstractive summaries. The encoder consists of Bidirectional RNN <ref type="bibr" target="#b29">[30]</ref> and decoder consists of Uni-directional RNN <ref type="bibr" target="#b9">[10]</ref>. We used Long Short-Term Memory RNN <ref type="bibr" target="#b12">[13]</ref> with 128 hidden units for encoder and decoder and Bahdanau Attention[2] for attention mechanism. We also inserted Dropout <ref type="bibr" target="#b32">[33]</ref> modules between LSTM layers to regularize.</p><p>The forward LSTM of encoder reads the input sequence as it is ordered, and the backward LSTM reads the sequence in the reverse order. In this way, fixed-length vector from encoder contains the summaries of both preceding words and the following words. With the help of attention mechanism, decoder decides parts of the source sentence to pay attention to and focus only on the vectors that are essential for summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolution Block</head><p>The structure of our Convolution Block is based on TextCNN model proposed by <ref type="bibr">Kim[17]</ref> which gets n × k vectorized representation of text where n is the number of words inside the text and k is the dimension size of word embedding. Each filter windows with varying sizes(h) extracts one feature by performing h × k convolution operation over input and apply max-over-time pooling operation. The model uses multiple filters to multiple features. These features are passed to a fully-connected softmax layer whose output is the probability distribution over labels.</p><p>Convolution Block gets vectorized representation of summaries generated by Sequential Block. We used rectified linear units(ReLU) <ref type="bibr" target="#b35">[36]</ref> for non-linear activation function and filter windows of 3, 4, 5 with 32 filters each. We applied Batch Normalization <ref type="bibr" target="#b13">[14]</ref> after each convolution layer. Batch Normalization <ref type="bibr" target="#b13">[14]</ref> accelerates training by reducing internal covariate shift <ref type="bibr" target="#b13">[14]</ref>, the change in the distribution of network activations due to the change in network parameters during the training. We also applied batch normalization on vectorized representation of summaries to stabilize entire training procedure by reducing internal covariate shift between Sequential block and Convolution block. For regularization, we inserted Dropout <ref type="bibr" target="#b32">[33]</ref> module between max-pooling layer and fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Embedding Block</head><p>Word Embedding Block consists of word embedding layer which stores vectorized representations of each word and vocabulary lookup table that maps each word with corresponding vector representation. To make vocabulary dictionary, we extracted 20,000 to 30,000 words from training data with minimum word frequency(f ), excluding words that have appeared less than f times. Our word embedding layer is non-static and fine-tuned via back-propagation. In our implementation, we set word embedding dimension to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>The main objective of Seq2CNN is to classify texts of various lengths without losing important features of original context. For feature extraction, we use abstractive summarization method using Sequential Block. Although our model is mainly focused on classification, quality of summary must be guaranteed for Convolution Block to successfully perform classification task.</p><p>Taking everything into consideration, we trained our model to minimize an objective function which is weighted sum of losses in classification and summarization. Our loss function for a text is defined as:</p><formula xml:id="formula_0">L({p yi , y * i }, {t i , t * i , l i }) = 1 N [L cls (p yi , y * i ) + λL sum (t i , t * i , l i )] (1) L sum (t i , t * i , l i ) = 1 l i li j L vocabj (p wj , w * j )<label>(2)</label></formula><p>In <ref type="formula">(1)</ref>, i is the index of a text in a mini-batch of size N and p yi is the predicted probability of text i. classified as ground truth label y * i . The classification loss L cls is cross-entropy loss over classification classes. The summarization loss L sum is sequence loss between the summary output of Sequential Block t i with length l i and summary example t * i . In <ref type="bibr" target="#b1">(2)</ref>, p wj is the predicted probability of the jth word w j in generated summaryto match with the jth word of summary example w j * . We defined sequence loss as the average of the vocabulary losses L vocab s in t i . The vocabulary loss of jth word L vocabj is cross-entropy loss over vocabulary in dictionary stored in Word Embedding Block.</p><p>Total loss is weighted sum of L cls and L sum with a balancing weight λ, normalized with N . In our implementation, we used multi-class softmax layer to get p yi and p wj . We normalized our loss with the mini-batch size. We applied Gradual Weight Shift to λ. We explained more about Gradual Weight Shift in section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Gradual Weight Shift</head><p>In our implementation of Seq2CNN, we train Sequential Block and Convolution Block end-to-end by back-propagation and gradient optimizer using loss function defined in 3.4. This training scheme fine-tunes the model and reduces training time compared to training each model independently. However, using a constant value (&gt;1.0) for balancing weight λ caused sudden drops of validation accuracy in later stages of training (~20 epochs). We found the main cause of this phenomenon in Sequential Block.</p><p>In earlier stages of training (~10 epochs), Sequential Block does not generate practical summaries and omits UNK tokens instead. Huge difference in quality of summary throughout the training hinders the optimization of Convolution Block. Using larger λ (&gt;1.5) solves this problem a little bit by giving more weight to L sum , making Sequential Block to converge faster.</p><p>Using larger λ gives more weight to L sum than L cls , leading the model to focus on optimization of Sequential Block until the end. Since our model is designed for classification, we gradually shifted weight from L sum to L cls by exponentially decaying λ throughout time. Our exponential decay function for λ is defined as:</p><formula xml:id="formula_1">λ t = λ 0 δ t<label>(3)</label></formula><p>where λ 0 is initial value of lambda, δ is decay rate, and t is current time step. We could stabilize the training of our model and achieve higher test accuracy by applying Gradual Weight Shift to our loss function defined in section 3.4. We explain more about the result in section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Sharing Word Embedding for Summarization and Classification</head><p>We designed our model to share word embedding for summarization and classification. When back-propagation happens, Sequential Block tries to update word embedding layer in direction of minimizing sequence loss. On the other hand, Convolution Block tries to update word embedding layer in direction of minimizing classification loss. This fine-tunes word embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Optimization</head><p>In our implementation, we trained our model end-to-end by back-propagation and stochastic gradient descent(SGD) <ref type="bibr" target="#b3">[4]</ref> using Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with β 1 = 0.9, β 2 = 0.999, and = 0.1. We used learning rate of 0.001, decayed every epoch using an exponential rate of 0.95. Dropout rate for Dropout <ref type="bibr" target="#b32">[33]</ref> modules is 0.7. We also used gradient clipping <ref type="bibr" target="#b25">[26]</ref> with gradient norm limited to 5. We set loss balancing weight λ to 1.0 when training AG's News and 2.0 for other datasets.</p><p>We initialized convolution layers using He Normal <ref type="bibr" target="#b11">[12]</ref> initializer and fully-connected layer using Xavier <ref type="bibr" target="#b7">[8]</ref> initializer. All other weights were initialized with random values from a uniform distribution with minimum of -1 and maximum of 1. The implementation is done using Tensorflow <ref type="bibr" target="#b0">[1]</ref>. We trained our model using single NVIDIA Titan V GPU with mini-batch size 256. It took 3 hours(AG's News) to 1 day(Yahoo Answers) for training. We didn't used any ensemble or data augmentation techniques Detailed information about datasets are given in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated the performance of our model by comparing with basic TextCNN <ref type="bibr" target="#b16">[17]</ref>. We defined basic TextCNN used in experiments as Vanilla CNN. We used the same hyperparameters(filter windows of 3, 4, 5 with 32 filters each) for Vanilla CNN and Convolution Block in Seq2CNN. We trained Vanilla CNN with three different data preprocessing methods: full-text, crop&amp;pad, summarize. We defined the length of text as the number of words in each text.</p><p>full-text This is default data preprocessing method. We just removed unnecessary characters and stopwords from the input texts and padded them into fixed length with PAD token. Here, fixed length is the maximum length of text in each dataset. Same preprocessing method is also applied before crop&amp;pad and summarize.</p><p>crop&amp;pad Using fixed length as hyperparameter(numbers inside bracket in <ref type="table" target="#tab_1">Table 2</ref>), we cropped each text into fixed length. For example, if a text is longer than 20 words, we only used 20 words starting from the front. Texts shorter than fixed length is padded with PAD token.</p><p>summarize Instead of cropping each sentence, we generated extractive summary of input text using TextRank <ref type="bibr" target="#b20">[21]</ref> and processed the summary with crop&amp;pad. Same method was used to generate summary example used in training Sequential Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our model on three different datasets: AG's News, DBPedia, and Yahoo Answers <ref type="bibr" target="#b37">[38]</ref>.</p><p>To offer fair evaluation on the performance of Sequential Block in Seq2CNN, we removed input texts shorter than 50 words(maximum fixed length value in <ref type="table" target="#tab_1">Table 2</ref>), changing number of training samples in DBPedia and Yahoo Answers. The number of training samples in AG' News is the same as the  <ref type="table" target="#tab_0">AG's News  4  120,000  7,600  26,543  3  DBPedia  14  140,000  70,000  23,371  15  Yahoo Answers  10  150,000  60,000  28,451  15</ref> original. We didn't apply any changes to test samples for fair comparison with best published results. We also limited the size of vocabulary into 20,000 to 30,000 with minimum word frequency(f ) in <ref type="table" target="#tab_0">Table 1</ref>. Words in test samples are not included in vocabulary dictionary of Word Embedding Block. None of the datasets contains summary examples to train Seq2CNN model so we generated summary examples using TextRank <ref type="bibr" target="#b20">[21]</ref>. We didn't feed any summary samples while evaluating Seq2CNN with test samples. <ref type="bibr" target="#b0">1</ref> Detailed statistics of each dataset is given in <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="bibr" target="#b16">[17]</ref> model and "Seq2CNN" is our model. "Full" stands for "full-text", "Crop" stands for "crop&amp;pad", and "Sum" stands for "summarize". We labeled the best result of Vanilla CNN in blue and worst result in red. Best result of Seq2CNN is labeled in green. Extractive Text Classification <ref type="table" target="#tab_1">Table 2</ref> shows classification results of Seq2CNN and Vanilia CNN models. We first evaluated data preprocessing methods for Vanilla CNN with different fixed length sizes. We labeled the best result in blue and worst result in red. There was not a single data preprocessing method that derived best performance for all datasets. Summarization with TextRank <ref type="bibr" target="#b20">[21]</ref> tends to work well in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Abstractive Text Classification Our Seq2CNN model outperformed other models in all cases bringing average 1% growth compared to Vanilla CNN trained without any data preprocessing(Vanilla CNN Full). Best published result for AG's News using TextCNN <ref type="bibr" target="#b37">[38]</ref> is 90.09% with pretrained word2vec <ref type="bibr" target="#b21">[22]</ref> embedding and data augmentation technique <ref type="bibr" target="#b37">[38]</ref> using thesaurus. Our model achieved competitive result on AG's News dataset without any pretrained word embedding or data augmentation technique. We cannot directly compare other results due to the changes that we explained in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text Summarization</head><p>TextRank <ref type="bibr" target="#b20">[21]</ref> algorithm cannot generate proper summary if the original text is too short. As a result, classification with Vanilla CNN and TextRank(Vanilla CNN Sum <ref type="bibr" target="#b19">(20)</ref>) performed worst with Yahoo Answers dataset. 2 Seq2CNN is robust to short-length texts as it's shown in <ref type="table" target="#tab_3">Table 3</ref>. Even with short-length texts, Sequential Block successfully generate summaries by removing unimportant words from the original. TextRank <ref type="bibr" target="#b20">[21]</ref> algorithm failed to generate any summary for both examples. Steps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Sequence Loss without GWS with GWS </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Gradual Weight Shift</head><p>Optimization In sequence loss curve of <ref type="figure" target="#fig_2">Figure 2</ref>, the sequence loss of the model trained without GWS converges smoothly in the early stage of training, but starts to fluctuate after 2,000 steps. This also effects the total loss curve, unstabilizing the training of the model. In contrast, loss curves of the model with GWS converges smoothly until the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Performance</head><p>We evaluated the performance of Gradual Weight Shift with AG's News Dataset. In <ref type="table" target="#tab_4">Table 4</ref>, Seq2CNN model trained with GWS achieved better results compared to the same model trained without GWS. Although Seq2CNN model without GWS performed better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed Sequence-to-Convolution Neural Networks (Seq2CNN) for efficient and accurate text classification. Seq2CNN can be trained with texts of various lengths without any text preprocessing method such as cropping or summarizing. We also presented a new training theme for our model using Gradual Weight Shift(GWS), which can be applied to other models with multi-task loss function by changing number of balancing weights.</p><p>The true strength of Seq2CNN comes from its flexibility. Each blocks can be replaced with other models designed for the same tasks. For example, Sequential Block can be replaced with multilayer Seq2Seq model <ref type="bibr" target="#b0">[1]</ref> or Text Variational Autoencoder <ref type="bibr" target="#b30">[31]</ref>. Convolution Block can be replaced with other text classification models such as C-LSTM <ref type="bibr" target="#b38">[39]</ref>, Recurrent-CNN <ref type="bibr" target="#b18">[19]</ref>, Char-CNN <ref type="bibr" target="#b37">[38]</ref>, or VDCNN <ref type="bibr" target="#b6">[7]</ref>. We adopted general sequence loss function which uses predicted probability of jth word w j in generated summary matches jth word of summary example w j * to calculate vocabulary losses. However, we think the sequence loss cannot be evaluated accurately by comparing only the words in the same position. Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> suggested specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks and brought significant performance improvements.</p><p>We also didn't use any pretrained word embeddings to initialize the Word Embedding Block. Previous results on word-based TextCNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19]</ref> suggests that initializing embedding layers with pretrained word vectors such as word2vec <ref type="bibr" target="#b21">[22]</ref>, FastText <ref type="bibr" target="#b15">[16]</ref>, or GloVe <ref type="bibr" target="#b26">[27]</ref> helps improves performances of models.</p><p>In the future, we are planning to improve Seq2CNN by reassembling our model with every possible methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>depicts the overall structure of Seq2CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our Seq2CNN model that consists of Sequential Block(left) and Convolution Block(right). Both blocks interact with Word Embedding Layer to get vectorized representation of words used in summarization and classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Total loss curve(left) and sequence loss curve(right) on the AG's News Dataset with and without Gradual Weight Shift(GWS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statisics of datasets. Vocabulary size is number of words used to train the model. Min Freq is minimum word frequency f used to decide vocabulary size of each dataset.DatasetsClasses Training Set Test Set Vocabulary Size Min Freq</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Classification results of all models. Numbers are test accuracy in percentage. "Vanilla CNN" is basic TextCNN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Examples of output produced by Sequential Block with short-length texts. TextRank failed to generate any summary, returning the first sentence of input instead.</figDesc><table><row><cell></cell><cell>Type</cell><cell>Label</cell><cell cols="2">Sentence</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Great Britain's Amir Khan, who looked so impressive</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">in winning the 132-pound championship at the Junior</cell></row><row><cell></cell><cell>Original</cell><cell>Sports</cell><cell cols="4">International Invitational Boxing Championships here last</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">summer, has a chance for an Olympic gold medal in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">lightweight division today.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Great Britain's Amir Khan, who looked so impressive</cell></row><row><cell></cell><cell>TextRank</cell><cell>Sports</cell><cell cols="4">in winning the 132-pound championship at the Junior International Invitational Boxing Championships here last</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">summer, great britain amir khan looked impressive winning pound</cell></row><row><cell></cell><cell cols="2">Sequential Block Sports</cell><cell cols="4">championship junior international invitational boxing championships last summer chance olympic gold medal</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">lightweight division today</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">A ROBOT that will generate its own power by eating flies</cell></row><row><cell></cell><cell>Original</cell><cell>Sci/Tech</cell><cell cols="4">is being developed by British scientists. The idea is to produce electricity by catching flies and digesting them in</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">special fuel cells that will break</cell></row><row><cell></cell><cell>TextRank</cell><cell>Sci/Tech</cell><cell cols="4">A ROBOT that will generate its own power by eating flies is being developed by British scientists.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">robot generate power eating flies developed british sci-</cell></row><row><cell></cell><cell cols="2">Sequential Block Sci/Tech</cell><cell cols="4">entists idea produce electricity catching flies digesting</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">special fuel cells break</cell></row><row><cell></cell><cell></cell><cell>Total Loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell cols="3">without GWS</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell cols="2">with GWS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell>6</cell></row><row><cell>Loss</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>0.2 0.4 0.6 0.8</cell><cell>1</cell><cell>1.2</cell><cell>0.2 0.4 0.6 0.8</cell><cell>1</cell><cell>1.2</cell></row><row><cell></cell><cell></cell><cell>Steps</cell><cell cols="2">·10 4</cell><cell></cell><cell>·10 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification results on the AG's News Dataset with and without Gradual Weight Shift(GWS). Best result using Vanilla CNN is also included for comparison. Vanilla CNN models, it could not outperform previous best published result of Zhang et al.<ref type="bibr" target="#b37">[38]</ref>.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Vanilla CNN</cell><cell>89.67</cell></row><row><cell>Seq2CNN without GWS(20)</cell><cell>89.75</cell></row><row><cell>Seq2CNN without GWS(50)</cell><cell>89.87</cell></row><row><cell>Seq2CNN with GWS(20)</cell><cell>90.18</cell></row><row><cell>Seq2CNN with GWS(50)</cell><cell>90.36</cell></row><row><cell>than any other</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our implementation with Tensorflow, we used greedy embedding helper instead of training helper for inference layer.<ref type="bibr" target="#b1">2</ref> We designed our implementation of TextRank algorithm to return the first sentence if the original text was too short.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Mané</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Task loss estimation for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1511.06456</idno>
		<imprint>
			<date type="published" when="2015-01" />
			<publisher>CoRR</publisher>
			<pubPlace>Nan Rosemary Ke</pubPlace>
		</imprint>
	</monogr>
	<note>Philemon Brakel</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<editor>Yves Lechevallier and Gilbert Saporta</editor>
		<meeting>COMPSTAT&apos;2010<address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Physica-Verlag HD</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature selection for text classification with naïve bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingnian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houkuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youli</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5432" to="5435" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Yee Whye Teh and Mike Titterington</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text classification with summaries generated using latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlene</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayapal</forename><surname>Rajasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRSAE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML-98</title>
		<editor>Claire Nédellec and Céline Rouveirol</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification. CoRR, abs/1408</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5882</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Text summarization with tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<editor>Dekang Lin and Dekai Wu</editor>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recurrent models of visual attention. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence rnns for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX &apos;12</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem. CoRR, abs/1211</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5063</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno>abs/1702.02390</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An improved tf-idf approach for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Zhang Yun-Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yong-Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Zhejiang University-SCIENCE A</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="55" />
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1509.01626</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A C-LSTM neural network for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<idno>abs/1511.08630</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
