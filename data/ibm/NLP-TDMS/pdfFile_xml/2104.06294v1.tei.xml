<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online and Offline Reinforcement Learning by Planning with a Learned Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Mandhane</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadamin</forename><surname>Barekatain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<title level="a" type="main">Online and Offline Reinforcement Learning by Planning with a Learned Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efficient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of offline Reinforcement Learning (offline RL). Combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm for any data budget, including offline RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or offline RL settings. MuZero Unplugged sets new stateof-the-art results in the RL Unplugged offline RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Offline reinforcement learning holds the promise of learning useful policies from many existing real-world datasets in a wide range of important problems such as robotics, healthcare or education . Learning effectively from offline data is crucial for such tasks where interaction with the environment is costly or comes with safety concerns, but a large amount of logged and other offline data is often available.</p><p>A wide variety of effective reinforcement learning (RL) * Equal contribution 1 DeepMind, London, UK. Correspondence to: Julian Schrittwieser &lt;swj@google.com&gt;.</p><p>Under submission, please do not distribute. algorithms for the online case have been described in the literature, achieving impressive results in video games <ref type="bibr" target="#b27">(Mnih et al., 2015)</ref>, robotic control <ref type="bibr" target="#b1">(Akkaya et al., 2019)</ref> and many other problems. However, applying these online RL algorithms to offline datasets often remains challenging due to off-policy issues, with the best results in offline RL so far obtained by specialised offline algorithms <ref type="bibr" target="#b37">Wang et al., 2020;</ref><ref type="bibr" target="#b0">Agarwal et al., 2020)</ref>.</p><p>At the same time, model-based reinforcement learning (RL) has long focused on learning efficiently from little data, even going as far as learning completely within a model of the environment <ref type="bibr" target="#b10">(Hafner et al., 2018)</ref> -an approach ideally suited for offline RL.</p><p>So far, these developments have been relatively independent, with no unified algorithm that could achieve state-of-the art results in both the online and offline settings.</p><p>In this paper, we describe the Reanalyse algorithm, a simple yet effective technique for policy and value improvement at any data budget, including the fully offline case. A preliminary version of Reanalyse was briefly introduced in the context of MuZero <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref>, but limited to data efficiency improvements in the discrete action case. Here, we delve deeper into the algorithm and push its capabilities much further -ultimately to the point where most or all of the data is reanalysed.</p><p>Starting with the possible uses of Reanalyse, we show how it can be used for data efficient learning and offline RL, leading to MuZero Unplugged. We demonstrate its effectiveness for the online case through results on Atari and for the offline case through results on the RL Unplugged benchmark for Atari and DM Control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent work by  provides a thorough review of offline RL literature and presents an excellent introduction to the subject.</p><p>A lot of recent work has focused on regularising the value or policy learning to counteract off-policy issues and learn only from high quality data. Critic-Regularized Regression (CRR) uses a critic to filter out bad actions and uses only arXiv:2104.06294v1 <ref type="bibr">[cs.</ref>LG] 13 Apr 2021 good actions to train the policy . Random Ensemble Mixture (REM) regularises q-value estimation by using random convex combinations of ensemble members during training, and the ensemble mean during evaluation . Conservative Q-Learning (CQL) learns a conservative Q-function, used to lower bound the value of the current policy . Pessimistic Offline Policy Optimization (POPO) also uses a pessimistic value function for policy learning <ref type="bibr" target="#b12">(He &amp; Hou, 2021)</ref>.</p><p>Existing work has also demonstrated the promise of modelbased RL for offline learning <ref type="bibr" target="#b26">(Matsushima et al., 2020;</ref><ref type="bibr" target="#b2">Argenson &amp; Dulac-Arnold, 2020</ref>), but has often been restricted to tasks with low-dimensional action or state spaces, and has not been applied to visually more complex tasks such as Atari <ref type="bibr" target="#b5">(Bellemare et al., 2013)</ref>.</p><p>Model-Based Offline Reinforcement Learning (MOReL) implements a two-step procedure, first learning a pessimistic MDP from offline data using Gaussian dynamics models, then a policy within this learned MDP <ref type="bibr" target="#b19">(Kidambi et al., 2020)</ref>. Results are presented for state-based control tasks.</p><p>Model-based Offline Policy Optimization (MOPO) penalises rewards by the uncertainty of the model dynamics to avoid distributional shift issues .</p><p>Offline Reinforcement Learning from Images with Latent Space Models (LOMPO) extends MOPO to image based tasks <ref type="bibr" target="#b28">(Rafailov et al., 2020)</ref>. Results are reported on newly introduced datasets with image observations, which the authors aim to open-source in the near future.</p><p>All these approaches have in common that they primarily use the learned model for uncertainty estimation and to train a policy; they do not directly use the learned model for planning over action sequences.</p><p>In contrast, our method focuses on using the learned model directly for policy and value improvement through planning both offline (when learning from data) and online (when interacting with an environment). It requires no regularisation of the value or policy function either in the online or offline case, works well even in very high dimensional state spaces and is equally applicable to both discrete and continuous action spaces.</p><p>A combination of MuZero Unplugged with regularisation approaches such as introduced in the previous work discussed above <ref type="bibr" target="#b19">(Kidambi et al., 2020;</ref><ref type="bibr" target="#b39">Yu et al., 2020;</ref><ref type="bibr" target="#b28">Rafailov et al., 2020)</ref> is possible; we leave such investigations for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reanalyse</head><p>Reanalyse takes advantage of model-based value and policy improvement operators to generate new value and policy training targets for a given state (Algorithm 1). In this work, we will use MuZero's Monte Carlo Tree Search (MCTS) planning algorithm combined with its learned model of the environment dynamics as the improvement operator. 1 As the learned model and its predictions are updated and improved throughout training, Reanalyse can be repeatedly applied to the same state to generate better and better training targets. The improved training targets in turn are used to improve the model and predictions, leading to a virtuous cycle of improvement.</p><p>Algorithm 1 The Reanalyse algorithm.</p><p>for</p><formula xml:id="formula_0">step ← 0...N do t ∼ random(1 : T ) s t = representation(o 1:t , θ) for i ← 0...k do π i t , ν i t = improve(representation(o 1:t+i , θ), θ) p i t , v i t = predict(s i t , θ) r i+1 t , s i+1 t = dynamics(s i t , θ) end for l = loss(h t:t+k , {r, p, v, u, π, ν} 0:k t , θ) ∆θ = optimise(l, θ) end for</formula><p>To run MCTS and compute new targets for a training point, the representation function of MuZero maps the input observations into an embedding. The search over possible future action sequences then takes place entirely in this embedding space, by rolling the dynamics forward and applying prediction functions at every step. These predictions output the key quantities required by planning: the policy, value function and reward. The resulting MCTS statistics at the root of the search tree -visit counts for the actions and value estimate averaged over the tree -are then used as new training targets. During reanalysis, no actions are selected -instead the agent updates its model and prediction parameters based on the data it has already experienced.</p><p>Specifically, MuZero Reanalyse jointly adjusts its parameters θ to repeatedly optimise the following loss at every time-step t, applied to a model that is unrolled 0...K steps into the future,</p><formula xml:id="formula_1">l t (θ) = K k=0 l p (π t+k , p k t ) + K k=0 l v (z t+k , v k t ) + K k=1 l r (u t+k , r k t )</formula><p>(1) <ref type="figure">Figure 1</ref>. Final scores in Ms. Pac-Man for different Reanalyse fractions. By scaling the Reanalyse fraction, MuZero can be trained at any desired data budget. All other parameters are held constant. Note the logarithmic x-axis: Linear improvements in score require exponentially more data, matching scaling laws such as described by <ref type="bibr" target="#b18">(Kaplan et al., 2020)</ref> for language models.</p><p>where p k t , v k t , and r t k are respectively the policy, value and reward prediction produced by the k-step unrolled model. The respective targets for these predictions are drawn from the corresponding time-step t+k of the real trajectory: π t+k is the improved policy generated by the search tree, z t+k is an n-step return, and u t+k is the true reward.</p><p>The policy and value predictions are then updated towards the new training targets, in the same way they would be for targets computed based on environment interactionsthrough minimising losses l p , l v and l r . In other words, Reanalyse requires no changes on the part of the learner and can be implemented purely in terms of adapting the actors to generate improved targets based on stored data instead of environment interactions.</p><p>Since the actual MCTS procedure used to Reanalyse a state is the same as the one used to choose an action when interacting with an environment, it is straightforward to perform a mix of both. We refer to this ratio between targets computed from direct interactions with the environment, and targets computed by reanalysing existing data points as the Reanalyse fraction. A Reanalyse fraction of 0% refers to training by only interacting with the environment, no Reanalyse of stored data, whereas a fraction of 100% refers to the fully offline case with no environment interaction at all.</p><p>Since Reanalyse only uses stored data points and the learned model to compute improved targets, it can be employed flexibly for many different purposes:</p><p>• Data Efficiency. The simplest use of Reanalyse is to improve data efficiency by repeatedly computing updated targets on previously collected data throughout training. By scaling the Reanalyse fraction as described in Section 4, learning can be optimised for any data budget. For this purpose, the data to be reanalysed is sampled from the N most recent environment interactions; in the limit this includes all interactions throughout training.</p><p>• Offline RL. When increasing the Reanalyse fraction to 100%, learning takes place entirely from stored offline data as described in Section 5, without any interaction with the environment. Offline data may be obtained from a variety of sources, such as other agents, logged data from a heuristic control system or human examples.</p><p>• Learning from Demonstrations. Reanalyse can also be used to quickly bootstrap learning from demonstrations containing good or desirable behaviour that might otherwise be hard to discover -collected for instance from humans -while still interacting with the environment, learning from both sources of data at the same time. This is useful to skip past what might otherwise be hard exploration problems while still improving beyond the quality of the initial demonstration data.</p><p>• Exploitation of good episodes. When using Reanalyse to improve data efficiency, Reanalyse is applied to the most recently collected data. If instead data is ordered by some other metric, such as episode reward, Reanalyse can be used to quickly learn from rare events, such as rewards observed in hard-exploration tasks. This variant is most useful in deterministic environments, as it could otherwise bias the value estimates in stochastic environments.</p><p>In this paper, we will focus on the data efficiency and offline RL cases. Remaining cases require no adjustments to the algorithm and only differ in the source of data to be reanalysed. Further combinations of the cases above are also possible, such as a mix of exploitation and data efficiency Reanalyse which we leave for future work.</p><p>The Reanalyse algorithm has some similarities to experience replay <ref type="bibr" target="#b23">(Lin, 1992)</ref>. Whereas replay performs multiple gradient descent updates for the same data point and target, Reanalyse uses model-based improvement operators to generate multiple training targets for the same data point.</p><p>Reanalyse and replay have independent effects and can be combined to further improve data efficiency of learning; in fact we do so for all experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Reanalyse for Data Efficiency</head><p>By adjusting the ratio between targets computed from interactions with the environment and from stored trajectories (Reanalyse fraction), Reanalyse can be used to train MuZero at any desired data budget, as shown in <ref type="figure">Figure 1</ref> and <ref type="table">Table 1</ref>. For both figures, the total amount of computation for each training run (number of updates on the learner and number of searches on the actors) is held constant.</p><p>As training progresses, the policy produced by MCTS with the latest network weights will increasingly differ from the policy originally used to generate the trajectories that are being reanalysed. This can bias both the state distribution used for training as well as the computation of value targets along those trajectories when using n-step temporal-difference (TD).</p><p>The policy prediction p t for a state s t is always updated towards the MCTS statistics π t for that same state. In this way, the policy can be learned completely independently from the trajectory; no off-policy issues can arise.</p><p>The reward prediction only depends on the state and the action that was taken from this state and is not affected by off-policy issues as such. However, if the state distribution is very biased -in the extreme an action may never be observed -the reward function will be unable to learn the correct reward prediction for these cases, limiting the maximum policy improvement step.</p><p>The situation for the value function depends on the choice of training target; when using an n-step TD return such as in Atari (n = 5), the value function will depend on the trajectory and off-policy issues can potentially arise. Whether this is an issue depends on how different the data distribution is from the policy that is being learned. Empirically, we observed that the gain from bootstrapping with the actually observed environment rewards seems to outweigh any harm from being off-policy. We speculate that the bias introduced by early bootstrapping may be larger than the bias introduced by off-policy targets, as also seen in prior work <ref type="bibr" target="#b36">(Vinyals et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MuZero Unplugged: Offline RL with Reanalyse</head><p>We obtain MuZero Unplugged, an offline version of MuZero, by adjusting the Reanalyse fraction to 100% -learning without any environment interactions, purely from stored trajectories. In contrast to previous work, we perform no off-policy corrections or adjustments to the value and policy learning: the exact same algorithm applies to both the online and offline case. We used the RL Unplugged  benchmark dataset for all offline RL experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reanalyse</head><p>To demonstrate the generality of the approach, we report results for both discrete and continuous action spaces as well as state and pixel based data, specifically:</p><p>• DM Control Suite, 9 different tasks, number of frames varies by task <ref type="table" target="#tab_3">(Table 5</ref>). Continuous action space with 1 to 21 dimensions, state observations.</p><p>• Atari, 46 games with 200M frames each. Discrete action space, pixel observations, stochasticity through sticky actions .</p><p>MuZero Unplugged was highly effective in either setting, outperforming baseline algorithms in Atari <ref type="table">(Table 2)</ref> as well as the DM Control Suite <ref type="table" target="#tab_3">(Table 5</ref>). We performed no tuning of hyperparameters for these experiments, instead using the same hyperparameter values as for the online RL case <ref type="bibr" target="#b31">(Schrittwieser et al., 2020;</ref><ref type="bibr" target="#b16">Hubert et al., 2021)</ref>.</p><p>To add another strong baseline for the Atari benchmark, we also implemented Critic Regularized Regression (CRR), a recent offline RL algorithm . For the critic value required by CRR we used the value head of MuZero model, trained by 5-step TD with respect to a target network, as in previous work <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref> and the same as used for MuZero Unplugged. Using CRR to train the policy head led to improved results in Atari <ref type="table">(Table  2a</ref>, CRR), matching results reported for continuous action tasks, but did not reach the same performance as MuZero <ref type="figure">Figure 2</ref>. Atari performance improvement. Improvement of performance with respect to the online DQN version that was used to generate the training data for the 46 Atari games from RL Unplugged, calculated as snormalized = sagent−s random s dqn −s random . 0 is random performance, 1 is the same performance as the training data, and larger than 1 represents an improvement. MuZero Unplugged shows very robust performance, reaching the same or higher score in 44 games, with a small decrease in only 2 games. Unplugged.</p><p>Performance of MuZero Unplugged was robust across the whole range of 46 Atari games in the RL Unplugged benchmark, reaching the same or better performance as the DQN policy used to generate the data in 44 games, and slightly worse performance in only 2 games <ref type="figure">(Figure 2</ref>). Improvements in performance with respect to the training data were considerable, exceeding a 20 times increase in score in several games.</p><p>To examine the performance of MuZero Unplugged in detail and ascertain the contributions of action selection methods and training losses, we also performed a set of ablations <ref type="table" target="#tab_2">(Tables 4 and 9</ref>) based on the Atari dataset. We chose Atari because the large number of diverse levels enables robust performance estimates and its discrete action space allows us to cleanly disentangle the contributions of value and policy predictions as well as planning with MCTS. In contrast, for continuous action spaces such as in the DM Control suite, the contributions of policy and value are entangled, as the value function can only evaluate actions already sampled from the policy.</p><p>For our ablations, we considered three possible action selection methods: Sampling actions according to the policy network probabilities, selecting the action with the maximum value, or selecting actions based on the MCTS visit count distribution (rows of  As expected, the policy prediction was insensitive to the choice of model depth, but benefited from an improved training target: the CRR loss significantly improved results. Best results were obtained when using the rich MCTS visit count distribution from the Reanalyse loss as a training target (top row of <ref type="table" target="#tab_2">Table 4</ref>).</p><p>When selecting actions according to the value estimate for each action (middle row of <ref type="table" target="#tab_2">Table 4</ref>), the depth of the learned model was surprisingly important. The difference between estimating q-values (0-step model) and state-values (1-step model) was small, with both attaining results similar to the IQN baseline <ref type="table" target="#tab_2">(Table 4a</ref>) -expected, since all of these results use a distributional value prediction. However, learning a full 5-step model led to a big improvement even though only 1-step value predictions were used for evaluation. We speculate that learning a full 5-step model is beneficial because it regularises the network representation and acts as a useful auxiliary loss. 2</p><p>Keeping the 5-step model but changing the loss for the policy head, we observed that CRR had no effect on the quality of the value prediction for action selection, while the richer MCTS visit count distribution from the Reanalyse loss led to another big improvement. Even though the policy head is not used when selecting actions according to the maximum 1-step value, we hypothesise that the auxiliary loss has a strong regularising effect and further improved the internal representation of the model. This matches the results of <ref type="bibr" target="#b33">(Silver et al., 2017)</ref> that training a single combined network to estimate both policy and value led to improved value prediction accuracy.</p><p>Finally, using MCTS to select actions at evaluation time (bottom row of <ref type="table" target="#tab_2">Table 4</ref>) improved results no matter which loss was used at training time, with best results obtained when using MCTS for both training and evaluation -the full MuZero Unplugged algorithm.</p><p>We also verified that our training setup correctly interpreted the offline data 3 and reproduced the baseline performance when using the same loss: Using the actions played in the training data as a supervised policy target to train a policy head using cross-entropy loss and sampling from it for evaluation <ref type="table">(Table 2</ref>, policy BC a and b) reproduced the behaviour cloning (BC) baseline results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Offline RL and Continuous Action Spaces</head><p>An important motivation for offline RL is the application to real-world systems such as robotics, which often have continuous and high-dimensional action spaces. To investigate the applicability of MuZero Unplugged to this setting, we used the DM Control Suite dataset from the RL Unplugged dataset. DM Control is a collection of physics based benchmark tasks <ref type="bibr" target="#b35">(Tassa et al., 2018)</ref> with a variety of robotic bodies of different action and state dimensionalities <ref type="table" target="#tab_3">(Table  5</ref>).</p><p>In order to use planning and Reanalyse with continuous action spaces, we used the sample based search extension of MuZero introduced by <ref type="bibr" target="#b16">(Hubert et al., 2021)</ref>. This extension uses a policy head to produce a set of candidate actions to search over, where the MCTS considers only the sampled actions instead of fully enumerating the action space. Finally, the policy is updated towards the search distribution only at the sampled actions.</p><p>When applying Reanalyse for data efficiency improvements to data generated by the agent itself, no modifications are used as the target for the policy loss of the model. <ref type="bibr">3</ref> We spent a surprisingly large amount of time tracking down action space mismatches, data discrepancies and compression artefacts. We recommend that any offline RL paper should first reproduce baseline results for the chosen dataset before attempting modifications and improvements to the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>MuZero required to use sample based search and Reanalyse together.</p><p>In offline RL or when reanalysing demonstrations from a source other than the agent itself, the policy that generated the actions making up the dataset is often quite different from the one learned by MuZero Unplugged, and unlikely to sample the same actions, at least at the beginning of training. Since in this case the MCTS (and by extension, Reanalyse) can only consider actions that have been sampled from the policy, it would be unlikely to learn about the actions contained in the dataset, and thus unable to sample them from the policy in the future. This effect is most pronounced in very high dimensional action spaces.</p><p>To prevent this issue, we explicitly included the action from the trajectory being reanalysed in the sample of actions searched over at the root of the MCTS tree. This serves the same purpose as the Dirichlet exploration noise used in standard MuZero -encouraging the MCTS to explore actions it would not otherwise consider. For the prior of the injected action we therefore use the same value as for the Dirichlet probability mass, 25%, though the algorithm is not sensitive to the exact value. This step is redundant for discrete action spaces (such as in Atari) where the policy already always produces a prior for all possible actions.</p><p>We compared the performance of MuZero Unplugged to offline RL algorithms from the literature such as D4PG <ref type="bibr" target="#b4">(Barth-Maron et al., 2018)</ref>, BRAC <ref type="bibr" target="#b38">(Wu et al., 2019)</ref> and RABM <ref type="bibr" target="#b9">Gulcehre et al., 2020)</ref>  <ref type="table" target="#tab_3">(Table 5)</ref>, as well as the recent Critic Regularized Regression (CRR)  algorithm <ref type="table" target="#tab_4">(Table 6</ref>, shown separately as CRR was evaluated by selecting the maximum performance throughout training and results are thus not comparable to the other baselines).</p><p>We first measured the performance of Behaviour Cloning (BC) when implemented using the MuZero network to ensure we used the offline dataset correctly and that it matches the evaluation environment. Overall performance indeed approximately matches the BC baseline <ref type="table" target="#tab_3">(Table 5)</ref>.</p><p>MuZero Unplugged outperformed baseline algorithms both in individual tasks and for the mean return 4 averaged across all tasks. It did best in difficult high-dimensional tasks such as humanoid.run or the manipulator tasks, classified as "hard" by ), compared to "easy" for the other tasks. Performance in the simplest tasks, especially cartpole, was somewhat lower -primarily due to the very small datasets 5 leading to overfitting of the learned model and value function throughout training: in cartpole, performance of the best checkpoint ( <ref type="figure">Figure 6</ref>) was much better than performance at the end of training ( <ref type="figure">Figure 5</ref>). Additional regularisation techniques such as dropout (Hinton et al., 2012) could be employed to prevent this. We leave this for future work since we are primarily interested in performance on complex tasks that we consider most representative of real-world problems. <ref type="bibr">4</ref> Using the mean is appropriate in DM Control as the return for all tasks is in [0, 1000], with the return for the optimal policy close to 1000. Therefore no outlier can dominate the mean; this is unlike the situation in Atari where scores of wildly varying magnitude require usage of the median. <ref type="bibr">5</ref> The amount of information contained in the dataset is the product of the number of episodes and the size of the state and action space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">MuZero Implementation</head><p>All experiments in this paper are based on a JAX <ref type="bibr" target="#b6">(Bradbury et al., 2018)</ref> implementation of MuZero, closely following the description in <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref>. For experiments in environments with continuous actions, we used the extension to MuZero proposed in <ref type="bibr" target="#b16">(Hubert et al., 2021)</ref>. To facilitate a more direct comparison with other algorithms, we used the same Gaussian policy representation as used for data generation and baselines in RL Unplugged .</p><p>The original MuZero did not use sticky actions ) (a 25% chance that the selected action is ignored and that instead the previous action is repeated) for Atari experiments. To make comparisons against other algorithms easier and match the data from RL Unplugged, our implementation did use sticky actions. As shown in <ref type="table">Table 7</ref> (MuZero vs MuZero sticky), this did not affect performance despite introducing slight stochasticity into the environment.</p><p>Additionally, we updated the network architecture of the MuZero learned model to use ResNet v2 style pre-activation residual blocks <ref type="bibr" target="#b11">(He et al., 2016)</ref> coupled with Layer Normalisation <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> and use the Adam optimiser <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> with decoupled weight decay <ref type="bibr" target="#b24">(Loshchilov &amp; Hutter, 2017)</ref> for training. This significantly improved both mean and median normalized performance, setting a new state of the art for Atari at the 200 million frame budget -see <ref type="table">Table 7</ref> for details.  <ref type="bibr" target="#b8">(Espeholt et al., 2018)</ref>, 2 <ref type="bibr" target="#b14">(Hessel et al., 2018)</ref>, 3 <ref type="bibr" target="#b17">(Jaderberg et al., 2016)</ref>, 4 <ref type="bibr" target="#b30">(Schmitt et al., 2019)</ref>, 5 <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref> a Hyper-parameters were tuned per game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>In this paper we have investigated the Reanalyse algorithm and its applications to both data efficient online RL at any data budget and completely offline RL. We combined Reanalyse with MuZero to obtain MuZero Unplugged, a unified model-based RL algorithm that achieved a new state of the art in both online and offline reinforcement learning. Specifically, MuZero Unplugged outperformed prior baselines in the Atari Learning Environment both using a standard online budget of 200 million frames and other data budgets spanning multiple orders of magnitude. Furthermore, MuZero Unplugged also outperformed offline baselines in the RL Unplugged benchmark for Atari and continuous control. Unlike previous approaches, MuZero Unplugged uses the same algorithm for multiple regimes without any special treatment for off-policy or offline data.</p><p>This work represents a further step towards the vision of a single algorithm that can address a wide range of reinforcement learning applications, extending the capabilities of model-based planning algorithms to encompass new dimensions such as online and offline learning, using discrete and continuous action spaces, across pixel and state-based observation spaces, in addition to the wide array of challenging planning tasks addressed by prior work .</p><p>Reanalyse TD steps Median Mean # Frames 99.5% 5 126.6% 450.6% 20M 99.5% 0 115.3% 385.8% 20M <ref type="table">Table 8</ref>. Comparison of value targets in Atari. Mean and median human normalized scores over 57 Atari games, comparing a 5-step TD update towards a target network compared with direct regression against the search value for a state ("0-step TD"). All other parameters are held constant. Even in the 20M frame setting, where learning is almost entirely off-policy from reanalysed data and a trajectory independent value target might be expected to give better results, bootstrapping along the trajectory (TD 5) was better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>For all experiments in this work we used a network architecture based on the one introduced by MuZero (Schrittwieser et al., 2020), but updated to use use ResNet v2 style preactivation residual blocks <ref type="bibr" target="#b11">(He et al., 2016)</ref> coupled with Layer Normalisation <ref type="bibr" target="#b3">(Ba et al., 2016)</ref>.</p><p>Both the representation function and the dynamics function were implemented by a ResNet with 10 blocks, each block containing 2 layers. For image inputs each layer was convolutional with a kernel size of 3x3 and 256 planes; for state based inputs each layer was fully-connected with a hidden size of 512.</p><p>To implement the network, we used the modules provided by the Haiku neural network library <ref type="bibr" target="#b13">(Hennigan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Policy Representation</head><p>For domains with discrete action spaces, i.e. Atari, we used the same categorical policy representation as MuZero, trained by minimising the KL-divergence.</p><p>In continuous action spaces, we used the Gaussian policy representation used for the data generation policies in RL Unplugged. We did not observe any benefit from using a Gaussian mixture, so instead in all our experiments we used a single Gaussian with diagonal covariance. We trained the Gaussian policy by maximising the log-likelihood of the training target: for behaviour cloning the log-likelihood of the action from the trajectory; for Reanalyse the loglikelihood of each searched action sample, weighted by its normalized visit count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Value Learning</head><p>We follow the approach described in previous work for value learning as well.</p><p>In Atari, we followed the MuZero training and use Temporal Difference (TD) learning with a TD step size of 5 towards a target network that is updated every 100 training steps by copying the weights of the network being trained <ref type="bibr" target="#b31">(Schrittwieser et al., 2020)</ref>.</p><p>For DM Control, we followed <ref type="bibr" target="#b16">(Hubert et al., 2021)</ref> and directly regressed the value prediction for a state against the MCTS value estimate for that state. This allows the value to be learned independently from the trajectory and is helpful to prevent overfitting, useful when only very little training data is available. When more training data is available, TD 5 training with a target network often led to better results <ref type="table">(Table 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>All experiments used the Adam optimiser <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> with decoupled weight decay <ref type="bibr" target="#b24">(Loshchilov &amp; Hutter, 2017)</ref> for training. We used a weight decay scale of 10 −4 and an initial learning rate of 10 −4 , decayed to 0 over 1 million training batches using a cosine schedule: lr = lr init 1 + cos π step max steps 2 where lr init = 10 −4 and max steps = 10 6 .</p><p>The batch size was 1024 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation</head><p>Unless otherwise noted, all results were obtained by evaluating the final network checkpoint, at the end of training for 1 million mini-batches. Results are reported as the mean for 300 evaluation episodes.</p><p>In domains with continuous action spaces, we followed previous work  in reducing the scale of the Gaussian policy close to 0 to obtain the performance of the policy mode. However, since setting the scale to 0 would not allow us to sample a set of different actions for MCTS to consider, we instead use a softer approach: scale eval = min(scale predicted , 0.05). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Other Hyperparameters</head><p>Our hyperparameters follow previous work <ref type="bibr" target="#b31">(Schrittwieser et al., 2020;</ref><ref type="bibr" target="#b16">Hubert et al., 2021)</ref>, but we reproduce them here for convenience.</p><p>We used a discount of 0.997 for Atari and 0.99 for DM Control experiments.</p><p>For replay, we kept a buffer of the most recent 50000 subsequences in Atari and 2000 in DM Control, splitting episodes into subsequences of length up to 500. Samples were drawn from the replay buffer according to prioritised replay , with priority P (i) = p α i k p α k , where p i = |ν i −z i |, ν is the search value and z the observed n-step return. To correct for sampling bias introduced by the prioritised sampling, we scaled the loss using the importance sampling ratio w i = ( 1 N · 1 P (i) ) β . In all our experiments, we set α = β = 1.  <ref type="table">Table 13</ref>. Evaluation of MuZero in RL Unplugged for individual games. Best result for each game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>yse loss. These ablations allow us to separately measure the contribution of MCTS at training time (rightmost column) and evaluation time (bottom row), with the combination of MCTS at evaluation time and Reanalyse loss (bottom right cell) corresponding to MuZero Unplugged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Performance throughout training in RL Unplugged Control Suite. The x-axis shows thousands of training batches, the y-axis mean reward. Performance of MuZero Unplugged was better than Behaviour Cloning (BC) throughout training. For some tasks with a small dataset such as cartpole, walker or finger.turn hard, MuZero Unplugged performance peaked at the beginning of training and subsequently declined due to overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Reanalyse scaling in Atari. Mean and median human normalised scores over 57 Atari games, at different Reanalyse fractions. All other parameters are held constant. Varying the Reanalyse fraction alone is enough to learn efficiently at data budgets differing by orders of magnitude. Overall results for the RL Unplugged Atari benchmark. Mean and median normalised scores over the 46 Atari games from the RL Unplugged benchmark. a) Results for offline RL baseline algorithms. CRR results are for our own reimplementation, other results are from (Gulcehre et al., 2020). b) Results using the MuZero network architecture. Behaviour cloning (BC) with the MuZero network replicated the baseline BC results from a), confirming correct import of the dataset and evaluation settings. Critic Regularized Regression (CRR) (Wang et al., 2020) significantly improved performance of the policy. MuZero Unplugged training with Reanalyse loss and MCTS for action selection led to overall best performance.</figDesc><table><row><cell>Median</cell><cell cols="2">Mean # Frames</cell></row><row><cell cols="2">50.0% 1331.7% 4094.4%</cell><cell>2000M</cell></row><row><cell cols="2">95.0% 1006.4% 2856.2%</cell><cell>200M</cell></row><row><cell>99.5% 126.6%</cell><cell>450.6%</cell><cell>20M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>). We also considered different losses and network architectures: the leftmost three columns use variants of the MuZero learned model with 0 (no model at all), 1 or 5 steps of model unroll, all trained usingTable 4. Median score in RL Unplugged Atari: ablations of action selection and training loss. Median normalized scores over the 46 Atari games from RL Unplugged. Rows of the table correspond to different action selection methods: sampling according to the policy probabilities, selecting the action with the highest value or selecting according to MCTS visit counts. Columns correspond to different number of unroll steps of the MuZero learned model and different losses. The leftmost three columns use the action from the training data as a supervised policy target, the rightmost two columns use the CRR and the Reanalyse loss respectively. For the case of 0 unroll steps, an action-value</figDesc><table><row><cell>Loss</cell><cell></cell><cell>supervised</cell><cell></cell><cell cols="2">CRR Reanalyse</cell></row><row><cell>Unroll</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell cols="2">policy 60.6</cell><cell>61.4</cell><cell cols="2">54.0 155.6</cell><cell>203.2</cell></row><row><cell>value</cell><cell cols="4">92.2 105.0 159.2 153.0</cell><cell>239.9</cell></row><row><cell>MCTS</cell><cell>-</cell><cell cols="3">137.3 169.7 172.5</cell><cell>265.3</cell></row></table><note>the supervised behaviour cloning policy target and a 5- step TD value target based on a target network for the. The next column used CRR to train the policy. The last column used the the MCTS visit count distribution from the Reanal-head is used to predict action values, instead of the state-value predicted by the normal model. All columns use a 5-step TD bootstrap towards a target network as the value target. For all action selection methods, Reanalyse loss led to the best performance; for all losses, MCTS action selection also led to the best performance. Overall, the combination of MCTS action selection and Reanalyse loss -the MuZero Unplugged algorithm - led to the best results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results for DM Control benchmark from RL Unplugged. Mean final score on 9 DM Control tasks, as well as mean score across all tasks. First three columns indicate task, action dimensonality and dataset size, subsequent four columns reproduce baseline results from. Final columns show performance of Behaviour Cloning (BC) with the MuZero network and results for MuZero Unplugged. As the data sets for the DM Control tasks are very small and vary a hundredfold between tasks, to keep the number of model parameters per datapoint constant and prevent memorisation, we scaled the neural network according to channels = datapoints layers .</figDesc><table><row><cell>Task</cell><cell cols="2"># dims # episodes</cell><cell cols="4">BC D4PG BRAC RABM</cell><cell>BC Unplugged</cell></row><row><cell>cartpole.swingup</cell><cell>1</cell><cell cols="3">40 386.0 856.0</cell><cell>869.0</cell><cell cols="2">798.0 143.7</cell><cell>343.3</cell></row><row><cell>finger.turn hard</cell><cell>2</cell><cell cols="3">500 238.0 714.0</cell><cell>227.0</cell><cell cols="2">433.0 308.8</cell><cell>405.0</cell></row><row><cell>fish.swim</cell><cell>5</cell><cell cols="3">200 444.0 180.0</cell><cell>222.0</cell><cell cols="2">504.0 542.8</cell><cell>585.4</cell></row><row><cell>manipulator.insert ball</cell><cell>5</cell><cell cols="3">1500 385.0 154.0</cell><cell>55.6</cell><cell cols="2">409.0 412.7</cell><cell>557.0</cell></row><row><cell>manipulator.insert peg</cell><cell>5</cell><cell cols="2">1500 279.0</cell><cell>50.4</cell><cell>49.5</cell><cell cols="2">290.0 309.9</cell><cell>432.7</cell></row><row><cell>walker.stand</cell><cell>6</cell><cell cols="3">200 386.0 930.0</cell><cell>829.0</cell><cell cols="2">689.0 444.4</cell><cell>759.8</cell></row><row><cell>walker.walk</cell><cell>6</cell><cell cols="3">200 380.0 549.0</cell><cell>786.0</cell><cell cols="2">651.0 496.3</cell><cell>901.5</cell></row><row><cell>cheetah.run</cell><cell>6</cell><cell cols="3">300 408.0 308.0</cell><cell>539.0</cell><cell cols="2">304.0 592.9</cell><cell>798.9</cell></row><row><cell>humanoid.run</cell><cell>21</cell><cell cols="2">3000 382.0</cell><cell>1.7</cell><cell>9.6</cell><cell cols="2">303.0 408.5</cell><cell>633.4</cell></row><row><cell>mean</cell><cell></cell><cell></cell><cell cols="2">365.3 415.9</cell><cell>398.5</cell><cell cols="2">486.8 406.7</cell><cell>601.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparison of MuZero Unplugged and CRR. Results for CRR were reported by selecting the checkpoint with the highest mean reward from each training run. Since this does not follow the offline policy selection guidelines from RL Unplugged and is therefore not directly comparable to the baseline results, we compared to it separately. The same highest mean reward evaluation scheme as used in CRR was used for MuZero Unplugged results in this table as well. All other tables report results at the end of training.</figDesc><table><row><cell></cell><cell></cell><cell>MuZero</cell></row><row><cell>Task</cell><cell>CRR</cell><cell cols="2">BC Unplugged</cell></row><row><cell>cartpole.swingup</cell><cell cols="2">664.0 501.8</cell><cell>594.3</cell></row><row><cell>finger.turn hard</cell><cell cols="2">714.0 333.8</cell><cell>759.0</cell></row><row><cell>fish.swim</cell><cell cols="2">517.0 556.8</cell><cell>681.6</cell></row><row><cell cols="3">manipulator.insert ball 625.0 465.6</cell><cell>659.2</cell></row><row><cell cols="3">manipulator.insert peg 387.0 325.9</cell><cell>556.0</cell></row><row><cell>walker.stand</cell><cell cols="2">797.0 473.3</cell><cell>887.2</cell></row><row><cell>walker.walk</cell><cell cols="2">901.0 637.9</cell><cell>949.5</cell></row><row><cell>cheetah.run</cell><cell cols="2">577.0 765.3</cell><cell>869.9</cell></row><row><cell>humanoid.run</cell><cell cols="2">586.0 416.5</cell><cell>643.1</cell></row><row><cell>mean</cell><cell cols="2">640.9 497.4</cell><cell>733.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 9. Mean score in RL Unplugged Atari: ablations of action selection and training loss. AsTable 4, but showing mean normalized score instead of median normalized score.</figDesc><table><row><cell>Loss</cell><cell></cell><cell>supervised</cell><cell></cell><cell cols="2">CRR Reanalyse</cell></row><row><cell>Unroll</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>policy</cell><cell>50.7</cell><cell>49.6</cell><cell cols="2">46.9 271.2</cell><cell>433.0</cell></row><row><cell>value</cell><cell cols="4">143.2 151.2 359.4 346.7</cell><cell>549.7</cell></row><row><cell>MCTS</cell><cell>-</cell><cell cols="3">248.4 394.8 408.3</cell><cell>595.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 10. Impact of injecting the trajectory action when reanalysing. When reanalysing continuous action offline data or demonstrations from other agents, the learned policy is unlikely to sample the same actions as occur in the data, preventing the MCTS from considering those actions. This effect is especially pronounced in high dimensional tasks. Injecting the trajectory actions as one of the actions for MCTS to consider avoids this issue.Table 11. Varying network size in DM Control benchmark from RL Unplugged. In tasks with very small amounts of training data, networks with a large number of parameters overfit easily.</figDesc><table><row><cell>Game</cell><cell></cell><cell cols="3">Random</cell><cell>Human</cell><cell cols="2">MuZero no-op starts mean normalized</cell><cell cols="3">MuZero sticky actions mean normalized</cell><cell cols="3">MuZero Res2 Adam mean normalized</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">supervised</cell><cell></cell><cell>MuZero</cell></row><row><cell>Game</cell><cell>Random</cell><cell>Online DQN</cell><cell></cell><cell>BC</cell><cell>DQN</cell><cell>IQN</cell><cell>BCQ</cell><cell>REM</cell><cell>policy</cell><cell>CRR</cell><cell>max-v</cell><cell>MCTS</cell><cell>Unplugged</cell></row><row><cell>Task</cell><cell cols="5"># dims no inject inject</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fish.swim</cell><cell></cell><cell>5</cell><cell></cell><cell cols="2">79.7 585.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">manipulator.insert ball</cell><cell>5</cell><cell></cell><cell cols="2">21.2 557.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">manipulator.insert peg</cell><cell>5</cell><cell></cell><cell cols="2">90.1 432.7</cell><cell></cell><cell></cell><cell cols="2"># hidden size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>walker.stand</cell><cell>Task</cell><cell>6</cell><cell cols="4">735.6 759.8 # dims # episodes</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell></cell></row><row><cell>cheetah.run</cell><cell cols="3">6 cartpole.swingup</cell><cell cols="2">101.3 798.9 1</cell><cell cols="6">40 605.0 343.3 293.2 302.6 259.3</cell><cell></cell></row><row><cell>humanoid.run</cell><cell cols="2">21 fish.swim</cell><cell></cell><cell cols="2">3.3 633.4 5</cell><cell cols="6">200 511.9 566.0 579.1 614.7 458.2</cell><cell></cell></row><row><cell></cell><cell cols="2">walker.stand</cell><cell></cell><cell></cell><cell>6</cell><cell cols="6">200 820.2 889.4 868.4 782.6 676.1</cell><cell></cell></row><row><cell></cell><cell cols="2">walker.walk</cell><cell></cell><cell></cell><cell>6</cell><cell cols="6">200 922.8 954.0 920.6 919.9 904.5</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Other model-based algorithms can be used as improvement operators as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These results suggest that an n-step model can also be used as an auxiliary loss to improve the performance of otherwise modelfree algorithms. For value-based algorithms without an explicit policy prediction, the distribution used for action selection can be</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Caglar Gulcehre for providing very detailed feedback and helpful suggestions to improve the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An optimistic perspective on offline reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ribas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Model-based offline planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributed Distributional Deterministic Policy Gradients</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unplugged: Benchmarks for Offline Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paduraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2006.13888" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davidson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">POPO: Pessimistic Offline Policy Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiku</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<title level="m">Sonnet for JAX</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning and Planning in Complex Action Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MOReL : Model-Based Offline Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachims</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conservative Q-learning for Offline Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="321" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.05101" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5699</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deployment-efficient reinforcement learning via model-based offline optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning from images with latent space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11547</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Off-policy actor-critic with shared experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11583</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08396</idno>
		<title level="m">Keep doing what worked: Behavioral modelling priors for offline reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deepmind control suite</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<title level="m">Critic Regularized Regression. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11361</idno>
		<title level="m">Behavior regularized offline reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MOPO: Model-based Offline Policy Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">821</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>alien 227.75 7,127.80 84,300.22 1,218.4 % 56,834.58 745.37 7,413.8 % 42,742.00 8,183.0 % 33,292.22 6,364.4 % asterix 210.00 8,503.33 860,812.50 10,377.0 % 879,375.00 10,600.9 % 862,406.65 10,396.3 % asteroids 719.10 47,388.67 265,336.41 567.0 % 374,146.38 800.2 % 476,412.00 1,019.3 % atlantis 12,850.00 29,028.13 1,055,658.62 6,445.8 % 1,353,616.62 8,287.5 % 1,137,475.12 926.53 125,085.20 753.0 % 201,154.00 1,212.3 % 333,077.44 2,008.8 % berzerk 123.65 2,630.42 5</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Human normalized score is calculated as snormalized = sagent−s random s human −s random . The original MuZero was trained without sticky actions and evaluated with 30 random no-op starts, we reproduce the results from (Schrittwieser et al., 2020) here for easy reference. Our version of MuZero was trained and evaluated with sticky actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Machado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>As shown in the table, this does not negatively impact performance, mean and median score are essentially unchanged. Finally, our version of MuZero with using Res v2 (He et al., 2016) and trained with the Adam optimiser shows clear improvements. alien 199.8 2766.8 2670.0 1690.0 2860.0 2090.0</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
