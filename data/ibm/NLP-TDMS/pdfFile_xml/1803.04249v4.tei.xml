<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SO-Net: Self-Organizing Network for Point Cloud Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M Chen</forename><surname>Gim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SO-Net: Self-Organizing Network for Point Cloud Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>After many years of intensive research, convolutional neural networks (ConvNets) is now the foundation for many state-of-the-art computer vision algorithms, e.g. image recognition, object classification and semantic segmentation etc. Despite the great success of ConvNets for 2D images, the use of deep learning on 3D data remains a challenging problem. Although 3D convolution networks (3D ConvNets) can be applied to 3D data that is rasterized into voxel representations, most computations are redundant because of the sparsity of most 3D data. Additionally, the performance of naive 3D ConvNets is largely limited by the resolution loss and exponentially growing computational cost. Meanwhile, the accelerating development of depth sensors, and the huge demand from applications such as autonomous vehicles make it imperative to process 3D data efficiently. Recent availability of 3D datasets including ModelNet <ref type="bibr" target="#b36">[37]</ref>, ShapeNet <ref type="bibr" target="#b7">[8]</ref>, 2D-3D-S <ref type="bibr" target="#b1">[2]</ref> adds on to the popularity of research on 3D data. <ref type="bibr">Figure 1</ref>. Our SO-Net applies hierarchical feature aggregation using SOM. Point clouds are converted into SOM node features and a global feature vector that can be applied to classification, autoencoder reconstruction, part segmentation and shape retrieval etc.</p><p>To avoid the shortcomings of naive voxelization, one option is to explicitly exploit the sparsity of the voxel grids <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>. Although the sparse design allows higher grid resolution, the induced complexity and limitations make it difficult to realize large scale or flexible deep networks <ref type="bibr" target="#b29">[30]</ref>. Another option is to utilize scalable indexing structures including kd-tree <ref type="bibr" target="#b3">[4]</ref>, octree <ref type="bibr" target="#b24">[25]</ref>. Deep networks based on these structures have shown encouraging results. Compared to tree based structures, point cloud representation is mathematically more concise and straight-forward because each point is simply represented by a 3-vector. Additionally, point clouds can be easily acquired with popular sensors such as the RGB-D cameras, LiDAR, or conventional cameras with the help of the Structure-from-Motion (SfM) algorithm. Despite the widespread usage and easy acquisition of point clouds, recognition tasks with point clouds still remain challenging. Traditional deep learning methods such as ConvNets are not applicable because point clouds are spatially irregular, and can be permutated arbitrarily. Due to these difficulties, few attempts has been made to apply deep learning techniques directly to point clouds until the very recent PointNet <ref type="bibr" target="#b25">[26]</ref>.</p><p>Despite being a pioneer in applying deep learning to point clouds, PointNet is unable to handle local feature extraction adequately. PointNet++ <ref type="bibr" target="#b27">[28]</ref> is later proposed to address this problem by building a pyramid-like feature aggregation scheme, but the point sampling and grouping strategy in <ref type="bibr" target="#b27">[28]</ref> does not reveal the spatial distribution of the input point cloud. Kd-Net <ref type="bibr" target="#b17">[18]</ref> build a kd-tree for the input point cloud, followed by hierarchical feature extractions from the leaves to root. Kd-Net explicitly utilizes the spatial distri-bution of point clouds, but there are limitations such as the lack of overlapped receptive fields.</p><p>In this paper, we propose the SO-Net to address the problems in existing point cloud based networks. Specifically, a SOM <ref type="bibr" target="#b18">[19]</ref> is built to model the spatial distribution of the input point cloud, which enables hierarchical feature extraction on both individual points and SOM nodes. Ultimately, the input point cloud can be compressed into a single feature vector. During the feature aggregation process, the receptive field overlap is controlled by performing point-to-node k-nearest neighbor (kNN) search on the SOM. The SO-Net theoretically guarantees invariance to the order of input points, by the network design and our permutation invariant SOM training. Applications of our SO-Net include point cloud based classification, autoencoder reconstruction, part segmentation and shape retrieval, as shown in <ref type="figure">Fig. 1</ref>.</p><p>The key contributions of this paper are as follows:</p><p>• We design a permutation invariant network -the SO-Net that explicitly utilizes the spatial distribution of point clouds.</p><p>• With point-to-node kNN search on SOM, hierarchical feature extraction is performed with systematically adjustable receptive field overlap.</p><p>• We propose a point cloud autoencoder as pre-training to improve network performance in various tasks.</p><p>• Compared with state-of-the-art approaches, similar or better performance is achieved in various applications with significantly faster training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>It is intuitive to represent 3D shapes with voxel grids because they are compatible with 3D ConvNets. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> use binary variable to indicate whether a voxel is occupied or free. Several enhancements are proposed in <ref type="bibr" target="#b26">[27]</ref> -overfitting is mitigated by predicting labels from partial subvolumes, orientation pooling layer is designed to fuse shapes with various orientations, and anisotropic probing kernels are used to project 3D shapes into 2D features. Brock et al. <ref type="bibr" target="#b5">[6]</ref> propose to combine voxel based variational autoencoders with object recognition networks. Despite its simplicity, voxelization is able to achieve state-of-the-art performance. Unfortunately, it suffers from loss of resolution and the exponentially growing computational cost. Sparse methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref> are proposed to improve the efficiency. However, these methods still rely on uniform voxel grids and experience various limitations such as the lack of parallelization capacity <ref type="bibr" target="#b20">[21]</ref>. Spectral ConvNets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> are explored to work on non-Euclidean geometries, but they are mostly limited to manifold meshes.</p><p>Rendering 3D data into multi-view 2D images turns the 3D problem into a 2D problem that can be solved using standard 2D ConvNets. View-pooling layer <ref type="bibr" target="#b32">[33]</ref> is designed to aggregate features from multiple rendered images. Qi et al. <ref type="bibr" target="#b26">[27]</ref> substitute traditional 3D to 2D rendering with multi-resolution sphere rendering. Wang et al. <ref type="bibr" target="#b33">[34]</ref> further propose the dominant set pooling and utilize features like color and surface normal. Despite the improved efficiency compared to 3D ConvNets, multi-view strategy still suffers from information loss <ref type="bibr" target="#b17">[18]</ref> and it cannot be easily extended to tasks like per-point labeling.</p><p>Indexing techniques such as kd-tree and octree are scalable compared to uniform grids, and their regular structures are suitable for deep learning techniques. To enable convolution and pooling operations over octree, Riegler et al. <ref type="bibr" target="#b29">[30]</ref> build a hybrid grid-octree structure by placing several small octrees into a regular grid. With bit string representation, a single voxel in the hybrid structure is fully determined by its bit index. As a result, simple arithmetic can be used to visit the parent or child nodes. Similarly, Wang et al. <ref type="bibr" target="#b35">[36]</ref> introduce a label buffer to find correspondence of octants at various depths. Klokov et al. propose the Kd-Net <ref type="bibr" target="#b17">[18]</ref> that computes vectorial representations for each node of the prebuilt balanced kd-tree. A parent feature vector is computed by applying non-linearity and affine transformation on its two child feature vectors, following the bottom-up fashion.</p><p>PointNet <ref type="bibr" target="#b25">[26]</ref> is the pioneer in the direct use of point clouds. It uses the channel-wise max pooling to aggregate per-point features into a global descriptor vector. PointNet is invariant to order permutation of input points because the per-point feature extraction is identical for every point and max pooling operation is permutation invariant. A similar permutation equivariant layer <ref type="bibr" target="#b28">[29]</ref> is also proposed at almost the same time as <ref type="bibr" target="#b25">[26]</ref>, with the major difference that the permutation equivariant layer is max-normalized. Although the max-pooling idea is proven to be effective, it suffers from the lack of ConvNet-like hierarchical feature aggregation. PointNet++ <ref type="bibr" target="#b27">[28]</ref> is later designed to group points into several groups in different levels, so that features from multiple scales could be extracted hierarchically.</p><p>Unlike networks based on octree or kd-tree, the spatial distribution of points is not explicitly modeled in Point-Net++. Instead, heuristic grouping and sampling schemes, e.g. multi-scale and multi-resolution grouping, are designed to combine features from multiple scales. In this paper, we propose our SO-Net that explicitly models the spatial distribution of input point cloud during hierarchical feature extraction. In addition, adjustable receptive field overlap leads to more effective local feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Organizing Network</head><p>The input to the network is a point set P = {p i ∈ R 3 , i = 0, · · · , N − 1}, which will be processed into M </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Permutation Invariant SOM</head><p>SOM is used to produce low-dimensional, in this case two-dimensional, representation of the input point cloud. We construct a SOM with the size of m × m, where m ∈ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>, i.e. the total number of nodes M ranges from 25 to 121. SOM is trained with unsupervised competitive learning instead of the commonly used backpropagation in deep networks. However, naive SOM training schemes are not permutation invariant for two reasons: the training result is highly related to the initial nodes, and the per-sample update rule depends on the order of the input points.</p><p>The first problem is solved by assigning fixed initial nodes for any given SOM configuration. Because the input point cloud is normalized to be within [−1, 1] in all three axes, we generate a proper initial guess by dispersing the nodes uniformly inside a unit ball, as shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>. Simple approaches such as the potential field can be used to construct such a uniform initial guess. To solve the second problem, instead of updating nodes once per point, we perform one update after accumulating the effects of all the points. This batch update process is deterministic <ref type="bibr" target="#b18">[19]</ref> for a given point cloud, making it permutation invariant. Another advantage of batch update is the fact that it can be implemented as matrix operations, which are highly efficient on GPU. Details of the initialization and batch training algorithms can be found in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, SOM is a guide for hierarchical feature extraction, and a tool to systematically adjust the receptive field overlap. Given the output of the SOM, we search for the k nearest neighbors (kNN) on the SOM nodes S for each point p i , i.e., point-to-node kNN search:</p><formula xml:id="formula_0">s ik = kNN(p i | s j , j = 0, · · · , M − 1).<label>(1)</label></formula><p>Each p i is then normalized into k points by subtraction with its associated nodes:</p><formula xml:id="formula_1">p ik = p i − s ik .<label>(2)</label></formula><p>The resulting kN normalized points are forwarded into a series of fully connected layers to extract individual point features. There is a shared fully connected layer on each level l, where φ is the non-linear activation function. The output of level l is given by</p><formula xml:id="formula_2">p l+1 ik = φ(W l p l ik + b l ).<label>(3)</label></formula><p>The input to the first layer p 0 ik can simply be the normalized point coordinates p ik , or the combination of coordinates and other features like surface normal vectors.</p><p>Node feature extraction begins with max-pooling the kN point features into M node features following the above kNN association. We apply a channel-wise max pooling operation to get the node feature s 0 j for those point features associated with the same node s j :</p><formula xml:id="formula_3">s 0 j = max({p l ik , ∀s ik = s j }).<label>(4)</label></formula><p>Since each point is normalized into k coordinates according to the point-to-node kNN search, it is guaranteed that the receptive fields of the M max pooling operations are overlapped. Specifically, M nodes cover kN normalized points. k is an adjustable parameter to control the overlap. Each node feature produced by the above max pooling operation is further concatenated with the associated SOM node. The M augmented node features are forwarded into a series of shared layers, and then aggregated into a feature vector that represents the input point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature aggregation as point cloud separation and assembly</head><p>There is an intuitive reason behind the SOM feature extraction and node concatenation. Since the input points to the first layer are normalized with M SOM nodes, they are actually separated into M mini point clouds as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Each mini point cloud contains a small number of points in a coordinate whose origin is the associated SOM node. For a point cloud of size 2048, and M = 64 and k = 3, a typical mini point cloud may consist of around 90 points inside a small space of x, y, z ∈ [−0.3, 0.3]. The number and coverage of points in a mini point cloud are determined by the SOM training and kNN search, i.e. M and k.</p><p>The first batch of fully connected layers can be regarded as a small PointNet that encodes these mini point clouds. The concatenation with SOM nodes plays the role of assembling these mini point clouds back into the original point cloud. Because the SOM explicitly reveals the spatial distribution of the input point cloud, our separate-and-assemble process is more efficient than the grouping strategy used in PointNet++ <ref type="bibr" target="#b27">[28]</ref>, as shown in Sec. 4.</p><p>Permutation Invariance There are two levels of feature aggregation in SO-Net, from point features to node features, and from node features to global feature vector. The first phase applies a shared PointNet to M mini point clouds. The generation of these M mini point clouds is irrelevant to the order of input points, because the SOM training in Sec. 3.1 and kNN search in <ref type="figure" target="#fig_1">Fig. 3</ref> are deterministic. Point-Net <ref type="bibr" target="#b25">[26]</ref> is permutation invariant as well. Consequently, both the node features and global feature vector are theoretically guaranteed to be permutation invariant.</p><p>Effect of suboptimal SOM training It is possible that the training of SOM converges into a local minima with isolated nodes outside the coverage of the input point cloud. In some situations no point will be associated with the isolated nodes during the point-to-node kNN search, and we set the corresponding node features to zero. This phenomenon is quite common because the initial nodes are dispersed uniformly in a unit ball, while the input point cloud may occupy only a small corner. Despite the existence of suboptimal SOM, the proposed SO-Net still out-performs state-of-the-art approaches in applications like object classification. The ef-fect of invalid node features is further investigated in Sec. 4 by inserting noise into the SOM results.</p><p>Exploration with ConvNets It is interesting to note that the node feature extraction has generated an image-like feature matrix, which is invariant to the order of input points. It is possible to apply standard ConvNets to further fuse the node features with increasing receptive field. However, the classification accuracy decreased slightly in our experiments, where we replaced the second batch of fully connected layers with 2D convolutions and pooling. It remains as a promising direction to investigate the reason and solution to this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extension to Segmentation</head><p>The extension to per-point annotations, e.g. segmentation, requires the integration of both local and global features. The integration process is similar to the invert operation of the encoder in Sec. 3.2. The global feature vector can be directly expanded and concatenated with the kN normalized points. The M node features are attached to the points that are associated with them during the encoding process. The integration results in kN features that combine point, node and global features, which are then forwarded into a chain of shared fully connected layers.</p><p>The kN features are actually redundant to generate N per-point classification scores because of the receptive field overlap. Average or max pooling are methods to fuse the redundant information. Additionally, similar to many deep <ref type="figure">Figure 4</ref>. The architecture of the decoder that takes 5000 points and reconstructs 4608 points. The up-convolution branch is designed to recover the main body of the input, while the more flexible fully connected branch is to recover the details. The "upconv" module consists of a nearest neighbor upsampling layer and a 3 × 3 convolution layer. The "conv2pc" module consists of two 1 × 1 convolution layers. networks, early, middle or late fusion may exhibit different performance <ref type="bibr" target="#b8">[9]</ref>. With a series of experiments, we found that middle fusion with average pooling is most effective compared to other fusion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Autoencoder</head><p>In this section, we design a decoder network to recover the input point cloud from the encoded global feature vector. A straightforward design is to stack series of fully connected layers on top of the feature vector, and generate an output vector of length 3N , which can be reshaped intô N × 3. However, the memory and computation footprint will be too heavy ifN is sufficiently large.</p><p>Instead of generating point clouds with fully connected layers <ref type="bibr" target="#b0">[1]</ref>, we design a network with two parallel branches similar with <ref type="bibr" target="#b12">[13]</ref>, i.e, a fully connected branch and a convolution branch as shown in <ref type="figure">Fig. 4</ref>. The fully connected branch predictsN 1 points by reshaping an output of 3N 1 elements. This branch enjoys high flexibility because each coordinate is predicted independently. On the other hand, the convolution branch predicts a feature matrix with the size of 3 × H × W , i.e.N 2 = H × W points. Due to the spatial continuity of convolution layers, the predictedN 2 point may exhibit more geometric consistency. Another advantage of the convolution branch is that it requires much less parameters compared to the fully connected branch.</p><p>Similar to common practice in many depth estimation networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, the convolution branch is designed as an up-convolution (upconv) chain in a pyramid style. Instead of deconvolution layers, each upconv module consists of a nearest neighbor upsampling layer and a 3 × 3 convolution layer. According to our experiments, this design is much more effective than deconvolution layers in the case of point cloud autoencoder. In addition, intermediate upconv products are converted to coarse reconstructed point clouds and compared with the input. The conversion from upconv products to point clouds is a 2-layer 1 × 1 convolution stack in order to give more flexibility to each recovered point. The coarse-to-fine strategy produces another boost in the reconstruction performance.</p><p>To supervise the reconstruction process, the loss function should be differentiable, ready for parallel computation and robust against outliers <ref type="bibr" target="#b12">[13]</ref>. Here we use the Chamfer loss:</p><formula xml:id="formula_4">d(P s , P t ) = 1 |P s | x∈Ps min y∈Pt x − y 2 + 1 |P t | y∈Pt min x∈Ps x − y 2 .<label>(5)</label></formula><p>where P s and P t ∈ R 3 represents the input and recovered point cloud respectively. The numbers of points in P s and P t are not necessarily the same. Intuitively, for each point in P s , Eq. (5) computes its distance to the nearest neighbor in P t , and vice versa for points in P t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, the performance of our SO-Net is evaluated in three different applications, namely point cloud autoencoder, object classification and object part segmentation. In particular, the encoder trained in the autoencoder can be used as pre-training for the other two tasks. The encoder structure and SOM configuration remain identical among all experiments without delicate finetuning, except for the 2D MNIST classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Detail</head><p>Our network is implemented with PyTorch on a NVIDIA GTX1080Ti. We choose a SOM of size 8 × 8 and k = 3 in most experiments. We optimize the networks using Adam <ref type="bibr" target="#b16">[17]</ref> with an initial learning rate of 0.001 and batch size of 8. For experiments with 5000 or more points as input, the learning rate is decreased by half every 20 epochs, otherwise the learning rate decay is executed every 40 epochs. Generally the networks converge after around 5 times of learning rate decay. Batch-normalization and ReLU activation are applied to every layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>As a 2D toy example, we adopt the MNIST dataset <ref type="bibr" target="#b19">[20]</ref> in Sec. 4.4. For each digit, 512 two-dimensional points are sampled from the non-zero pixels to serve as our input.</p><p>Two variants of the ModelNet <ref type="bibr" target="#b36">[37]</ref>, i.e. ModelNet10 and ModelNet40, are used as the benchmarks for the autoencoder task in Sec. 4.3 and the classification task in Sec. 4.4.</p><p>The ModelNet40 contains 13,834 objects from 40 categories, among which 9,843 objects belong to training set and the other 3,991 samples for testing. Similarly, the Mod-elNet10 is split into 2,468 training samples and 909 testing samples. The original ModelNet provides CAD models represented by vertices and faces. Point clouds are generated by sampling from the models uniformly. For fair comparison, we use the prepared ModelNet10/40 dataset from <ref type="bibr" target="#b27">[28]</ref>, where each model is represented by 10,000 points. Various sizes of point clouds, e.g., 2,048 or 5,000, can be sampled from the 10k points in different experiments.</p><p>Object part segmentation is demonstrated with the ShapeNetPart dataset <ref type="bibr" target="#b37">[38]</ref>. It contains 16,881 objects from 16 categories, represented as point clouds. Each object consists of 2 to 6 parts, and in total there are 50 parts in the dataset. We sample fixed size point clouds, e.g. 1,024, in our experiments.  In this section, we demonstrate that a point cloud can be reconstructed from the SO-Net encoded feature vector, e.g. a vector with length of 1024. The nearest neighbor search in Chamfer distance (Eq. 5) is conducted with Facebook's faiss <ref type="bibr" target="#b15">[16]</ref>. There are two configurations for the decoder to reconstruct different sizes of point clouds. The first configuration generates 64 × 64 points from the convolution branch and 512 points from the fully connected branch. The other one produces 32 × 32 and 256 points respectively, by removing the last upconv module of <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Point Cloud Autoencoder</head><p>It is difficult to provide quantitative comparison for the point cloud autoencoder task because little research has been done on this topic. The most related work is the point set generation network <ref type="bibr" target="#b12">[13]</ref> and the point cloud generative models <ref type="bibr" target="#b0">[1]</ref>. Examples of our reconstructed ShapeNetPart point clouds are visualized in <ref type="figure" target="#fig_3">Fig. 5</ref>, where 1024 points recovered from the convolution branch are denoted in red and the other 256 points in green. The overall testing Chamfer distance (Eq. 5) is 0.033. Similar to the results in <ref type="bibr" target="#b12">[13]</ref>, the convolution branch recovers the main body of the object, while the more flexible fully connected branch focuses on details such as the legs of a table. Nevertheless, many finer details are lost. For example, the reconstructed earphone is blurry. This is probably because the encoder is still not powerful enough to capture fine-grained structures.</p><p>Despite the imperfect reconstruction, the autoencoder enhances SO-Net's performance in other tasks by providing a pre-trained encoder, illustrated in Sec. 4.4 and 4.5. More results are visualized in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification Tasks</head><p>To classify the point clouds, we attach a 3-layer multilayer perceptron (MLP) on top of the encoded global feature vector. Random dropout is applied to the last two layers with keep-ratio of 0.4. <ref type="table" target="#tab_0">Table 1</ref> illustrates the classification accuracy for state-of-the-art methods using scalable 3D representations, such as point cloud, kd-tree and octree. In MNIST dataset, our network achieves a relative 13.7% error rate reduction compared with PointNet++. In ModelNet10 and ModelNet40, our approach out-performs state-of-theart methods by 1.7% and 1.5% respectively in terms of instance accuracy. Our SO-Net even out-performs single networks using multi-view images or uniform voxel grids as input, like qi-MVCNN <ref type="bibr" target="#b26">[27]</ref> (ModelNet40 at 92.0%) and VRN <ref type="bibr" target="#b5">[6]</ref> (ModelNet40 at 91.3%). Methods that integrate multiple networks, i.e., qi-MVCNN-MultiRes <ref type="bibr" target="#b26">[27]</ref> and VRN Ensemble <ref type="bibr" target="#b5">[6]</ref>, are still better than SO-Net in ModelNet classification, but their multi-view / voxel grid representations are far less scalable and flexible than our point cloud representation, as illustrated in Sec. 1 and 2.</p><p>Effect of pre-training The performance of the network can be improved with pre-training using the autoencoder in Sec. 3.4. The autoencoder is trained with ModelNet40, using 5000 points and surface normal vectors as input. The autoencoder brings a boost of 0.5% in ModelNet10 classification, but only 0.2% in ModelNet40 classification. This is not surprising because pre-training with a much larger   dataset may lead to convergence basins <ref type="bibr" target="#b11">[12]</ref> that are more resistant to over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to point corruption</head><p>We train our network with point clouds of size 2048 but test it with point dropout. As shown in <ref type="figure" target="#fig_4">Fig. 6(a)</ref>, our accuracy drops by 1.7% with 50% points missing (2048 to 1024), and 14.2% with 75% points missing (2048 to 512). As a comparison, the accuracy of PN drops by 3.8% with 50% points (1024 to 512).</p><p>Robustness to SOM corruption One of our major concern when designing the SO-Net is whether the SO-Net relies too much on the SOM. With results shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we demonstrate that our SO-Net is quite robust to the noise or corruption of the SOM results. In <ref type="figure" target="#fig_4">Fig. 6(b)</ref>, we train a network with SOM of size 8 × 8 as the noise-free version, but test the network with SOM sizes varying from 5 × 5 to 11×11. It is interesting that the performance decay is much slower if the SOM size is larger than training configuration, which is consistent with the theory in Sec. 3.2. The SO-Net separates the input point cloud into M mini point clouds, encodes them into M node features with a mini PointNet, and assembles them during the global feature extraction. In the case that the SOM becomes smaller during testing, the mini point clouds are too large for the mini PointNet to encode. Therefore the network performs worse when the testing SOM is smaller than expected. In <ref type="figure" target="#fig_4">Fig. 6(c)</ref>, we add Gaussian noise N (0, σ) onto the SOM during testing. Given the fact that input points have been normalized into a unit cube, a Gaussian noise with σ = 0.2 is rather considerable, as shown in <ref type="figure" target="#fig_4">Fig. 6(d)</ref>. Even in that difficult case, our network achieves the accuracy of 91.1% in ModelNet40 and 94.6% in ModelNet10.</p><p>Effect of hierarchical layer number Our framework shown in <ref type="figure" target="#fig_1">Fig. 3</ref> can be made to further out-perform state-ofthe-art methods by simply adding more layers. The vanilla SO-Net is a 2-layer structure "grouping&amp;PN(PointNet) -PN", where the grouping is based on SOM and point-tonode kNN. We make it a 3-layer structure by simply repeat- ing the SOM/kNN based "grouping&amp;PN" with this protocol: for each SOM node, find k = 9 nearest nodes and process the k node features with a PointNet. The output is a new SOM feature map of the same size but larger receptive field. Shown in <ref type="table" target="#tab_0">Table 1</ref>, our 3-layer SO-Net increases the accuracy to 1.5% higher (relatively 19% lower error rate) than PN++ on ModelNet40, and 1.7% higher (relatively 28% lower error rate) than Kd-Net on ModelNet10. The effect of hierarchical layer number is illustrated in <ref type="figure" target="#fig_5">Fig. 7</ref>, where too many layers may lead to over-fitting.</p><p>Training speed The batch training of SOM allows parallel implementation on GPU. Moreover, the training of SOM is completely deterministic in our approach, so it can be isolated as data preprocessing before network optimization. Compared to the randomized kd-tree construction in <ref type="bibr" target="#b17">[18]</ref>, our deterministic design provides great boosting during training. In addition to the decoupled SOM, the hierarchical feature aggregation based on SOM can be implemented efficiently on GPU. As shown in <ref type="table" target="#tab_0">Table 1</ref>, it takes about 3 hours to train our best network on ModelNet40 with a GTX1080Ti, which is significantly faster than state-ofthe-art networks that can provide comparable performance. We formulate the object part segmentation problem as a per-point classification task, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. The net-works are evaluated using the mean Intersection over Union (IoU) protocol proposed in <ref type="bibr" target="#b25">[26]</ref>. For each instance, IoU is computed for each part that belongs to that object category. The mean of the part IoUs is regarded as the IoU for that instance. Overall IoU is calculated as the mean of IoUs over all instances, and category-wise IoU is computed as an average over instances under that category. Similar with O-CNN <ref type="bibr" target="#b35">[36]</ref> and PointNet++ <ref type="bibr" target="#b27">[28]</ref>, surface normal vectors are fed into the network together with point coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Part Segmentation on ShapeNetPart</head><p>By optimizing per-point softmax loss functions, we achieve competitive results as reported in <ref type="table" target="#tab_1">Table 2</ref>. Although O-CNN reports the best IoU, it adopts an additional dense conditional random field (CRF) to refine the output of their network while others do not contain this post-processing step. Some segmentation results are visualized in <ref type="figure" target="#fig_6">Fig. 8</ref> and we further visualize one instance per category in the supplementary material. Although in some hard cases our network may fail to annotate the fine-grained details correctly, generally our segmentation results are visually satisfying. The low computation cost remains as one of our advantages. Additionally, pre-training with our autoencoder produces a performance boost, which is consistent with our classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the novel SO-Net that performs hierarchical feature extraction for point clouds by explicitly modeling the spatial distribution of input points and systematically adjusting the receptive field overlap. In a series of experiments including point cloud reconstruction, object classification and object part segmentation, our network achieves competitive performance. In particular, we outperform state-of-the-art deep learning approaches in point cloud classification and shape retrieval, with significantly faster training speed. As the SOM preserves the topological properties of the input space and our SO-Net converts point clouds into feature matrice accordingly, one promising future direction is to apply classical ConvNets or graph-based ConvNets to realize deeper hierarchical feature aggregation.</p><p>Our object classification network can be easily extended to the task of 3D shape retrieval by regarding the classification score as the feature vector. Given a query shape and a shape library, the similarity between the query and the candidates can be computed as their feature vector distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Dataset</head><p>We perform 3D shape retrieval task using the ShapeNet Core55 dataset, which contains 51,190 shapes from 55 categories and 204 subcategories. Specifically, we adopt the dataset split provided by the 3D Shape Retrieval Contest 2016 (SHREC16), where 70% of the models are used for training, 10% for validation and 20% for testing. Since the 3D shapes are represented by CAD models, i.e., vertices and faces, we sample 5,000 points and surface normal vectors from each CAD model. Data augmentation is identical with the previous classification and segmentation experimentsrandom jitter and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Procedures</head><p>We train a classification network on the ShapeNet Core55 dataset using identical configurations as our Model-Net40 classification experiment, i.e. a SOM of size 8×8 and k = 3. For simplicity, the softmax loss is minimized with only the category labels (without any subcategory information). The classification score vector of length 55 is used as the feature vector. We calculate the L2 feature distance between each shape in the test set and all shapes in the same predicted category from the test set (including itself). The corresponding retrieval list is constructed by sorting these shapes according to the feature distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Performance</head><p>SHREC16 provides several evaluation metrics including Precision-Recall curve, F-score, mean average precision (mAP), normalized discounted cumulative gain (NDCG). These metrics are computed under two contexts -macro and micro. Macro metric is a simple average across all categories while micro metric is a weighted average according to the number of shapes in each category. As shown in <ref type="table">Table 3</ref>, our SO-Net out-performs state-of-the-art approaches with most metrics. The precision-recall curves are illustrated in <ref type="figure">Fig. 9</ref>, where SO-Net demonstrates the largest area under curve (AUC). Some shape retrieval results are visualized in <ref type="figure" target="#fig_9">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time and Space Complexity</head><p>We evaluate the model size, forward (inference) time and training time of several point cloud based networks in the task of ModelNet40 classification, as shown in <ref type="table" target="#tab_5">Table 4</ref>. The forward timings are acquired with a batch size of 8 and input point cloud size of 1024. In the comparison, we choose the networks with the best classification accuracy among various configurations of PointNet and PointNet++, i.e., Point-Net with transformations and PointNet++ with multi-scale grouping (MSG). Because of the parallelizability and simplicity of our network design, our model size is smaller and the training speed is significantly faster compared to Point-Net and its successor PointNet++. Meanwhile, our inference time is around 1/3 of that of PointNet++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Permutation Invariant SOM</head><p>We apply two methods to ensure that the SOM is invariant to the permutation of the input points -fixed initialization and deterministic training rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Initialization for SOM Training</head><p>In addition to permutation invariance, the initialization should be reasonable so that the SOM training is less prone to local minima. Suboptimal SOM may lead to many isolated nodes outside the coverage of the input points. For simplicity, we use fixed initialization for any point cloud input although there are other initialization approaches that are permutation invariant, e.g., principal component initialization. We generate a set of node coordinates that are uniformly distributed in an unit ball to serve as a reasonable initialization because the input point clouds are in various shapes. Unfortunately, as shown in <ref type="figure" target="#fig_7">Fig. 10</ref>, isolated nodes are inevitable even with uniform initialization. Isolated nodes may not be associated during the kNN search, and their corresponding node features will be set to zero, i.e. the node features are invalid. Nevertheless, our SO-Net is robust to small amount of invalid nodes as demonstrated in the experiments.</p><p>We propose a simple algorithm based on potential field methods to generate the initialization as shown in Algorithm 1. S = {s j ∈ R 3 , j = 0, · · · , M − 1} represents the SOM nodes and η is the learning rate. The key idea is to apply a repulsion force between any pair of nodes, and external forces to attract nodes toward the origin. The parameter λ is used to control the weighting between the re-pulsion and attraction force, so that the resulting nodes are within the unit ball. </p><formula xml:id="formula_5">w xy (x, y|p, q, σ x , σ y ) = exp − 1 2 (x − µ) T Σ −1 (x − µ) (2π) 2 |Σ| µ = p q T Σ = σ 2 x 0 0 σ 2 y .<label>(6)</label></formula><p>The pseudo code of the training scheme is shown in Algorithm 2. P = {p i ∈ R 3 , i = 0, · · · , N − 1} and S = {s j ∈ R 3 , j = 0, · · · , M − 1} represent the input points and SOM nodes respectively. The learning rate η t and neighborhood parameter (σ x , σ y ) should be decreased slowly during training. In addition, Algorithm 2 can be easily implemented as matrix operations which are highly efficient on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. MNIST Classification</head><p>We evaluate our network using the 2D MNIST dataset, which contains 60,000 28 × 28 images for training and  <ref type="figure">Figure 9</ref>. Precision-recall curve for micro (a) and macro (b) metrics in the 3D shape retrieval task. In both curves, the SO-Net demonstrates the largest AUC.   10,000 images for testing. 2D coordinates are extracted from the non-zero pixels of the images. In order to upsample these 2D coordinates into point clouds of a certain size, e.g., 512 in our experiment, we augment the original pixel coordinates with Gaussian noise N (0, 0.01). Other than the acquisition of point clouds, the data augmentation is exactly the same as other experiments using ModelNet or ShapeNetPart. We reduce the SOM size to 4 × 4 and set k = 4 because the point clouds are in 2D and the cloud size is relatively small. The neurons in the shared fully connected layers are reduced as well: 2-64-64-128-128 during point feature extraction and (128+2)-256-512-512-1024 during node feature extraction. Similar to 3D classification tasks, our network outperforms existing point cloud based deep networks although the best performance is still from the well engineered 2D ConvNets as shown in <ref type="table">Table 6</ref>. Despite using point cloud representation instead of images, our network demonstrates better results compared with ConvNets such as Network in Network <ref type="bibr" target="#b21">[22]</ref>, LeNet5 <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Classification with SOM Only</head><p>There are two sources of information utilized by the SO-Net -the point cloud and trained SOM. The information from SOM is explicitly used when the nodes are concatenated with the node features at the beginning of node feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error rate (%) Multi-column DNN <ref type="bibr" target="#b9">[10]</ref> 0.23 Network in Network <ref type="bibr" target="#b21">[22]</ref> 0.47 LeNet5 <ref type="bibr" target="#b19">[20]</ref> 0.80 Multi-layer perceptron <ref type="bibr" target="#b30">[31]</ref> 1.60 PointNet <ref type="bibr" target="#b25">[26]</ref> 0.78 PointNet++ <ref type="bibr" target="#b27">[28]</ref> 0.51 Kd-Net <ref type="bibr" target="#b17">[18]</ref> 0.90 ECC <ref type="bibr" target="#b31">[32]</ref> 0.63 Ours 0.44 <ref type="table">Table 6</ref>. MNIST classification results.</p><p>extraction. Additionally, the SOM is implicitly utilized because point normalization, kNN search and the max pooling are based on the nodes. We perform classification using the SOM nodes without the point coordinates of the point cloud to analyze the contribution of the SOM. We feed the SOM nodes into a 3-layer MLP with MNIST, ModelNet10 and ModelNet40 dataset. Similarly in the Kd-Net <ref type="bibr" target="#b17">[18]</ref>, experiments are conducted using the kd-tree split directions without point information, i.e. feeding directions of the splits into a MLP. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>It is interesting that we can achieve reasonable performance in the classification tasks by combining SOM and a simple MLP. But there is still a large gap between this variant and the full SO-Net, which suggests that the integration of SOM and point clouds is important. Another intriguing phenomenon is that the SOM based MLP achieves better results than split-based MLP. It suggests that maybe SOM is more expressive than kd-trees in the context of classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Result Visualization</head><p>To visualize the shape retrieval results, we present the top 5 retrieval results for a few shapes as shown in <ref type="figure" target="#fig_9">Fig. 11</ref> For the point cloud autoencoder, we present results from two networks. The first network consumes 1024 points and reconstructs 1280 points with the ShapeNetPart dataset ( <ref type="figure" target="#fig_0">Fig. 12)</ref>, while the second one consumes 5000 points and reconstructs 4608 points using the ModelNet40 dataset ( <ref type="figure" target="#fig_1">Fig. 13</ref>). We present one instance for each category.</p><p>For results of object part segmentation using ShapeNet-Part dataset, we visualize one instance per category in     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) The initial nodes of an 8 × 8 SOM. For each SOM configuration, the initial nodes are fixed for every point cloud. (b) Example of a SOM training result. SOM nodes S = {s j ∈ R 3 , j = 0, · · · , M − 1} as shown in Sec. 3.1. Similarly, in the encoder described in Sec. 3.2, individual point features are max-pooled into M node features, which can be further aggregated into a global feature vector. Our SO-Net can be applied to various computer vision tasks including classification, per-point segmentation (Sec. 3.3), and point cloud reconstruction (Sec. 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of the SO-Net and its application to classification and segmentation. In the encoder, input points are normalized with the k-nearest SOM nodes. The normalized point features are later max-pooled into node features based on the point-to-node kNN search on SOM. k determines the receptive field overlap. In the segmentation network, M node features are concatenated with the kN normalized points following the same kNN association. Finally kN features are aggregated into N features by average pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Input point clouds are normalized to be zero-mean inside a unit cube. The following data augmentations are applied at training phase: (a) Gaussian noise N (0, 0.01) is added to the point coordinates and surface normal vectors (if applicable). (b) Gaussian noise N (0, 0.04) is added to the SOM nodes. (c) Point clouds, surface normal vectors (if applicable) and SOM nodes are scaled by a factor sampled from an uniform distribution U(0.8, 1.2). Further augmentation like random shift or rotation do not improve the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples of point cloud autoencoder results. First row: input point clouds of size 1024. Second row: reconstructed point clouds of size 1280. From left to right: chair, table, earphone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Robustness test on point or SOM corruption. (a) The network is trained with point clouds of size 2048, while there is random point dropout during testing. (b) The network is trained with SOM of size 8 × 8, but SOMs of various sizes are used at testing phase. (c) Gaussian noise N (0, σ) is added to the SOM during testing. (d) Example of SOM with Gaussian noise N (0, 0.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Effect of layer number on classification accuracy with ModelNet40 (left) and ModelNet10 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of object part segmentation results. First row: ground truth. Second row: predicted segmentation. From left to right: chair, lamp, table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Results of SOM training with uniform initialization. Isolated nodes are inevitable even with uniform initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .</head><label>14</label><figDesc>The inputs to the network are point clouds of size 1024 and the corresponding surface normal vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Top 5 retrieval results. First column: query shapes. Column 2-6: retrieved shapes ordered by feature similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Results of our ShapeNetPart autoencoder. Red points are recovered by the convolution branch and green ones are by the fully connected branch. Odd rows: input point clouds. Even rows: reconstructed point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Results of our ModelNet40 autoencoder. Red points are recovered by the convolution branch and green ones are by the fully connected branch. Odd rows: input point clouds. Even rows: reconstructed point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>Results of object part segmentation. Odd rows: ground truth segmentation. Even rows: predicted segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Object classification results for methods using scalable 3D representations like point cloud, kd-tree and octree. Our network produces the best accuracy with significantly faster training speed. * represents pre-training.</figDesc><table><row><cell>Method</cell><cell>Representation</cell><cell>Input</cell><cell cols="5">ModelNet10 Class Instance Class Instance Training ModelNet40</cell><cell cols="2">MNIST Input Error rate</cell></row><row><cell>PointNet, [26]</cell><cell>points</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>86.2</cell><cell>89.2</cell><cell>3-6h</cell><cell>256 × 2</cell><cell>0.78</cell></row><row><cell cols="3">PointNet++, [28] points + normal 5000 × 6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.9</cell><cell>20h</cell><cell>512 × 2</cell><cell>0.51</cell></row><row><cell cols="2">DeepSets, [29, 39] points</cell><cell>5000 × 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kd-Net, [18]</cell><cell>points</cell><cell cols="2">2 15 × 3 93.5</cell><cell>94.0</cell><cell>88.5</cell><cell>91.8</cell><cell>120h</cell><cell>1024 × 2</cell><cell>0.90</cell></row><row><cell>ECC, [32]</cell><cell>points</cell><cell cols="2">1000 × 3 90.0</cell><cell>90.8</cell><cell>83.2</cell><cell>87.4</cell><cell>-</cell><cell>-</cell><cell>0.63</cell></row><row><cell>OctNet, [30]</cell><cell>octree</cell><cell>128 3</cell><cell>90.1</cell><cell>90.9</cell><cell>83.8</cell><cell>86.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>O-CNN, [36]</cell><cell>octree</cell><cell>64 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (2-layer)*</cell><cell cols="3">points + normal 5000 × 6 94.9</cell><cell>95.0</cell><cell>89.4</cell><cell>92.5</cell><cell>3h</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (2-layer)</cell><cell cols="3">points + normal 5000 × 6 94.4</cell><cell>94.5</cell><cell>89.3</cell><cell>92.3</cell><cell>3h</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (2-layer)</cell><cell>points</cell><cell cols="2">2048 × 3 93.9</cell><cell>94.1</cell><cell>87.3</cell><cell>90.9</cell><cell>3h</cell><cell>512 × 2</cell><cell>0.44</cell></row><row><cell>Ours (3-layer)</cell><cell cols="3">points + normal 5000 × 6 95.5</cell><cell>95.7</cell><cell>90.8</cell><cell>93.4</cell><cell>3h</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Intersection over Union (IoU) mean air bag cap car chair ear. gui. knife lamp lap. motor mug pistol rocket skate table PointNet [26] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 PointNet++ [28] 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6 Kd-Net [18] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3 O-CNN + CRF [36] 85.9 85.5 87.1 84.7 77.0 91.1 85.1 91.9 87.4 83.3 95.4 56.9 96.2 81.6 53.5 74.1 84.4 Ours (pre-trained) 84.9 82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0 Ours 84.6 81.9 83.5 84.8 78.1 90.8 72.2 90.1 83.6 82.3 95.2 69.3 94.2 80.0 51.6 72.1 82.6 Object part segmentation results on ShapeNetPart dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Potential field method for SOM initializationSet random seed.Instead of updating the SOM once per point, the batch update rule conducts one update after accumulating the effect of all points in the point cloud. As a result, each SOM update iteration is unrelated to the order of point, i.e., permutation invariant. During SOM training, each training sample affects the winner node and all its neighbors. We define the neighborhood function as a Gaussian distribution as follows:</figDesc><table><row><cell cols="4">Random initialization: S ← U(−1, 1)</cell></row><row><cell>repeat</cell><cell></cell><cell></cell></row><row><cell cols="2">for all s j ∈ S do</cell><cell></cell></row><row><cell>f wall j</cell><cell>← −s j</cell><cell></cell></row><row><cell>f node j</cell><cell>← 0</cell><cell></cell></row><row><cell cols="3">for all s k ∈ S, k = j do f node j ← f node j + λ</cell><cell>sj −s k sj −s k 2 2</cell></row><row><cell cols="2">end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="2">for all s j ∈ S do</cell><cell></cell></row><row><cell cols="2">s j ← s j + η(f wall j</cell><cell cols="2">+ f node j</cell><cell>)</cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="2">until converge</cell><cell></cell></row><row><cell cols="3">D.2. Batch Update Training</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 3. 3D shape retrieval results with SHREC16. Our SO-Net out-performs state-of-the-art deep networks with most metrics.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell cols="9">Micro P@N R@N F1@N mAP NDCG@N P@N R@N F1@N mAP NDCG@N Macro</cell></row><row><cell cols="2">Tatsuma</cell><cell></cell><cell></cell><cell cols="3">0.427 0.689 0.472 0.728</cell><cell cols="2">0.875</cell><cell cols="4">0.154 0.730 0.203 0.596</cell><cell>0.806</cell></row><row><cell cols="3">Wang CCMLT</cell><cell></cell><cell cols="3">0.718 0.350 0.391 0.823</cell><cell cols="2">0.886</cell><cell cols="4">0.313 0.536 0.286 0.661</cell><cell>0.820</cell></row><row><cell cols="7">Li ViewAggregation 0.508 0.868 0.582 0.829</cell><cell cols="2">0.904</cell><cell cols="4">0.147 0.813 0.201 0.711</cell><cell>0.846</cell></row><row><cell cols="3">Bai GIFT [3]</cell><cell></cell><cell cols="3">0.706 0.695 0.689 0.825</cell><cell cols="2">0.896</cell><cell cols="4">0.444 0.531 0.454 0.740</cell><cell>0.850</cell></row><row><cell cols="3">Su MVCNN [33]</cell><cell></cell><cell cols="3">0.770 0.770 0.764 0.873</cell><cell cols="2">0.899</cell><cell cols="4">0.571 0.625 0.575 0.817</cell><cell>0.880</cell></row><row><cell cols="3">Kd-Net [18]</cell><cell></cell><cell cols="3">0.760 0.768 0.743 0.850</cell><cell cols="2">0.905</cell><cell cols="4">0.492 0.676 0.519 0.746</cell><cell>0.864</cell></row><row><cell cols="3">O-CNN [36]</cell><cell></cell><cell cols="3">0.778 0.782 0.776 0.875</cell><cell cols="2">0.905</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.799 0.800 0.795 0.869</cell><cell cols="2">0.907</cell><cell cols="4">0.615 0.673 0.622 0.805</cell><cell>0.888</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bai_GIFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Bai_GIFT</cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell cols="2">Li_ViewAggregation Su_MVCNN</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell cols="3">Li_ViewAggregation Su_MVCNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tatsuma_DB-FMCD-FUL-LCDR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Tatsuma_DB-FMCD-FUL-LCDR</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell>Wang_CCMLT SO-Net</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell cols="2">Wang_CCMLT SO-Net</cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P</cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.3</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Time and space complexity of point cloud based networks in ModelNet40 classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Classification results using structure information -SOM nodes and kd-tree split directions. Algorithm 2 SOM batch update rule Initialize m × m SOM S with Algorithm 1 for t &lt; MaxIter do Set update vectors to zero for all s xy ∈ S do D xy ← 0 end for Accumulate the effect of all points for all p i ∈ P do Obtain nearest neighbor coordinate p, q for all s xy ∈ S do w xy ← Eq. (6) D xy ← D xy + w xy (p i − s xy ) end for end for Conduct one update for all s xy ∈ S do s xy ← s xy + η t D xy end for t ← t + 1 Adjust σ x , σ y and η t end for</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell cols="3">MNIST ModelNet10 ModelNet40</cell></row><row><cell cols="2">Kd-Net split based MLP [18] splits</cell><cell>82.40</cell><cell>83.4</cell><cell>73.2</cell></row><row><cell>Kd-Net depth 10 [18]</cell><cell>point</cell><cell>99.10</cell><cell>93.3</cell><cell>90.6</cell></row><row><cell>Ours -SOM based MLP</cell><cell>SOM nodes</cell><cell>91.37</cell><cell>88.9</cell><cell>75.7</cell></row><row><cell>Ours</cell><cell>point</cell><cell>99.56</cell><cell>94.5</cell><cell>92.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lijx10/SO-Net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is supported partially by a ODPRT start-up grant R-252-000-636-133 from the National University of Singapore.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This supplementary document provides more technical details and experimental results to the main paper. Shape retrieval experiments are demonstrated with ShapeNet Core55 dataset in Sec. B. The time and space complexity is analyzed in Sec. C, followed by detailed illustration of our permutation invariant SOM training algorithms in Sec. D. More experiments and results are presented in Sec. E.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02392</idno>
		<title level="m">Learning representations and generative models for 3d point clouds</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gift: A real-time and scalable 3d shape search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03677</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01222</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Octree encoding: A new technique for the representation, manipulation and display of arbitrary 3-d objects by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Meagher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
		<respStmt>
			<orgName>Electrical and Systems Engineering Department Rensseiaer Polytechnic Institute Image Processing Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02901</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dominant set clustering and pooling for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
