<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
							<email>dzabraev.maksim@intsys.msu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
							<email>kalashnikov.maxim@intsys.msu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
							<email>stepan.komkov@intsys.msu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
							<email>petyushko.alexander1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Moscow Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MDMMT: Multidomain Multimodal Transformer for Video Retrieval A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video</term>
					<term>language</term>
					<term>retrieval</term>
					<term>multi-modal</term>
					<term>cross-modal</term>
					<term>temporality</term>
					<term>transformer</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new state-of-the-art on the text to video retrieval task on MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions by a large margin. Moreover, state-of-the-art results are achieved with a single model on two datasets without finetuning. This multidomain generalisation is achieved by a proper combination of different video caption datasets. We show that training on different datasets can improve test results of each other. Additionally we check intersection between many popular datasets and found that MSRVTT has a significant overlap between the test and the train parts, and the same situation is observed for ActivityNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video is a quite popular data format, 500+ hours of video are uploaded on YouTube every minute. Many personal mobile phones have gigabytes of video. Since video format gets more popular every year the importance of modern search methods is increasing as well.</p><p>In this work we present our research about text to video retrieval task. In this task system should return for a given textual query the most relevant video segments from a gallery. The query is a textual description of what we want to find in the video. The query may describe objects, actions, sounds, ..., and relations between them.</p><p>Such search methods are a promising direction for mobile devices because every year manufacturers increase the available memory on devices. The large part of the memory is filled by media data. For end users it is getting difficult to search for a video made one or two years ago. But users can easily describe the content of the video using natural language, which can be effectively used as a search query.</p><p>There are two major directions which allow calculate the relevance between a textual search query and a video segment. The first direction is single stream approaches <ref type="bibr" target="#b31">[32]</ref>, where a query and a video together are given to a network and then become fused from the beginning of the processing. The schematic illustration of this approach is presented in <ref type="figure" target="#fig_5">Fig. 1a.</ref> (a) Scheme for a single-stream neural network.</p><p>(b) Scheme for a two-stream neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Two types of fusion</head><p>This type of approaches have access to all input data from the beginning of its processing and can make a strong verdict about data. But these approaches have a significant drawback because it is not scalable: every new query the search system should calculate the full forward pass for this query and for each video segment from the gallery.</p><p>Another direction is two stream neural networks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b7">[8]</ref>, where a textual query and a video are processed by two different neural networks. As a result the networks produce embeddings inside the same embedding space, where semantically close textual queries and video segments will be placed next to each other. The schematic illustration is presented in <ref type="figure" target="#fig_5">Fig. 1b</ref>.</p><p>The two stream approach is scalable, it allows to precompute video embeddings for all videos from the gallery, and to do only one forward pass with the text network for arXiv:2103.10699v1 [cs.CV] <ref type="bibr" target="#b18">19</ref> Mar 2021 each new query and then to compute the cosine similarity between the new query embedding and all precomputed embeddings.</p><p>To make a strong video retrieval solution it is important to show to the model a lot of situations, actions and objects from real life. There exist a lot of video datasets, but none of them cover a significant portion of real life situations. One of the first steps to tackle this problem is to formulate the rules for combining different existing datasets to a single large train database.</p><p>Text to video retrieval is a modern direction, where one of the first works was published at 2016 <ref type="bibr" target="#b32">[33]</ref>. One of the most universal solution for video retrieval task is Multi Modal Transformer <ref type="bibr" target="#b7">[8]</ref> architecture which uses BERT <ref type="bibr" target="#b3">[4]</ref> backbone for a video network. It allows in a natural way to process the temporal dependencies inside the multi modal data source.</p><p>To train a text to video retrieval neural network the training database should consists of pairs: (a video segment, a textual description of this video segment). Traditionally such sort of datasets was created for a video captioning task. But it turns out that these datasets perfectly can be used for a video retrieval task. One of the first video captioning dataset was MSVD, which was created in 2010. Today there exist more than a dozen of different video captioning datasets.</p><p>The most popular datasets for text to video retrieval is MSRVTT <ref type="bibr" target="#b38">[39]</ref>, ActivityNet <ref type="bibr" target="#b16">[17]</ref> and LSMDC <ref type="bibr" target="#b28">[29]</ref>. Many researchers test their solutions mostly on these three datasets.</p><p>Our main contributions in this work are the following:</p><p>• We present a new state-of-the-art (SotA) result on MSRVTT and LSMDC benchmarks; • We present a model which shows good results on three different benchmarks without finetuning: MSRVTT (SotA), LSMDC (SotA) and ActivityNet at the same time; • We present a practical approach which helps us to find the overlap between the train and the test parts of used datasets.</p><p>2 Related work 2.1 Datasets MSRVTT <ref type="bibr" target="#b38">[39]</ref> was created in 2016. This dataset is traditionally used by researchers as the main dataset for testing text to video retrieval models. This dataset consists of 10k video segments, each segment has 20 captions. The authors collected 257 popular search queries and gathered from YouTube 118 most relevant videos for each of them. The dataset has 42 hours of video. The captions were made by 1327 amazon workers.</p><p>Today there are three different test/train splits. The official split is called full split, where the train part has 7k videos and the test part has 3k videos. There are two important properties of this split: 1. there are no two video segments cropped from the same video so as the first segment is placed in the train part and the second segment is placed in the test part; 2. there are no two video segments, retrieved from the same query so as the first one is placed in the train part and the second one is placed in the test part.</p><p>Another two splits are called 1k-A <ref type="bibr" target="#b39">[40]</ref> (sometimes called jsfusion) and 1k-B <ref type="bibr" target="#b20">[21]</ref>  Another problem is that all these splits have the overlap between the test and train parts, see C.2 for details. To be strict we remove the overlap between the test part and the train part of MSRVTT full split. We called this split MSRVTT full clean, and refer to it as M c . It is worth to mention that we do not modify the test part, we only remove some videos from the train part.</p><p>The Large Scale Movie Description Challenge (LSMDC) <ref type="bibr" target="#b28">[29]</ref> is the extension of two independent datasets: MPII Movie Description Dataset (MPII-MD) <ref type="bibr" target="#b27">[28]</ref>, and Montreal Video Annotation Dataset (M-VAD) <ref type="bibr" target="#b33">[34]</ref>.</p><p>Video segments for this dataset were cropped from movies, where movie textualized transcriptions were used as captions. A movie transcription is an audio description of a video segment that helps blind people to watch movies by describing what happens, who appears in this time, what is on background right now and so on.</p><p>In this work for testing we use LSMDC public test, which consists of 1k video segments.</p><p>ActivityNet captions dataset <ref type="bibr" target="#b16">[17]</ref> consists of 20k videos and 100k captions, where captions cover the full video length for the most of videos, and neighbour captions may intersect. The annotations were made with Amazon Mechanical Turk.</p><p>The situation when some video segments may overlap makes a problem for text to video retrieval testing. Suppose we have two video-caption pairs (S 1 , C 1 ) and (S 2 , C 2 ) where the video segment S 1 has a non empty overlap with the video segment S 2 . Now suppose that for query C 1 the system returns the video segment S 2 . Is it mistake or not? What to do in this case?</p><p>Many previous works used ActivityNet test dataset in a paragraph retrieval mode. In this mode all captions for all video segments are concatenated, then the concatenated text is used as a textual query and the whole video should be retrieved for this query. Such mode has two drawbacks. The first one is that paragraph retrieval is not a classical video retrieval mode, it is another task. One can ask: if a model is good in paragraph retrieval will it be good for video retrieval? The second drawback is that queries will be long, video segments will be long (compared to a classical video retrieval mode). This issue requires to enlarge the input for the model.</p><p>Another way to use the test part of ActivityNet is just to sample once a single random segment from each video. As a result we will have non intersected video segments and captions with usual length. We use ActivityNet test part in this way. We take all videos from val1 and val2 parts, and sample a single random segment from each video. All results on ActivityNet are reported on this split.</p><p>Additionally in this work the following datasets are used: NIST TRECVID Twitter vines <ref type="bibr" target="#b0">[1]</ref>, TGIF <ref type="bibr" target="#b17">[18]</ref>, MSVD <ref type="bibr" target="#b1">[2]</ref>, YouCook2 <ref type="bibr" target="#b42">[43]</ref>, Something-something V2 <ref type="bibr" target="#b9">[10]</ref>, Kinetics 700 <ref type="bibr" target="#b30">[31]</ref>, HowTo100M <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior Art</head><p>A dominant approach to train video retrieval models is contrastive learning. The idea of this approach is that we have a set of pairs (video i , text i ) and elements of each pair should be placed next to each other in some metric space: distance(video i , text i ) → 0, at the same time the element video i should be far from all other text j ), j = i: distance(video i , text j ) → +∞. The bi-directional maxmargin ranking loss <ref type="bibr" target="#b12">[13]</ref> represents this idea.</p><p>When training data have a lot of noise the MIL NCE loss <ref type="bibr" target="#b21">[22]</ref> can be applied in the training procedure. Suppose that we know that a video i should be close to one of (or several) texts text i1 , ..., text ik . This approach tries to reduce the distance between the video i and all text i1 , ..., text ik at the same time.</p><p>All video captions datasets have the following problem. Suppose the distance between (video i , text i ) is to be minimized while the distance between (video i , text j ), j = i is to be maximized, but text i and text j are quite similar (from the semantical point of view). Maybe the optimal scenario in this situation is to minimize the distance between (video i , text j ), j = i. In <ref type="bibr" target="#b24">[25]</ref> the authors show the approach which deals with this problem.</p><p>As far as an input video is the temporal sequence of tokens (frames or video segments) it is important to efficiently aggregate the information from all tokens. Many ideas for such aggregation in the previous works are borrowed from the natural language processing. Convolution filters for aggregation are used in <ref type="bibr" target="#b24">[25]</ref>, a transformer encoder as a video aggregator is used in <ref type="bibr" target="#b7">[8]</ref>, many different aggregation functions are tested in <ref type="bibr" target="#b25">[26]</ref>.</p><p>We think that the most promising aggregation method is Multi Modal Transformer (MMT) <ref type="bibr" target="#b7">[8]</ref>. MMT is a two stream solution designed for a text to video retrieval task. The extraction of features from the input video stream is done in the following way. An input video is preprocessed by several pretrained frozen neural networks (these networks are called experts). Original solution uses seven modalities: motion, RGB, scene, face, OCR, speech, audio, and one pretrained network for each modality is used. The motion modality is processed with video recognition networks like S3D, SlowFast, irCSN, where several input frames are used as a single input. The RGB modality uses a single frame as an input. The audio modality uses the raw input sound from a video. After embeddings are extracted from input data by these experts, it will be augmented by adding positional encoding tokens (representing time) and expert tokens. Then the augmented embeddings are passed through MMT backbone. MMT backbone is a standard transformer encoder architecture. Each input modality produces one embedding, so in total there are seven output embedding from MMT.</p><p>For encoding the textual query the authors use pretrained BERT model where the output [CLS] token is used. The output is postprocessed with shallow networks (one network per modality) to extract the modality related information, in total seven feature vectors will be produced. In addition to embeddings from the text query seven weights representing how much the query describes one of seven modalities are produced. For example, if a query does not represent the sound, the small weight for the audio modality should be produced.</p><p>The final similarity score is done by a sum of seven weighted dot products of embeddings.</p><p>The MMT is trained with the bi-directional max-margin ranking loss <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_0">1 B B i=1 j =i max(0, s ij −s ii +m)+max(0, s ji −s ii +m)</formula><p>where B, s ij , m represent the batch size, the similarity between the i-th query and the j-th video inside this batch, and some predefined margin correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our work is mostly based on MMT. We use the same loss and a similar architecture, but with different hyperparameters. In this work we study the following questions:</p><p>• Which publicly available pretrained motion expert is the best for text to video retrieval nowadays, Sec. 3.1. • How to combine several video caption datasets in order to train a strong model without specialisation for a particular dataset, Sec. 3.2. • How to find and prevent the overlap between the test and train parts when combining datasets, Sec. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motion experts</head><p>The MMT video backbone does not process the raw input video stream, and instead the input video stream is processed by one or more pretrained experts, where each expert produces time series of features. The most important modality is motion: a motion expert processes several video frames as a single input unit and extracts the information about actions and objects within a segment.</p><p>We may say that the motion modality is the basis of the MMT. If a motion expert doesn't extract some information, there is a high probability that MMT won't know about some events in the video stream. That's why improving the motion expert is very important.</p><p>We consider several best solutions from Kinetics <ref type="bibr" target="#b14">[15]</ref> benchmark as well as several promising video recognition models and check which one works in the best way as a motion expert. We present all details in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset creation</head><p>It is possible to train a video retrieval model by two means. The first way is the way of specialization for a single domain. For example: create model that will work good only for MSRVTT benchmark (or domain) but at the same time this model will show poor results on other datasets (domains). In this way MMT <ref type="bibr" target="#b7">[8]</ref> was trained. The authors trained three different models for MSRVTT, ActivityNet and LSMDC datasets. Each of these three networks works good on domain X if and only if it was trained on X, but at the same time works poor on another domain Y = X. A proof of this statement we provide in Tab. 6.</p><p>The second way is to create a model that will work good for all domains at the same time. We use this way.</p><p>Obviously the model trained in the first way can't work good with real users, because the event when a user writes a search query similar to some caption from a small train database is very rare.</p><p>The second drawback here is that each video retrieval train dataset is not that big, and it causes the situation that model doesn't see many words and real life situations during training. For example, MSRVTT has only 9k videos and 200k captions in total for training, obviously this is not enough to train a neural network that will know most of real life situations, different items and persons. To tackle with this problem we can take several datasets with videos and captions and concatenate it.</p><p>Different datasets have the different number of videos, the different number of captions per video, some datasets may have long captions, some may have short captions, different rules for creating captions were used by human writers, and so on. Due to these factors some datasets may contain more information and require longer training time, some datasets may contain less information and require shorter training time. On the other hand, if we use long training time for a small dataset it could lead to overfitting on this dataset (the data will be memorized). The "information sizes" of some used datasets are illustrated in <ref type="figure">Fig</ref>  The key question is: what is the proper way for sampling examples from several datasets taking into account the different information size?</p><p>We use these obvious rules:</p><p>1. If a dataset X is larger than Y , we should sample from X more often than from Y ; 2. Training on X and Y combined requires longer train than training solely on X or Y ; 3. Training on X and Y combined may require a deeper model than for X or Y .</p><p>If we achieve the same results on X after combining X and Y it is still good because model gets better on Y . Our experiments show that the proper usage of rules 1-3 often improves the results for a specific test dataset (e.g. MSRVTT) after extending the train dataset.</p><p>We managed to combine the following datasets: MSRVTT, ActivityNet, LSMDC, TwitterVines, YouCook2, MSVD, TGIF and Something to something V2 (SomethingV2  <ref type="table">Table 1</ref>: The "Num video" column represents the number of video clips in the dataset, the "Num pairs" column represents the total number of video-caption pairs, the "Num unique captions" column represents the number of unique captions in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intersection</head><p>It is important to extend the training database carefully, not allowing the addition to the train part of video segments that already exist in the test part.</p><p>To find the intersection between the test part and the train part we use the two stage filtration. The first stage is to use the YouTube ID, if it is available. We should not allow to use in the test and train parts simultaneously any two video segments sampled from the same video. In the second stage we compute the similarity score between each video from the test part and each video from the train part, then we manually assess the pairs with the highest scores. In total we assessed more than 100K pairs of the most relevant segments, see Sec. C.1 for details.</p><p>We found the significant overlap between the MSRVTT 1k-A test and train parts, and the similar situation is with the 1k-B test and train parts and the less significant overlap is found between the MSRVTT full split test and train parts. The similar situation is with the ActivityNet train and validation 1,2 parts.</p><p>Additionally we estimate (but did not find) the overlap between HowTo100M and MSRVTT, and found that it may be significant. Our approach allows to approximately estimate the total number of videos in the intersection without finding the exact intersection, please see the details in Sec. C.1.2. The similar estimation is for ActivityNet and Kinetics700, an our approximation shows that there may be a significant overlap, see all details in Sec. C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviate</head><p>Composition</p><formula xml:id="formula_1">M MSRVTT full split M c MSRVTT full clean split, see Sec. 2.1 M 1k-A MSRVTT 1k-A split M 1k-B MSRVTT 1k-B split A ActivityNet A val1</formula><p>ActivityNet val1 validation set A val2</p><p>ActivityNet val2 validation set A p/r ActivityNet paragraph retrieval, see Sec  <ref type="table">Table 2</ref>: The left column represents the abbreviate name for the set of datasets from the right column.</p><formula xml:id="formula_2">. 2.1 L LSMDC K Kinetics700 V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>We use exactly the same neural network architecture as original MMT <ref type="bibr" target="#b7">[8]</ref>, our method is significantly based on their codebase. The difference is in the following: 1. we use the more aggressive dropout equals to 0.2 for the text BERT and the video BERT (against the original value of 0.1); 2. we found that the deeper and wider transformer encoder for a video network gives better results -we use 6 layers and 8 heads for the motion only modality and 9 layers and 8 heads for the motion + audio setting (against 4 layer and 4 head in the original implementation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stronger motion experts</head><p>As the input data for MMT is embeddings from experts, the obvious question can arise: if a better expert is used, will we have a stronger model? To answer this question we train MMT on MSRVTT dataset with the only motion modality. For motion experts we try several architectures pretrained on different datasets, these models are presented in Tab. 3. We take the architectures which show the best results on Kinetics 400 benchmark having publicly available pretrained weights: <ref type="bibr" target="#b37">[38]</ref> [7] <ref type="bibr" target="#b35">[36]</ref> [35] <ref type="bibr" target="#b8">[9]</ref> .</p><p>The results in Tab. 3 are made with the same hyperparameters as in <ref type="bibr" target="#b7">[8]</ref>. For the train dataset we use only MSRVTT full clean split. The first line in Tab. 3 represents the motion feature extractor from the original MMT paper.</p><p>As we can see, usually stronger models provide better results, but not always. Refer to r(2+1)d 152 rows, this network demonstrates one of the best performance on Kinetics 400 benchmark, but works poorly as motion expert. Maybe this network is over specialized for Kinetics 400. More shallow analogue of r(2+1)d 152 is r(2+1)d 34 which shows much better results.</p><p>An interesting observation is that the best results are achieved with the networks trained in the unsupervised manner. CLIP and models trained on IG65M outperform all other models trained on Kinetics in the supervised manner. Another weakly supervised dataset is Sports1M <ref type="bibr" target="#b13">[14]</ref>. Models trained on this dataset provide weak embeddings similar to the weak s3d model trained on Kinetics dataset. The CLIP <ref type="bibr" target="#b26">[27]</ref> (ViT-B/32) image feature extractor with a large margin outperforms all other models. The model s3dg MIL-NCE is a video encoder from the work <ref type="bibr" target="#b21">[22]</ref>. This network was trained from scratch on HowTo100M dataset.</p><p>As we show in Sec. C Kinetics dataset has an overlap with MSRVTT dataset, and we don't know whether it affects to overfitting or not. Also it is worth to mention that IG65M and CLIP datasets are not publicly available, so we do not know if there is an overlap with MSRVTT and other video retrieval datasets.</p><p>For more details about our usage of pretrained video experts please refer to Sec. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets combination</head><p>In this section we show our experiments about the combination of different datasets. Nowadays video-caption datasets are not big enough to capture all real life situations, also some datasets may be biased. The combination of different datasets may help to tackle this problem.</p><p>Our experiments show that the proper combination of datasets allows to train a single model that can capture the knowledge from all used datasets. Important thing here is that in most cases the model trained on the combination of datasets is better than the model trained on a single dataset.</p><p>In our experiments we combine all datasets presented in Tab. 5. The important thing is how to sample minibatches during training. In our experiments we first sample a dataset, then we uniformly sample a video segment, if this sampled video segment has more than one caption we sample a single caption uniformly. Column weight in Tab. 5 describes the probability of sampling the corresponding dataset. To obtain the probability of sampling the dataset with the weight w we should divide w by the sum of all weights.</p><p>The weights for all datasets are manually adjusted. It is important to find a good weight combination, because if some weight will be larger than needed, this dataset will be overseen and as a result the performance will be lower comparing to the optimal case. The opposite case is when a small weight was selected, this causes the situation when </p><formula xml:id="formula_3">): 150K = 150K × (p MSRVTT + p ActivityNet + p LSMDC + p Twitter Vines + p YouCook2 + p MSVD + p TGIF + p Something V2 ).</formula><p>If some dataset is removed from the training, we remove the corresponding coefficient from this sum, so the resulting length will be 150K multiplied by a value less than 1.</p><p>As far as we use the configurations M c , A, L as the baselines, we need to be sure that the results for these configurations are the optimal values. In addition to the rule described above we try several values for a number of examples per epoch parameter, and report the results for the best found value.</p><p>Tab. 6 summarizes our experiments on the datasets combination (for more details please refer to Sec. B). The main point here is that the proper combination of datasets leads to the best solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Final result</head><p>In this section we compare our solution with the prior art. Our two best solution uses three modalities: the audio, the motion and the RGB. To fuse modalities we use MMT architecture with 9 layers and 8 heads. As a feature extractor for the audio stream the vggish <ref type="bibr" target="#b11">[12]</ref>     <ref type="table">Table 5</ref>: These datasets were used in our train procedure. The "Weight" column describes how often we sample examples from the dataset. The probability of obtaining an example from the dataset with the weight w equals to w divides by a sum of all weights.</p><p>vious solutions on all splits: full, 1k-A and 1k-B. Our solution is better than the previous SotA (on R@5) on 8.7%, 10.5% and 14.4% on full, 1k-A and 1k-B correspondingly. It is also worth to mention that our MDMMT (using only the motion, the RGB and the audio modalities) We also report the results for the original CLIP <ref type="bibr" target="#b26">[27]</ref>. The CLIP model has an image encoder and a text encoder, both pretrained in an unsupervised way. To test the CLIP model we take a single frame from the middle of the video (this is the original testing protocol for CLIP). The row CLIP agg <ref type="bibr" target="#b25">[26]</ref> represents the usage of CLIP model with several frames using some specific aggregation procedure from this work.</p><p>In Tab. 8 we report the results on LSMDC. On this benchmark we outperform the previous SotA solution by 8.6%.</p><p>As we mention in Sec. 2.1 we do not use the standard ActivityNet paragraph retrieval test protocol. Instead we use the text to video retrieval protocol. To compare our solution with the previous work we take the previous SotA approach (MMT) in text to video retrieval and test it on our split. The results are reported in Tab. 4. Our solution outperforms MMT by 22.6%. The row MMT (A p/r ) mo-tion+audio means that this network was trained only on ActivityNet dataset with the paragraph retrieval mode. It is also worth to mention that CLIP shows very bad results on this benchmark. We try to aggregate with the mean pooling of 2, 4 and 16 uniformly taken embeddings, take the first 10, 20 and 70 words from a caption, and no method improves the results.</p><p>The important property of our model is that we train a single model and test it on different test sets. The authors of previous SotA approach (MMT) trained three different models for MSRVTT, ActivityNet and LSMDC, while in Tab. <ref type="bibr" target="#b5">6</ref> we show that the model trained in such a manner has poor generalization and can show good performance on the test part of the dataset X if and only if it was trained on the train part of the dataset X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussion</head><p>In this work we present a new text to video retrieval stateof-the-art model on MSRVTT and LSMDC benchmarks. We do not use ActivityNet dataset in the paragraph retrieval mode as many previous works do, so we can't compare with them. But we show that on ActivityNet in the video retrieval mode we outperform the previous state-of-the-art model (MMT) by a large margin. Our model has captured knowledge from many video caption datasets, thus it is able to show the best results on several datasets at the same time without finetuning.</p><p>We also present a practical approach to find the overlap between two different video datasets. Using this approach we find the overlap between several datasets. Especially we find a large overlap between the MSRVTT test and train parts, and between the ActivityNet test and train parts. Removing this overlap from the MSRVTT train part significantly decreases the performance of previous best models on MSRVTT benchmark.   A Pretrain experts usage</p><p>The important data preparing stage is how to sample frames from a video to achieve the best performance. For s3d experiments the input video is converted to 30 frames per second, for all other experiments we convert the input video to 32 frames per second. As a result we compute a single embedding for each second, having 1 second window with 1 second shift (no overlapping).</p><p>The input frame size is important. We use the different sizes for the different models. For each model we use the recommended input size. For s3d we resize a video to 256 on the short side and then take a 224x224 center crop.</p><p>For SlowFast 32x2 R101 we resize a video to 256 on the short side and then take a 256x256 center crop. For ipCSN 152 and irCSN 152 we resize a video to 224 on the short side and take a 224x224 center crop. For r(2+1)d 152 and r(2+1)d 34 we resize a video to 112 on the short side and then take a 112x112 center crop.</p><p>Pretrained models for ipCSN, irCSN and r(2+1)d are available here 1 , for SlowFast 32x2 R101 here 2 , and for s3d here <ref type="bibr" target="#b2">3</ref> .</p><p>For the CLIP model <ref type="bibr" target="#b26">[27]</ref> we resize a video to 224 on the short side and take a center crop, then we extract 1 frame per second. We use a publicly available image encoder. We do not use the text encoder from CLIP.</p><p>Model s3dg MIL-NCE is a video encoder from the work <ref type="bibr" target="#b21">[22]</ref>. This network was trained from scratch on HowTo100M dataset. For this network we resize the input video stream to the size of 228x228 pixels, then take a center crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets combination</head><p>In <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Test and train intersection</head><p>In this section we present our analysis of overlapping of popular text to video datasets. Since we compose the train dataset from several different datasets it is important to be sure that there is no the same video segment in the train part and in the test part. Our aim is to find the overlap  between the train part of used datasets -MSRVTT, Activ-ityNet, LSMDC, YouCook2, MSVD, TGIF, TwitterVines, HowTo100M, Kinetics700 and the test parts of MSRVTT, ActivityNet and LSMDC, and then to remove found duplicates from the train parts.</p><p>Note that for training we use Something to Something V2 dataset, but we do not try to find overlap between it and test datasets because this dataset is artificially created, thus the probability to find duplicates is very low.</p><p>We decided to find the overlap only for MSRVTT, Activ-ityNet and LSMDC because these are the most popular datasets and we do not have enough human resources to find the overlap for the test part of all other datasets.</p><p>Our cleaning method consists of two stages. The first stage is to match video segments by the YouTube ID (if the ID is available) and remove from train parts all video segments that have the corresponding pair in test parts. In Tab. 1 the information about the availability of YouTube IDs in datasets is presented. We collect the YouTube ID for all videos from MSRVTT full test and ActivityNet validation 1,2 and remove corresponding video segments from the train part.</p><p>The second stage is based on matching frames by embeddings. For each video we compute several embeddings then we compute the similarity between each video from the train part and the test part. After we manually assess several thousands of video segments with highest scores for each pair of datasets. Then we extend found duplicates by either the YouTube ID or the internal dataset ID. This means that if a video V 1 is marked as a duplicate and a video V 2 is not marked as a duplicate, but they have the same YouTube ID or same internal dataset ID, we will remove V 1 and V 2 from the train part. In case of LSMDC we do not have the YouTube ID, but have the name of the movie from which the video segment was taken, so if a video segment V 1 is marked as a duplicate, we remove all segments taken from the movie of V 1 . The detailed description of the second stage is described in Sec. C. This circumstance means that researchers should carefully use HowTo100M and Kinetics700 along with MSRVTT and ActivityNet correspondingly, because for today we don't know whether a neural network overfits for some portion of this intersection or not.</p><p>All duplicates can be considered as two groups of pairs. Pairs from the first group have the same videos, but different brightness, aspect ratio, size, presence/absence of a logo and so on. The second group has pairs with quite similar videos, for example it can be the same person on the same background, doing the same things, but wearing different clothes. We think that it is better to remove such videos from the train part to prevent overfitting. Several found examples are presented in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Near duplicate video search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Approach</head><p>In this section we explain our approach that is used to find the same or quite similar video segments in test and train parts.</p><p>Suppose we have two sets of videos Q = {q 1 , ...., q k } and G = {g 1 , ..., g n } called the query set and the gallery set. We want to find all pairs (q i , g j ) where q i and g j have a common video segment.</p><p>From each q i and g j we extract 1 frame per second. Each video is then represented by a sequence of pictures: q i = [q 1 i , ...., q si i ] and g j = [g 1 j , ..., g pj j ]. Then a 2D pretrained neural network is used to extract features from each image:</p><formula xml:id="formula_4">q b a = neuralnet(q b a ) andḡ b a = neuralnet(g b a )</formula><p>. Then we compute the matrix of cosines between the features from Q and G:</p><formula xml:id="formula_5">s ab ij = &lt;q a i ,ḡ b j &gt; ||q b a ||2||ḡ b a ||2</formula><p>. Now each pair (q i , g j ) is represented by the matrix:</p><formula xml:id="formula_6">g 1 j ... g pj j q 1 i s 11 ij ... s 1pj ij ... q si i s si1 ij ... s sipj ij</formula><p>Suppose that videos q i and g j are intersected at time moments t q and t g , it is naturally to assume that the next several seconds t q +1, ..., t q +K −1 and t g +1, ..., t g +K −1 (K ≤ min(s i , p j )) represent the same video segment. Motivated by this fact we compute the mean cosine for each interval of K seconds (we use K=4): S Finally we sorted all S ij in the descending order and manually assess candidate pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Number of pairs to assess</head><p>Suppose we search duplicates in datasets Q and G and we have seen N pairs with the highest scores and find M pairs with duplicates. The important question is: what is the total number of duplicates and how many percents of them have we found.</p><p>For each pair of Q and G we construct the following test procedure. The first step is to augment Q, and let us call the result of augmentation asQ. To augment a dataset we apply two transformations: 1. we randomly crop a side of each video, where each side can be 70%-100% of original side length (aspect ratio can be changed); 2. we randomly shift the start of the video by a random value between 0 and 1 seconds.</p><p>Having Q,Q and G we compute sets of positive and negative scores: Pos and Neg. The Pos is the set of scores between i-th video from Q and the corresponding augmented video fromQ. Neg is the set of scores between each video from Q and G. Having Pos and Neg sets we can plot a curve, where x axis represents the fraction of found pairs with duplicates and Y axis represents the number of negative pairs that we need to assess to find fraction x of positive pairs, call this curve F (x). We present the algorithm that computes F using Pos and Neg sets in Lst.  The key component of a duplicate search system is a feature extractor. A good feature extractor significantly reduces the number of pairs for manual assessment. To compare different 2D feature extractors we use the following test procedure. The test consists of two datasets. The first dataset is the train part from the MSRVTT full split. The second dataset is random 596k videos from the HowTo100M dataset. From each video of the taken part of HowTo100M we take a random 30 seconds segment. We apply random augmentation to MSRVTT, as described in Sec. C.1.2. Define MSRVTT as Q, the augmented MSRVTT dataset asQ and the taken part of HowTo100M as G. For each feature extractor we compute curve F (x), as described in Sec. C.1.2.</p><p>The best expert has the lowest curve. For example, if we want to find 95% of duplicates, we should see many of candidates, some of them are duplicates, but majority of them are not. So, the value F (0.95) is the approximation of how many not duplicates we need to see to find 95% of duplicates. Ideally F (0.95) = 0, where all seen candidates are duplicates. So, a lower value F (0.95) requires to see less number of false candidates, that's why the lower curve is better.</p><p>We consider several feature extractors: resnet18 and resnet101 <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref>, resnet50 pretrained on Places365 <ref type="bibr" target="#b41">[42]</ref> and resnext101-32x8d, resnext101-32x32d, resnext101-32x48d pretrained on one billion images from Instagram <ref type="bibr" target="#b19">[20]</ref> and finetuned on Ima-geNet. We report search curves F (x) for these pretrained networks in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>There exist networks <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b15">[16]</ref> trained especially for match the duplicate frames or video segments, but they are not publicly available. Curve F is used to estimate the minimal number of negative pairs (y = F (x)) that human assessors need to inspect before they find the fraction x of positive pairs. The lower the curve F the better (need to inspect manually less pairs). The curves are built with the query set Q = MSRVTT full train, the gallery set G = random 596k videos from HowTo100M.</p><p>As we see resnext101-32x48d-wsl shows the best result. We use this network for searching for duplicates.</p><p>It is worth to mention that here we just compare different networks on a fixed benchmark, and pick the best one. But the search curve F (x) significantly depends on data. This curve should be estimated for each used pair of datasets Q and G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.4 Black frames</head><p>Often two consecutive video segments are glued with several black frames. The cosine similarity of embeddings of two black or near black frames are close to 1. In this case the most probable candidates for duplicates are black video segments. To prevent this we apply the following rule. Suppose we have a frame U and the unit length embedding v computed from U . We find the prevalent color in U and compute the area S 0 filled by this color. Then we compute the value S 0 /(hw), where h and w are the height and width of U . If this fraction is greater than 0.7 we define µ v = 1 − S 0 /(hw), otherwise µ v = 1. To calculate similarity between embeddings v 1 and v 2 we use weighted cosine similarity: µ 1 µ 2 cosv 1 , v 2 , instead of classical cosine similarity. This rule removes majority of all near black frames from the most relevant candidates for duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.5 Screensavers detection</head><p>Many videos from ActivityNet, HowTo100m, YouCook2 contain screensavers at the beginning or at the end. It causes a problem like mentioned above with near black frames, because most of relevant proposals are the same screensavers, but the video content of the remainder video part are different.</p><p>Using the system described in Sec. C.1.6 we search for duplicates in the ActivityNet dataset, where a lot of the most relevant segments are screensavers. We collect several hundreds of screensavers and then compute embeddings for each of them. Let us call the resulting set of embeddings as E. Then we apply the following rule: if some embedding v has the similarity greater that 0.9 to one of embeddings from E, we set v = 0. So if the video segment has a part of a screensaver, it will never be in the most relevant proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.6 GUI</head><p>The important part of the video duplicate search system is the user interface. Without ergonomic and fast interface it is impossible to assess tens thousands of video pairs. Our system is presented in <ref type="figure">Fig. 8</ref>.</p><p>The system shows video pairs with the highest scores on top. A user needs to scroll down a web page (new videos are loaded dynamically with ajax), and if a video duplicate is detected, a user should press the Duplicate button, if there are no duplicates in the current viewport, no action is required. When a user scrolls a web page, all non-duplicate pairs automatically are saved to a log file. Additionally several users at the same time can assess video pairs. Column "test" means how many video segments are in the test part that have the corresponding pair in the train part either with the same YouTube ID or manually marked as a duplicate. Column "train" represents the number of video segments in the train part that have corresponding pair in the test dataset either with the same YouTube ID or manually marked as a duplicate. All segments counted in the "train" column are removed from the train part. For example consider the column "A" and the row "M". train=10 means that the MSRVTT train part contains 10 video segments that have a pair in the ActivityNet test part. These 10 videos must be removed from train part when dataset are combined. test=6 means that ActivityNet test has 6 video segments that have a pair in the MSRVTT test part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Cleaning results</head><p>Recall that our cleaning method consists of two stages. In the first stage we throw out from the train part all video segments that have a pair with the same YouTube ID in test parts of MSRVTT or ActivityNet. The second stage is matching video segments by embeddings and manually assess several thousands pairs with the highest score.</p><p>In Tab. 9 we report how many duplicates are found for each pairs of datasets. This table represents the final result after applying these two stages.</p><p>Separate results for the first and the second stages are reported in Sec. C.2.1.</p><p>Note that columns "test" and "train" in Tab. 9 may have different values. Consider the situation when the test part have a video segment A, and the train part have two video segments A1 and A2. And both are marked as duplicates with A. In this case the video segment A brings +1 to the "test" column and A1, A2 bring +2 to the "train" column.</p><p>The most problematic datasets in terms of the number of duplicates are MSRVTT and ActivityNet. These datasets overlap with itself (e.g. MSRVTT test overlap with MSRVTT train). We found more than 100 duplicate pairs for both of them. Other problematic datasets are HowTo100M and Kinetics700, these datasets are large, so we can't assess the required number of video pairs to find 95% or 99% of duplicates. But we can assess a smaller number of pairs and using search curves F (see Sec. C.1.2) can extrapolate this value to 100%. We found that HowTo100M may have the intersection with MSRVTT test full by about 300 videos (10% of the MSRVTT test full). The similar situation is about the ActivityNet test set and Kinetics700, the intersection could be near 500-600 videos (10% of the ActivityNet test set).</p><p>In Tab. 10 we report results on MSRVTT for MMT retraining with no cleaning, after cleaning by the YouTube ID and cleaning combination by the YouTube ID and the manual assessment. The manual cleaning for 1k-A and 1k-B is incomplete because we only do cleaning for the full split. The following situation takes place for 1k-A, 1k-B splits: when 1k videos from the full test are taken for test and the remaining 2k videos are moved to the train part, the additional overlapping is introduced, because these 1k and 2k videos are overlapping. We do not remove this overlap in this research.  <ref type="table">Table 10</ref>: Comparison for original MMT trained (7 modalities) on MSRVTT without cleaning, with cleaning by the YouTube ID only, and with cleaning by the YouTube ID plus the manual assessment.</p><p>As you can see after cleaning the performance is significantly decreased on 1k-A and 1k-B splits for original MMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Intersection by YouTube ID and embeddings</head><p>In Tab. 11 we report the intersection by the YouTube ID between test parts of MSRVTT (full, 1k-A, 1k-B) and ActivityNet with train parts of MSRVTT (full, 1k-A, 1k-B), ActivityNet, Kinetics700, YouCook2, HowTo100m, MSVD.   <ref type="table">Table 12</ref>: Second stage. The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets. Column "seen" represents the number of video segments that we manually assess for a given pair of datasets. Column "found" represents the number of videos in the test part for which there exists the corresponding duplicate video segment in the train part. Column "total" represents the approximately estimated total number of videos from the test part that have a duplicate pair in the train part. Symbol "-" means that the intersection is not computed because it requires too much human resources.</p><p>In Tab. 12 we report the statistics for the second deduplication stage (searching by embeddings). We do not compute an intersection for MSRVTT 1k-A and 1k-B splits.</p><p>In this table we present the number of manually found duplicates and the estimated maximum number of duplicates for a given pair of datasets. We managed to find the intersection for almost all pairs of datasets.</p><p>The maximum number of duplicates is computed based on the search curve F (x). As we told in Sec. C.1.2 the search curve significantly depends on data. We compute the search curve for all pairs of datasets in Tab. 12. The search curve for each particular pair of datasets is build exactly in the same way as described in Sec. C.1.2. For example, to compute the search curve for MSRVTT test and ActivityNet train we define MSRVTT test as Q, Activ-ityNet train as G, then augment Q to produceQ, and use the algorithm described in Sec. C.1.2.</p><p>Using the column "seen" from Tab. 12 we can compute how many pairs need to be assessed to find the full overlap between datasets. For example, inspect 5k pairs for HowTo100M dataset and MSRVTT (the row "HT100M" and the column "M"), we found 15 duplicates, so the approximate maximum number of duplicates is 320: 5k * (320 / 15) = 106k. So, to find the full overlap using the current version of algorithm it is needed to manually assess 106k video pairs and it is too much, that's why we do not find full intersection for this specific pair of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>To train our best networks (MMT(MALVYMTS) L9H8 CLIP+audio, MDMMT(MALVYMTS) L9H8 irCSN152+audio and MMT(MALVYMTS) L9H8 CLIP+irCSN152+audio) we use 50 epochs and define a single epoch as 150K examples per GPU (in total 1.2M examples per epoch on 8 GPUs). We use Adam optimizer without weight decay, the initial value for a learning rate is 5e-5, after each epoch we multiply the learning rate by 0.95. Batch size of 32 examples per GPU is used. We do not exchange embeddings between GPUs. We use bi-directional max-margin ranking loss with margin 0.05. In Bert and the video transformer encoder we use dropout 0.2 in attention and in FFN block. We use 8 Nvidia V100 32GB GPUs. The training time is about 14 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Pretrained model</head><p>The well known method to boost the performance in video retrieval tasks is to use a pretrained model. First the neural network is trained on some large dataset, then at second stage it is finetuned for target target dataset. In video retrieval task HowTo100M dataset is often used for pretraining. In this work we use HowTo100M for pretraining in the same way.</p><p>In our training procedure we use <ref type="bibr" target="#b7">8</ref>     GPU. Initial learning rate is 5e-5. After each epoch we multiply learning rate by 0.98. We use the full HowTo00M dataset. The model is trained either with two modalities: motion/RGB and audio or with three modalities: motion, RGB and audio, depending on how many modalities are used in final model. The total training time is about 24 hours. We use bi-directional max-margin ranking loss with margin 0.05.</p><p>In Tab. 13, 14 and 15 we compare two our models: MDMMT(M c ALVYMTS) L9H8 irCSN152+audio and MDMMT(M c ALVYMTS) L9H8 CLIP+audio when they are trained from the pretrained model or not. In these three tables we present the same four models (no special finetuning for the target dataset) tested on different datasets.</p><p>As we can see in Tab. 13 the pretrained model increases R1 metric by 1% and R5 by 2%. The pretrained model also increase performance on ActivityNet dataset, see Tab. 14.</p><p>For R1 metric the improvement is about 2% and for R5 metric is about 4%. For LSMDC dataset, see <ref type="bibr">Tab 15,</ref><ref type="bibr"></ref> we have approximately the same results with and without pretraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Radius of the ball represent the "information size" of dataset. The biggest balls have more diversity in data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>is made with a simple algorithm. We take the original training procedure of MMT and for a given dataset we change the number of examples that will be shown to a network during training. We define the radius of the ball as the number of training examples after which the performance gets saturated (i.e. increasing the training time does not give the better model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Increasing R@5 metric on the MSRVTT full clean split while enriching the train part. Increasing R@5 metric on the ActivityNet test set while enriching the train part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Increasing R@5 metric on the LSMDC test set while enriching the train part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>+...+s tq +K−1,tg +K−1 ij K . The sum in the numerator is the sum of diagonal elements started with s tqtj ij . We define the intersection score between (q i , g j ) as S ij = max a=1,...,si−K b=1,...,pj −K S ab ij and the corresponding video segments as (a, a + K), (b, b + K) where a, b = argmax a=1,...,si−K b=1,...,pj −K S ab ij</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Suppose we have seen N + M pairs and have found M pairs with duplicates. The total number of pairs with duplicates can be estimated as M/F −1 (N ). By the definition F (x) connects the fraction of found positive pairs with the number of seen negative pairs. The value F −1 (N ) represents approximation of the fraction of found positive pairs. So if we know, that M is approximately 100 * F −1 (N )% of positive pairs, then we can approximately compute 100% of positive pairs as M/F −1 (N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The left image is taken from the MSRVTT test split and the right one from MSRVTT Train. The numbers in the upper left corner represent the MSRVTT video ID. The faces are blurred in order to avoid legal claims. # first element is highest P = np . sort ( P )[:: -1] # Pos N = np . sort ( N )[:: -1] # Neg xs = [] ys = [] for x , p in enumerate ( P ): # how many negative scores # greater than p ? j = np . searchsorted (N , p ) xs . append ( x ) ys . append ( j ) Listing 1: Numpy pseudocode for building the search curve F (x) C.1.3 Best 2D feature extractor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Search curves F for different pretrained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Internet and captions being created by humans have quite a rich structure.</figDesc><table><row><cell></cell><cell cols="2">Num Num</cell><cell>Num</cell><cell>Has</cell></row><row><cell>Dataset</cell><cell cols="2">video pairs</cell><cell>unique</cell><cell>YouTube</cell></row><row><cell></cell><cell></cell><cell></cell><cell>captions</cell><cell>Id</cell></row><row><cell>MSRVTT</cell><cell>10k</cell><cell>200k</cell><cell>167k</cell><cell>Yes</cell></row><row><cell>ActivityNet</cell><cell>14k</cell><cell>70k</cell><cell>69k</cell><cell>Yes</cell></row><row><cell>LSMDC</cell><cell cols="2">101k 101k</cell><cell>101k</cell><cell>No</cell></row><row><cell>TwitterVines</cell><cell>6.5k</cell><cell>23k</cell><cell>23k</cell><cell>No</cell></row><row><cell>YouCook2</cell><cell>1.5k</cell><cell>12k</cell><cell>12k</cell><cell>Yes</cell></row><row><cell>MSVD</cell><cell>1.5k</cell><cell>80k</cell><cell>64k</cell><cell>Yes</cell></row><row><cell>TGIF</cell><cell cols="2">102k 125k</cell><cell>125k</cell><cell>No</cell></row><row><cell>Sum above</cell><cell cols="2">236k 611k</cell><cell>561k</cell><cell>-</cell></row><row><cell cols="3">SomethingV2 193k 193k</cell><cell>124k</cell><cell>No</cell></row><row><cell>Sum above</cell><cell cols="2">429k 804k</cell><cell>685k</cell><cell>-</cell></row></table><note>). In total we increase the number of video segments by 40 times and the number of unique captions by 4 times compared with MSRVTT dataset. In Tab. 1 we summarize the sizes of used datasets. We separate SomethingV2 dataset from all other datasets because: 1. all video segments are created artificially, 2. the structure of text captions is quite limited. At the same time videos for all other datasets are collected from the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>during training a network does not see the required number of examples from this dataset. For experiments in this section we use MMT with the only motion modality. Embeddings for the motion modality are computed with irCSN152 pretrained on IG65M. All configurations are trained with 50 epochs and different number examples per epoch. The initial learning rate is 5e-5. After each epoch we multiply learning rate by 0.95. The MALVYMTS (see Tab. 2 for abbreviations.) configuration is trained with 150K examples per epoch. Configurations with the less number of datasets are trained with the less number of examples per epoch. The number of examples per epoch can be represented as a product of 150K by a sum of normalized weights (weights from Tab. 5 divided by a sum of all weights) for each dataset (the initial sum equals to 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>network is used. For the video encoding we use CLIP ViT-B/32 (RGB modality) and irCSN152 (motion modality) pretrained on IG65M dataset. The details about preprocessing videos for both networks are presented in Sec. A. Additionally we report separate results for motion + audio encoders and RGB + audio encoders because we do not know whether the IG65M or CLIP train database has a significant overlap with any of the test datasets or not. All our models presented in Tab. 4,7 and 8 are trained based on the pretrain HowTo100M model. We present the details about pretraining in Sec. E. ±0.1 24.0 ±0.2 34.9 ±0.2 129.6 ±1.0 23.7 ±0.5 SlowFast 32x2 R101 Kinetics 600 9.3 ±0.1 27.5 ±0.1 39.1 ±0.1 110.8 ±1.1 18.7 ±0.5 ipCSN152 IG65M 9.5 ±0.1 27.9 ±0.2 39.6 ±0.2 106.1 ±1.1 18.0 ±0.0 ipCSN152 IG65M → K400 8.3 ±0.1 25.2 ±0.1 36.5 ±0.2 124.3 ±0.2 21.0 ±0.0 ipCSN152 Sports1M 7.4 ±0.2 22.4 ±0.1 32.7 ±0.2 140.6 ±1.0 27.0 ±0.0 ipCSN152 Sports1M → K400 7.8 ±0.1 24.2 ±0.1 35.2 ±0.1 129.9 ±0.2 23.0 ±0.0 irCSN152 IG65M 9.5 ±0.1 27.9 ±0.2 39.5 ±0.2 105.5 ±0.4 18.0 ±0.0 irCSN152 IG65M → K400 8.4 ±0.1 25.3 ±0.1 36.5 ±0.2 120.4 ±0.4 21.0 ±0.0 irCSN152 Sports1M 6.9 ±0.1 21.6 ±0.1 31.6 ±0.1 141.9 ±0.4 28.7 ±0.5 irCSN152 Sports1M → K400 7.7 ±0.1 24.1 ±0.1 35.1 ±0.1 127.6 ±0.6 23.0 ±0.1 18.5 ±0.1 27.8 ±0.1 178.5 ±1.5 37.7 ±0.9 ±0.1 18.1 ±0.1 27.3 ±0.1 184.1 ±1.2 39.3 ±0.5 r(2+1)d 152 Sports1M → K400 5.3 ±0.1 17.3 ±0.1 26.0 ±0.1 193.4 ±3.6 42.3 ±0.5 ±0.2 27.2 ±0.2 38.7 ±0.2 108.1 ±0.0 19.0 ±0.2 25.3 ±0.3 36.7 ±0.1 120.8 ±0.7 21.0 ±0.</figDesc><table><row><cell>Video expert</cell><cell>Dataset</cell><cell>Text → Video R@1↑ R@5↑ R@10↑ MnR↓</cell><cell>MdR↓</cell></row><row><cell>s3d</cell><cell>Kinetics 600</cell><cell cols="2">7.7 ±0.0</cell></row><row><cell cols="4">r(2+1)d 152 5.7 r(2+1)d 152 IG65M IG65M → K400 5.5 r(2+1)d 34 IG65M 9.1 ±0.0</cell></row><row><cell>r(2+1)d 34</cell><cell>IG65M → K400</cell><cell>8.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">The results for MSRVTT are presented in Tab. 7. As</cell></row><row><cell></cell><cell></cell><cell cols="2">we can see our solution MDMMT(MALVYMTS) L9H8</cell></row><row><cell></cell><cell></cell><cell cols="2">CLIP+irCSN152+audio significantly outperforms all pre-</cell></row><row><cell></cell><cell></cell><cell>6</cell><cell></cell></row></table><note>0 CLIP CLIP 14.4 ±0.1 37.4 ±0.3 50.2 ±0.3 70.3 ±0.3 10.3 ±0.5 s3dg MIL-NCE HowTo100M 8.6 ±0.4 26.3 ±0.5 37.9 ±0.7 104.4 ±2.2 19.3 ±0.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>±0.1 38.3 ±0.1 51.5 ±0.3 92.4 ±2.3 10.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio 17.7 ±0.1 41.6 ±0.3 54.3 ±0.2 76.0 ±1.0 8.3 ±0.5 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+irCSN152+audio 20.1 ±0.5 45.1 ±0.5 58.0 ±0.6 70.8 ±0.1 7.0 ±0.0</figDesc><table><row><cell>model</cell></row></table><note>Comparison of the best available pretrain models as the motion experts for MMT. IG65M → K400 means that model was trained on IG65M and then fine tuned on Kinetics400. Results for each experiment are computed over three runs with random seeds. The results are reported on MSRVTT full clean split.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test results on our split (see Sec. 2.1) on ActivityNet.</figDesc><table><row><cell>Dataset</cell><cell>Weight</cell></row><row><cell>MSRVTT</cell><cell>140</cell></row><row><cell>ActivityNet</cell><cell>100</cell></row><row><cell>LSMDC</cell><cell>70</cell></row><row><cell>Twitter Vines</cell><cell>60</cell></row><row><cell>YouCook2</cell><cell>9</cell></row><row><cell>MSVD</cell><cell>9</cell></row><row><cell>TGIF</cell><cell>102</cell></row><row><cell>Something V2</cell><cell>169</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>±0.1 32.0 ±0.2 26.5 ±0.7 M c ALVYMT 33.8 ±0.1 32.3 ±0.2 27.3 ±0.4 M c ALVYMTS 34.5 ±0.1 32.4 ±0.5 27.4 ±0.6 See abbreviations for the first column in Tab. 2. The first three rows M c ,A,L report the quality of models trained on a single domain, and tested on other domains. Italic means that the model did not see data from this domain during training. In this table the only motion modality (irCSN152) is used. outperforms the original MMT (the motion, the RGB and the audio and 4 other modalities) by 8.7%, 10.5% and 14.4</figDesc><table><row><cell>Dataset</cell><cell cols="3">Test Text → Video R@5 ↑ MSRVTT ActivityNet LSMDC</cell></row><row><cell>M c</cell><cell cols="2">29.0 ±0.2 13.4 ±0.3</cell><cell>12.9 ±0.6</cell></row><row><cell>A</cell><cell cols="2">14.7 ±0.1 30.9 ±0.6</cell><cell>10.4 ±0.3</cell></row><row><cell>L</cell><cell>8.8 ±0.1</cell><cell>7.2 ±0.2</cell><cell>24.7 ±0.6</cell></row><row><cell>M c ALV</cell><cell>32.1</cell><cell></cell><cell></cell></row></table><note>(R@5) on full, 1k-A and 1k-B correspondingly.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>±0.1 29.0 ±0.3 41.2 ±0.2 86.8 ±0.3 16.0 ±0.0 MMT (M) 7mod [8] 10.7 ±0.2 31.1 ±0.1 43.4 ±0.2 88.2 ±0.7 15.0 ±0.0 ±0.1 38.8 ±0.1 51.1 ±0.2 76.0 ±0.7 10.0 ±0.0 Ours MDMMT(MALVYMTS) L9H8 CLIP+audio 21.7 ±0.2 47.6 ±0.3 59.8 ±0.1 55.9 ±0.2 6.0 ±0.0 Ours MDMMT(MALVYMTS) L9H8 CLIP+irCSN152+audio 23.1 ±0.1 49.8 ±0.1 61.8 ±0.1 52.8 ±0.2 6.0 ±0.0 ±0.1 30.2 ±0.4 42.3 ±0.2 89.4 ±0.6 15.7 ±0.5 Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio 15.8 ±0.1 38.9 ±0.1 51.0 ±0.1 76.4 ±0.5 10.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio 21.5 ±0.1 47.4 ±0.2 59.6 ±0.1 57.7 ±0.4 6.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+irCSN152+audio 22.8 ±0.2 49.5 ±0.1 61.5 ±0.1 53.8 ±0.3 6.0 ±0.0 ±1.0 57.1 ±1.0 69.6 ±0.2 24.0 ±0.8 4.0 ±0.0 ±0.1 60.4 ±1.2 71.8 ±1.0 24.0 ±0.4 3.0 ±0.0 Ours MDMMT(M 1k-A ALVYMTS) L9H8 CLIP+audio 38.9 ±1.0 68.3 ±0.7 78.8 ±0.2 17.3 ±0.5 2.0 ±0.0 Ours MDMMT(M 1k-A ALVYMTS) L9H8 CLIP+irCSN152+audio 38.9 ±0.6 69.0 ±0.1 79.7 ±0.6 16.5 ±0.4 2.0 ±0.0 ±0.7 46.0 ±0.4 60.7 ±0.2 35.3 ±1.1 7.0 ±0.0 ±0.5 54.4 ±0.8 68.0 ±0.5 26.6 ±0.2 4.7 ±0.5 ±0.9 58.8 ±0.3 71.2 ±0.3 28.5 ±0.5 3.7 ±0.5 Ours MDMMT(M 1k-B ALVYMTS) L9H8 CLIP+audio 35.1 ±0.1 66.5 ±0.9 77.6 ±0.3 21.5 ±0.4 2.7 ±0.5 Ours MDMMT(M 1k-B ALVYMTS) L9H8 CLIP+irCSN152+audio 37.4 ±1.5 68.8 ±0.4 79.4 ±0.4 21.3 ±0.4 2.0 ±0.0</figDesc><table><row><cell>model</cell><cell>split</cell><cell cols="4">MSRVTT text → video R@1↑ R@5↑ R@10↑ MnR↓</cell><cell>MdR↓</cell></row><row><cell>Random baseline</cell><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.3</cell><cell>1500</cell><cell>1500</cell></row><row><cell>VSE [24]</cell><cell></cell><cell>5.0</cell><cell>16.4</cell><cell>24.6</cell><cell>-</cell><cell>47</cell></row><row><cell>VSE++ [24]</cell><cell></cell><cell>5.7</cell><cell>17.1</cell><cell>24.8</cell><cell>-</cell><cell>65</cell></row><row><cell>Multi Cues [24]</cell><cell></cell><cell>7.0</cell><cell>20.9</cell><cell>29.7</cell><cell>-</cell><cell>38</cell></row><row><cell>W2VV [5]</cell><cell></cell><cell>6.1</cell><cell>18.7</cell><cell>27.5</cell><cell>-</cell><cell>45</cell></row><row><cell>Dual Enc. [6]</cell><cell></cell><cell>7.7</cell><cell>22.0</cell><cell>31.8</cell><cell>-</cell><cell>32</cell></row><row><cell cols="3">full 10.0 CLIP [27] CE [19] 15.1</cell><cell>31.8</cell><cell>40.4</cell><cell>184.2</cell><cell>21</cell></row><row><cell>CLIP agg [26]</cell><cell></cell><cell>21.5</cell><cell>41.1</cell><cell>50.4</cell><cell>-</cell><cell>4</cell></row><row><cell cols="3">Ours MDMMT(MALVYMTS) L9H8 irCSN152+audio 15.7 MMT (M c ) 7mod [8] full clean 10.4 Random baseline 0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500.0</cell><cell>500.0</cell></row><row><cell>JSFusion [40]</cell><cell></cell><cell>10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>-</cell><cell>13</cell></row><row><cell>E2E [22]</cell><cell></cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>-</cell><cell>29.5</cell></row><row><cell>HT [23]</cell><cell></cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>-</cell><cell>9</cell></row><row><cell cols="7">CE [19] CLIP [27] MMT (M 1k-A ) 7mod [8] 26.6 AVLnet[30] 20.9 ±1.2 48.8 ±0.6 62.4 ±0.8 28.2 ±0.8 6.0 ±0.0 22.5 44.3 53.7 61.7 8 1k-A 27.1 55.6 66.6 -4</cell></row><row><cell>SSB [25]</cell><cell></cell><cell>30.1</cell><cell>58.5</cell><cell>69.3</cell><cell>-</cell><cell>3.0</cell></row><row><cell>CLIP agg [26]</cell><cell></cell><cell>31.2</cell><cell>53.7</cell><cell>64.2</cell><cell>-</cell><cell>4</cell></row><row><cell cols="3">Ours MDMMT(M 1k-A ALVYMTS) L9H8 irCSN152+audio 31.3 Random baseline 0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500.0</cell><cell>500.0</cell></row><row><cell>MEE [21]</cell><cell></cell><cell>13.6</cell><cell>37.9</cell><cell>51.0</cell><cell>-</cell><cell>10.0</cell></row><row><cell>JPose [37]</cell><cell></cell><cell>14.3</cell><cell>38.1</cell><cell>53.0</cell><cell>-</cell><cell>9</cell></row><row><cell>MEE-COCO [21]</cell><cell></cell><cell>14.2</cell><cell>39.2</cell><cell>53.8</cell><cell>-</cell><cell>9.0</cell></row><row><cell cols="3">1k-B 18.2 MMT (M 1k-B ) 7mod [8] CE [19] 24.5 CLIP [27] 24.5</cell><cell>46.2</cell><cell>56.8</cell><cell>60.9</cell><cell>7</cell></row><row><cell>Ours MDMMT(M 1k-B ALVYMTS) L9H8 irCSN152+audio</cell><cell></cell><cell>28.8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on MSRVTT dataset. ±0.4 26.9 ±1.1 34.8 ±2.0 96.8 ±5.0 25.3 ±3.1 ±0.7 38.5 ±0.4 47.9 ±0.7 58.0 ±1.1 12.3 ±0.5</figDesc><table><row><cell>model</cell></row></table><note>9 ±0.1 29.9 ±0.7 40.1 ±0.8 75.0 ±1.2 19.3 ±0.2 Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio 13.1 ±0.5 31.3 ±0.3 40.1 ±0.0 74.5 ±0.7 19.3 ±0.5 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio 17.2 ±0.6 34.9 ±0.4 45.3 ±1.0 65.6 ±0.8 14.0 ±0.8 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+irCSN152+audio 18.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Test results on LSMDC public test (1k video) 9</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>,4,5 we present 6 models. Abbreviations M c ALV, M c ALVYMTS and M c ALVYMTS represent the same three models on these figures. The first model, called M c , is trained on the MSRVTT full clean split only, the second one, called A, is trained on ActivityNet only. And the third model, called L, is trained on LSMDC only. These three models are taken as baselines. Adding more datasets should be not worse than these baseline. The forth model is called M c ALV. This model is trained on the combination of MSRVTT, ActivityNet, LSMDC and TwitterVines. As we can see M c →M c ALV gives +3.07% on MSRVTT (full clean split), A→M c ALV gives +1.06% on ActivityNet, and L→M c ALV gives +1.77% on LSMDC. The next model is called M c ALVYMT and it is trained on combination of MSRVTT, ActivityNet, LSMDC, TwitterVines, YouCook2, MSVD, TGIF. The transitions M c →M c ALVYMT, A→M c ALVYMT, L→M c ALVYMT give +4.85%, +1.45% and +2.63% correspondingly. The last transitions M c →M c ALVYMTS, A→M c ALVYMTS, L→M c ALVYMTS slightly improve the performance on ActivityNet and LSMDC and significantly improve the performance on MSRVTT. Finally, the combination of all datasets gives +5.5% for MSRVTT, +1.47% for Activi-tyNet and +2.74% for LSMDC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>10% of the ActivityNet test) that may have duplicates in ActivityNet validation 1,2. Another problem with the Kinetics dataset is that many motion models are pretrained on it.</figDesc><table><row><cell>video segments (</cell></row><row><cell>1.</cell></row><row><cell>Surprisingly we found that the MSRVTT test has a signifi-</cell></row><row><cell>cant overlap with the MSRVTT train part. This problem is</cell></row><row><cell>relevant for the full, 1k-A and 1k-B splits. The ActivityNet</cell></row><row><cell>dataset suffers from the same problem.</cell></row><row><cell>For large datasets like HowTo100M and Kinetics700 we</cell></row><row><cell>can not find the whole intersection, but we estimate the</cell></row><row><cell>approximate number of videos in the intersection. We</cell></row><row><cell>found that HowTo100M may have about 300 (10% of the</cell></row><row><cell>MSRVTT full test part) video segments that can be in the</cell></row><row><cell>MSRVTT full test part.</cell></row><row><cell>The similar situation is about Kinetics700 and ActivityNet</cell></row><row><cell>datasets. Kinetics700 may have approximately 500-600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets.</figDesc><table><row><cell cols="7">Figure 8: Web system used to find duplicates. Images on</cell></row><row><cell cols="7">the first and third row are not duplicates and the second</cell></row><row><cell cols="2">row contains duplicate.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dataset</cell><cell cols="6">M test train test train test train A L</cell></row><row><cell>M</cell><cell cols="2">114 223</cell><cell>6</cell><cell>10</cell><cell>0</cell><cell>0</cell></row><row><cell>A</cell><cell>10</cell><cell>6</cell><cell cols="2">127 163</cell><cell>0</cell><cell>0</cell></row><row><cell>L</cell><cell cols="2">6 2744</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>YouCook2</cell><cell cols="2">13 27</cell><cell>7</cell><cell>10</cell><cell>0</cell><cell>0</cell></row><row><cell>MSVD</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>TGIF</cell><cell>6</cell><cell>8</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Twitter Vines</cell><cell>3</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Kinetics700</cell><cell>4</cell><cell>5</cell><cell cols="2">456 464</cell><cell>0</cell><cell>0</cell></row><row><cell cols="5">HowTo100M 177 154 209 209</cell><cell>0</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>±0.1 31.1 ±0.1 30.2 ±0.4 1k-A 54.8 ±0.5 50.7 ±0.9 49.4 ±0.5 1k-B 51.1 ±0.9 46.1 ±0.1 46.4 ±0.6</figDesc><table><row><cell>split</cell><cell>no clean</cell><cell>by ID</cell><cell>by ID + manual</cell></row><row><cell>full</cell><cell>31.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>First stage. The leftmost column represents train parts of datasets, and the upper row represents test parts of datasets. Column "test" represents number of video segments in test part that have corresponding video in train part with the same YouTube ID. Column "train" represents number of video in train part that have corresponding pair in test part with the same ID. For example: if we combine M and YouCook2, we should remove 4 video from YouCook2 train.It is worth to mention that MSRVTT 1k-A test and 1k-Btest have a large overlap ratio by the YouTube ID with the 1k-A train and the 1k-B train parts correspondingly. Both splits have the overlap ratio of about 38% between the train part and the test part. We also emphasize that the original MSRVTT full split does not overlap by the YouTube ID between the test and train parts.</figDesc><table><row><cell>data</cell><cell></cell><cell>M</cell><cell cols="2">M 1k-a</cell><cell></cell><cell>M 1k-b</cell><cell>A</cell><cell></cell></row><row><cell>set</cell><cell>test</cell><cell cols="7">train test train test train test train</cell></row><row><cell>M</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="4">0 104 179 2</cell><cell>4</cell></row><row><cell cols="8">M 1k-a 2362 1990 372 415 827 1007 2</cell><cell>4</cell></row><row><cell cols="8">M 1k-b 1689 1367 563 634 380 407 2</cell><cell>4</cell></row><row><cell>A</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>K</cell><cell>5</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell cols="3">0 408 408</cell></row><row><cell>Y</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell cols="9">HT100M 147 117 39 38 57 53 175 175</cell></row><row><cell>MSVD</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Nvidia V100 32Gb GPUs, we train for 200 epochs where one epoch is defined as 80k examples on each GPU (in total network sees 640k examples on 8 GPUs per epoch). We use batch size 64 for each GPU and do not exchange embeddings between model pretr MSRVTT full clean text → video R@1↑ R@5↑ R@10↑ MnR↓ MdR↓ Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio yes 15.8 ±0.1 38.9 ±0.1 51.0 ±0.1 76.4 ±0.5 10.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio no 14.5 ±0.1 36.8 ±0.3 48.8 ±0.3 82.2 ±0.6 11.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio yes 21.5 ±0.1 47.4 ±0.2 59.6 ±0.1 57.7 ±0.4 6.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio no 20.0 ±0.1 45.1 ±0.1 57.3 ±0.1 63.1 ±0.1 7.0 ±0.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Performance on the MSRVTT full clean split with and without pretrained model (HowTo100m). ALVYMTS) L9H8 irCSN152+audio yes 15.1 ±0.1 38.3 ±0.1 51.5 ±0.3 92.4 ±2.3 10.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio no 12.0 ±0.1 33.7 ±0.4 46.3 ±0.3 119.9 ±2.1 13.0 ±0.0 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio yes 17.7 ±0.1 41.6 ±0.3 54.3 ±0.2 76.0 ±1.0 8.3 ±0.5 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio no 15.2 ±0.3 37.9 ±0.3 50.1 ±0.2 93.4 ±2.0 10.3 ±0.5</figDesc><table><row><cell>model</cell><cell>pretr</cell><cell>ActivityNet text → video R@1↑ R@5↑ R@10↑ MnR↓</cell><cell>MdR↓</cell></row><row><cell>Ours MDMMT(M c</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Performance on ActivityNet with and without pretrained model (HowTo100m). The performance reported for the text to video retrieval task on our own subset of the original ActivityNet test part. See Sec. 2.1 for details. ALVYMTS) L9H8 irCSN152+audio yes 13.1 ±0.5 31.3 ±0.3 40.1 ±0.0 74.5 ±0.7 19.3 ±0.5 Ours MDMMT(M c ALVYMTS) L9H8 irCSN152+audio no 12.6 ±0.7 30.2 ±1.5 39.6 ±0.9 76.1 ±0.8 19.7 ±1.3 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio yes 17.2 ±0.6 34.9 ±0.4 45.3 ±1.0 65.6 ±0.8 14.0 ±0.8 Ours MDMMT(M c ALVYMTS) L9H8 CLIP+audio no 16.2 ±1.1 35.4 ±1.3 45.1 ±0.7 64.9 ±1.9 14.7 ±0.5</figDesc><table><row><cell>model</cell><cell>pretr</cell><cell>LSMDC text → video R@1↑ R@5↑ R@10↑ MnR↓</cell><cell>MdR↓</cell></row><row><cell>Ours MDMMT(M c</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Performance on LSMDC with and without pretrained model (HowTo100m).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Andrey Ivanyuta and other colleagues from Intelligent Systems and Data Science Lab for helping to find the overlap between datasets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TRECVID 2020: comprehensive campaign for evaluating video retrieval tasks across multiple application domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID 2020. NIST, USA. 2020</title>
		<meeting>TRECVID 2020. NIST, USA. 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P11-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019</title>
		<imprint/>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting Visual Features From Text for Image and Video Caption Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2018.2832602</idno>
		<ptr target="http://dx.doi.org/10.1109/TMM.2018.2832602" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dual Encoding for Zero-Example Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">SlowFast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-modal Transformer for Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10639</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00561</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04261</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CNN Architectures for Large-Scale Audio Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09430</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
	<note>NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Kinetics Human Action Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense-Captioning Events in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TGIF: A New Dataset and Benchmark on Animated GIF Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Weakly Supervised Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kumar Mahajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Straightforward Framework For Video Retrieval Using CLIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús Andrés</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Marín</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12443</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>In: Image 2 (</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Dataset for Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02530</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03705[cs.CV</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Learning Audio-Visual Language Representations from Instructional Videos. 2020</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10864</idno>
		<title level="m">A Short Note on the Kinetics-700-2020 Human Action Dataset. 2020</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Joint Model for Video and Language Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning Language-Visual Embedding for Movie Understanding with Natural-Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11248</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video Classification with Channel-Separated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02811</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03477</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Joint Sequence Fusion Model for Video Question Answering and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Places: A 10 million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02834</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
