<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<email>k.zhou@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Key Lab of Computer Vision and Virtual Reality</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Key Lab of Computer Vision and Virtual Reality</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decisionmaking process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learningbased framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other stateof-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Driven by the exponential growth in the amount of online videos in recent years, research in video summarization has gained increasing attention, leading to various methods proposed to facilitate large-scale video browsing <ref type="bibr" target="#b4">(Gygli et al. 2014;</ref><ref type="bibr" target="#b4">Gygli, Grabner, and Van Gool 2015;</ref><ref type="bibr" target="#b12">Zhang et al. 2016a;</ref><ref type="bibr" target="#b9">Panda and Roy-Chowdhury 2017;</ref><ref type="bibr" target="#b9">Mahasseni, Lam, and Todorovic 2017;</ref><ref type="bibr" target="#b9">Potapov et al. 2014)</ref>.</p><p>Recently, recurrent neural network (RNN), especially with the long short-term memory (LSTM) cell <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref>, has been exploited to model the sequential patterns in video frames, as well as to tackle the end-to-end training problem. <ref type="bibr" target="#b12">Zhang et al. (Zhang et al. 2016b</ref>) proposed a deep architecture that combines a bidirectional LSTM network with a Determinantal Point Process (DPP) module that increases diversity in summaries, referring to as DPP-LSTM. They trained DPP-LSTM with super-vised learning, using both video-level summaries and framelevel importance scores. At test time, DPP-LSTM predicts importance scores and outputs feature vectors simultaneously, which are together used to construct a DPP matrix. Due to the DPP modeling, DPP-LSTM needs to be trained in a two-stage manner.</p><p>Although DPP-LSTM <ref type="bibr" target="#b12">(Zhang et al. 2016b</ref>) has shown state-of-the-art performances on several benchmarks, we argue that supervised learning cannot fully explore the potential of deep networks for video summarization because there does not exist a single ground truth summary for a video. This is grounded by the fact that humans have subjective opinions on which parts of a video should be selected as the summary. Therefore, devising more effective summarization methods that rely less on labels is still in demand.</p><p>Mahasseni et al. <ref type="bibr" target="#b9">(Mahasseni, Lam, and Todorovic 2017)</ref> developed an adversarial learning framework to train DPP-LSTM. During the learning process, DPP-LSTM selects keyframes and a discriminator network is used to judge whether a synthetic video constructed by the keyframes is real or not, in order to enforce DPP-LSTM to select more representative frames. Although their framework is unsupervised, the adversarial nature makes the training unstable, which may result in model collapse. In terms of increasing diversity, DPP-LSTM cannot benefit maximally from the DPP module without the help of labels. Since a RNN-based encoder-decoder network following DPP-LSTM for video reconstruction requires pretraining, their framework requires multiple training stages, which is not efficient in practice.</p><p>In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN has an encoder-decoder architecture, where the encoder is a convolutional neural network (CNN) that performs feature extraction on video frames and the decoder is a bidirectional LSTM network that produces probabilities based on which actions are sampled to select frames. To train our DSN, we propose an end-to-end, reinforcement learning-based framework with a diversity-representativeness (DR) reward function that jointly accounts for diversity and representativeness of generated summaries, and does not rely on labels or user interactions at all.</p><p>The DR reward function is inspired by the general criteria of what properties a high-quality video summary should have. Specifically, the reward function consists of a diversity reward and a representativeness reward. The diversity reward measures how dissimilar the selected frames are to each other, while the representativeness reward computes distances between frames and their nearest selected frames, which is essentially the k-medoids problem. These two rewards complement to each other and work jointly to encourage DSN to produce diverse, representative summaries. The intuition behind this learning strategy is closely concerned with how humans summarize videos. To the best of our knowledge, this paper is the first to apply reinforcement learning to unsupervised video summarization.</p><p>The learning objective of DSN is to maximize the expected rewards over time. The rationale for using reinforcement learning (RL) to train DSN is two-fold. Firstly, we use RNN as part of our model and focus on the unsupervised setting. RNN needs to receive supervision signals at each temporal step but our rewards are computed over the whole video sequence, i.e., they can only be obtained after a sequence finishes. To provide supervision from a reward that is only available in the end of sequence, RL becomes a natural choice. Secondly, we conjecture that DSN can benefit more from RL because RL essentially aims to optimize the action (frame-selection) mechanism of an agent by iteratively enforcing the agent to take better and better actions. However, optimizing action mechanism is not particularly highlighted in a normal supervised/unsupervised setting.</p><p>As the training process does not require labels, our method can be fully unsupervised. To fit the case where labels are available, we further extend our unsupervised method to the supervised version by adding a supervised objective that directly maximizes the log-probability of selecting annotated keyframes. By learning the high-level concepts encoded in labels, our DSN can recognize globally important frames and produce summaries that highly align with human-annotated summaries.</p><p>We conduct extensive experiments on two datasets, SumMe <ref type="bibr" target="#b4">(Gygli et al. 2014)</ref> and TVSum , to quantitatively and qualitatively evaluate our method. The quantitative results show that our unsupervised method not only outperforms other state-of-the-art unsupervised alternatives, but also is comparable to or even superior than most of published supervised methods. More impressively, the qualitative results illustrate that DSN trained with our unsupervised learning algorithm can identify important frames that coincide with human selections.</p><p>The main contributions of this paper are summarized as follows: (1) We develop an end-to-end, reinforcement learning-based framework for training DSN, where we propose a label-free reward function that jointly accounts for diversity and representativeness of generated summaries. To the best of our knowledge, our work is the first to apply reinforcement learning to unsupervised video summarization.</p><p>(2) We extend our unsupervised approach to the supervised version to leverage labels. (3) We conduct extensive experiments on two benchmark datasets to show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Video summarization. Research in video summarization has been significantly advanced in recent years, leading to approaches of various characteristics. Lee et al. <ref type="bibr" target="#b8">(Lee, Ghosh, and Grauman 2012)</ref> identified important objects and people in summarizing videos. <ref type="bibr" target="#b4">Gygli et al. (Gygli et al. 2014</ref>) learned a linear regressor to predict the degree of interestingness of video frames and selected keyframes with the highest interestingness scores. <ref type="bibr" target="#b4">Gygli et al. (Gygli, Grabner, and Van Gool 2015)</ref> cast video summarization as a subset selection problem and optimized submodular functions with multiple objectives. Ejaz et al. <ref type="bibr" target="#b3">(Ejaz, Mehmood, and Baik 2013)</ref> applied an attention-modeling technique to extracting keyframes of visual saliency. Zhang et al. <ref type="bibr" target="#b12">(Zhang et al. 2016a</ref>) developed a nonparametric approach to transfer structures of known video summaries to new videos with similar topics. Auxiliary resources have also been exploited to aid the summarization process such as web images/videos <ref type="bibr" target="#b6">Khosla et al. 2013;</ref><ref type="bibr" target="#b1">Chu, Song, and Jaimes 2015)</ref> and category information <ref type="bibr" target="#b9">(Potapov et al. 2014</ref>). Most of these non-deep summarization methods processed video frames independently, thus ignoring the inherent sequential patterns. Moreover, non-deep summarization methods usually do not support end-to-end training, which causes extra costs at test time. To address the aforementioned issues, we model video summarization via a deep RNN to capture longterm dependencies in video frames, and propose a reinforcement learning-based framework to train the network end to end.</p><p>Reinforcement learning (RL). RL has become an increasingly popular research area due to its effectiveness in various tasks. Mnih et al. <ref type="bibr" target="#b9">(Mnih et al. 2013</ref>) successfully approximated Q function with a deep CNN, and enabled their agent to beat a human expert in several Atari games. Later on, many researchers have applied RL algorithms to vision-related applications such as image captioning <ref type="bibr" target="#b11">(Xu et al. 2015)</ref> and person re-identification <ref type="bibr" target="#b7">(Lan et al. 2017</ref>). In the domain of video summarization, our work is not the first to use RL. Previously, Song et al. <ref type="bibr" target="#b10">(Song et al. 2016</ref>) has applied RL to training a summarization network for selecting category-specific keyframes. Their learning framework requires keyframe-labels and category information of training videos. However, our work significantly differs from the work of Song et al. and other RL-based work in the way that labels or user interactions are not required at all during the learning process, which is attributed to our novel reward function. Therefore, our summarization method can be fully unsupervised and is more practical to be deployed for largescale video summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach</head><p>We formulate video summarization as a sequential decisionmaking process. In particular, we develop a deep summarization network (DSN) to predict probabilities for video frames and make decisions on which frames to select based on the predicted probability distributions. We present an end-to-end, reinforcement learning-based framework for training our DSN, where we design a diversity- is computed based on the quality of the summary, i.e., diversity and representativeness.</p><formula xml:id="formula_0">LSTM LSTM CNN Videos V 1 V 2 V M . . . LSTM LSTM LSTM LSTM p t 1 p t p t+1 Deep Summarization Network (DSN) video V i = {v t } T t=1 summary S = {v yi |a yi = 1, i = 1, 2, ...} actions A = {a t |a t 2 {0, 1}, t = 1, ..., T } R(S) = R div |{z} Eq.(3) + R rep |{z} Eq.(5) Reward Function reward R(S) BiRNN</formula><p>representativeness reward function, which directly assesses how diverse and representative the generated summaries are. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overall learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Summarization Network</head><p>We adopt the encoder-decoder framework for our deep summarization network (DSN). The encoder is a convolutional neural network (CNN) that extracts visual features {x t } T t=1 from the input video frames {v t } T t=1 with the length T . The decoder is a bidirectional recurrent neural network (BiRNN) topped with a fully connected (FC) layer. The BiRNN takes as input the entire visual features {x t } T t=1 and produces corresponding hidden states {h t } T t=1 . Each h t is the concatenation of the forward hidden state h f t and the backward hidden state h b t , which encapsulate the future information and the past information with a strong emphasis on the parts surrounding the t th frame. The FC layer that ends with the sigmoid function predicts for each frame a probability p t , from which a frame-selection action a t is sampled:</p><formula xml:id="formula_1">p t = σ(W h t ), (1) a t ∼ Bernoulli(p t ),<label>(2)</label></formula><p>where σ represents the sigmoid function, a t ∈ {0, 1} indicates whether the t th frame is selected or not. The bias in Eq.</p><p>(1) is omitted for brevity. A video summary is composed of the selected frames, S = {v yi |a yi = 1, i = 1, 2, ...}.</p><p>In practice, we use the GoogLeNet (Szegedy et al. 2015) pretrained on ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>) as the CNN model. The visual feature vectors {x t } T t=1 are extracted from the penultimate layer of the GoogLeNet. For the RNN cell, we employ long short-term memory (LSTM) to enhance RNN's ability for capturing long-term dependencies in video frames. During training, we only update the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversity-Representativeness Reward Function</head><p>During training, DSN will receive a reward R(S) that evaluates the quality of generated summaries, and the objective of DSN is to maximize the expected rewards over time by producing high-quality summaries. In general, a high-quality video summary is expected to be both diverse and representative of the original video so that temporal information across the entire video can be maximally preserved. To this end, we propose a novel reward that assesses the degree of diversity and representativeness of generated summaries. The proposed reward is composed of a diversity reward R div and a representativeness reward R rep , which we detail as follows.</p><p>Diversity reward. We evaluate the degree of diversity of a generated summary by measuring the dissimilarity among the selected frames in the feature space. Let the indices of the selected frames be Y = {y i |a yi = 1, i = 1, ..., |Y|}, we compute R div as the mean of the pairwise dissimilarities among the selected frames:</p><formula xml:id="formula_2">R div = 1 |Y|(|Y| − 1) t∈Y t ∈Y t =t d(x t , x t ),<label>(3)</label></formula><p>where d(·, ·) is the dissimilarity function calculated by</p><formula xml:id="formula_3">d(x t , x t ) = 1 − x T t x t ||x t || 2 ||x t || 2 .<label>(4)</label></formula><p>Intuitively, the more diverse (or more dissimilar) the selected frames to each other, the higher the diversity reward that the agent can receive. However, Eq. (3) treats video frames as randomly permutable items which ignore the temporal structure inherent in sequential data. In fact, the similarity between two temporally distant frames should be ignored because they are essential to the storyline construction <ref type="bibr" target="#b4">(Gong et al. 2014)</ref>. To overcome this problem, we set d(x t , x t ) = 1 if |t − t | &gt; λ, where λ controls the degree of temporal distance. We will validate this hypothesis in the Experiments section.</p><p>Representativeness reward. This reward measures how well the generated summary can represent the original video. To this end, we formulate the degree of representativeness of a video summary as the k-medoids problem <ref type="bibr" target="#b4">(Gygli, Grabner, and Van Gool 2015)</ref>. In particular, we want the agent to select a set of medoids such that the mean of squared errors between video frames and their nearest medoids is minimal. Therefore, we define R rep as</p><formula xml:id="formula_4">R rep = exp(− 1 T T t=1 min t ∈Y ||x t − x t || 2 ).<label>(5)</label></formula><p>With this reward, the agent is encouraged to select frames that are close to the cluster centers in the feature space. An alternative formulation of R rep can be the inverse reconstruction errors achieved by the selected frames, but this formulation is too computationally expensive.</p><p>Diversity-representativeness reward. R div and R rep complement to each other and work jointly to guide the learning of DSN:</p><formula xml:id="formula_5">R(S) = R div + R rep .<label>(6)</label></formula><p>During training, R div and R rep are similar in the order of magnitude. In fact, it is non-trivial to keep R div and R rep at the same order of magnitude during training, thus none of them would dominate in gradient computation. We give zero reward to DSN when no frames are selected, i.e., the sampled actions are all zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training with Policy Gradient</head><p>The goal of our summarization agent is to learn a policy function π θ with parameters θ by maximizing the expected rewards</p><formula xml:id="formula_6">J(θ) = E p θ (a 1:T ) [R(S)],<label>(7)</label></formula><p>where p θ (a 1:T ) denotes the probability distributions over possible action sequences, and R(S) is computed by Eq. <ref type="formula" target="#formula_5">(6)</ref>. π θ is defined by our DSN. Following the REINFORCE algorithm proposed by Williams <ref type="bibr" target="#b11">(Williams 1992)</ref>, we can compute the derivative of the objective function J(θ) w.r.t. the parameters θ as</p><formula xml:id="formula_7">θ J(θ) = E p θ (a 1:T ) [R(S) T t=1 θ log π θ (a t |h t )],<label>(8)</label></formula><p>where a t is the action taken by DSN at time t and h t is the hidden state from the BiRNN. Since Eq. (8) involves the expectation over highdimensional action sequences, which is hard to compute directly, we approximate the gradient by running the agent for N episodes on the same video and then taking the average gradient</p><formula xml:id="formula_8">θ J(θ) ≈ 1 N N n=1 T t=1 R n θ log π θ (a t |h t ),<label>(9)</label></formula><p>where R n is the reward computed at the n th episode. Eq. <ref type="formula" target="#formula_8">(9)</ref> is also known as the episodic REINFORCE algorithm. Although the gradient in Eq. (9) is a good estimate, it may contain high variance which will make the network hard to converge. A common countermeasure is to subtract the reward by a constant baseline b, so the gradient becomes</p><formula xml:id="formula_9">θ J(θ) ≈ 1 N N n=1 T t=1 (R n − b) θ log π θ (a t |h t ),<label>(10)</label></formula><p>where b is simply computed as the moving average of rewards experienced so far for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>Since selecting more frames will also increase the reward, we impose a regularization term on the probability distributions p 1:T produced by DSN in order to constrain the percentage of frames selected for the summary. Inspired by (Mahasseni, Lam, and Todorovic 2017), we minimize the following term during training,</p><formula xml:id="formula_10">L percentage = || 1 T T t=1 p t − || 2 ,<label>(11)</label></formula><p>where determines the percentage of frames to be selected.</p><p>In addition, we also add the 2 regularization term on the weight parameters θ to avoid overfitting</p><formula xml:id="formula_11">L weight = i,j θ 2 i,j .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>We optimize the policy function's parameters θ via stochastic gradient-based method. By combing the gradients computed from Eq. (10), Eq. (11) and Eq. (12), we update θ as</p><formula xml:id="formula_12">θ = θ − α θ (−J + β 1 L percentage + β 2 L weight ),<label>(13)</label></formula><p>where α is learning rate, and β 1 and β 2 are hyperparameters that balance the weighting. In practice, we use Adam (Kingma and Ba 2014) as the optimization algorithm. As a result of learning, the logprobability of actions taken by the network that have led to high rewards is increased, while that of actions that have resulted in low rewards is decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension to Supervised Learning</head><p>Given the keyframe indices for a video, Y * = {y * i |i = 1, ..., |Y * |}, we use Maximum Likelihood Estimation (MLE) to maximize the log-probability of selecting keyframes specified by Y * , log p(t; θ) where t ∈ Y * . p(t; θ) is computed from Eq. (1). The objective is formalized as</p><formula xml:id="formula_13">L MLE = t∈Y * log p(t; θ).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Generation</head><p>For a test video, we apply a trained DSN to predict the frame-selection probabilities as importance scores. We compute shot-level scores by averaging frame-level scores within the same shot. For temporal segmentation, we use KTS proposed by <ref type="bibr" target="#b9">(Potapov et al. 2014</ref>). To generate a summary, we select shots by maximizing the total scores while ensuring that the summary length does not exceed a limit, which is usually 15% of the video length. The maximization step is essentially the 0/1 Knapsack problem, which is known as NP-hard. We obtain a near-optimal solution via dynamic programming .</p><p>Besides evaluating generated summaries in the Experiments part, we also qualitatively analyze the raw predictions of DSN so as to exclude the effect of this summary generation step, by which we can better understand what DSN has learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setup</head><p>Datasets. We evaluate our methods on SumMe <ref type="bibr" target="#b4">(Gygli et al. 2014)</ref> and TVSum . SumMe consists of 25 user videos covering various topics such as holidays and sports. Each video in SumMe ranges from 1 to 6 minutes and is annotated by 15 to 18 persons, thus there are multiple ground truth summaries for each video. TVSum contains 50 videos, which include the topics of news, documentaries, etc. The duration of each video varies from 2 to 10 minutes. Similar to SumMe, each video in TVSum has 20 annotators that provide frame-level importance scores. Following <ref type="bibr" target="#b12">Zhang et al. 2016b</ref>), we convert importance scores to shot-based summaries for evaluation. In addition to these two datasets, we exploit two other datasets, OVP 1 that has 50 videos and YouTube <ref type="bibr" target="#b2">(De Avila et al. 2011</ref>) that has 39 videos excluding cartoon videos, to evaluate our method in the settings where training data is augmented <ref type="bibr" target="#b12">(Zhang et al. 2016b;</ref><ref type="bibr" target="#b9">Mahasseni, Lam, and Todorovic 2017)</ref>.</p><p>Evaluation metric. For fair comparison with other approaches, we follow the commonly used protocol from <ref type="bibr" target="#b12">(Zhang et al. 2016b)</ref> to compute F-score as the metric to assess the similarity between automatic summaries and ground truth summaries. We also follow <ref type="bibr" target="#b12">(Zhang et al. 2016b)</ref> to deal with multiple ground truth summaries.</p><p>Evaluation settings. We use three settings as suggested in <ref type="bibr" target="#b12">(Zhang et al. 2016b</ref>) to evaluate our method. (1) Canonical: we use the standard 5-fold cross validation (5FCV), i.e., 80% of videos for training and the rest for testing. (2) Augmented: we still use the 5FCV but we augment the training data in each fold with OVP and YouTube. (3) Transfer: for a target dataset, e.g. SumMe or TVSum, we use the other three datasets as the training data to test the transfer ability of our model. Implementation details. We downsample videos by 2 fps as did in <ref type="bibr" target="#b12">(Zhang et al. 2016b</ref>). We set the temporal distance λ to 20, the in Eq. 11 to 0.5, and the number of episodes 1 Open video project: https://open-video.org/.</p><p>N to 5. The other hyperparameters α, β 1 and β 2 in Eq. (13) are optimized via cross-validation. We set the dimension of hidden state in the RNN cell to 256 throughout this paper. Training is stopped when it reaches a maximum number of epochs (60 in our case). Early stopping is executed when reward creases to increase for a period of time (10 epochs in our experiments). We implement our method based on Theano (Al-Rfou et al. 2016) 2 .</p><p>Comparison. To compare with other approaches, we implement Uniform sampling, K-medoids and Dictionary selection <ref type="bibr" target="#b4">(Elhamifar, Sapiro, and Vidal 2012)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>We first compare our method with several baselines that differ in learning objectives. Then, we compare our methods with current state-of-the-art unsupervised/supervised approaches in the three evaluation settings.</p><p>Comparison with baselines. We set the baseline models as the ones trained with R div only and R rep only, which are denoted by D-DSN and R-DSN, respectively. We represent the model trained with the two rewards jointly as DR-DSN. The model that is extended to the supervised version is denoted by DR-DSN sup . We also validate the effectiveness of the proposed technique (we call this λ-technique from now on) that ignores the distant similarity when computing R div . We represent the D-DSN trained without the λ-technique as D-DSN w/o λ . To verify that DSN can benefit more from reinforcement learning than from supervised learning, we add another baseline as the DSN trained with the cross entropy loss using keyframe annotations, where a confidence penalty <ref type="bibr" target="#b9">(Pereyra et al. 2017</ref>) is imposed on the output distributions as a regularization term. This model is denoted by DSN sup .  <ref type="table" target="#tab_0">Table 1</ref> reports the results of different variants of our method on SumMe and TVSum. We can see that DR-DSN (a) Example frames from video 18 in TVSum (indexed as in ).  , which justifies our assumption that DSN can benefit more from reinforcement learning than from supervised learning. By adding the supervision signals of L MLE <ref type="figure" target="#fig_0">(Eq. (14)</ref>) to DR-DSN, the summarization performances are further improved (1.7% improvements on SumMe and 0.9% improvements on TVSum). This is because labels encode the highlevel understanding of the video content, which is exploited by DR-DSN sup to learn more useful patterns.</p><p>The performances of R-DSN are slightly better than those of D-DSN on the two datasets, which is because diverse summaries usually contain redundant information that are irrelevant to the video subject. We observe that the performances of D-DSN are better than those of D-DSN w/o λ that does not consider temporally distant frames. When using the λ-technique in training, around 50% ∼ 70% of the distance matrix was set to 1 (varying across different videos) at the early stage. As the training epochs increased, the percentage went up too, eventually staying around 80% ∼ 90%. This makes sense because selecting temporally distant frames can lead to higher rewards and DSN is encouraged to do so with the diversity reward function.</p><p>Comparison with unsupervised approaches. <ref type="table" target="#tab_2">Table 2</ref> shows the results of DR-DSN against other unsupervised approaches on SumMe and TVSum. It can be seen that DR-DSN outperforms the other unsupervised approaches on both datasets by large margins. On SumMe, DR-DSN is 5.9% better than the current state-of-the-art, GAN dpp . On TVSum, DR-DSN substantially beats GAN dpp by 11.4%.</p><p>Although our reward functions are analogous to the objectives of GAN dpp in concepts, ours directly model diversity and representativeness of selected frames in the feature space, which is more useful to guide DSN to find good solutions. In addition, the training performances of DR-DSN are 40.2% on SumMe and 57.2% on TVSum, which suggest that the model did not overfit to the training data (note that we do not explicitly optimize the F-score metric in the training objective function). Comparison with supervised approaches. <ref type="table" target="#tab_4">Table 3</ref> reports the results of our supervised model, DR-DSN sup , and other supervised approaches. In terms of LSTM-based methods, our DR-DSN sup beats the others, i.e., Bi-LSTM, DPP-LSTM and GAN sup , by 1.0% ∼ 12.0% on SumMe and 3.2% ∼ 7.2% on TVSum, respectively. It is also interesting to see that the summarization performance of our unsupervised method, DR-DSN, is even superior than the state-ofthe-art supervised approach on TVSum (57.6 vs. 56.3), and is better than most of the supervised approaches on SumMe.  Besides the F-score for each prediction, we also compute cross-correlation (XCorr) for each pair of prediction and ground truth to give a quantitative measure of similarity over two series of 1D arrays. The higher the XCorr, the more similar two arrays are to each other.</p><p>These results strongly prove the efficacy of our learning framework.  <ref type="table" target="#tab_5">Table 4</ref> compares our methods with current stateof-the-art LSTM-based methods in the A and T settings. The results in the Canonical setting are also provided to exhibit the improvements obtained by increased training data. In the A setting, DR-DSN sup performs marginally better than GAN sup on SumMe (43.9 vs. 43.6), whereas it is defeated by GAN sup on TVSum (59.8 vs. 61.2). This may be because the LSTM model in GAN sup has more hidden units (1024 vs. our 256). In the T setting, DR-DSN sup performs the best on both datasets, suggesting that our model is able to transfer knowledge between datasets. Furthermore, it is interesting to see that our unsupervised model, DR-DSN, is superior or comparable with other methods in both settings. Overall, we firmly believe that by using a larger model and/or designing a better network architecture, we can obtain better summarization performances with our learning framework.</p><p>We also experiment with different gated RNN units, i.e., LSTM vs. <ref type="bibr">GRU (Cho et al. 2014)</ref>, and find that LSTM-based models consistently beat GRU-based models (see <ref type="table" target="#tab_6">Table 5</ref>). This may be interpreted as that the memory mechanism in LSTM has a higher degree of complexity, thus allowing more complex patterns to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>Video summaries. We provide qualitative results for an exemplar video that talks about a man making a spicy sausage   <ref type="figure" target="#fig_2">Figure 2</ref>. In general, all four methods produce high-quality summaries that span the temporal structure, with only small variations observed in some frames. The peak regions of ground truth are almost captured. Nevertheless, the summary produced by the supervised model, DR-DSN sup , is much closer to the complete storyline conveyed by the original video i.e., from food preparation to cooking. This is because DR-DSN sup benefits from labels that allow high-level concepts to be better captured. Predicted importance scores. We visualize the raw predictions by DR-DSN and DSN sup in <ref type="figure" target="#fig_3">Figure 3</ref>. By comparing predictions with ground truth, we can better understand in more depth how well DSN has learned. It is worth highlighting that the curves of importance scores predicted by the unsupervised model resemble those predicted by the supervised model in several parts. More importantly, these parts coincide with the ones also considered as important by humans. This strongly demonstrates that reinforcement learning with our diversity-representativeness reward function can well imitate the human-learning process and effectively teach DSN to recognize important frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a label-free reinforcement learning algorithm to tackle unsupervised video summarization. Extensive experiments on two benchmark datasets showed that using reinforcement learning with our unsupervised reward function outperformed other state-of-the-art unsupervised alternatives, and produced results comparable to or even superior than most supervised methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training deep summarization network (DSN) via reinforcement learning. DSN receives a video V i and takes actions A (i.e., a sequence of binary variables) on which parts of the video are selected as the summary S. The feedback reward R(S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>by ourselves. We retrieve results of other approaches including Video-MMR (Li and Merialdo 2010), Vsumm (De Avila et al. 2011), Web image (Khosla et al. 2013), Online sparse coding (Zhao and Xing 2014), Co-archetypal (Song et al. 2015), Interestingness (Gygli et al. 2014), Submodularity (Gygli, Grabner, and Van Gool 2015), Summary transfer (Zhang et al. 2016a), Bi-LSTM and DPP-LSTM (Zhang et al. 2016b), GAN dpp and GAN sup (Mahasseni, Lam, and Todorovic 2017) from published papers. Due to space limit, we do not include these citations in tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Video summaries generated by different variants of our approach for video 18 in TVSum. The light-gray bars in (b) to (e) correspond to ground truth importance scores, while the colored areas correspond to the selected parts by different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ground truth (top) and importance scores predicted by DR-DSN (middle) and DSN sup (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results (%) of different variants of our method on SumMe and TVSum.</figDesc><table><row><cell>Method</cell><cell cols="2">SumMe TVSum</cell></row><row><cell>DSN sup</cell><cell>38.2</cell><cell>54.5</cell></row><row><cell>D-DSN w/o λ</cell><cell>39.3</cell><cell>55.7</cell></row><row><cell>D-DSN</cell><cell>40.5</cell><cell>56.2</cell></row><row><cell>R-DSN</cell><cell>40.7</cell><cell>56.9</cell></row><row><cell>DR-DSN</cell><cell>41.4</cell><cell>57.6</cell></row><row><cell>DR-DSN sup</cell><cell>42.1</cell><cell>58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results (%) of unsupervised approaches on SumMe and TVSum. Our DR-DSN performs the best, especially in TVSum where it exhibits a huge advantage over others.</figDesc><table><row><cell>Method</cell><cell cols="2">SumMe TVSum</cell></row><row><cell>Video-MMR</cell><cell>26.6</cell><cell>-</cell></row><row><cell>Uniform sampling</cell><cell>29.3</cell><cell>15.5</cell></row><row><cell>K-medoids</cell><cell>33.4</cell><cell>28.8</cell></row><row><cell>Vsumm</cell><cell>33.7</cell><cell>-</cell></row><row><cell>Web image</cell><cell>-</cell><cell>36.0</cell></row><row><cell>Dictionary selection</cell><cell>37.8</cell><cell>42.0</cell></row><row><cell>Online sparse coding</cell><cell>-</cell><cell>46.0</cell></row><row><cell>Co-archetypal</cell><cell>-</cell><cell>50.0</cell></row><row><cell>GAN dpp</cell><cell>39.1</cell><cell>51.7</cell></row><row><cell>DR-DSN</cell><cell>41.4</cell><cell>57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results (%) of supervised approaches on SumMe and TVSum. Our DR-DSN sup performs the best.</figDesc><table><row><cell>Method</cell><cell cols="2">SumMe TVSum</cell></row><row><cell>Interestingness</cell><cell>39.4</cell><cell>-</cell></row><row><cell>Submodularity</cell><cell>39.7</cell><cell>-</cell></row><row><cell>Summary transfer</cell><cell>40.9</cell><cell>-</cell></row><row><cell>Bi-LSTM</cell><cell>37.6</cell><cell>54.2</cell></row><row><cell>DPP-LSTM</cell><cell>38.6</cell><cell>54.7</cell></row><row><cell>GAN sup</cell><cell>41.7</cell><cell>56.3</cell></row><row><cell>DR-DSN sup</cell><cell>42.1</cell><cell>58.1</cell></row><row><cell cols="3">Comparison in the Augmented (A) and Transfer (T)</cell></row><row><cell>settings.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results (%) of the LSTM-based approaches on SumMe and TVSum in the Canonical (C), Augmented (A) and Transfer (T) settings, respectively. DSN sup 42.1 43.9 42.6 58.1 59.8 58.9</figDesc><table><row><cell>Method</cell><cell>C</cell><cell>SumMe A</cell><cell>T</cell><cell>C</cell><cell>TVSum A</cell><cell>T</cell></row><row><cell>Bi-LSTM</cell><cell cols="6">37.6 41.6 40.7 54.2 57.9 56.9</cell></row><row><cell cols="7">DPP-LSTM 38.6 42.9 41.8 54.7 59.6 58.7</cell></row><row><cell>GAN dpp</cell><cell cols="2">39.1 43.4</cell><cell>-</cell><cell cols="2">51.7 59.5</cell><cell>-</cell></row><row><cell>GAN sup</cell><cell cols="2">41.7 43.6</cell><cell>-</cell><cell cols="2">56.3 61.2</cell><cell>-</cell></row><row><cell>DR-DSN</cell><cell cols="6">41.4 42.8 42.4 57.6 58.4 57.8</cell></row><row><cell>DR-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results (%) of using different gated recurrent units.</figDesc><table><row><cell>Method</cell><cell cols="4">SumMe LSTM GRU LSTM GRU TVSum</cell></row><row><cell>DR-DSN</cell><cell>41.4</cell><cell>41.2</cell><cell>57.6</cell><cell>56.7</cell></row><row><cell>DR-DSN sup</cell><cell>42.1</cell><cell>41.5</cell><cell>58.1</cell><cell>57.8</cell></row><row><cell>sandwich in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Codes are available on https://github.com/KaiyangZhou/vsummreinforce</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ke Zhang and Wei-Lun Chao for discussions of details of their paper <ref type="bibr" target="#b12">(Zhang et al. 2016b)</ref>. This work was supported in part by National Key Research and Development Program of China (2016YFC1400704) and National Natural Science Foundation of China (U1613211, 61633021).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theano: A python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Rfou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<idno>arXiv:1409.1259</idno>
	</analytic>
	<monogr>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cho et al. 2014</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video co-summarization: Video summarization by visual co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3584" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient visual attention based framework for extracting key frames from videos. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sapiro</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vidal ; Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3090" to="3098" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2698" to="2705" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning attention selection for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghosh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<idno>arXiv:1701.06548</idno>
	</analytic>
	<monogr>
		<title level="m">WIAMIS, 1-4. IEEE</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Category driven deep recurrent neural network for video summarization</title>
	</analytic>
	<monogr>
		<title level="m">ICMEW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>Xu et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Show, attend and tell: Neural image caption generation with visual attention</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Summary transfer: Exemplar-based subset selection for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno>Zhang et al. 2016b</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quasi real-time summarization for consumer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2513" to="2520" />
		</imprint>
	</monogr>
	<note>and Xing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
