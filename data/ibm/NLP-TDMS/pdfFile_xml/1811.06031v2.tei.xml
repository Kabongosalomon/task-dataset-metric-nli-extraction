<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
								<address>
									<addrLine>20 Jay Street</addrLine>
									<settlement>Brooklyn</settlement>
									<region>New York</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
							<email>thomas@huggingface.co</email>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
								<address>
									<addrLine>20 Jay Street</addrLine>
									<settlement>Brooklyn</settlement>
									<region>New York</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<email>sebastian@ruder.io</email>
							<affiliation key="aff1">
								<orgName type="department">Insight Research Centre</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aylien Ltd</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Harmony Court</orgName>
								<address>
									<addrLine>Harmony Row</addrLine>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent Natural Language Processing (NLP) models heavily rely on rich distributed representations (typically word or sentence embeddings) to achieve good performance. One example are so-called "universal representations" <ref type="bibr" target="#b10">(Conneau et al. 2017)</ref> which are expected to encode a varied set of linguistic features, transferable to many NLP tasks. This kind of rich word or sentence embeddings can be learned by leveraging the training signal from different tasks in a multi-task setting. It is known that a model trained in a multi-task framework can take advantage of inductive transfer between the tasks, achieving a better generalization performance <ref type="bibr" target="#b6">(Caruana 1993)</ref>. Recent works in sentence embeddings <ref type="bibr" target="#b40">(Subramanian et al. 2018;</ref><ref type="bibr" target="#b18">Jernite, Bowman, and Sontag 2017)</ref> indicate that complementary aspects of the sentence (e.g. syntax, sentence length, word order) should be encoded in order for the model to produce sentence embeddings that are able to generalize over a wide range of tasks. Complementary aspects in representations can be naturally encoded by training a model on a set of diverse Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Organization (EMD/NER) tasks, such as, machine translation, sentiment classification or natural language inference. Although, (i) the selection of this diverse set of tasks, as well as, (ii) the control of the interactions between them are of great importance, a deeper understanding of (i) and (ii) is missing as highlighted in the literature <ref type="bibr" target="#b7">(Caruana 1997;</ref><ref type="bibr" target="#b28">Mitchell 1980;</ref><ref type="bibr" target="#b36">Ruder 2017)</ref>. This work explores this line of research by combining, in a single model, four fundamental semantic NLP tasks: Named Entity Recognition, Entity Mention Detection (also sometimes referred as mention detection), Coreference Resolution and Relation Extraction. This selection of tasks is motivated by the inter-dependencies these tasks share. In <ref type="table" target="#tab_0">Table 1</ref>, we give three simple examples to exemplify the reasons why these tasks should benefit from each other. For instance, in the last example knowing that the company and Dell are referring to the same real world entity, Dell is more likely to be an organization than a person.</p><p>Several prior works <ref type="bibr" target="#b42">(Yang, Salakhutdinov, and Cohen 2016;</ref><ref type="bibr" target="#b5">Bingel and Søgaard 2017)</ref> avoid the question of the linguistic hierarchies between NLP tasks. We argue that some tasks (so-called "low level" tasks) are simple and require a limited amount of modification to the input of the model while other tasks (so-called "higher level" tasks) require a deeper processing of the inputs and likely a more complex architecture. Following <ref type="bibr" target="#b17">(Hashimoto et al. 2017;</ref><ref type="bibr" target="#b38">Søgaard and Goldberg 2016)</ref>, we therefore introduce a hierarchy between the tasks so that low level tasks are supervised at lower levels of the architecture while keeping more com-plex interactions at deeper layers. Unlike previous works <ref type="bibr" target="#b25">(Li and Ji 2014;</ref><ref type="bibr" target="#b29">Miwa and Bansal 2016)</ref>, our whole model can be trained end-to-end without any external linguistic tools or hand-engineered features while giving stronger results on both Relation Extraction and Entity Mention Detection.</p><p>Our main contributions are the following: (1) we propose a multi-task architecture combining four different tasks that have not been explored together to the best of our knowledge. This architecture uses neural networks and does not involve external linguistic tools or hand-engineered features. We also propose a new sampling strategy for multi-task learning, proportional sampling.</p><p>(2) We show that this architecture can lead to state-of-the art results on several tasks e.g. Named Entity Recognition, Relation Extraction and Entity Mention Detection while using simple models for each of these tasks. This suggests that the information encoded in the embeddings is rich and covers a variety of linguistic phenomena.</p><p>(3) We study and give insights on the influence of multi-task learning on (i) the speed of training and (ii) the type of biases learned in the hierarchical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In this section, we describe our model beginning at the lower levels of the architecture and ascending to the top layers. Our model introduces a hierarchical inductive bias between the tasks by supervising low-level tasks (that are assumed to require less knowledge and language understanding) at the bottom layers of the model architecture and supervising higher-level tasks at higher layers. The architecture of the model is shown in <ref type="figure">Figure 1</ref>. Following <ref type="bibr" target="#b17">(Hashimoto et al. 2017)</ref>, we use shortcut connections so that top layers can have access to bottom layer representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words embeddings</head><p>Our model encodes words w t of an input sentence s = (w 1 , w 2 , ..., w n ) as a combination of three different types of embeddings. We denote the concatenation of the these three embeddings g e . Pre-trained word embeddings: We use GloVe <ref type="bibr" target="#b30">(Pennington, Socher, and Manning 2014)</ref> pre-trained word level embeddings. These embeddings are fine-tuned during training. Pre-trained contextual word embeddings: We also use contextualized ELMo embeddings <ref type="bibr" target="#b31">(Peters et al. 2018</ref>). These word embeddings differ from GloVe word embeddings in that each token is represented by a vector that is a function of the whole sentence (a word can thus have different representations depending on the sentence it is extracted from). These representations are given by the hidden states of a bidirectional language model. ELMo embeddings have been shown to give state-of-the-art results in multiple NLP tasks <ref type="bibr" target="#b31">(Peters et al. 2018</ref>). Character-level word embeddings: Following <ref type="bibr" target="#b8">(Chiu and Nichols 2015;</ref><ref type="bibr" target="#b23">Lample et al. 2016</ref>), we use character-level word embeddings to extract character-level features. Specifically, we use a convolutional neural network (CNN) (followed by a max pooling layer) for the ease of training since Recurrent Neural Network-based encodings do not significantly outperform CNNs while being computationally more expensive to train <ref type="bibr" target="#b26">(Ling et al. 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer BiLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Random Field</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer BiLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Mention Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Random Field</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention + Pair Scorer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Scorer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GloVe ElMo CNN-extracted Char Features Encoder</head><p>Multi-layer BiLSTM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Multi-layer BiLSTM <ref type="figure">Figure 1</ref>: Diagram of the model architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition (NER)</head><p>The first layers of our model are supervised by Named Entity Recognition labels. NER aims to identify mentions of named entities in a sequence and classify them into predefined categories. In accordance with previous work <ref type="bibr" target="#b8">(Chiu and Nichols 2015;</ref><ref type="bibr" target="#b23">Lample et al. 2016</ref>) the tagging module contains an RNN-based encoding layer followed by a sequence tagging module based on a conditional random field <ref type="bibr" target="#b22">(Lafferty, McCallum, and Pereira 2001)</ref>. We use multi-layer bi-directional LSTMs (Long Short-Term Memory) as encoders. The encoders take as input the concatenated word embeddings g e and produce (sequence) embeddings g ner . Specifically, g ner are the concatentation of the backward and forward hidden states of the top layer of the biLSTMs, which are then fed to the sequence tagging layer. We adopt the BILOU (Beginning, Inside, Last, Outside, Unit) tagging scheme. The tagging decisions are modeled using a CRF, which explicitly reasons about interactions between neighbour tokens tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Mention Detection (EMD)</head><p>A second group of layers of our model are supervised using Entity Mention Detection labels. EMD is similar in spirit to NER but more general as it aims to identify all the mentions related to a real life entity, whereas NER only focuses on the named entities. Let us consider an example: [The men] P ERS held on [the sinking vessel] V EH until [the passenger ship] V EH was able to reach them from [Corsica] GP E . Here, NER annotations would only tag Corsica, while EMD requires a deeper understanding of the entities in the sentence.</p><p>We formulate Mention Detection as a sequence tagging task using a BILOU scheme. We use a multi-layer biLSTM followed by a CRF tagging layer. We adopt shortcut connections so that each layer can build on top of the representations extracted by the lower layers in a hierarchical fashion. The encoder thus takes as input the concatenation of the lower layer representations [g e , g ner ] and outputs sequence embeddings denoted by g emd .</p><p>To be able to compare our results with previous works <ref type="bibr" target="#b3">(Bekoulis et al. 2018;</ref><ref type="bibr" target="#b29">Miwa and Bansal 2016;</ref><ref type="bibr" target="#b20">Katiyar and</ref> Cardie 2017) on EMD, we identify the head of the entity mention rather than the whole mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference Resolution (CR)</head><p>Ascending one layer higher in our model, CR is the task of identifying mentions that are referring to the same real life entity and cluster them together (typically at the level of a few sentences). For instance, in the example My mom tasted the cake. She said it was delicious, there are two clusters: (My mom, She) and (the cake, it). CR is thus a task which requires a form of semantic representation to cluster the mentions pointing to the same entity.</p><p>We use the model proposed in <ref type="bibr" target="#b24">(Lee et al. 2017)</ref>. This model considers all the spans in a document as potential mentions and learns to distinguish the candidate coreferent mentions from the other spans using a mention scorer to prune the number of mentions. The output of the mention scorer is fed to a mention pair scorer, which decides whether identified candidates mentions are coreferent. The main elements introduced in <ref type="bibr" target="#b24">(Lee et al. 2017</ref>) are the use of span embeddings to combine context-dependent boundary representation and an attention mechanism over the span to point to the mention's head. This model is trained fully end-to-end without relying on external parser pre-processing.</p><p>The encoder takes as input the concatenation of [g e , g emd ] and outputs representations denoted as g cr , which are then fed to the mention pair scorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction (RE)</head><p>The last supervision of our model is given by Relation Extraction (RE). RE aims at identifying semantic relational structures between entity mentions in unstructured text. Traditional systems treat this task as two pipelined tasks: (i) identifying mentions and (ii) classifying the relations between identified mentions. We use the Joint Resolution Model proposed by <ref type="bibr" target="#b3">Bekoulis et al. (2018)</ref> in which the selection of the mentions and classification of the relation between these mentions are performed jointly. Following previous work <ref type="bibr" target="#b25">(Li and Ji 2014;</ref><ref type="bibr" target="#b20">Katiyar and Cardie 2017;</ref><ref type="bibr" target="#b3">Bekoulis et al. 2018)</ref>, we only consider relations between the last token of the head mentions involved in the relation. Redundant relations are therefore not classified.</p><p>The RE encoder is a multi-layer BiLSTM which takes as input [g e , g emd ] and outputs a representation denoted g re . These contextualized representations are fed to a feedforward neural network. More specifically, considering two token's contextualized representations, g i and g j , both of size R l , we compute a vector score:</p><formula xml:id="formula_0">t(w i , w j ) = V φ(U g j + W g i + b) (1) where U ∈ R d×l , W ∈ R d×l , b ∈ R d ,</formula><p>and V ∈ R r×d are learned transformation weights, l is the size of the embeddings output by the encoder, d is the size of the hidden layer of the feedforward network, r is the number of possible relations, and φ is a non-linear activation function. The relation probabilities are then estimated as p</p><formula xml:id="formula_1">= σ(t(w i , w j )) ∈ R r where p k (1 ≤ k ≤ r)</formula><p>is the probability that token w i and token w j are respectively labeled as ARG1 and ARG2 in a relation of type k. The model predictions are computed by thresholding estimated probabilities. The parameters of the model V , U , W , and b are trained by minimizing a crossentropy loss.</p><p>In this formulation, a mention may be involved in several relations at the same time (for instance being the ARG1 and the ARG2 in two respective relations), which can occur in real life examples. If we replaced the sigmoid function by a softmax function, this is not possible.</p><p>In the model, the CR and RE modules are both on the same level. We did not find it helpful to introduce a hierarchical relation between these two tasks as they both rely on deeper semantic modeling, i.e. both trying to link mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setting Datasets and evaluation metrics</head><p>We use labeled data from different sources to train and evaluate our model. For NER, we use the English portion of OntoNotes 5.0 <ref type="bibr" target="#b33">(Pradhan et al. 2013)</ref>. Following <ref type="bibr" target="#b39">Strubell et al. (2017)</ref>, we use the same data split as used for coreference resolution in the CoNLL-2012 shared task <ref type="bibr" target="#b32">(Pradhan et al. 2012)</ref>. We report the performance on NER using span level F 1 score on the test set. The dataset covers a large set of document types (including telephone conversations, web text, broadcast news and translated documents), and a diverse set of 18 entity types (including PERSON, NORP, FACILITY, ORGANIZATION, GPE). Statistics of the corpus are detailed in <ref type="table" target="#tab_1">Table 2</ref>. We also report performance on more commonly used CoNLL2003 NER dataset.</p><p>For CR, EMD and RE, we use the Automatic Content Extraction (ACE) program ACE05 corpus <ref type="bibr" target="#b13">(Doddington et al. 2004</ref>). The ACE05 corpus is one of the largest corpus annotated with CR, EMD and RE making it a compelling dataset for multi-task learning. Mention tags in ACE05 cover 7 types of entities such as Person, Organization, or Geographical Entities. For each entity, both the mention boundaries and the head spans are annotated. ACE05 also introduces 6 relation types (including Organization-Affiliation (ORG-AFF), GEN-Affiliation (GEN-AFF), and Part-Whole (PART-WHOLE)). We use the same data splits as previous work <ref type="bibr" target="#b25">(Li and Ji 2014;</ref><ref type="bibr" target="#b29">Miwa and Bansal 2016;</ref><ref type="bibr" target="#b20">Katiyar and Cardie 2017)</ref> for both RE and EMD and report F 1 -scores, Precision, and Recall. We consider an entity mention correct if the model correctly predicted both the mention's head and its type. We consider a relation correct if the model correctly predicted the heads of the two arguments and the relation type. For CR, we use different splits to be able to compare to previous work <ref type="bibr" target="#b2">(Bansal and Klein 2012;</ref><ref type="bibr" target="#b15">Durrett and Klein 2014)</ref>. These splits (introduced in (Rahman and Ng 2009)) use the whole ACE05 dataset leaving 117 documents for test while having 482 documents for training (as in <ref type="bibr" target="#b2">(Bansal and Klein 2012)</ref>, we randomly split the training into a 70/30 ratio to form a validation set). We evaluate coreference on both splits. We compare all coreference systems using the commonly used metrics: MUC, B3, CEAFe (CEAFφ 4 ) as well as the average F 1 of the three metrics as computed by the official CoNLL-2012 scorer. Note that Durrett and Klein make use of external NLP tools including an automatic parser <ref type="bibr" target="#b14">(Durrett and Klein 2013)</ref>.</p><p>We compare our model to several previous systems that have driven substantial improvements over the past few years both using graphical models or neural-net-based models. These are the strongest baselines to the best of our knowledge. <ref type="bibr" target="#b40">Subramanian et al. (2018)</ref> observe that there is no clear consensus on how to correctly train a multi-task model. Specifically, there remain many open questions such as "when should the training schedule switch from one task to another task?" or "should each task be weighted equally?" One of the main issues that arises when training a multi-task model is catastrophic forgetting <ref type="bibr" target="#b16">(French 1999)</ref> where the model abruptly forgets part of the knowledge related to a previously learned task as a new task is learned. This phenomenon is especially present when multiple tasks are trained sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We selected the simple yet effective training method described in <ref type="bibr" target="#b38">(Søgaard and Goldberg 2016;</ref><ref type="bibr" target="#b35">Ruder et al. 2017</ref>): after each parameter update, a task is randomly selected and a batch of the dataset attached to this task is also sampled at random to train the model. This process is repeated until convergence (the validation metrics do not improve anymore). We tested both uniform and proportional sampling and found that proportional sampling performs better both in terms of performance and speed of convergence. In proportional sampling, the probability of sampling a task is proportional to the relative size of each dataset compared to the cumulative size of all the datasets. Note that unlike (Subramanian et al. 2018), the updates for a particular task affect the layers associated with this task and all the layers below but not the layers above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head><p>In this section, we present our main results on each task and dataset. The hierarchical model and multi-task learning framework presented in this work achieved state-of-the-art results on three tasks, namely NER (+0.52), EMD (+3.8) and RE (+6.8). <ref type="table" target="#tab_2">Table 3</ref> summarizes the results and introduces each setups' abbreviation (alphabetical letters). In the following subsections, we highlight a few useful observations.</p><p>To be able to compare our work on CR with the various baselines, we report results using different settings and splits. More precisely, GM indicates that gold mentions were used for evaluation and that coreference was trained using the ACE05 splits introduced in (Rahman and Ng 2009).</p><p>Using gold mentions is impossible in real settings so we also relax this condition leading to a more challenging task in which we make no use of external tools or metadata (such as speaker ID used by some systems <ref type="bibr" target="#b9">(Clark and Manning 2015)</ref>). Comparing setups A and A-GM shows how the supervision from one module (e.g. CR) can flow through the entire architecture and impact other tasks' performance: RE's F 1 score drops by ∼1 point on A. Note that the GM setup impacts the training exit condition (the validation metrics stop improving) and the evaluation metrics (it is well known that using gold mentions at evaluation time improves CR's performance). Similarly, the A-GM setup leads to the state-of-the-art on EMD and RE. It increases the F 1 by ∼1.5 points for EMD and ∼1 point for RE (A vs. A-GM). This suggests that having different type of information on different sentences brings richer information than having multiple types of information on the same sentences (setup A-CoNLL2012 -see <ref type="table" target="#tab_4">Table 4</ref>-supports this claim as CR trained on another dataset leads to comparable performance on the three other tasks).</p><p>To analyze which components of the model are driving the improvements and understand the interactions between the different tasks and modules, we performed an ablation study summarized in the following paragraphs and on Tables 3 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Task vs. Full Models</head><p>The largest difference between a single-task and a multi-task setup is observed on the RE task (A vs. D on <ref type="table" target="#tab_2">Table 3</ref>), while the results on NER are similar in multi-task and single-task setups (B vs. A &amp; A-GM). This further highlights how the RE module can be sensitive to information learned from other tasks. Results on EMD are in the middle, with the single task setup giving higher score than a multi-task-setup except for A-GM and I. More surprisingly, CR can give slightly better results when being single-task-trained (A vs. E).</p><p>Progressively adding tasks To better understand the contribution of each module, we vary the number of tasks in our training setup. The experiments show that training using RE helps both for NER and for EMD. Adding RE supervision leads to an increase of ∼1 point on NER while boosting both precision and recall on EMD (F vs. I). CR and RE can help NER as shown by comparing setups A and F: recall and F 1  for NER are ∼1 point stronger, while the impact on EMD is negative. Finally, training using CR supervision boosts NER (F vs. J) by increasing NER's recall while lowering EMD's precision and F 1 . In other words the information flowing along the hierarchical model (e.g. stacking of encoders and shortcut connections) enables higher levels' supervision to train lower levels to learn better representations. More generally, whenever the task RE is combined with another task, it always increases the F 1 score (most of the improvement coming from the precision) by 2-6 F 1 points.</p><formula xml:id="formula_2">- - - - - - - - - (C) EMD - - - 87.03 85.27 86.14 - - - - - - - (D) RE - - - - - - 60.47 52.14 55.99 - - - - (E) CR - - - - - - - - -</formula><p>Experimenting with the hierarchy order Comparing setups F vs. K and A vs. L in which we switched NER and EMD, provides evidence for the hierarchical relation between NER and EMD: supervising EMD at a lower level than NER is detrimental to the overall performance. This supports our intuition that the hierarchy should follow the intrinsic difficulty of the tasks.</p><p>Comparison to other canonical datasets We also compare our model on two other canonical datasets for NER <ref type="bibr">(CoNLL-2003)</ref> and CR <ref type="bibr">(CoNLL-2012)</ref>. Details are reported in <ref type="table" target="#tab_4">Table 4</ref>. We did not tune hyperparameters, keeping the same hyperparameters as used in the previous experiments. We reach performance comparable to previous work and the other tasks, demonstrating that our improvements are not dataset-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the embeddings</head><p>We perform an ablation study on the words and character embeddings g e . Results are reported in <ref type="table" target="#tab_5">Table 5</ref>. As expected, contextualized ELMo embeddings have a noticeable effect on each metrics. Removing ELMo leads to a ∼4 F 1 points drop on each task. Furthermore, character-level embeddings, which are sensitive to morphological features such as prefixes and suffixes and capitalization, also have a strong impact, in particular on NER, RE and CR. Removing character-level embeddings  does not affect EMD suggesting that the EMD module can compensate for this information. The main improvements on the CR task stem from the increase in B3 and Ceafe metrics. Note that the strong effect of removing a type of embedding is also a consequence of using shortcut connections: removing an embedding has a direct impact on the input to each task's module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What did the embeddings and encoders learn?</head><p>High scores on a specific task suggest that the representations learned by the encoders (and the embeddings) have somehow managed to capture relevant linguistic or statistical features for the task. However, using complex archi-tectures makes it difficult to understand what is actually encoded in the embeddings and hidden states and what type of linguistic information, if any, is being used by the model to make a particular prediction. To further understand our architecture, we analyze the inductive biases encoded in the embeddings and hidden states of the various layers. We follow <ref type="bibr" target="#b11">Conneau et al. (2018)</ref> who introduced 10 different probing tasks 1 to analyze the quality of sentence embeddings. These tasks aim at evaluating a wide set of linguistic properties from surface information, to syntactic information through semantic information.</p><p>We use a simple logistic regression classifier, which takes the sentence embeddings as inputs and predicts the linguistic property. We study both the word embeddings (g e ) and the hidden state representations (biLSTM encoders) specific to each module in our model. The sentence embedding of an input sequence of length L is computed from the L hidden states of an encoder by taking the maximum value over each dimension of the last layer activations as done in <ref type="bibr" target="#b10">(Conneau et al. 2017)</ref>. Sentence embeddings are obtained from word and character-level embeddings by max-pooling over a sentence's words. Averaging word embeddings is known to be a strong baseline for sentence embeddings <ref type="bibr" target="#b0">(Arora, Liang, and Ma 2017)</ref> and we also report the results of this simpler procedure in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare our results with two baselines from <ref type="bibr" target="#b11">(Conneau et al. 2018</ref>): bag-of-words computed from Fast-Text embeddings and SkipThought sentence embeddings <ref type="bibr" target="#b21">(Kiros et al. 2015)</ref>. We compare the base word embeddings g e of our model with the first baseline and the output of the task-specific encoders to the second baseline. A first observation is that the word embeddings already encode rich representations, having an accuracy higher than 70% on seven of the ten probing tasks. We suspect that shortcut connections are key to this good performances by allowing high level tasks to encode rich representations. The good performance on Bigram Shift (compared to BoV-FastText: +38.8) likely comes from the use of ELMo embeddings which are sensitive to word order. The same argument may also explain the strong performance on Sentence Length.</p><p>There are significant discrepancies between the results of the word embeddings g e and the encoder representations, indicating that the learned linguistic features are different between these two types of embeddings. Averaging the base embeddings surpasses encoder embeddings on almost all the probing tasks (except Coordination Inversion). The difference is particularly high on the Word Content task in which the results of the encoders embeddings barely rise above 11.0, indicating that the ability to recover a specific word is not a useful feature for our four semantic tasks.</p><p>The performance of the encoder representation is stronger on semantic probing tasks, compared to the low signal for surface and syntatic tasks. The only exception is the Sentence Length which suggest that this linguistic aspect is naturally encoded. The performances of the NER and EMD encoders are generally in the same range supporting the fact 1 A probing task is a classification task that focuses on a well identified linguistic property. that these two tasks are similar in nature. Finally, we observe that the highest scores for encoder representations always stem from the coreference encoder suggesting that CR is both the highest level task and that the CR module requires rich and diverse representations to make a decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task learning accelerating training</head><p>It is also interesting to understand the influence of a multitask learning framework on the training time of the model. In the following section, we compare the speed of training in terms of number of parameter updates (a parameter update being equal to a back-propagation pass) for each of the tasks in the multi-task framework against a single-task framework. The speed of training is defined as the number of updates necessary to reach convergence based on the validation metric.</p><p>Results are presented in <ref type="table">Table 7</ref> for the best performing multi-task model (A-GM). The multi-task framework needs less updates to reach comparable (or higher) F 1 score in most cases except on the RE task. This supports the intuition that knowledge gathered from one task is beneficial to the other tasks in the hierarchical architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Our work is most related to <ref type="bibr" target="#b17">Hashimoto et al. (2017)</ref> who develop a joint hierarchical model trained on syntactic and semantic tasks. The top layers of this model are supervised by semantic relatedness and textual entailment between two input sentences, implying that the lower layers need to be run two times on different input sentences. Our choice of tasks avoids such a setup. Our work also adopts a different approach to multi-task training (proportional sampling) therefore avoiding the use of complex regularization schemes to prevent catastrophic forgetting. Our results show that strong performances can be reached without these ingredients. In addition, the tasks examined in this work are more focused on learning semantic representations, thereby reducing the need to learn surface and syntactic information, as evidenced by the linguistic probing tasks.</p><p>Unlike <ref type="bibr" target="#b17">(Hashimoto et al. 2017</ref>) and other previous work <ref type="bibr" target="#b20">(Katiyar and Cardie 2017;</ref><ref type="bibr" target="#b3">Bekoulis et al. 2018;</ref><ref type="bibr" target="#b1">Augenstein, Ruder, and Søgaard 2018)</ref>, we do not learn label embeddings, meaning that the (supervised) output/prediction of a layer is not directly fed to the following layer through an embedding learned during training. Nonetheless, sharing embeddings and stacking hierarchical encoders allows us to share the supervision from each task along the full structure of our model and achieve state-of-the-art performance.</p><p>Unlike some studies on multi-task learning such as (Subramanian et al. 2018), each task has its own contextualized encoder (multi-layer BiLSTM) rather than a shared one, which we found to improve the performance.</p><p>Our work is also related to <ref type="bibr" target="#b38">Søgaard and Goldberg (2016)</ref> who propose to cast a cascade architecture into a multi-task learning framework. However, this work was focused on syntactic tasks and concluded that adding a semantic task like NER to a set of syntactic tasks does not bring any improvement. This confirms the intuition that multi-task learn-  <ref type="table">Table 7</ref>: Speed of training: Difference in number of updates necessary before convergence: Multi-task (Full Model: A-GM) compared to single task. We report the differences in F 1 performance. Lower time is better, higher performance is better.</p><formula xml:id="formula_3">Setup Model Time ∆ Performance ∆ (B) NER -16% -0.02 (C) EMD -44% +1.14 (D) RE +78% +6.76 (E-GM) Coref-GM -28% +0.91</formula><p>ing is mostly effective when tasks are related and that syntactic and semantic tasks may be too distant to take advantage of shared representations. The authors used a linear classifier with a softmax activation, relying on the richness on the embeddings. As a consequence the sequence tagging decisions are made independently for each token, in contrast to our work. One central question in multi-task learning is the training procedure. Several schemes have been proposed in the literature. <ref type="bibr" target="#b17">Hashimoto et al. (2017)</ref> train their hierarchical model following the model's architecture from bottom to top: the trainer successively goes through the whole dataset for each task before moving to the task of the following level. The underlying hypothesis is that the model should perform well on low-level tasks before being trained in more complicated tasks. Hashimoto et al. avoid catastrophic forgetting by introducing successive regularization using slack constraints on the parameters. <ref type="bibr" target="#b40">Subramanian et al. (2018)</ref> adopt a simpler strategy for each parameter update: a task is randomly selected and a batch of the associated dataset is sampled for the current update. More recently, <ref type="bibr" target="#b27">Mccann et al. (2018)</ref> explored various batch-level sampling strategies and showed that an anti-curriculum learning strategy <ref type="bibr" target="#b4">(Bengio et al. 2009</ref>) is most effective. In contrast, we propose a novel proportional sampling strategy, which we find to be more effective.</p><p>Regarding the selection of the set of tasks, our work is closest to <ref type="bibr" target="#b15">(Durrett and Klein 2014;</ref><ref type="bibr" target="#b37">Singh et al. 2013</ref>). Durrett and Klein (2014) combine coreference resolution, entity linking (sometimes referred to as Wikification) and mention detection. <ref type="bibr" target="#b37">Singh et al. (2013)</ref> combine entity tagging, coreference resolution and relation extraction. These two works are based on graphical models with hand-engineered factors.</p><p>We are using a neural-net-based approach fully trainable in an end-to-end fashion, with no need for external NLP tools (such as in <ref type="bibr" target="#b15">(Durrett and Klein 2014)</ref>) or hand-engineered features. Coreference resolution is rarely used in combination with other tasks. The main work we are aware of is <ref type="bibr" target="#b12">(Dhingra et al. 2018)</ref>, which uses coreference clusters to improve reading comprehension and the works on language modeling by <ref type="bibr" target="#b19">Ji et al. (2017)</ref> and .</p><p>Regarding the combination of entity mention detection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features like dependency trees <ref type="bibr" target="#b29">(Miwa and Bansal 2016)</ref> or handengineered heuristics <ref type="bibr" target="#b25">(Li and Ji 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a hierarchically supervised multi-task learning model focused on a set of semantic task. This model achieved state-of-the-art results on the tasks of Named Entity Recognition, Entity Mention Detection and Relation Extraction and competitive results on Coreference Resolution while using simpler training and regularization procedures than previous works. The tasks share common embeddings and encoders allowing an easy information flow from the lowest level to the top of the architecture. We analyzed several aspects of the representations learned by this model as well as the effect of each tasks on the overall performances of the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A few examples motivating our selection of tasks. Abbreviations: CR: Coreference Resolution, RE: Relation Extraction, EMD: Entity Mention Detection, NER: Named Entity Recognition, X Y: X is more likely to be Y.</figDesc><table><row><cell>Example</cell><cell>Predictions on one</cell><cell cols="3">...can help disambiguate</cell></row><row><cell></cell><cell>task...</cell><cell cols="2">other tasks</cell></row><row><cell>X works for Y</cell><cell>RE: {work, X, Y}</cell><cell>X</cell><cell cols="2">Person (EMD)</cell></row><row><cell></cell><cell></cell><cell>Y</cell><cell cols="2">Organization or</cell></row><row><cell></cell><cell></cell><cell cols="2">Person (NER)</cell></row><row><cell>I love Melbourne. I've</cell><cell>CR: (Melbourne,</cell><cell cols="2">Melbourne</cell><cell>Location</cell></row><row><cell>lived three years in this</cell><cell>this city)</cell><cell cols="2">(EMD/NER)</cell></row><row><cell>city.</cell><cell>RE: {live, I, this</cell><cell></cell><cell></cell></row><row><cell></cell><cell>city}</cell><cell></cell><cell></cell></row><row><cell>Dell announced a $500M</cell><cell>CR: (Dell, The</cell><cell>Dell</cell><cell></cell></row><row><cell>net loss. The company is</cell><cell>company)</cell><cell></cell><cell></cell></row><row><cell>near bankruptcy.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Data statistics</cell><cell></cell></row><row><cell>OntoNotes</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Documents</cell><cell>2,483</cell><cell>319</cell><cell>322</cell></row><row><cell>Sentences</cell><cell>59,924</cell><cell>8,529</cell><cell>8,262</cell></row><row><cell>Named Entities</cell><cell>81,828</cell><cell>11,066</cell><cell>11,257</cell></row><row><cell>Tokens</cell><cell cols="3">1,088,503 147,724 152,728</cell></row><row><cell>ACE05</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Documents</cell><cell>351</cell><cell>80</cell><cell>80</cell></row><row><cell>Sentences</cell><cell>7,273</cell><cell>1,765</cell><cell>1,535</cell></row><row><cell>Mentions</cell><cell>26,470</cell><cell>6,421</cell><cell>1,535</cell></row><row><cell>Relations</cell><cell>4,779</cell><cell>1,179</cell><cell>1,147</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results: Baselines and ablation study on the tasks. GM means that the same coreference module uses gold mentions at evaluation time and that we used the splits introduced in (Rahman and Ng 2009). Otherwise, we use for coreference the same splits as for EMD and RE (351/80/80). For coreference, figures that are comparable with<ref type="bibr" target="#b15">(Durrett and Klein 2014)</ref> are tagged with an *.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>NER</cell><cell></cell><cell></cell><cell>EMD</cell><cell></cell><cell></cell><cell>RE</cell><cell></cell><cell></cell><cell></cell><cell>CR</cell></row><row><cell>Setup</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>MUC</cell><cell>B3</cell><cell cols="2">Ceafe Avg. F 1</cell></row><row><cell></cell><cell>(Strubell et al. 2017)</cell><cell>-</cell><cell>-</cell><cell>86.99</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>(Katiyar and Cardie 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.0</cell><cell>81.3</cell><cell>82.6</cell><cell>57.9</cell><cell>54.0</cell><cell>55.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>(Miwa and Bansal 2016)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.9</cell><cell>83.9</cell><cell>83.4</cell><cell>57.2</cell><cell>54.0</cell><cell>55.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>(Li and Ji 2014)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.2</cell><cell>76.9</cell><cell>80.8</cell><cell>68.9</cell><cell>41.9</cell><cell>52.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>(Durrett and Klein 2014)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">81.03* 74.89* 72.56* 76.16*</cell></row><row><cell></cell><cell>(Bansal and Klein 2012)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.2*</cell><cell>72.5*</cell><cell>-</cell><cell>-</cell></row><row><cell>(A)</cell><cell>Full Model</cell><cell cols="9">87.52 87.21 87.36 85.68 85.69 85.69 68.53 54.48 61.30</cell><cell>73.89</cell><cell>61.34</cell><cell>59.11</cell><cell>64.78</cell></row><row><cell cols="2">(A-GM) Full Model -GM</cell><cell cols="13">87.12 87.09 87.10 87.15 87.33 87.24 70.40 56.40 62.69 82.49* 67.64* 60.75* 70.29*</cell></row><row><cell>(B)</cell><cell>NER</cell><cell cols="3">87.24 87.00 87.12</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Comparison to other canonical datasets on NER</cell></row><row><cell cols="3">(CoNLL-2003) and coreference (CoNLL-2012). A-CoNLL:</cell></row><row><cell cols="3">train A-RS-GM using CoNLL-2003 for NER; A-CoNLL-</cell></row><row><cell cols="3">2012: train A using CoNLL-2012 for coreference.</cell></row><row><cell>Model</cell><cell cols="2">NER (F 1 ) CR (F 1 )</cell></row><row><cell>Lample et al. (2016)</cell><cell>90.94</cell><cell>-</cell></row><row><cell>Strubell et al. (2017)</cell><cell>90.54</cell><cell>-</cell></row><row><cell>Peters et al. (2018)</cell><cell>92.22</cell><cell>-</cell></row><row><cell>(A-CoNLL-2003)</cell><cell>91.63</cell><cell>70.14</cell></row><row><cell>Durrett and Klein (2014)</cell><cell>-</cell><cell>61.71</cell></row><row><cell>Lee et al. (2017) (single)</cell><cell>-</cell><cell>67.2</cell></row><row><cell>Lee et al. (2017) (ensemble)</cell><cell>-</cell><cell>68.8</cell></row><row><cell>(A-CoNLL-2012)</cell><cell>86.90</cell><cell>62.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the embeddings. We remove one by one the embeddings on the first layer of the best performing model (A-RS-GM).</figDesc><table><row><cell>Model</cell><cell cols="4">NER (F 1 ) EMD (F 1 ) RE (F 1 ) CR (F 1 )</cell></row><row><cell>Glove + Char. embds + ELMo</cell><cell>87.10</cell><cell>87.24</cell><cell>62.69</cell><cell>70.29</cell></row><row><cell>Glove + Char. embds</cell><cell>84.33</cell><cell>83.13</cell><cell>57.47</cell><cell>66.44</cell></row><row><cell>Glove</cell><cell>79.81</cell><cell>83.00</cell><cell>53.77</cell><cell>64.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>SentEval Probing task accuracies. Classification is performed using a simple logistic regression enabling fair evaluation of the richness of a sentence embedding. We report two baselines fromConneau et al..    </figDesc><table><row><cell>Tasks</cell><cell cols="2">Surface Information</cell><cell cols="2">Syntatic Information</cell><cell></cell><cell></cell><cell cols="3">Semantic Information</cell><cell></cell></row><row><cell></cell><cell>SentLen</cell><cell>WC</cell><cell cols="8">TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv</cell></row><row><cell>Word Embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bov-fastText ((Conneau et al. 2018))</cell><cell>54.8</cell><cell>91.6</cell><cell>32.3</cell><cell>63.1</cell><cell>50.8</cell><cell>87.8</cell><cell>81.9</cell><cell>79.3</cell><cell>50.3</cell><cell>52.7</cell></row><row><cell>Our model (g emb ) -Max</cell><cell>62.4</cell><cell>43.0</cell><cell>32.5</cell><cell>76.3</cell><cell>74.5</cell><cell>88.1</cell><cell>85.7</cell><cell>82.7</cell><cell>54.7</cell><cell>56.9</cell></row><row><cell>Our model (g emb ) -Average</cell><cell>72.1</cell><cell>70.0</cell><cell>38.5</cell><cell>79.9</cell><cell>81.4</cell><cell>89.7</cell><cell>88.5</cell><cell>86.5</cell><cell>57.4</cell><cell>63.0</cell></row><row><cell>BiLSTM-max encoders</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SkipThought (Conneau et al.)</cell><cell>59.6</cell><cell>35.7</cell><cell>42.7</cell><cell>70.5</cell><cell>73.4</cell><cell>90.1</cell><cell>83.3</cell><cell>79.0</cell><cell>70.3</cell><cell>70.1</cell></row><row><cell>Our model (Encoder NER g ner )</cell><cell>50.7</cell><cell>3.24</cell><cell>19.5</cell><cell>34.2</cell><cell>57.2</cell><cell>66.6</cell><cell>63.5</cell><cell>61.6</cell><cell>50.7</cell><cell>52.0</cell></row><row><cell>Our model (Encoder EMD g emd )</cell><cell>43.3</cell><cell>1.8</cell><cell>19.3</cell><cell>30.0</cell><cell>56.3</cell><cell>64.0</cell><cell>60.1</cell><cell>57.9</cell><cell>51.3</cell><cell>50.4</cell></row><row><cell>Our model (Encoder RE g re )</cell><cell>56.8</cell><cell>1.2</cell><cell>19.3</cell><cell>24.5</cell><cell>53.9</cell><cell>62.3</cell><cell>60.8</cell><cell>57.1</cell><cell>50.4</cell><cell>52.2</cell></row><row><cell>Our model (Encoder CR g cr )</cell><cell>61.9</cell><cell>11.0</cell><cell>29.5</cell><cell>55.9</cell><cell>70.0</cell><cell>82.8</cell><cell>83.0</cell><cell>76.5</cell><cell>53.3</cell><cell>58.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Gianis Bekoulis and Victor Quach for helpful feedbacks and the anonymous reviewers for constructive comments on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coreference semantics from web features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<idno>CoRR abs/1702.08303</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<title level="m">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno>abs/1805.01070</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural models for reasoning over multiple mentions using coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1804.05922</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2013 -2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discourse-based objectives for fast unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno>abs/1705.00557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Dynamic Entity Representations in Neural Language Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Neural Architectures for Named Entity Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incremental Joint Extraction of Entity Mentions and Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<idno>abs/1508.02096</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">The Natural Language Decathlon : Multitask Learning as Question Answering. (Nips)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bjrkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08142</idno>
		<title level="m">Learning what to share between loosely related tasks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An Overview of Multi-Task Learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Neural Networks. arXiv</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<title level="m">Joint Inference of Entities, Relations, and Coreference. Automated Knowledge Base Construction (AKBC CIKM 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fast and Accurate Entity Recognition with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Iterated Dilated Convolutions</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning General Purpose Distributed Sentence Representations Via Large Scale Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Referenceaware language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1611.01628</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-Task Cross-Lingual Sequence Tagging from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
