<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
							<email>nkim@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Woo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Joun</roleName><forename type="first">Yeop</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and INMC</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Flow-based generative models are composed of invertible transformations between two random variables of the same dimension. Therefore, flow-based models cannot be adequately trained if the dimension of the data distribution does not match that of the underlying target distribution. In this paper, we propose SoftFlow, a probabilistic framework for training normalizing flows on manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. We experimentally show that SoftFlow can capture the innate structure of the manifold data and generate high-quality samples unlike the conventional flow-based models. Furthermore, we apply the proposed framework to 3D point clouds to alleviate the difficulty of forming thin structures for flow-based models. The proposed model for 3D point clouds, namely SoftPointFlow, can estimate the distribution of various shapes more accurately and achieves state-of-the-art performance in point cloud generation.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ever since <ref type="bibr" target="#b5">Dinh et al. (2014)</ref> first introduced Non-linear Independent Component Estimation (NICE) that exploits a change of variables for density estimation, flow-based generative models have been widely studied and have shown competitive performance in many applications such as image generation <ref type="bibr" target="#b15">(Kingma &amp; Dhariwal, 2018)</ref>, speech synthesis <ref type="bibr" target="#b22">(Prenger et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2018)</ref>, video prediction <ref type="bibr" target="#b17">(Kumar et al., 2019)</ref> and machine translation <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>. With this success, flowbased models are considered a potent technique for unsupervised learning due to their attractive merits: (i) exact log-likelihood evaluation, (ii) efficient synthetic data generation, and (iii) wellstructured latent variable space. These properties enable the flow-based model to learn complex dependencies within high-dimensional data, generate a number of synthetic samples in real-time, and learn a semantically meaningful latent space which can be used for downstream tasks or interpolation between data points.</p><p>There also have been some theoretical developments as well as various application of flow-based models in recent years. For example, unlike the conventional flow-based models which typically perform dequantization by adding uniform noise to discrete data points (e.g., image) as a pre-process for the change of variable formula <ref type="bibr" target="#b6">(Dinh et al., 2016;</ref><ref type="bibr" target="#b21">Papamakarios et al., 2017)</ref>, Flow++ <ref type="bibr" target="#b11">(Ho et al., 2019)</ref> proposed to leverage a variational dequantization technique to provide more natural and smoother density approximator of discrete data. Another example is a continuous normalizing flow (CNF) <ref type="bibr" target="#b9">Grathwohl et al., 2018)</ref>. While discrete flow-based models adopt a restricted architecture for ease of computing the determinant of the Jacobian, CNFs impose no limits on the choice of model architectures since the objective function of CNFs can be efficiently calculated via Hutchinson's estimator <ref type="bibr" target="#b12">(Hutchinson, 1990)</ref>.</p><p>In this paper, we further aim to overcome another limitation of existing flow-based models, i.e., normalizing flows on manifolds. There have been some interesting works on the similar topic <ref type="bibr" target="#b8">(Gemici et al., 2016;</ref><ref type="bibr" target="#b24">Rezende et al., 2020;</ref><ref type="bibr" target="#b2">Brehmer &amp; Cranmer, 2020</ref>) but they may not guarantee numerical stability, or require the information about manifolds (e.g., structure and dimensionality) in advance or additional training steps to estimate it. Here, we propose a novel probabilistic framework for training normalizing flows on manifolds without any assumption and prescribed information. To begin with, we show that conventional normalizing flows cannot accurately estimate the data distribution if the data resides on a low dimensional manifold. To circumvent this issue, the proposed method, namely SoftFlow, perturbs the data with random noise sampled from different distributions and estimates the conditional distribution of the perturbed data. Unlike the conventional normalizing flows, SoftFlow is able to capture the distribution of the manifold data and synthesize high-quality samples. Furthermore, we also propose SoftPointFlow for 3D point cloud generation which relieves the difficulty of forming thin structures. We experimentally demonstrate that SoftPointFlow achieves cutting-edge performance among many point cloud generation models. Our framework is intuitive, simple and easily applicable to the existing flow-based models including both discrete and continuous normalizing flows. To encourage reproducibility, we attach the code for both SoftFlow and SoftPointFlow used in the experiments 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Flow-based generative model</head><p>A normalizing flow <ref type="bibr" target="#b23">(Rezende &amp; Mohamed, 2015)</ref> consists of invertible mappings from a simple latent distribution p Z (z) (e.g., isotropic Gaussian) to a complex data distribution p X (x). Let f i be an invertible transformation from z i−1 to z i , z 0 = z and z n = x (z i ∈ R D for i = 0, ..., n). Then, the log-likelihood log p X (x) can be expressed in terms of the latent variable z following the change of variables theorem:</p><formula xml:id="formula_0">z = f −1 1 • f −1 2 • ... • f −1 n (x),<label>(1)</label></formula><formula xml:id="formula_1">log p X (x) = log p Z (z) − n i=1 log det ∂f i ∂z i−1 .<label>(2)</label></formula><p>Eqs.</p><p>(1) and (2) suggest that the optimization of flow-based models requires the tractability of computing f −1 i and log det ∂fi ∂z i−1 . After training, sampling process can be performed efficiently as follows:</p><formula xml:id="formula_2">z ∼ p Z (z),<label>(3)</label></formula><formula xml:id="formula_3">x = f n • f n−1 • ... • f 1 (z).<label>(4)</label></formula><p>Recently, <ref type="bibr" target="#b4">Chen et al. (2018)</ref> introduced a continuous normalizing flow (CNF) where the latent variable is assumed to be time-varying and the change of its log-density follows the instantaneous change of variables formula. More specifically, continuous-time analogs of Eqs.</p><p>(1) and (2) can be given by</p><formula xml:id="formula_4">z(t 0 ) = z(t 1 ) + t0 t1 f (z(t), t)dt,<label>(5)</label></formula><formula xml:id="formula_5">log p(z(t 1 )) = log p(z(t 0 )) − t1 t0 Tr ∂f (z(t), t) ∂z(t) dt,<label>(6)</label></formula><p>where f (z(t), t) = dz(t) dt , z(t 0 ) = z and z(t 1 ) = x. Unlike conventional normalizing flows, CNFs impose no restriction on the choice of model architecture since the trace operation in Eq. (6) can be efficiently computed using the Hutchinson's estimator <ref type="bibr" target="#b9">(Grathwohl et al., 2018)</ref> and the sampling process is performed by reversing the time interval in Eq. (5). However, due to the large computational cost of the ODE solver, CNFs usually require a long time for training (e.g., <ref type="bibr" target="#b9">Grathwohl et al. (2018)</ref> reported that they trained the CNF on MNIST for 5 days using 6 GPUs).  Although normalizing flows have shown promising results on various tasks such as image generation <ref type="bibr" target="#b15">(Kingma &amp; Dhariwal, 2018</ref>), voice conversion <ref type="bibr" target="#b25">(Serrà et al., 2019)</ref> and machine translation <ref type="bibr" target="#b20">(Ma et al., 2019)</ref>, current flow-based models are not suitable for estimating the distribution of the data sitting on a lower-dimensional manifold. We note that Eqs.</p><p>(2) and (6) are valid only when the data distribution and the target distribution have the same dimensions. To demonstrate this limitation, we trained 2 FFJORD models <ref type="bibr" target="#b9">(Grathwohl et al., 2018)</ref> on different data distributions and present the results in <ref type="figure" target="#fig_1">Fig. 1</ref> where the left column represents the data distribution, the central column represents the scatter plot of the corresponding latent variables warped from the data points through the trained networks, and the right column denotes the target latent distribution that we initially set for training. When the dimension of the data distribution matches to that of the target distribution, the FFJORD model properly transforms the data points into the latent points. However, when the FFJORD model is trained on 1D manifold data scattered over 2D space, the distribution of the latent variables corresponding to the data points is quite different from the target latent distribution. This simple experiment exhibits the shortcoming of the current normalizing flows that they cannot expand the 1D manifold data points to the 2D shape of the target distribution since the transformations used in flow networks are homeomorphisms <ref type="bibr" target="#b7">(Dupont et al., 2019)</ref>. If the transformed latent variables cannot represent the whole 2D space, it is unclear which part of the data space would match the latent points outside the lines. The observation suggests that training the normalizing flows on manifolds according to Eq.</p><p>(2) or Eq. <ref type="formula" target="#formula_5">(6)</ref>  In fact, the change of variables theorem used in Eq. <ref type="formula" target="#formula_1">(2)</ref> is not useful anymore if the dimension of the domain is lower than the dimension of the image. Let F be a function from the contented subset A ⊂ R m to a manifold M ⊂ R n where m &lt; n as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. If z ∈ A and x ∈ M satisfy F (z) = x, the m-dimensional infinitesimal volume dV x around x is given by</p><formula xml:id="formula_6">dV x = det(DF ) † DF dV z<label>(7)</label></formula><p>where DF is the n × m Jacobian matrix (i.e., DF ij = ∂xi ∂zj ), dV z is the infinitesimal volume around z, and † represents the transpose operation <ref type="bibr" target="#b8">(Gemici et al., 2016;</ref><ref type="bibr" target="#b1">Ben-Israel, 1999)</ref>. Therefore, the log-likelihood log p X (x) can be computed as follows:</p><formula xml:id="formula_7">log p X (x) = log p Z (z) − 1 2 log(det(DF ) † DF ).<label>(8)</label></formula><p>Unfortunately, however, it is not straightforward to design a flow-based model according to Eq. (8) for a few reasons. First, transforming x to z is problematic as F cannot be invertible in general. This prevents the use of maximum likelihood since flow-based models cannot be optimized according Eq. (8) without knowing z. Secondly, we are no longer able to employ the trick of setting the Jacobian to a lower triangular matrix as in the general flow models. It is because that det(DF ) † DF is always a symmetric matrix. This restriction may lead to O(m 3 ) computational cost for the determinant. Finally, we need prior knowledge on the dimension of the manifold data in order to exactly determine m. Otherwise, we may rely on a heuristic search for m. These difficulties motivated us to come up with a novel probabilistic framework for training normalizing flows on manifolds which is presented in the following section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SoftFlow</head><p>Our ultimate goal is to appropriately train the normalizing flows on manifolds and generate realistic samples. The main cause of the aforementioned difficulties is the inherent nature of normalizing flows that the network output is homeomorphic to the input. To bridge the gap between the dimensions of the data and the target latent variable, we propose to estimate a conditional distribution of the perturbed data. The key idea is to add noise sampled from a randomly selected distribution and use the distribution parameters as a condition. For implementation, we perform the following steps for the i-th data point x i : First, we sample a random value c i from the uniform distribution unif [a, b], and set the noise distribution to N (0, Σ i ) where Σ i = c 2 i I. Next, we sample a noise vector ν i from N (0, Σ i ) and obtain the perturbed data point x i by adding ν i to x i . Note that now the distribution of x i is not confined to a low dimensional manifold due to the addition of ν i . Let f (·|c i ) be the flow transformation from the latent variable z to x i (i.e., f (z|c i ) = x i ), then the final objective function is given by</p><formula xml:id="formula_8">log p X (x i |c i ) = log p Z (z) − log det ∂f (z|c i ) ∂z .<label>(9)</label></formula><p>A summary of the proposed training procedure is illustrated in <ref type="figure" target="#fig_4">Fig.3</ref>. Since the support of the perturbed data distribution spans the entire dimensions of the data space, the normalizing flow on manifolds can be reliably trained according to Eq. (9). In addition, during training, the flow networks observe various distributions with different volumes and learn to transform the randomly perturbed data points into the latent variables properly. This enables the flow networks to understand and generalize the relation between the shape of data distributions and the noise distribution parameters. As a result, we can synthesize a realistic sample x sp by setting c sp to a small value or even zero as follows:</p><formula xml:id="formula_9">z sp ∼ p Z (z),<label>(10)</label></formula><p>x sp = f (z sp |c sp ).</p><p>Furthermore, it is obvious that the method can be extended to any CNF by adopting the following dynamics:</p><formula xml:id="formula_11">dz(t) dt = f (z(t), t, c i ), z(t 0 ) = z, z(t 1 ) = x i + ν i .<label>(12)</label></formula><p>The proposed framework, namely SoftFlow, provides a new way to exploit a normalizing flow for manifold data. SoftFlow overcomes the dimension mismatch by estimating a perturbed data distribution which is conditioned on noise parameters. Both the training and sampling processes can be easily implemented within the existing flow-based frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on artificial data</head><p>We conducted a set of experiments in order to validate the proposed framework. To implement SoftFlow within the FFJORD architecture, we augmented another dimension for a noise distribution parameter to the conditioning networks. During training, we sampled c i from U nif [0, 0.1] and perturbed each data point x i by adding ν i which was drawn from N (0, c 2 i I). We scaled up c i by multiplying 20 to get c in i and passed c in i to the CNF networks for conditioning. We employed the same way of conditioning time t in FFJORD for conditioning c in i . SoftFlow and FFJORD were trained on the data sampled from 5 different distributions 23 using the Adam optimizer <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> with learning rate 10 −3 for 36K iterations. We also trained a 100-layer Glow model for 100K iterations. All the distribution shapes were set to be composed of only 1D lines to exclude volume components.</p><p>The generative performance of SoftFlow, Glow, and FFJORD is shown in <ref type="figure">Fig. 4</ref>. For sampling, c sp in SoftFlow was set to zero. We can observe that Glow showed the worst performance in most distributions, and some parts of the sample clusters generated by FFJORD failed to fit the data distribution. Especially in the case of the circles distribution, both Glow and FFJORD generated poor samples which were scattered around the circles and formed a curved line connecting the inner and outer circles. The results demonstrate that Glow and FFJORD suffer from difficulties in estimating the distribution of the manifold data and cannot synthesize appropriate samples that agree with the data distribution. In contrast, SoftFlow is optimized according to the adequate objective function for manifold data. As a result, SoftFlow is capable of generating high-quality samples that follow the data distribution quite perfectly.</p><p>To examine how well the proposed model understands and generalizes the relation between the shape of a distribution and a noise distribution parameter, we drew different groups of samples obtained from SoftFlow by varying c sp from 0 to 0.1. As shown in <ref type="figure">Fig. 5</ref>, SoftFlow generated various samples which faithfully follow different distributions as we intended. The experimental results imply that SoftFlow can be further exploited to estimate an unseen distribution or produce a plausible synthetic distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SoftPointFlow</head><p>3D point clouds are a compact representation of geometric details which leverage sparsity of the data. Point clouds are becoming popular and attractive since they are processed by simple geometric operations and can be efficiently acquired by using various range-scanning devices such as LiDARs. In light of the benefits of point clouds, some recent works have proposed generative models for point clouds <ref type="bibr" target="#b26">(Yang et al., 2019;</ref><ref type="bibr" target="#b0">Achlioptas et al., 2017;</ref><ref type="bibr" target="#b10">Groueix et al., 2018)</ref>. However, generative modelling of point clouds is still a challenging task due to the high complexity of the space of point clouds. PointFlow <ref type="bibr" target="#b26">(Yang et al., 2019)</ref> is the current state-of-the-art model for point cloud generation but still had difficulty forming thin structures. We assume that the difficulty stems from the inability of normalizing flows to estimate the density on lower-dimensional manifolds, and propose SoftPointFlow to mitigate the issue by applying the SoftFlow technique to PointFlow.</p><p>The overall architecture of SoftPointFlow 4 is shown in <ref type="figure">Fig. 6</ref>. SoftPointFlow models two-level hierarchical distributions of shape and points, following the same training framework of PointFlow. Given a point set X set consisting of M points, we first encode X set into a latent variable S using the reparameterization trick <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2013)</ref>. The encoder employs the same architecture as in PointFlow. We provide a more expressive and learnable prior for S by employing PriorFlow, a Glow-like architecture for estimating the likelihood of S. Each x i is randomly perturbed as follows:</p><formula xml:id="formula_12">x i = x i + ν i , ν i ∼ N (0, c 2 i I), c i ∼ unif [a, b].<label>(13)</label></formula><p>The perturbed point x i goes through DecoderFlow to compute the conditional likelihood given S and c i . DecoderFlow adopts an autoregressive function for flow transformation (AR Layer) which offers parallel computation for training. Even though AR Layer requires serial operations for sampling, the sampling speed will not be degraded significantly since AR Layer only processes a 3-dimensional vector. Our final objective function L(X set ; θ, ψ, φ) is given as follows:</p><formula xml:id="formula_13">L(X set ; θ, ψ, φ) = E q φ (S|Xset) [log p θ (X set |S) + log p ψ (S) − log q φ (S|X set )] ≈ E q φ (S|Xset) M i=1 E ci∼unif [a,b] log p(z i ) − log det ∂g θ (z i |S, c i ) ∂z i + log p(Z) − log det ∂f ψ (Z) ∂Z + H[q φ (S|X set )],<label>(14)</label></formula><p>where Z = f −1 ψ (S), z i = g −1 θ (x i |S, c i ), and H represents the entropy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on point clouds</head><p>We conducted a set of experiments using the ShapeNet Core dataset (v2) <ref type="bibr" target="#b3">(Chang et al., 2015)</ref> to evaluate the proposed framework for 3D point clouds. Three different categories were used for the experiments: airplane, chair, and car. We followed the same configuration for the training and test sets as in <ref type="bibr" target="#b26">Yang et al. (2019)</ref>, and used 5K points per shape in the training set to construct the validation set. We trained SoftPointFlow for 15K epochs with a batch size of 128 using four 2080-Ti GPUs. The initial learning rate of the Adam optimizer was set to 0.002 and decayed by half after every 5K epochs. Each point set X set was compoesd of 2048 points (M = 2048).</p><p>SoftPointFlow was built on two discrete normalizing flow networks, PriorFlow and DecoderFlow. PriorFlow consisted of 12 flow blocks of an actnorm, an invertible 1x1 convolution, and an affine coupling layer (N p = 12). For the affine coupling layer, PriorFlow employed 4 convolution layers with gated tanh nonlinearities as well as residual connections and skip connections with 256 channels. The latent variable S was squeezed to have 8 channels before going through PriorFlow and 2 of the channels were factored out after every 4 flow blocks following the multi-scale architecture <ref type="bibr" target="#b6">(Dinh et al., 2016)</ref>. On the other hand, DecoderFlow consisted of 9 flow blocks of an actnorm, an invertible 1x1 convolution, and an autoregressive (AR) layer (N D = 9). Each AR layer employed 3 linear layers with gated tanh units. We set the number of channels in the linear and gate layers to 256. A concatenated vector of the input point and the noise parameter was passed to the linear layers. Also, the latent variable S was used as a global condition by going through the gate layers. During training, c i was sampled from unif [0, 0.075] and scaled up to have a maximum value 2 for the AR layers. For sampling, c sp was set to zero.</p><p>We report some samples generated by PointFlow and SoftPointFlow in <ref type="figure">Fig. 7</ref>. The axis ratio was adjusted to highlight the difference between the samples. We can observe that PointFlow generated blurry samples that failed to form thin structures such as chair legs or wing tips. In contrast, SoftPointFlow captured fine details of an object well and produced high-quality samples. We provide various samples generated by each model in Appendices B and C.</p><p>What would happen if we sample latent variables from N (0, σ 2 z I) with different values of σ z , and transform them into the data space? If the latent variable is sampled from the vicinity of high density (i.e., a low value of σ z ), we can expect that the corresponding points in the data space would be focused on the main part of an object (e.g., a chair body). In the opposite case, the points may be gathered around the thin structure (e.g., chair legs). To investigate the representations that each model learned, we generated various point sets by varying σ z , and report the results in <ref type="figure">Fig. 8</ref>. The overall tendency is similar to what we expected. However, as σ z increases, we observe that PointFlow failed to capture an X-shaped structure beneath the chair body while SoftPointFlow produced points that form the X shape. Also, as σ z decreases, the samples generated by PointFlow are concentrated on the center of the chair while the point clouds of SoftPointFlow well preserve the whole structure. The results demonstrate that SoftPointFlow learns more desirable features from manifold data and is robust to the variance of latent variables. In order to compare SoftPointFlow with other generative models, we measured 1-nearest neighbor accuracy (1-NNA) of SoftPointFlow. The 1-NNA represents the leave-one-out accuracy of the 1-NN classifier and its ideal score is 50% <ref type="bibr" target="#b19">(Lopez-Paz &amp; Oquab, 2016)</ref>. To compute the 1-NNA, two different distance metrics, Chamfer distance (CD) and earth mover's distance (EMD), can be employed to measure the similarity between point clouds. The generation results on 1-NNA are shown in <ref type="table" target="#tab_0">Table 1</ref>. The results of l-GAN <ref type="bibr" target="#b0">(Achlioptas et al., 2017)</ref>, PC-GAN <ref type="bibr" target="#b18">(Li et al., 2018)</ref> and PointFlow are taken from <ref type="bibr" target="#b26">Yang et al. (2019)</ref>. In all categories, SoftPointFlow achieved the significantly better results than the GAN-based models. Compared to PointFlow, SoftPointFlow showed the competitive performance on the airplane and chair data sets, and recorded the slightly lower scores on the car data set. Since the proportion of thin structures in the car data set is relatively low, we believe the results still support the validity of the proposed framework.  <ref type="figure">Figure 9</ref>: Results on preference test.</p><p>We also conducted a preference test to evaluate the perceptual quality of samples. We randomly selected 60 point clouds for each category (airplane, chair, and car) and obtained the reconstructed point clouds from PointFlow and SoftPointFlow. We asked 31 participants to assess which sample is more similar to the reference and better in quality. Each question presented a reference point cloud and two reconstructed point clouds as shown in the left and center columns of <ref type="figure">Fig. 7</ref> but the order was random. The results are shown in <ref type="figure">Fig. 9</ref>. Surprisingly, SoftPointFlow received better scores than PointFlow in all cases. In particular, Soft-PointFlow outperformed by a large margin in the airplane shapes and the seen chair shapes. The overall results demonstrate that the proposed framework is considerably useful and suitable for modelling generative flows on point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have introduced a novel probabilistic framework, SoftFlow, for training a normalizing flow on manifolds. We experimentally demonstrated that SoftFlow is capable of capturing the innate structure of the manifold data and produces high-quality samples while the current flow-based models cannot. Also, we have successfully applied the proposed framework to point clouds to overcome the difficulty of modelling thin structures. Our generative model, SoftPointFlow, produced point clouds that describe more exactly the details of an object and achieved state-of-the-art performance for point cloud generation. We believe that our framework can be further improved by theoretically identifying which noise distribution is more useful for training or by searching an architecture to leverage noise parameters efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This paper introduces a new way of designing a generative model for manifold data. This paper will motivate other researchers and engineers to employ the proposed framework for various applications. Like other generative models, the proposed model could produce biased samples if the training set is not properly set up. However, we believe that this paper will not cause a bad influence to the society in general use. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of normalizing flow trained on 2D data distribution (top) and 1D manifold data distribution (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>may result in degenerated performance. Example of the function that maps the contented subset of R m to the manifold of R n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Proposed technique for training a normalizing flow on manifold data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Samples from SoftFlow, Glow, and FFJORD trained on 5 different distributions. Examples of synthetic data points sampled from SoftFlow with different values of c sp .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Schematic block diagram of SoftPointFlow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Examples of point clouds generated by PointFlow and SoftPointFlow. From left to right: reconstructed samples of seen data, reconstructed samples of unseen data, and synthetic samples. Reconstructed samples transformed from different latent distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>AFigure 15 :</head><label>15</label><figDesc>Code for the 2-sines and target distributions 1 i m p o r t numpy a s np 3 d e f g e t _ d a t a _ b a t c h ( bsz , d i s t ) : # b s z : b a t c h s i z e 5 r n g = np . random . R a n d o m S t a t e f d i s t == " t a r g e t " : s h a p e s = np . random . r a n d i n t ( 7 , s i z e = b s z p p e n d ( ( s h a p e s == i ) * 1 . ) # b o o l e a n t o f l o a t 19 t h e t a = np . l i n s p a c e ( 0 , 2 * np . p i , bsz , e n d p o i n t = F a l s e ) 21 x = ( mask [ 0 ] + mask [ 1 ] + mask [ 2 ] ) * ( r n g . r a n d ( b s z ) − 0 . 5 ) * 4 + \ ( − mask [ 3 ] + 0 * mask [ 4 ] + mask [ 5 ] ) * 2 * np . o n e s ( b s z ) + \ 23 mask [ 6 ] * np . c o s ( t h e t a ) 25 y = ( − mask [ 0 ] + 0 * mask [ 1 ] + mask [ 2 ] ) * 2 * np . o n e s ( b s z ) + \ ( mask [ 3 ] + mask [ 4 ] + mask [ 5 ] ) * ( r n g . r a n d ( b s z ) − 0 . 5 Synthetic point clouds generated by SoftPointFlow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">: Generation results on 1-NNA (%). Lower</cell></row><row><cell>is better.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>Model</cell><cell>CD</cell><cell>EMD</cell></row><row><cell></cell><cell cols="3">l-GAN (EMD) 87.65 85.68</cell></row><row><cell>Airplane</cell><cell>PC-GAN PointFlow</cell><cell cols="2">94.35 92.32 75.68 75.06</cell></row><row><cell></cell><cell cols="3">SoftPointFlow 70.92 69.44</cell></row><row><cell></cell><cell cols="3">l-GAN (EMD) 64.73 65.56</cell></row><row><cell>Chair</cell><cell>PC-GAN PointFlow</cell><cell cols="2">76.03 78.37 60.88 59.89</cell></row><row><cell></cell><cell cols="3">SoftPointFlow 59.95 63.51</cell></row><row><cell></cell><cell cols="3">l-GAN (EMD) 69.74 68.32</cell></row><row><cell>Car</cell><cell>PC-GAN PointFlow</cell><cell cols="2">92.19 90.87 60.65 62.36</cell></row><row><cell></cell><cell cols="3">SoftPointFlow 62.63 64.71</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ANLGBOY/SoftFlow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The implementation of the data distributions can be found in our code.3  We provide the code for the 2sines and target distributions in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The CNF networks in PointFlow are replaced with discrete normalizing flows for two reasons: (1) slow convergence of the CNF networks, and (2) validation of the proposed framework on discrete normalizing flows.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02392</idno>
		<title level="m">Learning representations and generative models for 3d point clouds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The change-of-variables formula using matrix volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Flows for simultaneous manifold learning and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13913</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02304</idno>
		<title level="m">Normalizing flows on riemannian manifolds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<title level="m">Improving flow-based generative models with variational dequantization and architecture design</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flowavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02155</idno>
		<title level="m">A generative flow for raw audio</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Videoflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point Cloud Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting classifier two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06545</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flowseq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02480</idno>
		<title level="m">Non-autoregressive conditional sequence generation with generative flow</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02428</idno>
		<title level="m">Normalizing flows on tori and spheres</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blow: a single-scale hyperconditioned flow for nonparallel raw-audio voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Perales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
