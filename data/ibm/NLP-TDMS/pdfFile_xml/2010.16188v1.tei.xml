<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Animal Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">End-to-end Animal Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. X, NO. X, X X 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image Matting</term>
					<term>Deep Learning</term>
					<term>Alpha Matte</term>
					<term>Image Composition</term>
					<term>Domain Gap !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting accurate foreground animals from natural animal images benefits many downstream applications such as film production and augmented reality. However, the various appearance and furry characteristics of animals challenge existing matting methods, which usually require extra user inputs such as trimap or scribbles. To resolve these problems, we study the distinct roles of semantics and details for image matting and decompose the task into two parallel sub-tasks: high-level semantic segmentation and low-level details matting. Specifically, we propose a novel Glance and Focus Matting network (GFM), which employs a shared encoder and two separate decoders to learn both tasks in a collaborative manner for end-to-end animal image matting. Besides, we establish a novel Animal Matting dataset (AM-2k) containing 2,000 high-resolution natural animal images from 20 categories along with manually labeled alpha mattes. Furthermore, we investigate the domain gap issue between composite images and natural images systematically by conducting comprehensive analyses of various discrepancies between foreground and background images. We find that a carefully designed composition route RSSN that aims to reduce the discrepancies can lead to a better model with remarkable generalization ability. Comprehensive empirical studies on AM-2k demonstrate that GFM outperforms state-of-the-art methods and effectively reduces the generalization error.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE matting refers to extracting the foreground alpha matte from an input image, requiring both hard labels for the explicit foreground or background and soft labels for the transition areas. Specifically, animal matting that aims at extracting the foreground animal from an image, plays an important role in many applications, e.g., virtual reality, augmented reality, entertainment, etc. Different categories of animals have the diverse appearance and furry details, which lay a great burden on image matting methods. How to recognize the semantic foreground or background as well as extract the fine detail for end-to-end animal matting remains challenging. However, this research topic is underexplored in the image matting community.</p><p>For image matting, an image I is assumed to be a linear combination of foreground F and background B via a soft alpha matte α ∈ [0, 1], i.e.,</p><formula xml:id="formula_0">I i = α i F i + (1 − α i ) B i<label>(1)</label></formula><p>where i denotes the pixel index. It is a typical ill-posed problem to estimate F, B, and α given I from Eq. (1) due to the under-determined nature. To address this issue, previous matting methods adopt extra user input such as trimap <ref type="bibr" target="#b0">[1]</ref> and scribbles <ref type="bibr" target="#b1">[2]</ref> as priors to decrease the degree of unknown. Based on sampling neighboring known pixels <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or defining an affinity matrix <ref type="bibr" target="#b5">[6]</ref>, the known alpha values (i.e., foreground or background) are propagated to the unknown pixels. Usually, some edge-aware smoothness constraints are used to make the problem tractable <ref type="bibr" target="#b1">[2]</ref>. However, either the sampling or calculating affinity matrix is based on low-level color or structural features, which is not so discriminative at indistinct transition areas or fine edges. Consequently, their performance is sensitive to the size of unknown areas and may suffer from fuzzy boundaries and color blending. To address this issue, deep convolutional neural network (CNN)-based matting methods have been proposed to leverage its strong representative ability and the learned discriminative features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Although CNN-based methods can achieve good matting results, the prerequisite trimap or scribbles make them unlikely to be used in automatic applications such as the augmented reality of live streaming and film production. For animal images, the issue becomes even worse since it may take great effort to indicate the transition areas regarding their furry nature and occlusions by the living environment, e.g., grassland and trees, in consideration of their protective coloration. To address this issue, end-to-end matting methods have been proposed <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> in recent years. Most of them can be categorized into two types. The first type shown in (i) of <ref type="figure" target="#fig_0">Figure 1</ref>(a) is a straightforward solution which is to perform global segmentation <ref type="bibr" target="#b11">[12]</ref> and local matting sequentially, where the former aims at trimap generation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> or foreground/background generation <ref type="bibr" target="#b7">[8]</ref> and the latter is image matting based on the trimap or other priors generated from the previous stage. The shortage of such a pipeline attributes to its sequential nature, since they may generate an erroneous semantic error which could not be corrected by the subsequent matting step. Besides, the separate training scheme in two stages may lead to a sub-optimal solution due to the mismatch between them. The second type is shown in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref> (a), global information is provided as guidance while performing local matting. For example, coarse alpha matte is generated and used in the following matting network in <ref type="bibr" target="#b9">[10]</ref> and in <ref type="bibr" target="#b8">[9]</ref>, spatial-and channel-wise attention is adopted to provide global appearance filtration to the matting network. Such methods avoid the problem of state-wise modeling and training but bring in new problems. Although global guidance is provided in an implicit way, it is challenging to generating alpha matte for both arXiv:2010.16188v1 [cs.CV] <ref type="bibr" target="#b29">30</ref> Oct 2020 foreground/background areas and transition areas simultaneously in a single network due to their distinct appearance and semantics. For animal matting, it is more difficult to generate reliable trimap and estimate accurate alpha matte that will challenge the above two pipelines due to the following two reasons. First, there are different categories of animals with diverse shapes, sizes, colors, and appearances in the animal matting task other than a single category in other matting tasks such as human portrait matting <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Second, the protective coloration of animals makes it hard to recognize the foreground and distinguish the animal fur from the background context. Thereby, how to design a novel framework to learn discriminative features and distinguish the semantic foreground/background from the fine details in the transition area for animal image matting remains challenging and under-explored.</p><p>Another challenge is the limitation of the current available matting dataset. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track, due to the laborious and costly labeling process, existing public matting datasets only have tens or hundreds of highquality annotations <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>. They either only provide foregrounds and alpha mattes <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref> as in (i) of Figure 1(b) ORI-Track, or provide fix-size and low-resolution (800 × 600) headshots with inaccurate alpha mattes <ref type="bibr" target="#b10">[11]</ref> generated by ensemble of existing matting algorithms as in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track. Moreover, the number of animal samples in these datasets is even less (about 100 images). Due to the unavailability of original natural images, as shown in (i) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) COMP-Track, a common practice for data augmentation in matting is to composite one foreground with various background images by alpha blending according to Eq. (1) to generate large-scale synthetic data. The background images are usually choosing from existing benchmarks for image classification and detection, such as MS COCO <ref type="bibr" target="#b13">[14]</ref> and PASCAL VOC <ref type="bibr" target="#b14">[15]</ref>. However, these background images are in low-resolution and may contain salient objects. In this paper, we point out that the training images following the above route have a significant domain gap with those natural images due to the composition artifacts, attributing to the resolution, sharpness, noise, and illumination discrepancies between foreground and background images. The artifacts serve as cheap features to distinguish foreground from background and will mislead the models during training, resulting in overfitted models with poor generalization on natural images.</p><p>To address the above issues in animal image matting, we study the distinct roles of semantics and details for animal image matting and explore the idea of decomposing the task into two parallel sub-tasks, semantic segmentation and details matting. Specifically, we propose a novel endto-end matting model named Glance and Focus Matting network (GFM). It consists of a shared encoder and two separate decoders to learn both tasks in a collaborative manner for animal matting, which is trained end-to-end in a single stage. Moreover, we also explore different data representation formats in the global decoder and gain useful empirical insights into the semantic-transition representation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a)(iii), compared with previous SOTA methods, GFM is a unified model that models both sub-tasks explicitly and collaboratively in a single network.</p><p>Besides, we make the first attempt to establish a largescale dataset for animal matting, i.e., AM-2k, which con-tains 2,000 high-resolution natural animal images from 20 categories along with manually carefully labeled fine alpha mattes. As shown in (iii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track, compared to the datasets in previous work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> as shown in (i) and (ii) which only provide foreground images or lowresolution inaccurate alpha mattes, AM-2k includes all the original natural images and high-resolution high-quality alpha mattes (more than 1080 pixel in the shorter side), which are beneficial to train models with better generalization on natural animal images. Besides, AM-2k also suggests several new research problems which will be discussed later.</p><p>Furthermore, we investigate the domain gap systematically and carry out comprehensive empirical analyses of the composition pipeline in image matting. We identify several kinds of discrepancies that lead to the domain gap and point out possible solutions to them. We then design a novel composition route names RSSN that can significantly reduce the domain gap arisen from the discrepancies of resolution, sharpness, noise, etc. Along with this, as shown in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) COMP-Track, we propose a large-scale high-resolution clean background dataset (BG-20k) without salient foreground objects, which can be used in generating high-resolution composite images. Extensive experiments demonstrate that the proposed composition route along with BG-20k can reduce the generalization error by 60% and achieve comparable performance as the model trained using original natural images. It opens an avenue for compositionbased image matting since obtaining foreground images and alpha mattes are much easier than those from original natural images by leveraging chroma keying.</p><p>The contributions of this paper are four-fold 1 :</p><p>• We propose a novel model named GFM for end-toend natural animal image matting, which simultaneously generates global semantic segmentation and local alpha matte without any priors as input but a single image.</p><p>• We construct the first natural animal image matting dataset AM-2k, which benefits training a better model with good generalization by its large scale, diverse categories, and high-quality annotations.</p><p>• We design a novel composition route RSSN to reduce various kinds of discrepancies and propose a largescale high-resolution background dataset BG-20k to serve as better candidates for generating high-quality composite images.</p><p>• Extensive experiments on AM-2k and BG-20k demonstrate that GFM outperforms state-of-the-art matting models and can be a strong baseline for future research. Moreover, the proposed composition route demonstrates its value by reducing the generalization error by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image Matting Most classical image matting methods are trimap-based <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. They sample or propagate foreground and background labels to the unknown areas based on local smoothness assumptions. Recently, CNN-based methods improve them by learning discriminative features rather than relying on hand-crafted lowlevel color features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Deep Matting <ref type="bibr" target="#b0">[1]</ref> 1. The source code, datasets, models, and a video demo will be made publicly available at https://github.com/JizhiziLi/animal-matting employed an encoder-decoder structure to extract high-level contextual features. IndexNet <ref type="bibr" target="#b19">[20]</ref> focused on boundary recovery by learning the activation indices during downsampling. However, trimap-based methods require user interaction, so are not likely to be deployed in automatic applications. Recently, Chen et al. <ref type="bibr" target="#b6">[7]</ref> proposed an end-toend model that first predicted the trimap then carried out matting. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> also devised a two-stage model that first segmented the foreground/background and then refined them with a fusion net. Both methods separate the process of segmentation and matting in different stages, which may generate erroneous segmentation results that mislead the matting step. Qiao et al. <ref type="bibr" target="#b8">[9]</ref> employed spatial and channel-wise attention to integrate appearance cues and pyramidal features while predicting, however, the distinct appearance and semantics of foreground/background areas and transition areas bring a lot of burden to a single-stage network and limit the quality of alpha matte prediction. Liu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a network to perform human matting by predicting the coarse mask first, then adopted a refinement network to predict a more detailed one. Despite the necessity of stage-wise training and testing, a coarse mask is not enough for guiding the network to refine the detail since the transition areas are not defined explicitly.</p><p>For animal image matting, the above issue becomes even worse since the appearance diversity of shape, size, texture among different categories of animals as well as the protective coloration of animals make it more challenging to recognize foreground animals or distinguish their fur details from background context. In contrast to previous methods, we devise a novel end-to-end matting model via multi-task learning, which addresses the segmentation and matting tasks simultaneously. It can learn both high-level semantic features and low-level structural features in a shared encoder, benefiting the subsequent segmentation and matting decoders collaboratively. One close related work with ours is AdaMatting <ref type="bibr" target="#b21">[22]</ref>, which also has a structure of a shared encoder and two decoders. There are several significant differences: 1) AdaMatting requires a coarse trimap as an extra input while our GFM model only takes a single image as input without any priors; 2) the trimap branch in AdaMatting aims to refine the input trimap, which is much easier than generating a global representation in our case because the initial trimap actually serves as an attention mask for learning semantical features; 3) both the encoder and decoder structures of GFM are specifically designed for end-to-end matting, which differs from AdaMatting; and 4) we systematically investigate the semantic-transition representations in the global decoder and gain useful empirical insights.</p><p>Matting Dataset Existing matting datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> only contain a small number of annotated alpha mattes, e.g., 27 training images and 8 test images in alphamatting <ref type="bibr" target="#b12">[13]</ref>, 431 training images and 50 test images in Composition-1k <ref type="bibr" target="#b0">[1]</ref>, and 596 training images and 50 test images in HAttMatting <ref type="bibr" target="#b8">[9]</ref>. As for animal matting, there is no large-scale dataset available. Composition-1k only contains 41 and HAttMatting only contains 61 animal images in total, which is far from enough in the deep learning era. We fill this gap by establishing the first natural animal image matting dataset named AM-2k. It contains 2,000 high-resolution natural animal images from 20 categories. We manually annotate the alpha matte and category label for each image. In contrast to previous datasets that only provide the foregrounds or composite images, we provide the original natural images. We empirically demonstrate that the model trained on AM-2k has a better generalization ability on natural animal images than the one trained on composite images. Image Composition. As the inverse problem of image matting and the typical way of generating synthetic dataset, image composition plays an important role in image editing. Researchers have been dedicated to improve the reality of composite images from the perspective of color, lighting, texture compatibility and geometric consistency in the past years <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Xue et al. <ref type="bibr" target="#b23">[24]</ref> conducted experiments to evaluate how the image statistical measure including luminance, color temperature, saturation, local contrast, and hue determine the realism of a composite. Tsai et al. <ref type="bibr" target="#b24">[25]</ref> proposed an end-to-end deep convolutional neural network to adjust the appearance of the foreground and background to be more compatible. Chen et al. <ref type="bibr" target="#b25">[26]</ref> proposed a generative adversarial network(GAN) architecture to learn geometrically and color consistent in the composites. Cong et al. <ref type="bibr" target="#b26">[27]</ref> contributed a large-scale image harmonization dataset and a network using a novel domain verification discriminator to reduce the inconsistency of foreground and background. Although they all done a good job in harmonizing the composites to be more realistic, the domain gap still exists when fitting the synthesis data into the matting model, the reason is a subjective agreed standard of harmonization by a human is not equivalent to a good training candidate for a machine learning model. In this paper, instead of working on generating consistent look composite images, we focus on generating composite images that can be used to reduce the generalization error on natural images of matting models.</p><p>Domain Adaptation Due to the tedious labor required to collect and annotate large-scale natural images from the real-world scenarios for different computer vision tasks such as image matting <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>, crowd counting <ref type="bibr" target="#b28">[29]</ref>, object detection <ref type="bibr" target="#b29">[30]</ref>, and semantic segmentation <ref type="bibr" target="#b30">[31]</ref>, leveraging synthetic data to increase the volume of training set for deep leaning method becomes gradually prevalent, e.g., obtaining synthetic images from 3D virtual game engine for semantic segmentation <ref type="bibr" target="#b31">[32]</ref>. However, the domain gap between the synthetic images and real-world images degrades the generalization ability of deep models.</p><p>To address this issue, many efforts have been made in the domain adaptation community. For example, Wang et al. <ref type="bibr" target="#b28">[29]</ref> proposed an SSIM Embedding (SE) Cycle GAN architecture to transform the synthetic domain to real-world domain. Dwibedi et al. <ref type="bibr" target="#b29">[30]</ref> adopted Poisson blending <ref type="bibr" target="#b32">[33]</ref> and Gaussian noise to the synthetic data to smooth the boundary artifacts. Sankaranarayanan et al. <ref type="bibr" target="#b31">[32]</ref> utilized an approach based on GAN to bring the embedding of synthetic data closer in the learned feature space. Mayer et al. <ref type="bibr" target="#b33">[34]</ref> evaluated the importance of object shape, motion type, camera lens distortion, and Bayer artifact in improving the performance of optical flow estimation tasks.</p><p>However, the domain gap between synthetic images and real-world natural images in image matting attributes to different types of discrepancies between the foreground and background images introduced during the specific alphacomposition process, e.g., resolution, sharpness, noise, etc. Sengupta et al. <ref type="bibr" target="#b27">[28]</ref> reduced the domain gap by augmenting the dataset and devising a context switching block and self-supervised train the network on real unlabeled input images. In this paper, we systematically investigate the discrepancies that lead to the domain gap and design a novel composition route that can significantly reduce it. As a result, we successfully reduce the generalization error by a large margin for different models trained on images generated by the proposed composition route.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GFM: GLANCE AND FOCUS MATTING NET-WORK</head><p>When tackling the animal image matting problem, we humans first glance at the image to quickly recognize the salient rough foreground or background areas and then focus on the transition areas to distinguish details from the background. It can be formulated as a rough segmentation stage and a matting stage roughly. Note that these two stages may be intertwined that there will be feedback from the second stage to correct the erroneous decision at the first stage, for example, in some ambiguous areas due to the protective coloration of animals or occlusions. To mimic the human experience and empower the matting model with proper abilities at both stages, it is reasonable to integrate them into a single model and explicitly model the collaboration. To this end, we propose a novel Glance and Focus Matting network for end-to-end natural animal image matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared Encoder</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, GFM has an encoder-decoder structure, where the encoder is shared by two subsequent decoders. The encoder takes a single image as input and processes it through five blocks E 0 ∼ E 4 , where each reduces the resolution by half. We adopt the ResNet-34 <ref type="bibr" target="#b34">[35]</ref> or DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> pre-trained on the ImageNet training set as our backbone encoder. Specifically, for DenseNet-121, we add a convolution layer to reduce the output feature channels to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Glance Decoder (GD)</head><p>The glance decoder aims to recognize the easy semantic parts and leave the others as unknown areas. To this end, the decoder should have a large receptive field to learn highlevel semantics. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we symmetrically stack five blocks D G 4 ∼ D G 0 as the decoder, each of which consists of three sequential 3 × 3 convolutional layers and an upsampling layer. To enlarge the receptive field further, we add a pyramid pooling module (PPM) <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> after E 4 to extract global context, which is connected to each decoder block D G i by element-wise summation. We adopt a sigmoid activation function after the decoder output.</p><p>Loss Function The training loss for glance decoder is a cross-entropy loss, denoted L CE in Eq. (2). where G c p ∈ [0, 1] is the predicted probability for cth class, G c g ∈ {0, 1} is the ground truth label. The output of GD is a two-or three-channel (C = 2 or 3) class probability map depends on the semantic-transition representation, which will be detailed in Section 3.4.</p><formula xml:id="formula_1">L CE = − C c=1 G c g log G c p ,<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Focus Decoder (FD)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, FD has the same basic structure as GD, i.e., symmetrically stacked five blocks D F 4 ∼ D F 0 . Different from GD, which aims to do roughly semantic segmentation, FD aims to extract details in the transition areas where low-level structural features are very useful. Therefore, we use a bridge block (BB) <ref type="bibr" target="#b38">[39]</ref> instead of the PPM after E 4 to leverage local context in different receptive fields. Specifically, it consists of three dilated convolutional layers. The features from both E 4 and BB are concatenated and fed into D F 4 . We follow the U-net <ref type="bibr" target="#b39">[40]</ref> style and add a shortcut between each encoder block E i and the decoder block D F i to preserve fine details.</p><p>Loss Function The training loss for FD (L F D ) is consist of an alpha-prediction loss L T α and a Laplacian loss L T lap in the unknown transition areas <ref type="bibr" target="#b0">[1]</ref>, i.e.,</p><formula xml:id="formula_2">L F D = L T α + L T lap<label>(3)</label></formula><p>Following <ref type="bibr" target="#b0">[1]</ref>, the alpha loss L T α is calculated as absolute difference between ground truth α and predicted alpha matte α F in the unknown transition region. It is defined as follows:</p><formula xml:id="formula_3">L T α = i α i − α F i × W T i 2 + ε 2 i W T i ,<label>(4)</label></formula><p>where i denotes pixel index, W T i ∈ {0, 1} denotes whether pixel i belongs to the transition region or not. We add ε = 10 −6 for computational stability.</p><p>Following <ref type="bibr" target="#b20">[21]</ref>, the Laplacian loss L T lap is defined as the L1 distance between the Laplacian pyramid of ground truth and that of prediction. We use five levels in the Laplacian pyramid. L T lap can be formulated as follows:</p><formula xml:id="formula_4">L T lap = i W T i 5 k=1 (Lap k (α i ) − Lap k (α F i ) 1 ,<label>(5)</label></formula><p>where Lap k denotes the kth level of the Laplacian pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RoSTa: Representation of Semantic and Transition Area</head><p>To investigate the impact of the representation format of the supervisory signal in our GFM, we adopt three kinds of Representations of Semantic and Transition areas (RoSTa) as bridge to link GD and FD.</p><p>• GFM-TT We use the classical 3-class trimap T as the supervisory signal for GD, which is generated by dilation and erosion from ground truth alpha matte with a kernel size of 25. We use the ground truth alpha matte α in the unknown transition areas as the supervisory signal for FD.</p><p>• GFM-FT We use the 2-class foreground segmentation mask F as the supervisory signal for GD, which is generated by the erosion of ground truth alpha matte with a kernel size of 50 to ensure the left foreground part is correctly labeled. In this case, the area of I (α &gt; 0) − F is treated as the transition area, where I (·) denotes the indicator function. We use the ground truth alpha matte α in the transition area as the supervisory signal for FD.</p><p>• GFM-BT We use the 2-class background segmentation mask B as the supervisory signal for glance decoder, which is generated by dilation of ground truth alpha matte with kernel size as 50 to ensure the left background part is correctly labeled. In this case, the area of B − I (α &gt; 0) is treated as the transition area. We use the ground truth alpha matte α in the transition area as the supervisory signal for FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Collaborative Matting (CM)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, CM merges the predictions from GD and FD to generate the final alpha prediction. Specifically, CM follows different rules when using different RoSTa as described in Section 3.4. In GFM-TT, CM replaces the transition area of the prediction of GD with the prediction of FD. In GFM-FT, CM adds the predictions from GD and FD to generate the final alpha matte. In GFM-BT, CM subtracts the prediction of FD from the prediction of GD as the final alpha matte. In this way, GD takes charge of recognizing rough foreground and background by learning global semantic features, and FD is responsible for matting details in the unknown areas by learning local structural features. Such task decomposition and specifically designed parallel decoders make the model simpler than the two-stage ones in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Besides, both decoders are trained simultaneously that the loss can be backpropagated to each of them via the merging node. In this way, our model enables interaction between both decoders that the erroneous prediction can be corrected in time by the responsible branch. Obviously, it is expected to be more effective than the two-stage framework, where the erroneous segmentation in the first stage could not be corrected by the subsequent one and thus mislead it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The training loss for collaborative matting (L CM ) consists of an alpha-prediction loss L α , a Laplacian loss L lap , and a composition loss L comp , i.e.,</p><formula xml:id="formula_5">L CM = L α + L lap + L comp .<label>(6)</label></formula><p>Here L α and L lap are calculated according to Eq. (4) and Eq. (5) but in the whole alpha matte. Following <ref type="bibr" target="#b0">[1]</ref>, the composition loss (L comp ) is calculated as the absolute difference between the composite images based on the ground truth alpha and the predicted alpha matte by referring to <ref type="bibr" target="#b1">[2]</ref>. It can be defined as follows:</p><formula xml:id="formula_6">L comp = i C(α i ) − C(α CM i ) 2 + ε 2 N ,<label>(7)</label></formula><p>where C (·) denotes the composited image accordingly, α CM is the predicted alpha matte by CM, N denotes the number of pixels in the alpha matte.</p><p>To sum up, the final loss used during training is calculated as the linear combination of L CE , L F D and L CM , i.e.,</p><formula xml:id="formula_7">L = λ 1 L CE + λ 2 L F D + λ 3 L CM ,<label>(8)</label></formula><p>where λ 1 ∼ λ 3 are loss weights to balance different losses.</p><p>They are set to 0.25 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RSSN: A NOVEL COMPOSITION ROUTE</head><p>Since labeling alpha matte of real-world natural images is very laborious and costly, a common practice is to generate large-scale composition images from a few of foreground images and the paired alpha mattes <ref type="bibr" target="#b0">[1]</ref>. The prevalent matting composition route is to paste one foreground with various background images by alpha blending according to Eq. (1). However, since the foreground and background images are usually sampled from different distributions, there will be a lot of composition artifacts in the composite images, which bring in a large domain gap between the composition images and natural ones. The composition artifacts may mislead the model by serving as cheap features, resulting in overfitting on the composite images and producing large generalizing errors on natural images. In this section, we systematically analyze the factors that cause the composition artifacts including Resolution discrepancy, Semantic ambiguity, Sharpness discrepancy, and Noise discrepancy. To address these issues, we propose a new composition route named RSSN and a large-scale highresolution background dataset named BG-20k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Resolution Discrepancy and Semantic Ambiguity</head><p>In the literature of image matting, the background images used for composition are usually chosen from existing benchmarks for image classification and detection, such as MS COCO <ref type="bibr" target="#b13">[14]</ref> and PASCAL VOC <ref type="bibr" target="#b14">[15]</ref>. However, these background images are in low-resolution and may contain salient objects, causing the following two types of discrepancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Resolution Discrepancy: A typical image in MS</head><p>COCO <ref type="bibr" target="#b13">[14]</ref> or Pascal VOC <ref type="bibr" target="#b14">[15]</ref> has a resolution about 389 × 466, which is much smaller compared to the high-resolution foreground images in matting dataset such as Compsition-1k <ref type="bibr" target="#b0">[1]</ref>. The resolution discrepancy between foreground and background images will result in obvious artifacts as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b). 2) Semantic ambiguity: Images in MS COCO <ref type="bibr" target="#b13">[14]</ref> and Pascal VOC <ref type="bibr" target="#b14">[15]</ref> are collected for classification and object detection tasks, which usually contain salient objects from different categories, including various animals. Directly pasting the foreground image with such background images will result in semantic ambiguity for end-to-end image matting. For example, as shown in <ref type="figure" target="#fig_2">Figure 3(b)</ref>, there is a dog in the background which is beside the leopard in the composite image. Training with such images will mislead the model to ignore the background animal, i.e., probably learning few about semantics but more about discrepancies.</p><p>To address these issues, we propose a large-scale highresolution dataset named BG-20k to serve as good background candidates for composition. We only selected those images whose shortest side has at least 1080 pixels to reduce the resolution discrepancy. Moreover, we removed those images containing salient objects to eliminate semantic ambiguity. The details of constructing BG-20k are presented as follows.</p><p>1) We collected 50k high-resolution (HD) images using the keywords such as HD background, HD view, HD scene, HD wallpaper, abstract painting, interior design, art, landscape, nature, street, city, mountain, sea, urban, suburb from websites with open licenses 2 , removed those images whose shortest side has less than 1080 pixels and resized the left images to have 1080 pixels at the shortest side while keeping the original aspect ratio. The average resolution of images in BG-20k is 1180 × 1539;</p><p>2. https://unsplash.com/ and https://www.pexels.com/  2) We removed duplicate images by a deep matching model <ref type="bibr" target="#b40">[41]</ref>. We adopted YOLO-v3 <ref type="bibr" target="#b41">[42]</ref> to detect salient objects and then manually double-checked to make sure each image has no salient objects. In this way, we built BG-20k containing 20,000 highresolution clean images; 3) We split BG-20k into a disjoint training set (15k) and validation set (5k).</p><p>An composition example using the background image from BG-20k is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(c) and <ref type="figure" target="#fig_2">Figure 3(d)</ref>. In (c), we use the foreground image computed by multiplying ground truth alpha matte with the original image for alpha blending, in (d), we use the foreground image computed by referring to the method in <ref type="bibr" target="#b1">[2]</ref> for alpha blending. As can be seen, there are obvious color artifacts in (c) that blends both colors of foreground and background in the fine details. The composite image in (d) is much more realistic than that in (c). Therefore, we adopt the method in <ref type="bibr" target="#b1">[2]</ref> for computing foreground images in our composition route. More examples of BG-20k are presented in <ref type="figure" target="#fig_3">Figure 4</ref> and the supplementary video to show its diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sharpness Discrepancy</head><p>In photography, it is usually to use a large aperture and focal length to capture a sharp and salient foreground image within the shallow depth-of-field, thus highlighting it from the background context, which is usually blurred due to the out-of-focus effect. An example is shown in <ref type="figure" target="#fig_2">Figure 3(a)</ref>, where the leopard is the center of interest and the background is blurred. Previous composition methods dismiss this effect, producing a domain gap of sharpness discrepancy between the composite images and natural photos. Since we target the animal matting task, where the animals are usually salient in the images, thereby we investigate this effect in our composition route. Specifically, we simulate it by adopting the averaging filter in OpenCV with a kernel size chosen from 20, 30, 40, 50, 60 randomly to blur the background images. Since some natural photos may not have blurred backgrounds, we only use this technique in our composition route with a probability of 0.5. An example is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(e), where the background is chosen from BG-20k and blurred using the averaging filter. As can be seen, it has a similar style to the original image in (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Noise Discrepancy</head><p>Since the foreground and background come from different image sources, they may contain different noise distributions. This is another type of discrepancy, which will mislead the model to search noise cues during training, resulting in overfitting. To address this discrepancy, we adopt BM3D <ref type="bibr" target="#b42">[43]</ref> to remove noise in both foreground and background images in RSSN. Furthermore, we add Gaussian noise with a standard deviation of 10 to the composite image such that the noise distributions in both foreground and background areas are the same. We find that it is effective in improving the generalization ability of trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">the Proposed Composition Route</head><p>We conclude the pipeline of our proposed composition route RSSN in Pipeline 1. The input of the pipeline is the matting dataset, e.g. our proposed AM-2k as will be introduced in Section 5, DIM <ref type="bibr" target="#b0">[1]</ref>, or DAPM <ref type="bibr" target="#b10">[11]</ref> and the proposed background image set BG-20k. If the matting dataset provides original images, e.g. AM-2K, DAPM <ref type="bibr" target="#b10">[11]</ref>, we compute the foreground from the original image given the alpha matte by referring to <ref type="bibr" target="#b1">[2]</ref>. We random sample K background candidates from BG-20k for each foreground for data augmentation. We set K = 5 in our experiments. For each foreground image and background image, we carried out the denoising step with a probability of 0.5. To simulate the effect of large-aperture, we carried out the blur step on the background image with a probability of 0.5, where the blur kernel size was randomly sampled from {20, 30, 40, 50, 60}. We then generated the composite image according to the alpha-blending equation Eq. (1). Finally, with a probability of 0.5, we added Gaussian noise to the composite image to ensure the foreground and background areas have the same noise distribution. To this end, we generate a composite image set that has reduced many kinds of discrepancies, thereby narrowing the domain gap with natural images. for each k ∈ [1, K] do <ref type="bibr">11:</ref> Sample a background candidate B ik ∈ BG-20k <ref type="bibr">12:</ref> if random() &lt; 0.5 then <ref type="bibr">13:</ref> F i = Denoise(F i ) //denoising by BM3D <ref type="bibr" target="#b42">[43]</ref> 14:</p><formula xml:id="formula_8">B ik = Denoise(B ik ) 15:</formula><p>end if <ref type="bibr">16:</ref> if random() &lt; 0.5 then <ref type="bibr">17:</ref> Sample a blur kernel size r ∈ {20, 30, 40, 50, 60}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>B ik = Blur(B ik , r) // the averaging filter <ref type="bibr">19:</ref> end if <ref type="bibr">20:</ref> Alpha blending:</p><formula xml:id="formula_9">C ik = F i × α i + B ik × (1 − α i ) 21:</formula><p>if random() &lt; 0.5 then <ref type="bibr">22:</ref> C ik = AddGaussianN oise(C ik ) <ref type="bibr">23:</ref> end if <ref type="bibr">24:</ref> end for <ref type="bibr" target="#b24">25</ref>: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AM-2K: ANIMAL MATTING DATASET</head><p>In this section, we present the details of our AM-2k dataset, including the process of collection and labeling, a brief comparison with other matting datasets, the dataset partition, and benchmark tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Collection and Labeling</head><p>The collection and labeling process of AM-2k are as follows:</p><p>1) collect about 5,000 animal-related images from websites with open licenses as in Section 4.1, make sure they are free to use and their shortest sides have more than 1080 pixels; 2) retain images containing salient animals of clear appearance and remove duplicate images by a deep matching model <ref type="bibr" target="#b40">[41]</ref>; 3) resize the retained images to have 1080 pixels at the shortest side while keeping the original aspect ratio; 4) manually annotate the category label for each image and the alpha matte using open-source image editing software, e.g. Adobe Photoshop, GIMP, etc.</p><p>After the above process, we build our AM-2k dataset which contains 2,000 animal images in total from 20 categories evenly. All the categories are listed as follows in alphabetical order: alpaca, antelope, bear, camel, cat, cattle, deer, dog, elephant, giraffe, horse, kangaroo, leopard, lion, monkey, rabbit, rhinoceros, sheep, tiger, zebra. We present some examples from our AM-2k dataset in <ref type="figure" target="#fig_5">Figure 5</ref>, where the original images and the corresponding alpha mattes are displayed. More examples can be found in the supplementary video.</p><p>We make a brief comparison between our AM-2k dataset and other representative matting datasets in the literature from the perspective of availability of original images (AOI), dataset volume, average resolution, and the volume of animal images. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Compared with DAPM <ref type="bibr" target="#b10">[11]</ref> and LF <ref type="bibr" target="#b7">[8]</ref>, images and alpha mattes in our AM-2k have higher resolution, which is very important for high-definition image matting, e.g., extracting fine details in images. Besides, AM-2k is specifically constructed for animal matting while other datasets only contain a few or no animal images. Compared to Comp-1k <ref type="bibr" target="#b0">[1]</ref>, LF <ref type="bibr" target="#b7">[8]</ref>, and HAtt <ref type="bibr" target="#b8">[9]</ref>, AM-2k has more images and also provides the original images, where the others only provide the foreground images. As we will demonstrate in the experiment part, the common practice that training on composite images based on the foregrounds will result in a model with poor generalization on natural images, since it may bias towards the composition artifacts. On the contrary, the model trained on AM-2k has no such a drawback since we provide the original natural images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset Partition and Benchmark Tracks</head><p>After constructing AM-2k, we then partition it into disjoint training and validation sets and set up two evaluation tracks for different purposes. Details are given below:</p><p>1) ORI-Track (Original Images Based Track) is set to perform end-to-end matting tasks on the original natural images. We randomly select 90 images in each category to form the training set while using the left 10 images in each category to form the validation set. We also reserve another 500 images from the other five categories of animals that are disjoint from the training and validation sets, serving as the test set. The ORI-Track is the primary benchmark track. 2) COMP-Track (Composite Images Based Track) is set to carry out domain adaptation studies for image matting. As discussed before, the composite images have a large domain gap with natural images due to the composition artifacts. If we can reduce the domain gap and learn a domain-invariant feature representation, we can obtain a model with better generalization. To this end, we set up this track by making the first attempt to this research direction. Specifically, we construct the composite training set by alpha-blending each foreground with five background images from the COCO dataset <ref type="bibr" target="#b13">[14]</ref> (denote as COMP-COCO) and our BG-20k dataset (denote as COMP-BG20K), or adopting the composition route RSSN proposed in Section 4.4 based our BG-20K (denote as COMP-RSSN). Moreover, unlike previous benchmarks that evaluate matting methods on composite images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, we evaluate matting methods on natural images of the validation set in the ORI-Track to validate their generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EMPIRICAL STUDIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>Datasets. Experiments were carried out on two tracks of our AM-2k dataset: 1) to compare our proposed method with state-of-the-art (SOTA) methods, we trained and evaluated them on the ORI-Track; 2) to evaluated the side effect of domain gap caused by composition and the proposed composition route, we trained and evaluated GFM and SOTA methods on the COMP-Track, i.e., COMP-COCO, COMP-BG20k, and COMP-RSSN, respectively. Evaluation Metrics. Following the common practice in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we used the mean squared error (MSE), the sum of absolute differences (SAD), gradient (Grad.), and connectivity (Conn.) as the major metrics to evaluate the quality of alpha matte predictions. Note that the MSE and SAD metrics evaluate the quantitative difference between the prediction and ground truth alpha matte, while the gradient and connectivity metrics favor clear details. Besides, we also use some auxiliary metrics such as Mean Absolute Difference (MAD), SAD-TRAN (SAD in the transition areas), SAD-FG (SAD in the foreground areas), and SAD-BG (SAD in the background areas) to comprehensively evaluate the quality of alpha matte prediction. While MAD evaluates the average quantitative difference regardless of the image size, SAD-TRAN, SAD-FG, and SAD-BG evaluate SAD in different semantic areas, respectively. In addition, we also compared the model complexity of different methods in terms of the number of parameters, computational complexity, and inference time.</p><p>Implementation Details During training, we used multi-scale augmentation similar to <ref type="bibr" target="#b0">[1]</ref>. Specifically, we cropped each of the selected images with size from {640 × 640, 960 × 960, 1280 × 1280} randomly, resized the cropped image to 320 × 320, and randomly flipped it with a probability of 0.5. The encoder of GFM was initialized with the ResNet-34 <ref type="bibr" target="#b34">[35]</ref> or DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> pre-trained on the ImageNet dataset. GFM was trained on two NVIDIA Tesla V100 GPUs. The batch size was 4 for DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> and 32 for ResNet-34 <ref type="bibr" target="#b34">[35]</ref>. For COMP-Track, we composite five training images by using five different backgrounds for each foreground on-the-fly during training. It took about two days to train GFM for 500 epochs on ORI-Track and 100 epochs for COMP-Track. The learning rate was fixed to 1×10 −4 for the ORI-Track and 1×10 −5 for the COMP-Track.</p><p>For baseline methods LF <ref type="bibr" target="#b7">[8]</ref> and SSS <ref type="bibr" target="#b11">[12]</ref>, we used the official codes released by authors. For SHM <ref type="bibr" target="#b6">[7]</ref>, HAtt <ref type="bibr" target="#b8">[9]</ref> and SHMC <ref type="bibr" target="#b9">[10]</ref> with no public codes, we re-implemented them according to the papers. For SHMC <ref type="bibr" target="#b9">[10]</ref> which does not specify the backbone network, we used ResNet-34 <ref type="bibr" target="#b34">[35]</ref> for a fair comparison. These models were trained using the training set on ORI-Track or COMP-Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quantitative and Subjective Evaluation</head><p>Results on the ORI-Track. We benchmarked several SOTA methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> on the ORI-Track of AM-2k. The results are summarized in the top of <ref type="table" target="#tab_1">Table 2</ref>. GFM-TT, GFM-FT, and GFM-BT denote the proposed GFM model with different RoSTa as described in Section 3.4. (d) stands for using DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> as the backbone encoder while (r) stands for using ResNet-34 <ref type="bibr" target="#b34">[35]</ref> as the backbone encoder. There are several empirical findings from <ref type="table" target="#tab_1">Table 2</ref>.</p><p>First, SSS <ref type="bibr" target="#b11">[12]</ref> achieved the worst performance with a large foreground SAD error of 401.66 compare with others, the reason can be twofold: 1) they adopt the pretrained Deeplab-ResNet-101 <ref type="bibr" target="#b43">[44]</ref> model as the semantic feature extractor to calculate affinities. The pre-trained model may generate limited representative features on our highresolution animal matting dataset which degrade the performance; and 2) this method aims to extract all the semantic regions in the image while other matting methods are trained to extract only the salient animal foreground. Second, SHMC using global guidance <ref type="bibr" target="#b9">[10]</ref> and stage-wise method LF <ref type="bibr" target="#b7">[8]</ref> perform better than SSS <ref type="bibr" target="#b11">[12]</ref> in all evaluation metrics. However, the SAD errors in the transition area dominate the total errors, which is 35.23 and 19.68 respectively. The reason is that both of them have not explicitly define the transition area, thereby the matting network has limited ability to distinguish the details in the transition area when it needs segment foreground and background areas using the same network at the same time. It can also be confirmed by the scores of Grad. and Conn. Third, HAtt <ref type="bibr" target="#b8">[9]</ref> performs better than SHMC <ref type="bibr" target="#b9">[10]</ref> and LF <ref type="bibr" target="#b7">[8]</ref> in terms of SAD error in the transition area and foreground area because the attention module it adopted can provide better global appearance filtration. However, using a single network to model both the foreground and background areas and the transition areas of plentiful details makes it hard to powerful representative features for both areas, resulting in large SAD errors especially in the background areas as well as large Grad. and Conn. errors.</p><p>Fourth, SHM <ref type="bibr" target="#b6">[7]</ref> performs the best among all the SOTA methods. It reduces the SAD error in the transition area from 13.36 to 10.26 and the SAD error in the background area from 13.29 to 6.95 compared with HAtt <ref type="bibr" target="#b8">[9]</ref>. We believe the improvement credits to the explicit definition of RoSTa and the PSPNet <ref type="bibr" target="#b36">[37]</ref> used in the first stage which has a good semantic segmentation capability. However, SHM <ref type="bibr" target="#b6">[7]</ref> still has large error in the background area due to its stagewise pipeline, which will accumulate the segmentation error into the matting network. Last, compare with all the SOTA methods, GFM outperforms them in all evaluation metrics, achieving the best performance by simultaneously segmenting the foreground and background and matting on the transition areas, no matter which kind of RoSTa it uses. For example, it achieves the lowest SAD error in different areas, i.e. 8.45 v.s. 10.26 in the transition area, 0.57 v.s. 0.60 in the foreground area, and 0.96 v.s. 6.95 in the background area compared with the previous best method SHM <ref type="bibr" target="#b6">[7]</ref>. The results of using different RoSTa are comparable, especially for FT and BT, since they both define two classes in the image for segmentation by the Glance Decoder. GFM using TT as RoSTa performs the best due to its explicit definition of the transition area as well as the foreground and background areas. We also tried two different backbone networks, ResNet-34 <ref type="bibr" target="#b34">[35]</ref> and DenseNet-121 <ref type="bibr" target="#b35">[36]</ref>. Both of them achieve the best performance compared with other SOTA methods, while DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> is marginally better.</p><p>The reason of GFM's superiority over other methods can be explained as follows. First, compared with stagewise methods e.g. SHM <ref type="bibr" target="#b6">[7]</ref>, SSS <ref type="bibr" target="#b11">[12]</ref> and LF <ref type="bibr" target="#b7">[8]</ref>, GFM can be trained in a single stage and the collaboration module acts as an effective gateway to propagate matting errors to the responsible branch adaptively. Second, compared with methods that adopt global guidance e.g. HAtt <ref type="bibr" target="#b8">[9]</ref> and SHMC <ref type="bibr" target="#b9">[10]</ref>, GFM explicitly model the end-to-end matting task into two separate but collaborate sub-tasks by two distinct decoders. Moreover, it uses a collaboration module to merge the predictions according to the definition of RoSTa, which explicitly defines the role of each decoder.</p><p>From <ref type="figure" target="#fig_6">Figure 6</ref>, we can find similar observations. SHM <ref type="bibr" target="#b6">[7]</ref>, LF <ref type="bibr" target="#b7">[8]</ref>, and SSS <ref type="bibr" target="#b11">[12]</ref> fail to segment some foreground parts, implying inferiority of its stage-wise network structure, since they do not distinguish the foreground/background and the transition areas explicitly in the model. It is hard to balance the role of semantic segmentation for the former and matting details for the latter, which requires global semantic and local structural features, respectively. HAtt <ref type="bibr" target="#b8">[9]</ref> and SHMC <ref type="bibr" target="#b9">[10]</ref> struggle to obtain clear details in the transition areas since the global guidance is helpful for recognizing the semantic areas while being less useful for matting of details. Compared to them, GFM achieves the best results owing to the virtue of a unified model, which deals with the foreground/background and transition areas using separate decoders and optimizes them in a collaborative manner. More results of GFM can be found in the supplementary video.</p><p>Results on the COMP-Track. We evaluated the best performed SOTA method SHM <ref type="bibr" target="#b6">[7]</ref> and our GFM with two different backbones on AM-2k's COMP-Track including COMP-COCO, COMP-BG20K, and COMP-RSSN. The results are summarized in the bottom of <ref type="table" target="#tab_1">Table 2</ref>, from which we have several empirical findings. First, when training matting models using images from MS COCO dataset <ref type="bibr" target="#b13">[14]</ref> as backgrounds, GFM performs much better than SHM <ref type="bibr" target="#b6">[7]</ref>, i.e 46.16 and 30.05 v.s. 182.70 in whole image SAD, confirming the superiority of the proposed model over the twostage one for generalization. Second, GFM using ResNet-34 <ref type="bibr" target="#b34">[35]</ref> performs better than using DenseNet-121 <ref type="bibr" target="#b35">[36]</ref>, which is different from the results on the ORI-TRACK. We suspect that DenseNet-121 has a slightly better representation capacity than ResNet-34, thereby it can achieve better results on the ORI-TRACK while being a little overfitting of composite images on the COMP-TRACK. Third, when training matting models using background images from the proposed BG-20k dataset, the errors of all the methods are significantly reduced, especially for SHM <ref type="bibr" target="#b6">[7]</ref>, i.e., from 182.70 to 52.36, which mainly attributes to the reduction of SAD error in the background area, i.e., from 134.43 to 33.52. There is the same trend for GFM(d) and GFM(r) as well. These results confirm the value of our BG-20k, which helps to reduce resolution discrepancy and eliminate semantic ambiguity in the background area. Fourth, when training matting models using the proposed composition route, the errors can be reduced further, i.e., from 52.36 to 23.94 for SHM <ref type="bibr" target="#b6">[7]</ref>, from 25.19 to 19.19 for GFM(d), and from 16.44 to 15.88 for GFM(r). The performance improvement is attributed to the composition techniques in our RSSN: 1) we simulate the large-aperture effect to reduce sharpness discrepancy; and 2) we remove the noise of foreground/background and add noise to the composite image to reduce noise discrepancy. It is noted that the SAD error of SHM <ref type="bibr" target="#b6">[7]</ref> has dramatically reduced about 87% from 182.70 to 23.93 when using the proposed RSSN for composition compared with using the traditional composition method based on MS COCO dataset, which is even comparable with the one obtained by training using original images, i.e., 17.81. It demonstrates that the proposed composition route RSSN can significantly narrow the domain gap between the composite images and realworld natural images. Last, We also conducted experiments by using different RoSTa in GFM(d) on COMP-RSSN, their results have a similar trend to that on the ORI-TRACK. Model Ensemble Since we propose three different RoSTa for GFM, it is interesting to investigate their complementary. To this end, we calculated the result by a model ensemble which takes the median of the alpha predictions from three models as the final prediction. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the result of the model ensemble is better than any single one, i.e., 9.21 v.s. 10.27 for GFM-TT(d), 9.92 v.s. 10.89 for GFM-TT(r), confirming the complementarity between different RoSTa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Ensemble and Hybrid-resolution Test</head><p>Hybrid-resolution Test For our GFM, we also proposed a hybrid-resolution test strategy to balance GD and FD. Specifically, we first fed a down-sampled image to GFM to get an initial result. Then, we used the full resolution image as input and only used the predicted alpha matte from FD to replace the initial prediction in the transition areas. For simplicity, we denote the down-sampling ratio at each step as d 1 and d 2 , which are subject to d 1 ∈ {1/2, 1/3, 1/4}, d 2 ∈ {1/2, 1/3, 1/4}, and d 1 ≤ d 2 . We summarize the results of GFM-TT(d) in <ref type="table" target="#tab_2">Table 3</ref>. A smaller d 1 increases the effective receptive field and benefits the Glance Decoder, while a larger d 2 stands for a higher resolution image and benefits the Focus Decoder with clear details. Finally, we set d 1 = 1/3 and d 2 = 1/2 for a trade-off. Results on the ORI-Track. To further verify the benefit of designed structure in GFM, we conducted ablation studies on several variants of GFM on AM-2k's ORI-Track including 1) motivated by Qin et.al <ref type="bibr" target="#b38">[39]</ref>, in GFM encoder when using ResNet-34 <ref type="bibr" target="#b34">[35]</ref> as the backbone, we modified the convolution kernel of E 0 from 7 × 7 with stride 2 to 3 × 3 with stride 1, removed the first max pooling layer in E 0 , and added two more encoder layers E 5 and E 6 after E 4 , each of which had a max pooling layer with stride 2 and three basic res-blocks with 512 filters, denoting "r2b"; 2) using a single decoder to replace both FD and GD in GFM, denoting "SINGLE"; 3) excluding the pyramid pooling module (PPM) in GD, and 4) excluding the bridge block(BB) in FD. The results are summarized in the top of <ref type="table" target="#tab_3">Table 4</ref>. First, when using r2b structure, all the metrics have been improved compared with GFM-TT(r), which is attributed to the larger feature maps at the early stage of the encoder part. However, it has more parameters and computations than GFM-TT(r), which will be discussed later. Second, using only single decoder results in worse performance, i.e., SAD increases from 10.27  <ref type="bibr" target="#b6">[7]</ref>, LF <ref type="bibr" target="#b7">[8]</ref>, SSS <ref type="bibr" target="#b11">[12]</ref>, HAtt <ref type="bibr" target="#b8">[9]</ref>, SHMC <ref type="bibr" target="#b9">[10]</ref>, and GFM. to 13.79 for GFM-TT(d) and 10.89 to 15.50 for GFM-TT(r), which confirms the value of decomposing the end-to-end image matting task into two collaborative sub-tasks. Third, without PPM, SAD increases from 10.86 to 10.27 for GFM-TT(d) and 11.90 to 10.89 for GFM-TT(r), demonstrating that the global context features by PPM due to its larger receptive field are beneficial for semantic segmentation in GD. Fourth, without BB, SAD increases from 10.27 to 11.27 for GFM-TT(d) and 10.89 to 11.29 for GFM-TT(r), demonstrating that the learned local structural features by BB due to its dilated convolutional layers are beneficial for matting in FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Study</head><p>Results on the COMP-Track. In order to verify the different techniques in the proposed composition route RSSN, we conducted ablation studies on several variants of RSSN: 1) only using the simulation of large-aperture effect, denoting "w/ blur"; 2) only removing foreground and background noise, denoting "w/ denoise"; 3) only adding noise on the composite images, denoting "w/ noise"; and 4) using all the techniques in RSSN, denoting "w/ RSSN". We used the BG-20k for sampling background images in these experiments. The results are summarized at the bottom of <ref type="table" target="#tab_3">Table 4</ref>. First, compared with the baseline model listed in the first row, which was trained using the composite images by alpha-blending, each technique in the proposed composition route is helpful to improve the matting performance in terms of all the metrics. Second, simulation of large-aperture effect and adding noise on the composite images are more effective than denoising. Third, different techniques are complementary to each other that they contribute to the best performance achieved by RSSN collaboratively. We compared the number of model parameters (million, denoting "M"), computing complexity (denoting "GMac"), and inference time (seconds, denoting "s") of each method on an image resized to 800×800. All methods are performed on a server with an Intel Xeon CPU (2.30GHz) and an NIVDIA Tesla V100 GPU (16GB memory). As shown in <ref type="table" target="#tab_4">Table 5</ref>, GFM using either DenseNet-121 <ref type="bibr" target="#b35">[36]</ref> or ResNet-34 <ref type="bibr" target="#b34">[35]</ref> as the backbone surpasses SHM <ref type="bibr" target="#b6">[7]</ref>, LF <ref type="bibr" target="#b7">[8]</ref>, Hatt <ref type="bibr" target="#b8">[9]</ref> and SHMC <ref type="bibr" target="#b9">[10]</ref> in running speed, i.e., taking about 0.2085s and 0.1734s to process an image. In terms of parameters, GFM has fewer parameters than all the SOTA methods except for LF <ref type="bibr" target="#b7">[8]</ref>. For computing complexity, GFM has fewer computations than all the SOTA methods when adopting ResNet-34 <ref type="bibr" target="#b34">[35]</ref> as the backbone, i.e., 132.28 GMacs. When adopting DenseNet-121 <ref type="bibr" target="#b35">[36]</ref>, it only has more computations than SHMC <ref type="bibr" target="#b9">[10]</ref> while being smaller. As for GFM(r2b), it has more parameters and computations. Although it can achieve better results, a trade-off between performance and complexity should be made for practical applications. Generally, GFM is light-weight and computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Model Complexity Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORKS</head><p>In this paper, we propose a novel image matting model for natural animal image matting. It addresses two challenges in this matting problem: 1) recognizing various foreground animals with diverse shapes, sizes, and textures from different categories; and 2) extracting fur details from ambiguous context background. Specifically, a Glance Decoder is devised for the first task and a Focus Decoder is devised for the latter one, while they share an encoder and are trained jointly. Therefore, they collaboratively accomplish the endto-end animal matting task and achieve superior performance than state-of-the-art matting methods. Besides, we also point out the domain discrepancy between composite images and natural ones which suggests that the common practice for data augmentation may not be suitable for training end-to-end matting models. To remedy this issue, we establish the first large-scale natural animal image matting dataset, which contains 2,000 high-resolution animal images from 20 categories and manually labeled alpha mattes. We also construct a background dataset containing 20,000 high-resolution images without salient objects. Furthermore, we systematically analyze the factors affecting composition quality and propose a novel composition route, which can effectively address the domain discrepancy issue. Extensive experiments validate the superiority of the proposed methods over state-of-the-art methods. We believe the proposed matting method and composition route will benefit the research for both trimap-based and end-to-end image matting. Moreover, the proposed dataset can provide a testbed to study the matting problem including the domain discrepancy issue.</p><p>Although GFM outperforms state-of-the-art methods in terms of both objective metrics and subjective evaluation, there are some limitations to be addressed in future work. First, after taking a detailed analysis of the error source as evidenced by SAD-TRAN, SAD-FG, and SAD-BG, the error in the transition areas is larger than in that in the foreground and background areas, i.e., 8.45 v.s. 1.83, even if the size of transition areas is usually much smaller than that of foreground and background areas. It tells that the performance could be further enhanced by devising a more effective Focus Decoder as well as leveraging some structure-aware and perceptual losses. Second, there is still room to improve for the composite-based model since the cost required to generate a composite-based training set is much easier than constructing a natural images-based one. Given that alpha matting and alpha blending are inverse problems, it is interesting to see whether or not these two tasks benefit each other if we model them in a single framework. Third, it is interesting to investigate the impact of synthesizing appearance-like composite images as natural ones to reduce the domain gap and augmentation techniques to avoid overfitting as well as their differences in improving the generalization ability of matting models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The comparison between SOTA end-to-end-matting methods in (i) and (ii) and our GFM in (iii). (b) The comparison between existing matting datasets and our proposed AM-2k as well as the comparison between existing composition methods and our RSSN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Diagram of the proposed Glance and Focus Matting (GFM) network, consists of a shared encoder and two separate decoders responsible for whole image rough segmentation and transition area's details matting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of different image composition methods. (a) Original natural image. (b) Composite with background from MS COCO [14] with foreground computed by [2]. (c) Composite with background from our proposed BG-20k by alpha blending of original image directly. (d) Composite with background from our proposed BG-20k with foreground computed by [2]. (e) Composite with large-aperture effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Some examples from our BG-20k dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Pipeline 1 : 2 : 3 : 4 : 5 :</head><label>12345</label><figDesc>the Proposed Composition Route: RSSN Input: The matting dataset M containing |M | images and the background image set BG-20k Output: The composite image set C 1: for each i ∈ [1, |M |] do if there are original images in M , e.g. AM-2k, then Sample an original image I i ∈ M Sample the paired alpha matte α i ∈ M Compute the foreground F i given (I i , α i ) image F i ∈ M 8: Sample the paired alpha matte α i ∈ M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Some examples from our AM-2k dataset. The alpha matte is displayed beside the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative comparison of different methods on the AM-2k ORI-Track, including SHM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Comparison between AM-2k and existing matting datasets.</figDesc><table><row><cell>Dataset DAPM [11]</cell><cell cols="2">AOI Volume √ 2,000</cell><cell>Resolution 800 × 600</cell><cell>Animal 0</cell></row><row><cell>Comp-1k [1]</cell><cell>×</cell><cell>481</cell><cell>1, 083 × 1, 298</cell><cell>41</cell></row><row><cell>LF [8]</cell><cell>×</cell><cell>257</cell><cell>756 × 553</cell><cell>0</cell></row><row><cell>HAtt [9] AM-2k</cell><cell>× √</cell><cell>646 2,000</cell><cell>1730 × 1572 1, 196 × 1, 470</cell><cell>61 2,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Results on AM-2k's ORI-Track, COMP-Track. (d) stands for DenseNet-121<ref type="bibr" target="#b35">[36]</ref> backbone, (r) stands for ResNet-34<ref type="bibr" target="#b34">[35]</ref> backbone. Representations of T T , F T and BT can refer to Section 3.4.</figDesc><table><row><cell>Track</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Model</head><label>3</label><figDesc>Ensemble and Hybrid-resolution Test.</figDesc><table><row><cell></cell><cell>Track</cell><cell></cell><cell></cell><cell>Ensemble</cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell>Grad.</cell><cell>Conn.</cell></row><row><cell cols="2">GFM-ENS(d)</cell><cell>9.21</cell><cell>0.0021</cell><cell>0.0054</cell><cell>8.00</cell><cell>8.16</cell></row><row><cell cols="2">GFM-ENS(r)</cell><cell>9.92</cell><cell>0.0024</cell><cell>0.0058</cell><cell>8.82</cell><cell>8.92</cell></row><row><cell></cell><cell>Track</cell><cell></cell><cell></cell><cell>Hybrid</cell><cell></cell><cell></cell></row><row><cell>d 1</cell><cell>d 2</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell>Grad.</cell><cell>Conn.</cell></row><row><cell>1/2</cell><cell>1/2</cell><cell>12.57</cell><cell>0.0041</cell><cell>0.0074</cell><cell>9.26</cell><cell>11.75</cell></row><row><cell>1/3</cell><cell>1/2</cell><cell>10.27</cell><cell>0.0027</cell><cell>0.0060</cell><cell>8.80</cell><cell>9.37</cell></row><row><cell>1/3</cell><cell>1/3</cell><cell>11.58</cell><cell>0.0028</cell><cell>0.0067</cell><cell>11.67</cell><cell>10.67</cell></row><row><cell>1/4</cell><cell>1/2</cell><cell>13.23</cell><cell>0.0045</cell><cell>0.0078</cell><cell>10.00</cell><cell>12.26</cell></row><row><cell>1/4</cell><cell>1/3</cell><cell>14.65</cell><cell>0.0047</cell><cell>0.0086</cell><cell>12.46</cell><cell>13.68</cell></row><row><cell>1/4</cell><cell>1/4</cell><cell>17.29</cell><cell>0.0055</cell><cell>0.0102</cell><cell>16.50</cell><cell>16.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Ablation study of GFM on ORI-Track and COMP-Track.</figDesc><table><row><cell>Track</cell><cell></cell><cell></cell><cell>ORI</cell></row><row><cell>Method</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Comparison of model parameters, computing complexity, and running time. (d) and (r) stand for DenseNet-121<ref type="bibr" target="#b35">[36]</ref> and ResNet-34<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell>Method</cell><cell>Parameters</cell><cell>Complexity</cell><cell>Running</cell></row><row><cell></cell><cell>(M)</cell><cell>(GMac)</cell><cell>time (s)</cell></row><row><cell>SHM [7]</cell><cell>79.27</cell><cell>870.16</cell><cell>0.3346</cell></row><row><cell>LF [8]</cell><cell>37.91</cell><cell>2821.14</cell><cell>0.3623</cell></row><row><cell>HAtt [9]</cell><cell>106.96</cell><cell>1502.46</cell><cell>0.5176</cell></row><row><cell>SHMC [10]</cell><cell>78.23</cell><cell>139.55</cell><cell>0.4863</cell></row><row><cell>GFM-TT(d)</cell><cell>46.96</cell><cell>244.0</cell><cell>0.2085</cell></row><row><cell>GFM-TT(r)</cell><cell>55.29</cell><cell>132.28</cell><cell>0.1734</cell></row><row><cell>GFM-TT(r2b)</cell><cell>126.85</cell><cell>1526.79</cell><cell>0.2268</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An iterative optimization approach for unified image segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzymatte: A computationally efficient scheme for interactive matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting semantic human matting with coarse annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8563" to="8572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Poisson matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Knn matting. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4130" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8819" to="8828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunc Ozan</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3055" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding and improving the realism of image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3789" to="3797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward realistic image compositing with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Bor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8415" to="8424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dovenet: Deep image harmonization via domain verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8394" to="8403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Background matting: The world is your green screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2291" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8198" to="8207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What makes good synthetic training data for learning disparity and optical flow estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="942" to="960" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bm3d image denoising with shape-adaptive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPARS&apos;09-Signal Processing with Adaptive Sparse Structured Representations</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
