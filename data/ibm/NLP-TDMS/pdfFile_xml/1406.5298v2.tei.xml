<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Learning with Deep Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
							<email>d.p.kingma@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Group</orgName>
								<orgName type="institution">Univ. of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
							<email>danilor@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Group</orgName>
								<orgName type="institution">Univ. of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
							<email>shakir@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Group</orgName>
								<orgName type="institution">Univ. of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>m.welling@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Group</orgName>
								<orgName type="institution">Univ. of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Learning with Deep Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.</p><p>For an updated version of this paper, please see</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semi-supervised learning considers the problem of classification when only a small subset of the observations have corresponding class labels. Such problems are of immense practical interest in a wide range of applications, including image search <ref type="bibr" target="#b5">(Fergus et al., 2009)</ref>, genomics <ref type="bibr" target="#b19">(Shi and Zhang, 2011)</ref>, natural language parsing <ref type="bibr" target="#b10">(Liang, 2005)</ref>, and speech analysis <ref type="bibr" target="#b11">(Liu and Kirchhoff, 2013)</ref>, where unlabelled data is abundant, but obtaining class labels is expensive or impossible to obtain for the entire data set. The question that is then asked is: how can properties of the data be used to improve decision boundaries and to allow for classification that is more accurate than that based on classifiers constructed using the labelled data alone. In this paper we answer this question by developing probabilistic models for inductive and transductive semi-supervised learning by utilising an explicit model of the data density, building upon recent advances in deep generative models and scalable variational inference <ref type="bibr" target="#b8">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b16">Rezende et al., 2014)</ref>.</p><p>Amongst existing approaches, the simplest algorithm for semi-supervised learning is based on a self-training scheme <ref type="bibr" target="#b18">(Rosenberg et al., 2005)</ref> where the the model is bootstrapped with additional labelled data obtained from its own highly confident predictions; this process being repeated until some termination condition is reached. These methods are heuristic and prone to error since they can reinforce poor predictions. Transductive SVMs (TSVM) <ref type="bibr" target="#b6">(Joachims, 1999)</ref> extend SVMs with the aim of max-margin classification while ensuring that there are as few unlabelled observations near the margin as possible. These approaches have difficulty extending to large amounts of unlabelled data, and efficient optimisation in this setting is still an open problem. Graph-based methods are amongst the most popular and aim to construct a graph connecting similar observations; label information propagates through the graph from labelled to unlabelled nodes by finding the minimum energy (MAP) configuration <ref type="bibr" target="#b1">(Blum et al., 2004;</ref><ref type="bibr" target="#b25">Zhu et al., 2003)</ref>. Graph-based approaches are sensitive to the graph structure and require eigen-analysis of the graph Laplacian, which limits the scale to which these methods can be applied -though efficient spectral methods are now available <ref type="bibr" target="#b5">(Fergus et al., 2009)</ref>. Neural network-based approaches combine unsupervised and supervised learning by training feed-forward classifiers with an additional penalty from an auto-encoder or other unsupervised embedding of the data <ref type="bibr" target="#b15">(Ranzato and Szummer, 2008;</ref><ref type="bibr" target="#b23">Weston et al., 2012)</ref>. The Manifold Tangent Classifier (MTC) <ref type="bibr" target="#b17">(Rifai et al., 2011)</ref> trains contrastive auto-encoders (CAEs) to learn the manifold on which the data lies, followed by an instance of TangentProp to train a classifier that is approximately invariant to local perturbations along the manifold. The idea of manifold learning using graph-based methods has most recently been combined with kernel (SVM) methods in the Atlas RBF model <ref type="bibr" target="#b14">(Pitelis et al., 2014)</ref> and provides amongst most competitive performance currently available.</p><p>In this paper, we instead, choose to exploit the power of generative models, which recognise the semi-supervised learning problem as a specialised missing data imputation task for the classification problem. Existing generative approaches based on models such as Gaussian mixture or hidden Markov models <ref type="bibr" target="#b24">(Zhu, 2006)</ref>, have not been very successful due to the need for a large number of mixtures components or states to perform well. More recent solutions have used non-parametric density models, either based on trees <ref type="bibr" target="#b7">(Kemp et al., 2003)</ref> or Gaussian processes <ref type="bibr" target="#b0">(Adams and Ghahramani, 2009</ref>), but scalability and accurate inference for these approaches is still lacking. Variational approximations for semi-supervised clustering have also been explored previously <ref type="bibr" target="#b9">(Li et al., 2009;</ref><ref type="bibr" target="#b22">Wang et al., 2009)</ref>. Thus, while a small set of generative approaches have been previously explored, a generalised and scalable probabilistic approach for semi-supervised learning is still lacking. It is this gap that we address through the following contributions:</p><p>• We describe a new framework for semi-supervised learning with generative models, employing rich parametric density estimators formed by the fusion of probabilistic modelling and deep neural networks. • We show for the first time how variational inference can be brought to bear upon the problem of semi-supervised classification. In particular, we develop a stochastic variational inference algorithm that allows for joint optimisation of both model and variational parameters, and that is scalable to large datasets. • We demonstrate the performance of our approach on a number of data sets providing stateof-the-art results on benchmark problems. • We show qualitatively generative semi-supervised models learn to separate the data classes (content types) from the intra-class variabilities (styles), allowing in a very straightforward fashion to simulate analogies of images on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Generative Models for Semi-supervised Learning</head><p>We are faced with data that appear as pairs (X, Y) = {(x 1 , y 1 ), . . . , (x N , y N )}, with the i-th observation x i ∈ R D and the corresponding class label y i ∈ {1, . . . , L}. Observations will have corresponding latent variables, which we denote by z i . We will omit the index i whenever it is clear that we are referring to terms associated with a single data point. In semi-supervised classification, only a subset of the observations have corresponding class labels; we refer to the empirical distribution over the labelled and unlabelled subsets as p l (x, y) and p u (x), respectively. We now develop models for semi-supervised learning that exploit generative descriptions of the data to improve upon the classification performance that would be obtained using the labelled data alone.</p><p>Latent-feature discriminative model (M1): A commonly used approach is to construct a model that provides an embedding or feature representation of the data. Using these features, a separate classifier is thereafter trained. The embeddings allow for a clustering of related observations in a latent feature space that allows for accurate classification, even with a limited number of labels. Instead of a linear embedding, or features obtained from a regular auto-encoder, we construct a deep generative model of the data that is able to provide a more robust set of latent features. The generative model we use is:</p><formula xml:id="formula_0">p(z) = N (z|0, I); p θ (x|z) = f (x; z, θ),<label>(1)</label></formula><p>where f (x; z, θ) is a suitable likelihood function (e.g., a Gaussian or Bernoulli distribution) whose probabilities are formed by a non-linear transformation, with parameters θ, of a set of latent variables z. This non-linear transformation is essential to allow for higher moments of the data to be captured by the density model, and we choose these non-linear functions to be deep neural networks.</p><p>Approximate samples from the posterior distribution over the latent variables p(z|x) are used as features to train a classifier that predicts class labels y, such as a (transductive) SVM or multinomial regression. Using this approach, we can now perform classification in a lower dimensional space since we typically use latent variables whose dimensionality is much less than that of the observations. These low dimensional embeddings should now also be more easily separable since we make use of independent latent Gaussian posteriors whose parameters are formed by a sequence of non-linear transformations of the data. This simple approach results in improved performance for SVMs, and we demonstrate this in section 4.</p><p>Generative semi-supervised model (M2):</p><p>We propose a probabilistic model that describes the data as being generated by a latent class variable y in addition to a continuous latent variable z. The data is explained by the generative process:</p><formula xml:id="formula_1">p(y) = Cat(y|π); p(z) = N (z|0, I); p θ (x|y, z) = f (x; y, z, θ),<label>(2)</label></formula><p>where Cat(y|π) is the multinomial distribution, the class labels y are treated as latent variables if no class label is available and z are additional latent variables. These latent variables are marginally independent and allow us, in case of digit generation for example, to separate the class specification from the writing style of the digit. As before, f (x; y, z, θ) is a suitable likelihood function, e.g., a Bernoulli or Gaussian distribution, parameterised by a non-linear transformation of the latent variables. In our experiments, we choose deep neural networks as this non-linear function. Since most labels y are unobserved, we integrate over the class of any unlabelled data during the inference process, thus performing classification as inference. Predictions for any missing labels are obtained from the inferred posterior distribution p θ (y|x). This model can also be seen as a hybrid continuous-discrete mixture model where the different mixture components share parameters.</p><p>Stacked generative semi-supervised model (M1+M2): We can combine these two approaches by first learning a new latent representation z 1 using the generative model from M1, and subsequently learning a generative semi-supervised model M2, using embeddings from z 1 instead of the raw data x. The result is a deep generative model with two layers of stochastic variables:</p><formula xml:id="formula_2">p θ (x, y, z 1 , z 2 ) = p(y)p(z 2 )p θ (z 1 |y, z 2 )p θ (x|z 1 ),</formula><p>where the priors p(y) and p(z 2 ) equal those of y and z above, and both p θ (z 1 |y, z 2 ) and p θ (x|z 1 ) are parameterised as deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalable Variational Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lower Bound Objective</head><p>In all our models, computation of the exact posterior distribution is intractable due to the nonlinear, non-conjugate dependencies between the random variables. To allow for tractable and scalable inference and parameter learning, we exploit recent advances in variational inference <ref type="bibr" target="#b8">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b16">Rezende et al., 2014)</ref>. For all the models described, we introduce a fixed-form distribution q φ (z|x) with parameters φ that approximates the true posterior distribution p(z|x). We then follow the variational principle to derive a lower bound on the marginal likelihood of the model -this bound forms our objective function and ensures that our approximate posterior is as close as possible to the true posterior.</p><p>We construct the approximate posterior distribution q φ (·) as an inference or recognition model, which has become a popular approach for efficient variational inference <ref type="bibr" target="#b2">(Dayan, 2000;</ref><ref type="bibr" target="#b8">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b16">Rezende et al., 2014;</ref><ref type="bibr" target="#b20">Stuhlmüller et al., 2013)</ref>. Using an inference network, we avoid the need to compute per data point variational parameters, but can instead compute a set of global variational parameters φ. This allows us to amortise the cost of inference by generalising between the posterior estimates for all latent variables through the parameters of the inference network, and allows for fast inference at both training and testing time (unlike with VEM, in which we repeat the generalized E-step optimisation for every test data point). An inference network is introduced for all latent variables, and we parameterise them as deep neural networks whose outputs form the parameters of the distribution q φ (·). For the latent-feature discriminative model (M1), we use a Gaussian inference network q φ (z|x) for the latent variable z. For the generative semi-supervised model (M2), we introduce an inference model for each of the latent variables z and y, which we we assume has a factorised form q φ (z, y|x) = q φ (z|x)q φ (y|x), specified as Gaussian and multinomial distributions respectively.</p><formula xml:id="formula_3">M1: q φ (z|x) = N (z|µ φ (x), diag(σ 2 φ (x))),<label>(3)</label></formula><formula xml:id="formula_4">M2: q φ (z|y, x) = N (z|µ φ (y, x), diag(σ 2 φ (x))); q φ (y|x) = Cat(y|π φ (x)),<label>(4)</label></formula><p>where σ φ (x) is a vector of standard deviations, π φ (x) is a probability vector, and the functions µ φ (x), σ φ (x) and π φ (x) are represented as MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Latent Feature Discriminative Model Objective</head><p>For this model, the variational bound J (x) on the marginal likelihood for a single data point is:</p><formula xml:id="formula_5">log p θ (x) ≥ E q φ (z|x) [log p θ (x|z)] − KL[q φ (z|x) p θ (z)] = −J (x),<label>(5)</label></formula><p>The inference network q φ (z|x) (3) is used during training of the model using both the labelled and unlabelled data sets. This approximate posterior is then used as a feature extractor for the labelled data set, and the features used for training the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Generative Semi-supervised Model Objective</head><p>For this model, we have two cases to consider. In the first case, the label corresponding to a data point is observed and the variational bound is a simple extension of equation <ref type="formula" target="#formula_5">(5)</ref>:</p><formula xml:id="formula_6">log p θ (x, y) ≥ E q φ (z|x,y) [log p θ (x|y, z) + log p θ (y) + log p(z) − log q φ (z|x, y)] = −L(x, y),<label>(6)</label></formula><p>For the case where the label is missing, it is treated as a latent variable over which we perform posterior inference and the resulting bound for handling data points with an unobserved label y is:</p><formula xml:id="formula_7">log p θ (x) ≥ E q φ (y,z|x) [log p θ (x|y, z) + log p θ (y) + log p(z) − log q φ (y, z|x)] = y q φ (y|x)(−L(x, y)) + H(q φ (y|x)) = −U(x).<label>(7)</label></formula><p>The bound on the marginal likelihood for the entire dataset is now:</p><formula xml:id="formula_8">J = (x,y)∼ p l L(x, y) + x∼ pu U(x)<label>(8)</label></formula><p>The distribution q φ (y|x) (4) for the missing labels has the form a discriminative classifier, and we can use this knowledge to construct the best classifier possible as our inference model. This distribution is also used at test time for predictions of any unseen data.</p><p>In the objective function <ref type="formula" target="#formula_8">(8)</ref>, the label predictive distribution q φ (y|x) contributes only to the second term relating to the unlabelled data, which is an undesirable property if we wish to use this distribution as a classifier. Ideally, all model and variational parameters should learn in all cases. To remedy this, we add a classification loss to <ref type="formula" target="#formula_8">(8)</ref>, such that the distribution q φ (y|x) also learns from labelled data. The extended objective function is:</p><formula xml:id="formula_9">J α = J + α · E p l (x,y) [− log q φ (y|x)] ,<label>(9)</label></formula><p>where the hyper-parameter α controls the relative weight between generative and purely discriminative learning. We use α = 0.1 · N in all experiments. While we have obtained this objective function by motivating the need for all model components to learn at all times, the objective 9 can also be derived directly using the variational principle by instead performing inference over the parameters π of the categorical distribution, using a symmetric Dirichlet prior over these parameterss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimisation</head><p>The bounds in equations (5) and (9) provide a unified objective function for optimisation of both the parameters θ and φ of the generative and inference models, respectively. This optimisation can be done jointly, without resort to the variational EM algorithm, by using deterministic reparameterisations of the expectations in the objective function, combined with Monte Carlo approximationreferred to in previous work as stochastic gradient variational Bayes (SGVB) <ref type="bibr" target="#b8">(Kingma and Welling, 2014)</ref> or as stochastic backpropagation <ref type="bibr" target="#b16">(Rezende et al., 2014)</ref>. We describe the core strategy for the latent-feature discriminative model M1, since the same computations are used for the generative semi-supervised model.</p><p>When the prior p(z) is a spherical Gaussian distribution p(z) = N (z|0, I) and the variational distribution q φ (z|x) is also a Gaussian distribution as in <ref type="formula" target="#formula_3">(3)</ref>, the KL term in equation <ref type="formula" target="#formula_5">(5)</ref> can be computed</p><formula xml:id="formula_10">Algorithm 1 Learning in model M1 while generativeTraining() do D ← getRandomMiniBatch() z i ∼ q φ (z i |x i ) ∀x i ∈ D J ← n J (x i ) (g θ , g φ ) ← ( ∂J ∂θ , ∂J ∂φ ) (θ, φ) ← (θ, φ) + Γ(g θ , g φ ) end while while discriminativeTraining() do D ← getLabeledRandomMiniBatch() z i ∼ q φ (z i |x i ) ∀{x i , y i } ∈ D trainClassifier({z i , y i } ) end while Algorithm 2 Learning in model M2 while training() do D ← getRandomMiniBatch() y i ∼ q φ (y i |x i ) ∀{x i , y i } / ∈ O z i ∼ q φ (z i |y i , x i ) J α ← eq. (9) (g θ , g φ ) ← ( ∂L α ∂θ , ∂L α ∂φ ) (θ, φ) ← (θ, φ) + Γ(g θ , g φ ) end while</formula><p>analytically and the log-likelihood term can be rewritten, using the location-scale transformation for the Gaussian distribution, as:</p><formula xml:id="formula_11">E q φ (z|x) [log p θ (x|z)] = E N ( |0,I) log p θ (x|µ φ (x) + σ φ (x) ) ,<label>(10)</label></formula><p>where indicates the element-wise product. While the expectation (10) still cannot be solved analytically, its gradients with respect to the generative parameters θ and variational parameters φ can be efficiently computed as expectations of simple gradients:</p><formula xml:id="formula_12">∇ {θ,φ} E q φ (z|x) [log p θ (x|z)] = E N ( |0,I) ∇ {θ,φ} log p θ (x|µ φ (x) + σ φ (x)</formula><p>) .</p><p>The gradients of the loss (9) for model M2 can be computed by a direct application of the chain rule and by noting that the conditional bound L(x n , y) contains the same type of terms as the loss <ref type="formula" target="#formula_9">(9)</ref>. The gradients of the latter can then be efficiently estimated using (11) .</p><p>During optimization we use the estimated gradients in conjunction with standard stochastic gradientbased optimization methods such as SGD, RMSprop or AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2010)</ref>. This results in parameter updates of the form:</p><formula xml:id="formula_14">(θ t+1 , φ t+1 ) ← (θ t , φ t ) + Γ t (g t θ , g t φ )</formula><p>, where Γ is a diagonal preconditioning matrix that adaptively scales the gradients for faster minimization. The training procedure for models M1 and M2 are summarised in algorithms 1 and 2, respectively. Our experimental results were obtained using AdaGrad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computational Complexity</head><p>The overall algorithmic complexity of a single joint update of the parameters (θ, φ) for M1 using the estimator (11) is C M1 = M SC MLP where M is the minibatch size used , S is the number of samples of the random variate , and C MLP is the cost of an evaluation of the MLPs in the conditional distributions p θ (x|z) and q φ (z|x). The cost C MLP is of the form O(KD 2 ) where K is the total number of layers and D is the average dimension of the layers of the MLPs in the model. Training M1 also requires training a supervised classifier, whose algorithmic complexity, if it is a neural net, it will have a complexity of the form C MLP .</p><p>The algorithmic complexity for M2 is of the form C M2 = LC M1 , where L is the number of labels and C M1 is the cost of evaluating the gradients of each conditional bound J y (x), which is the same as for M1. The stacked generative semi-supervised model has an algorithmic complexity of the form C M1 + C M2 . But with the advantage that the cost C M2 is calculated in a low-dimensional space (formed by the latent variables of the model M1 that provides the embeddings).</p><p>These complexities make this approach extremely appealing, since they are no more expensive than alternative approaches based on auto-encoder or neural models, which have the lowest computational complexity amongst existing competitive approaches. In addition, our models are fully probabilistic, allowing for a wide range of inferential queries, which is not possible with many alternative approaches for semi-supervised learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Open source code, with which the most important results and figures can be reproduced, is available at http://github.com/dpkingma/nips14-ssl. For the latest experimental results, please see http://arxiv.org/abs/1406.5298.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Classification</head><p>We test performance on the standard MNIST digit classification benchmark. The data set for semisupervised learning is created by splitting the 50,000 training points between a labelled and unlabelled set, and varying the size of the labelled from 100 to 3000. We ensure that all classes are balanced when doing this, i.e. each class has the same number of labelled points. We create a number of data sets using randomised sampling to confidence bounds for the mean performance under repeated draws of data sets.</p><p>For model M1 we used a 50-dimensional latent variable z. The MLPs that form part of the generative and inference models were constructed with two hidden layers, each with 600 hidden units, using softplus log(1+e x ) activation functions. On top, a transductive SVM (TSVM) was learned on values of z inferred with q φ (z|x). For model M2 we also used 50-dimensional z. In each experiment, the MLPs were constructed with one hidden layer, each with 500 hidden units and softplus activation functions. In case of SVHN and NORB, we found it helpful to pre-process the data with PCA. This makes the model one level deeper, and still optimizes a lower bound on the likelihood of the unprocessed data. <ref type="table" target="#tab_0">Table 1</ref> shows classification results. We compare to a broad range of existing solutions in semisupervised learning, in particular to classification using nearest neighbours (NN), support vector machines on the labelled set (SVM), the transductive SVM (TSVM), and contractive auto-encoders (CAE). Some of the best results currently are obtained by the manifold tangent classifier (MTC) <ref type="bibr" target="#b17">(Rifai et al., 2011)</ref> and the AtlasRBF method <ref type="bibr" target="#b14">(Pitelis et al., 2014)</ref>. Unlike the other models in this comparison, our models are fully probabilistic but have a cost in the same order as these alternatives.</p><p>Results: The latent-feature discriminative model (M1) performs better than other models based on simple embeddings of the data, demonstrating the effectiveness of the latent space in providing robust features that allow for easier classification. By combining these features with a classification mechanism directly in the same model, as in the conditional generative model (M2), we are able to get similar results without a separate TSVM classifier.</p><p>However, by far the best results were obtained using the stack of models M1 and M2. This combined model provides accurate test-set predictions across all conditions, and easily outperforms the previously best methods. We also tested this deep generative model for supervised learning with all available labels, and obtain a test-set performance of 0.96%, which is among the best published results for this permutation-invariant MNIST classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conditional Generation</head><p>The conditional generative model can be used to explore the underlying structure of the data, which we demonstrate through two forms of analogical reasoning. Firstly, we demonstrate style and content separation by fixing the class label y, and then varying the latent variables z over a range of values. <ref type="figure">Figure 1</ref> shows three MNIST classes in which, using a trained model with two latent variables, and the 2D latent variable varied over a range from -5 to 5. In all cases, we see that nearby regions of latent space correspond to similar writing styles, independent of the class; the left region represents upright writing styles, while the right-side represents slanted styles.</p><p>As a second approach, we use a test image and pass it through the inference network to infer a value of the latent variables corresponding to that image. We then fix the latent variables z to this   value, vary the class label y, and simulate images from the generative model corresponding to that combination of z and y. This again demonstrate the disentanglement of style from class. <ref type="figure">Figure 1</ref> shows these analogical fantasies for the MNIST and SVHN datasets <ref type="bibr" target="#b12">(Netzer et al., 2011)</ref>. The SVHN data set is a far more complex data set than MNIST, but the model is able to fix the style of house number and vary the digit that appears in that style well. These generations represent the best current performance in simulation from generative models on these data sets.</p><p>The model used in this way also provides an alternative model to the stochastic feed-forward networks (SFNN) described by <ref type="bibr" target="#b21">Tang and Salakhutdinov (2013)</ref>. The performance of our model significantly improves on SFNN, since instead of an inefficient Monte Carlo EM algorithm relying on importance sampling, we are able to perform efficient joint inference that is easy to scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Classification</head><p>We demonstrate the performance of image classification on the SVHN, and NORB image data sets. Since no comparative results in the semi-supervised setting exists, we perform nearest-neighbour and TSVM classification with RBF kernels and compare performance on features generated by our latent-feature discriminative model to the original features. The results are presented in tables 2 and 3, and we again demonstrate the effectiveness of our approach for semi-supervised classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization details</head><p>The parameters were initialized by sampling randomly from N (0, 0.001 2 I), except for the bias parameters which were initialized as 0. The objectives were optimized using minibatch gradient ascent until convergence, using a variant of RMSProp with momentum and initialization bias correction, a constant learning rate of 0.0003, first moment decay (momentum) of 0.1, and second moment decay of 0.001. For MNIST experiments, minibatches for training were generated by treating normalised pixel intensities of the images as Bernoulli probabilities and sampling binary images from this distribution. In the M2 model, a weight decay was used corresponding to a prior of (θ, φ) ∼ N (0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>The approximate inference methods introduced here can be easily extended to the model's parameters, harnessing the full power of variational learning. Such an extension also provides a principled ground for performing model selection. Efficient model selection is particularly important when the amount of available data is not large, such as in semi-supervised learning.</p><p>For image classification tasks, one area of interest is to combine such methods with convolutional neural networks that form the gold-standard for current supervised classification methods. Since all the components of our model are parametrised by neural networks we can readily exploit convolutional or more general locally-connected architectures -and forms a promising avenue for future exploration.</p><p>A limitation of the models we have presented is that they scale linearly in the number of classes in the data sets. Having to re-evaluate the generative likelihood for each class during training is an expensive operation. Potential reduction of the number of evaluations could be achieved by using a truncation of the posterior mass. For instance we could combine our method with the truncation algorithm suggested by <ref type="bibr" target="#b13">Pal et al. (2005)</ref>, or by using mechanisms such as error-correcting output codes <ref type="bibr" target="#b3">(Dietterich and Bakiri, 1995)</ref>. The extension of our model to multi-label classification problems that is essential for image-tagging is also possible, but requires similar approximations to reduce the number of likelihood-evaluations per class.</p><p>We have developed new models for semi-supervised learning that allow us to improve the quality of prediction by exploiting information in the data density using generative models. We have developed an efficient variational optimisation algorithm for approximate Bayesian inference in these models and demonstrated that they are amongst the most competitive models currently available for semisupervised learning. We hope that these results stimulate the development of even more powerful semi-supervised classification methods based on generative models, of which there remains much scope.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Handwriting styles for MNIST obtained by fixing the class label and varying the 2D latent variable z (b) MNIST analogies (c) SVHN analogies Figure 1: (a) Visualisation of handwriting styles learned by the model with 2D z-space. (b,c) Analogical reasoning with generative semi-supervised models using a high-dimensional z-space. The leftmost columns show images from the test set. The other columns show analogical fantasies of x by the generative model, where the latent variable z of each row is set to the value inferred from the test-set image on the left by the inference network. Each column corresponds to a class label y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmark results of semi-supervised classification on MNIST with few labels.</figDesc><table><row><cell>N</cell><cell>NN</cell><cell>CNN</cell><cell>TSVM</cell><cell>CAE</cell><cell>MTC</cell><cell>AtlasRBF</cell><cell>M1+TSVM</cell><cell>M2</cell><cell>M1+M2</cell></row><row><cell>100</cell><cell>25.81</cell><cell>22.98</cell><cell>16.81</cell><cell>13.47</cell><cell>12.03</cell><cell>8.10 (± 0.95)</cell><cell>11.82 (± 0.25)</cell><cell>11.97 (± 1.71)</cell><cell>3.33 (± 0.14)</cell></row><row><cell>600</cell><cell>11.44</cell><cell>7.68</cell><cell>6.16</cell><cell>6.3</cell><cell>5.13</cell><cell>-</cell><cell>5.72 (± 0.049)</cell><cell>4.94 (± 0.13)</cell><cell>2.59 (± 0.05)</cell></row><row><cell>1000</cell><cell>10.7</cell><cell>6.45</cell><cell>5.38</cell><cell>4.77</cell><cell>3.64</cell><cell>3.68 (± 0.12)</cell><cell>4.24 (± 0.07)</cell><cell>3.60 (± 0.56)</cell><cell>2.40 (± 0.02)</cell></row><row><cell>3000</cell><cell>6.04</cell><cell>3.35</cell><cell>3.45</cell><cell>3.22</cell><cell>2.57</cell><cell>-</cell><cell>3.49 (± 0.04)</cell><cell>3.92 (± 0.63)</cell><cell>2.18 (± 0.04)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised classification on the SVHN dataset with 1000 labels.</figDesc><table><row><cell>KNN</cell><cell>TSVM</cell><cell>M1+KNN</cell><cell>M1+TSVM</cell><cell>M1+M2</cell></row><row><cell>77.93</cell><cell>66.55</cell><cell>65.63</cell><cell>54.33</cell><cell>36.02</cell></row><row><cell>(± 0.08)</cell><cell>(± 0.10)</cell><cell>(± 0.15)</cell><cell>(± 0.11)</cell><cell>(± 0.10)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised classification on the NORB dataset with 1000 labels.</figDesc><table><row><cell>KNN</cell><cell>TSVM</cell><cell>M1+KNN</cell><cell>M1+TSVM</cell></row><row><cell>78.71</cell><cell>26.00</cell><cell>65.39</cell><cell>18.79</cell></row><row><cell>(± 0.02)</cell><cell>(± 0.06)</cell><cell>(± 0.09)</cell><cell>(± 0.05)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We are grateful for feedback from the reviewers. We would also like to thank the SURFFoundation for the use of the Dutch national e-infrastructure for a significant part of the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Archipelago: nonparametric Bayesian semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using randomized mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Rwebangira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page">44</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Solving multiclass learning problems via error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bakiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>arXiv preprint cs/9501101</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Machine Learning (ICML)</title>
		<meeting>eeding of the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stromsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variational approach to semi-supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning for phone and segment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast inference and learning with sparse belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using an unsupervised atlas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pitelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agapito</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the European Conference on Machine Learning (ECML)</title>
		<meeting>eddings of the European Conference on Machine Learning (ECML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8725</biblScope>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of compact document representations with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2294" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION&apos;05)</title>
		<meeting>the Seventh IEEE Workshops on Application of Computer Vision (WACV/MOTION&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning improves gene expression-based prediction of cancer recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3017" to="3023" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning stochastic inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3048" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A rate distortion approach for semi-supervised conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2008" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the International Conference on Machine Learning (ICML)</title>
		<meeting>eddings of the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
