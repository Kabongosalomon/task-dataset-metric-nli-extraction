<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><forename type="middle">Dipta</forename><surname>Das</surname></persName>
							<email>souryadipta.das@razorthink.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Razorthink Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Basak</surname></persName>
							<email>ayan.basak@razorthink.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Razorthink Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Dutta</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIT Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>COVID-19 · Language Model · Fake News · Ensemble · Heuristic</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world stay connected. With the COVID-19 pandemic raging, social media has become more relevant and widely used than ever before, and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe our Fake News Detection system that automatically identifies whether a tweet related to COVID-19 is "real" or "fake", as a part of CONSTRAINT COVID19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models that has helped us achieve a joint 8 th position on the leader board. We have achieved an F1-score of 0.9831 against a top score of 0.9869. Post completion of the competition, we have been able to drastically improve our system by incorporating a novel heuristic algorithm based on username handles and link domains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art results on the given dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fake news represents the press that is used to spread false information and hoaxes through conventional platforms as well as online ones, mainly social media. There has been an increasing interest in fake news on social media due to the political climate prevailing in the modern world <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, as well as several other factors. Detecting misinformation on social media is as important as it is technically challenging. The difficulty is partly due to the fact that even humans cannot accurately distinguish false from true news, mainly because it involves tedious evidence collection as well as careful fact checking. With the advent of technology and ever-increasing propagation of fake articles in social media, it has become really important to come up with automated frameworks for fake news identification. In this paper, we describe our system which performs a binary classification on tweets from social media and classifies it into "real" or "fake". We have used transfer learning in our approach as it has proven to be extremely effective in text classification tasks, with a reduced training time as we do not equal contribution arXiv:2101.03545v1 [cs.CL] 10 Jan 2021 need to train each model from scratch. The primary steps for our approach initially include text preprocessing, tokenization, model prediction, and ensemble creation using a soft voting schema. Post evaluation, we have drastically improved our fake news detection framework with a heuristic post-processing technique that takes into account the effect of important aspects of tweets like username handles and URL domains. This approach has allowed us to produce much superior results when compared to the top entry in the official leaderboard. We have performed an ablation study of the various attributes used in our post-processing approach. We have also provided examples of tweets where the post-processing approach has predicted correctly when compared to the initial classification output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional machine learning approaches have been quite successful in fake news identification problems. Reis et al. <ref type="bibr" target="#b4">[5]</ref> has used feature engineering to generate hand-crafted features like syntactic features, semantic features etc. The problem was then approached as a binary classification problem where these features were fed into conventional Machine Learning classifiers like K-Nearest Neighbor (KNN), Random Forest (RF), Naive Bayes, Support Vector Machine (SVM) and XGBOOST (XGB), where RF and XGB yielded results that were quite favourable. Shu et al. <ref type="bibr" target="#b5">[6]</ref> have proposed a novel framework TriFN, which provides a principled way to model tri-relationship among publishers, news pieces, and users simultaneously. This framework significantly outperformed the baseline Machine Learning models as well as erstwhile state-of-the-art frameworks. With the advent of deep learning, there has been a significant revolution in the field of text classification, and thereby in fake news detection. Karimi et al. <ref type="bibr" target="#b6">[7]</ref> has proposed a Multi-Source Multi-class Fake News Detection framework that can do automatic feature extraction using Convolution Neural Network (CNN) based models and combine these features coming from multiple sources using an attention mechanism, which has produced much better results than previous approaches that involved hand-crafted features. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> introduced a new diffusive unit model, namely Gated Diffusive Unit (GDU), that has been used to build a deep diffusive network model to learn the representations of news articles, creators and subjects simultaneously. Ruchansky et al. <ref type="bibr" target="#b8">[9]</ref> has proposed a novel CSI(Capture-Score-Integrate) framework that uses an Long Short-term Memory (LSTM) network to capture the temporal spacing of user activity and a doc2vec <ref type="bibr" target="#b20">[21]</ref> representation of a tweet, along with a neural network based user scoring module to classify the tweet as real or fake. It emphasizes the value of incorporating all three powerful characteristics in the detection of fake news: the tweet content, user source, and article response. Monti et al. <ref type="bibr" target="#b9">[10]</ref> has shown that social network structure and propagation are important features for fake news detection by implementing a geometric deep learning framework using Graph Convolutional Networks.</p><p>Language models: Most of the current state-of-the-art language models are based on Transformer <ref type="bibr" target="#b11">[12]</ref> and they have proven to be highly effective in text classification problems. They provide superior results when compared to previous state-of-the-art approaches using techniques like Bi-directional LSTM, Gated Recurrent Unit (GRU) based models etc. The models are trained on a huge corpus of data. The introduction of the BERT <ref type="bibr" target="#b12">[13]</ref> architecture has transformed the capability of transfer learning in Natural Language Processing. It has been able to achieve state-of-the art results on downstream tasks like text classification. RoBERTa <ref type="bibr" target="#b14">[15]</ref> is an improved version of the BERT model. It is derived from BERT's language-masking strategy, modifying its key hyperparameters, including removing BERT's next-sentence pre-training objective, and training with much larger mini-batches and learning rates, leading to improved performance on downstream tasks. XLNet <ref type="bibr" target="#b15">[16]</ref> is a generalized auto-regressive language method. It calculates the joint probability of a sequence of tokens based on the transformer architecture having recurrence. Its training objective is to calculate the probability of a word token conditioned on all permutations of word tokens in a sentence, hence capturing a bidirectional context. XLM-RoBERTa <ref type="bibr" target="#b13">[14]</ref> is a transformer <ref type="bibr" target="#b11">[12]</ref> based language model relying on Masked Language Model Objective. DeBERTa <ref type="bibr" target="#b16">[17]</ref> provides an improvement over the BERT and RoBERTa models using two novel techniques; first, the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, and second, the output softmax layer is replaced by an enhanced mask decoder to predict the masked tokens pre-training the model. ELECTRA <ref type="bibr" target="#b17">[18]</ref> is used for self-supervised language representation learning. It can be used to pre-train transformer networks using very low compute, and is trained to distinguish "real" input tokens vs "fake" input tokens, such as tokens produced by artificial neural networks. ERNIE 2.0 <ref type="bibr" target="#b18">[19]</ref> is a continual pre-training framework to continuously gain improvement on knowledge integration through multi-task learning, enabling it to learn various lexical, syntactic and semantic information through massive data much better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The dataset <ref type="bibr" target="#b10">[11]</ref> for CONSTRAINT COVID-19 Fake News Detection in English challenge was provided by the organizers on the competition website <ref type="bibr" target="#b2">3</ref> . It consists of data that have been collected from various social media and fact checking websites, and the veracity of each post has been verified manually. The "real" news items were collected from verified sources which give useful information about COVID-19, while the "fake" ones were collected from tweets, posts and articles which make speculations about COVID-19 that are verified to be false. The original dataset contains 10,700 social media news items, the vocabulary size (i.e., unique words) of which is 37,505 with 5141 words in common to both fake and real news. It is class-wise balanced with 52.34% of the samples consisting of real news, and 47.66% of fake samples. These are 880 unique username handle and 210 unique URL domains in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>We have approached this task as a text classification problem. Each news item needs to be classified into two distinct categories: "real" or "fake". Our proposed method consists of five main parts: (a) Text Preprocessing, (b) Tokenization, (c) Backbone Model Architectures, (d) Ensemble, and (e) Heuristic Post Processing. The overall architecture of our system is shown in <ref type="figure">Figure-</ref>1. More detailed description is given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Preprocessing</head><p>Some social media items, like tweets, are mostly written in colloquial language. Also, they contain various other information like usernames, URLs, emojis, etc. We have filtered out such attributes from the given data as a basic preprocessing step, before feeding it into the ensemble model. We have used the tweet-preprocessor 4 library from Python to filter out such noisy information from tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tokenization</head><p>During tokenization, each sentence is broken down into tokens before being fed into a model. We have used a variety of tokenization approaches 5 depending upon the pre-trained model that we have used, as each model expects tokens to be structured in a particular manner, including the presence of model-specific special tokens. Each model also has its corresponding vocabulary associated with its tokenizer, trained on a large corpus data like GLUE, wikitext-103, CommonCrawl data etc. During training, each model applies the tokenization technique with its corresponding vocabulary on our tweets data. We have used a combination of XLNet <ref type="bibr" target="#b15">[16]</ref>, RoBERTa <ref type="bibr" target="#b14">[15]</ref>, XLM-RoBERTa <ref type="bibr" target="#b13">[14]</ref>, DeBERTa <ref type="bibr" target="#b16">[17]</ref>, ERNIE 2.0 <ref type="bibr" target="#b18">[19]</ref> and ELECTRA <ref type="bibr" target="#b17">[18]</ref> models and have accordingly used the corresponding tokenizers from the base version of their pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Backbone Model Architectures</head><p>We have used a variety of pre-trained language models 6 as backbone models for text classification. For each model, an additional fully connected layer is added to its respective encoder sub-network to obtain prediction probabilities for each class-"real" and "fake" as a prediction vector. We have used transfer learning in our approach in this problem. Each model has used some pre-trained model weights as initial weights. Thereafter, it fine-tunes the model weights using the tokenized training data. The same tokenizer is used to tokenize the test data and the fine-tuned model checkpoint is used to obtain predictions during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble</head><p>In this method, we use the model prediction vectors from the different models to obtain our final classification result, i.e. "real" or "fake". To balance an individual model's limitations, an ensemble method can be useful for a collection of similarly well-performing models. We have experimented with two approaches: soft voting and hard voting, that are described in the following figure: Soft Voting : In this approach, we calculate a "soft probability score" for each class by averaging out the prediction probabilities of various models for that class. The class that has a higher average probability value is selected as the final prediction class. Probability for "real" class, P r (x) and probability for "fake" class , P f (x) for a tweet x is given by,</p><formula xml:id="formula_0">P r (x) = n ∑ i=1 P r i (x) n (1) P f (x) = n ∑ i=1 P f i (x) n<label>(2)</label></formula><p>where P r i (x) and P f i (x) are "real" and "fake" probabilities by the i-th model and n is the total number of models.</p><p>Hard Voting : In this approach, the predicted class label for a news item is the class label that represents the majority of the class labels predicted by each individual model. In other words, the class with the most number of votes is selected as the final prediction class. Votes for "real" class, V r (x) and Votes for "fake" class , V f (x) for a tweet x is given by,</p><formula xml:id="formula_1">V r (x) = n ∑ i=1 I(P r i (x) ≥ P f i (x)) (3) V f (x) = n ∑ i=1 I(P r i (x) &lt; P f i (x))<label>(4)</label></formula><p>where the value of I(a) is 1 if condition a is satisfied and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Heuristic Post-Processing</head><p>In this approach, we have augmented our original framework with a heuristic approach that can take into account the effect of username handles and URL domains present in some data, like tweets. This approach works well for data having URL domains and username handles; we rely only on ensemble model predictions for texts lacking these attributes. We create a new feature-set using these attributes. Our basic intuition is that username handles and URL domains are very important aspects of a tweet and they can convey reliable information regarding the genuineness of tweets. We have tried to incorporate the effect of these attributes along with our original ensemble model predictions by calculating probability vectors corresponding to both of them. We have used information about the frequency of each class for each of these attributes in the training set to compute these vectors. In our experiments, we observed that Soft-voting works better than Hard-voting. Hence our post-processing step takes Soft-voting prediction vectors into account. The steps taken in this approach are described as follows:</p><p>-First, we obtain the class-wise probability from the best performing ensemble model. These probability values form two features of our new feature-set. -We collect username handles from all the news items in our training data, and calculate how many times the ground truth is "real" or "fake" for each username. -We calculate the conditional probability of a particular username indicating a real news item, which is represented as follows:</p><formula xml:id="formula_2">P r (x|username) = n(A) n(A) + n(B)<label>(5)</label></formula><p>where n(A) = number of "real" news items containing the username and n(B) = number of "fake" news items containing the username. Similarly, the conditional probability of a particular username indicating a fake news item is given by,</p><formula xml:id="formula_3">P f (x|username) = n(B) n(A) + n(B)<label>(6)</label></formula><p>We obtain two probability vectors that form four additional features of our new dataset. -We collect URL domains from all the news items in our training data, obtained by expanding the shorthand URLs associated with the tweets, and calculate how many times the ground truth is "real" or "fake" for each domain. -We calculate the conditional probability of a particular URL domain indicating a real news item, which is represented as follows: </p><p>where n(P) = number of "real" news items containing the domain and n(Q) = number of "fake" news items containing the domain. Similarly, the conditional probability of a particular domain indicating a fake news item is given by,</p><formula xml:id="formula_5">P f (x|domain) = n(Q) n(P) + n(Q)<label>(8)</label></formula><p>We obtain two probability vectors that form the final two additional features of our new dataset.  <ref type="table" target="#tab_0">Table 1</ref> shows some samples of the conditional probability values of each label class given each of the two attributes, URL domain and username handle. We have also shown the frequency of those attributes in the training data. The details of the heuristic algorithm is explained in the following pseudocode (Algorithm-1). In our experiment, the value of threshold used is 0.88. The post-processing architecture is shown in <ref type="figure">Figure-</ref>2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Description</head><p>We have fine-tuned our pre-trained models using AdamW <ref type="bibr" target="#b19">[20]</ref> optimizer and crossentropy loss after doing label encoding on the target values. We have applied softmax on the logits produced by each model in order to obtain the prediction probability vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Heuristic Algorithm</head><p>Result: label ( "real" or "fake") 1: if P r (x|username) &gt; threshold AND P r (x|username) &gt; P f (x|username) then 2: label = "real" 3: else if P f (x|username) &gt; threshold AND P r (x|username) &lt; P f (x|username) then 4: label = "fake" 5: else if P r (x|domain) &gt; threshold AND P r (x|domain) &gt; P f (x|domain) then 6: label = "real" 7: else if P f (x|domain) &gt; threshold AND P r (x|domain) &lt; P f (x|domain) then 8: label = "fake" 9: else if P r (x) &gt; P f (x) then 10: label = "real" 11: else 12: label = "fake" 13: end if</p><p>The experiments were performed on a system with 16GB RAM and 2.2 GHz Quad-Core Intel Core i7 Processor, along with a Tesla T4 GPU, with batch size of 32. The maximum input sequence length was fixed at 128. Initial learning rate was set to 2e-5. The number of epochs varied from 6 to 15 depending on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of Individual Models</head><p>We have used each fine-tuned model individually to perform "real" vs "fake" classification. Quantitative results are tabulated in <ref type="table">Table-</ref>2. We can see that XLM-RoBERTa, RoBERTa, XLNet and ERNIE 2.0 perform really well on the validation set. However, RoBERTa has been able to produce the best classification results when evaluated on the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance of Ensemble Models</head><p>We tried out different combinations of pre-trained models with both the ensemble techniques: Soft Voting and Hard Voting. Performance for different ensembles are shown in <ref type="table">Table-3 and 4</ref>. From the results, we can infer that the ensemble models significantly outperform the individual models, and Soft-voting ensemble method performed better overall than Hard-voting ensemble method. Hard-voting Ensemble model consisting of RoBERTa, XLM-RoBERTa, XLNet, ERNIE 2.0 and DeBERTa models performed the best among other hard voting ensembles on both validation and test set. Among the Soft Voting Ensembles, the ensemble consisting of RoBERTa, XLM-RoBERTa, XL-Net, ERNIE 2.0 and Electra models achieved best accuracy overall on the validation set and a combination of XLNet, RoBERTa, XLM-RoBERTa and DeBERTa models produces the best classification result overall on the test set. Our system has been able to achieve an overall F1-score of 0.9831 and secure a joint 8 th rank in the leaderboard, against a top score of 0.9869. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance of Our Final Approach</head><p>We augmented our Fake News Detection System with an additional heuristic algorithm and achieved an overall F1-score of 0.9883, making this approach state-of-the-art on the given fake news dataset <ref type="bibr" target="#b10">[11]</ref>. We have used the best performing ensemble model consisting of RoBERTa, XLM-RoBERTa, XLNet and DeBERTa for this approach. We have shown the comparison of the results on the test set obtained by our model before and after applying the post-processing technique against the top 3 teams in the leaderboard in <ref type="table">Table 5</ref>. <ref type="table">Table 6</ref> shows a few examples where the post-processing algorithm corrects the initial prediction. The first example is corrected due to extracted domain which is "news.sky" and the second one is corrected because of presence of the username handle, "@drsanjaygupta". We're LIVE talking about COVID-19 (a vaccine transmission) with @drsanjaygupta. Join us and ask some questions of your own: https://t.co/e16G2RGdkA https://t.co/Js7lemT1Z6 real fake fake</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>We have performed an ablation study by assigning various levels of priority to each of the features (username and domain) and then checking which class's probability value for that feature is maximum for a particular tweet, so that we can assign the corresponding "real" or "fake" class label to that particular tweet. For example, in one iteration, we have given URL domains a higher priority than username handles to select the label class. We have also experimented with only one attribute mentioned above in our study. Results for different priority and feature set is shown in <ref type="table" target="#tab_4">Table 7</ref>.</p><p>Another important parameter that we have introduced for our experiment is a threshold on the class-wise probability values for the features. For example, if the probability that a particular username that exists in a tweet belongs to "real" class is greater than that of it belonging to "fake" class, and the probability of it belonging to the "real" class is greater than a specific threshold, we assign a "real" label to the tweet. The value of this threshold is a hyperparameter that has been tuned based on the classification accuracy on the validation set. We have summarized the results from our study with and without the threshold parameter in <ref type="table" target="#tab_4">Table 7</ref>. As we can observe from the results, domain plays a significant role for ensuring a better classification result when the threshold parameter is taken into account. The best results are obtained when we consider the threshold parameter and both the username and domain attributes, with a higher importance given to the username. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a robust framework for identification of fake tweets related to COVID-19, which can go a long way in eliminating the spread of misinformation on such a sensitive topic. In our initial approach, we have tried out various pre-trained language models. Our results have significantly improved when we implemented an ensemble mechanism with Soft-voting by using the prediction vectors from various combinations of these models. Furthermore, we have been able to augment our system with a novel heuristics-based post-processing algorithm that has drastically improved the fake tweet detection accuracy, making it state-of-the-art on the given dataset.</p><p>Our novel heuristic approach shows that username handles and URL domains form very important features of tweets and analyzing them accurately can go a long way in creating a robust framework for fake news detection. Finally, we would like to pursue more research into how other pre-trained models and their combinations perform on the given dataset. It would be really interesting to evaluate how our system performs on other generic Fake News datasets and also if different values of the threshold parameter for our post-processing system would impact its overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fake News Identification Initial Process Block Diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fake News Identification Post Process Block Diagram P r (x|domain) = n(P) n(P) + n(Q)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>In case there are multiple username handles and URL domains in a sentence, the final probability vectors are obtained by averaging out the vectors of the individual attributes.-At this point, we have new training, validation and test feature-sets obtained using class-wise probability vectors from ensemble model outputs as well as probability values obtained using username handles and URLs from the training data. We use a novel heuristic algorithm on this resulting feature set to obtain our final class predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Few Examples on URL Domain-name and Username attribute distribution data</figDesc><table><row><cell cols="4">Example of URL Domain Name Prob. Dist.</cell><cell cols="3">Example of UserName Prob. Dist.</cell><cell></cell></row><row><cell>URL Domain Name</cell><cell cols="3">P r (x|domain) P f (x|domain) Frequency</cell><cell cols="4">UserName P r (x|username) P f (x|username) Frequency</cell></row><row><cell>news.sky</cell><cell>1.0</cell><cell>0.0</cell><cell>274</cell><cell>MoHFW NDIA</cell><cell>0.963</cell><cell>0.037</cell><cell>162</cell></row><row><cell>medscape.com</cell><cell>1.0</cell><cell>0.0</cell><cell>258</cell><cell>DrTedros</cell><cell>1.0</cell><cell>0.0</cell><cell>110</cell></row><row><cell>thespoof.com</cell><cell>0.0</cell><cell>1.0</cell><cell>253</cell><cell>ICMRDELHI</cell><cell>0.9903</cell><cell>0.0097</cell><cell>103</cell></row><row><cell>newsthump.com</cell><cell>0.0</cell><cell>1.0</cell><cell>68</cell><cell>PIB ndia</cell><cell>1.0</cell><cell>0.0</cell><cell>83</cell></row><row><cell>theguardian.com</cell><cell>0.167</cell><cell>0.833</cell><cell>6</cell><cell>CDCMMWR</cell><cell>1.0</cell><cell>0.0</cell><cell>34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Individual model performance on validation and test set</figDesc><table><row><cell>Model Name</cell><cell cols="4">Validation Set Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score Test set</cell></row><row><cell cols="2">XLM-RoBERTa (base) 0.968</cell><cell>0.968 0.968 0.968</cell><cell>0.970</cell><cell>0.970 0.970 0.970</cell></row><row><cell>RoBERTa (base)</cell><cell>0.970</cell><cell>0.970 0.970 0.970</cell><cell>0.972</cell><cell>0.972 0.972 0.972</cell></row><row><cell>XLNet (base, cased)</cell><cell>0.975</cell><cell>0.975 0.975 0.975</cell><cell>0.966</cell><cell>0.966 0.966 0.966</cell></row><row><cell>DeBERTa (base)</cell><cell>0.964</cell><cell>0.964 0.964 0.964</cell><cell>0.964</cell><cell>0.964 0.964 0.964</cell></row><row><cell>ELECTRA (base)</cell><cell>0.948</cell><cell>0.948 0.948 0.948</cell><cell>0.953</cell><cell>0.953 0.953 0.953</cell></row><row><cell>ERNIE 2.0</cell><cell>0.976</cell><cell>0.976 0.976 0.976</cell><cell>0.969</cell><cell>0.969 0.969 0.969</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance of Soft Voting for different ensemble models on validation and test set Performance of Hard Voting for different ensemble models on validation and test set</figDesc><table><row><cell>Ensemble Model</cell><cell></cell><cell>Validation Set</cell><cell>Test set</cell></row><row><cell>Combination</cell><cell cols="3">Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score</cell></row><row><cell>RoBERTa+XLM-RoBERTa +XLNet</cell><cell>0.9827</cell><cell>0.9827 0.9827 0.9827 0.9808</cell><cell>0.9808 0.9808 0.9808</cell></row><row><cell>RoBERTa+XLM-RoBERTa +XLNet+DeBERT</cell><cell>0.9832</cell><cell>0.9832 0.9832 0.9832 0.9831</cell><cell>0.9831 0.9831 0.9831</cell></row><row><cell>RoBERTa+XLM-RoBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+XLNet+ERNIE 2.0</cell><cell>0.9836</cell><cell>0.9836 0.9836 0.9836 0.9822</cell><cell>0.9822 0.9822 0.9822</cell></row><row><cell>+DeBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa+XLM-RoBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+XLNet+ERNIE 2.0</cell><cell>0.9841</cell><cell>0.9841 0.9841 0.9841 0.9808</cell><cell>0.9808 0.9808 0.9808</cell></row><row><cell>+Electra</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensemble Model</cell><cell></cell><cell>Validation Set</cell><cell>Test set</cell></row><row><cell>Combination</cell><cell cols="3">Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score</cell></row><row><cell>RoBERTa+XLM-RoBERTa +XLNet</cell><cell>0.9818</cell><cell>0.9818 0.9818 0.9818 0.9804</cell><cell>0.9804 0.9804 0.9804</cell></row><row><cell>RoBERTa+XLM-RoBERTa +XLNet+DeBERT</cell><cell>0.9748</cell><cell>0.9748 0.9748 0.9748 0.9743</cell><cell>0.9743 0.9743 0.9743</cell></row><row><cell>RoBERTa+XLM-RoBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+XLNet+ERNIE 2.0</cell><cell>0.9832</cell><cell>0.9832 0.9832 0.9832 0.9813</cell><cell>0.9813 0.9813 0.9813</cell></row><row><cell>+DeBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa+XLM-RoBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+XLNet+ERNIE 2.0</cell><cell>0.9822</cell><cell>0.9822 0.9822 0.9822 0.9766</cell><cell>0.9766 0.9766 0.9766</cell></row><row><cell>+Electra</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Performance comparison on test set Qualitative comparison between our initial and final approach.</figDesc><table><row><cell>Method</cell><cell cols="4">Accuracy Precision Recall F1 Score</cell></row><row><cell>Team g2tmn (Rank 1)</cell><cell>0.9869</cell><cell cols="3">0.9869 0.9869 0.9869</cell></row><row><cell>Team saradhix (Rank 2)</cell><cell>0.9864</cell><cell cols="3">0.9865 0.9864 0.9864</cell></row><row><cell cols="2">Team xiangyangli (Rank 3) 0.9860</cell><cell cols="3">0.9860 0.9860 0.9860</cell></row><row><cell>Ensemble Model</cell><cell>0.9831</cell><cell cols="3">0.9831 0.9831 0.9831</cell></row><row><cell>Ensemble Model + Heuristic Post-Processing</cell><cell>0.9883</cell><cell cols="3">0.9883 0.9883 0.9883</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Initial</cell><cell>Final</cell></row><row><cell>Tweet</cell><cell></cell><cell></cell><cell>Classification</cell><cell>Classification</cell><cell>Ground Truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Output</cell><cell>Output</cell></row><row><cell cols="3">Coronavirus: Donald Trump ignores COVID-19 rules with 'reckless and selfish' indoor rally https://t.co/JsiHGLMwfO</cell><cell>fake</cell><cell>real</cell><cell>real</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation Study on Heuristic algorithm</figDesc><table><row><cell>Combination of Attributes</cell><cell cols="2">with Threshold</cell><cell cols="2">without Threshold</cell></row><row><cell>(in descending order of Attribute Priority)</cell><cell>F1 Score on</cell><cell>F1 Score on</cell><cell>F1 Score on</cell><cell>F1 Score on</cell></row><row><cell></cell><cell>Validation Set</cell><cell>Test Set</cell><cell>Validation Set</cell><cell>Test Set</cell></row><row><cell>{username, ensemble model pred }</cell><cell>0.9831</cell><cell>0.9836</cell><cell>0.9822</cell><cell>0.9804</cell></row><row><cell>{domain, ensemble model pred }</cell><cell>0.9917</cell><cell>0.9878</cell><cell>0.9635</cell><cell>0.9523</cell></row><row><cell>{domain, username, ensemble model pred }</cell><cell>0.9911</cell><cell>0.9878</cell><cell>0.9635</cell><cell>0.9519</cell></row><row><cell>{username, domain, ensemble model pred }</cell><cell>0.9906</cell><cell>0.9883</cell><cell>0.9645</cell><cell>0.9528</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://competitions.codalab.org/competitions/26655</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">pypi.org/project/tweet-preprocessor/ 5 huggingface.co/docs/tokenizers/python/latest/ 6 huggingface.co/models</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social media, political polarization, and political disinformation: A review of the scientific literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Guess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barberá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Sanovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Stukal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Nyhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Political polarization, and political disinformation: a review of the scientific literature</title>
		<imprint>
			<date type="published" when="2018-03-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Political ideology predicts perceptions of the threat of covid-19 (and susceptibility to fake news about it)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><forename type="middle">P</forename><surname>Calvillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><forename type="middle">M</forename><surname>Smelter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rutchick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1119" to="1128" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting deceptive opinions with profile compatibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="338" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linguistic Traces of a Scientific Fraud: The Case of Diederik Stapel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Markowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised learning for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><forename type="middle">Cs</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrício</forename><surname>Murai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrício</forename><surname>Benevenuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond news contents: The role of social context for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-source multi-class fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Proteek</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sari</forename><surname>Saba-Sadiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fakedetector: Effective fake news detection with deep diffusive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1826" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natali</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fake news detection on social media using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Patwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pykl</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><surname>Guptha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gitanjali</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03327</idno>
		<title level="m">Fighting an Infodemic: COVID-19 Fake News Dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">DeBERTa: Decodingenhanced BERT with Disentangled Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of CONSTRAINT 2021 Shared Tasks: Detecting English COVID-19 Fake News and Hindi Hostile Posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Parth Patwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><surname>Guptha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gitanjali</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pykl</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation (CONSTRAINT)</title>
		<meeting>the First Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation (CONSTRAINT)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
