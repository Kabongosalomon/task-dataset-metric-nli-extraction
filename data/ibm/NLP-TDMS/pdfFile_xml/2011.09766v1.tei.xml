<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
							<email>zhengzhuo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
							<email>zhongyanfei@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
							<email>maailong007@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Geospatial object segmentation, as a particular semantic segmentation task, always faces with larger-scale variation, larger intra-class variance of background, and foreground-background imbalance in the high spatial resolution (HSR) remote sensing imagery. However, general semantic segmentation methods mainly focus on scale variation in the natural scene, with inadequate consideration of the other two problems that usually happen in the large area earth observation scene. In this paper, we argue that the problems lie on the lack of foreground modeling and propose a foreground-aware relation network (FarSeg) from the perspectives of relation-based and optimization-based foreground modeling, to alleviate the above two problems. From perspective of relation, FarSeg enhances the discrimination of foreground features via foreground-correlated contexts associated by learning foreground-scene relation. Meanwhile, from perspective of optimization, a foregroundaware optimization is proposed to focus on foreground examples and hard examples of background during training for a balanced optimization. The experimental results obtained using a large scale dataset suggest that the proposed method is superior to the state-of-the-art general semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code has been made available at: https://github.com/Z-Zheng/FarSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High spatial resolution earth observation technique has provided a large number of high spatial resolution (HSR) remote sensing images that can finely describe various geospatial objects, such as ship, vehicle and airplane, etc. Automatically extracting objects of interest from HSR re- mote sensing imagery is very helpful for urban management, planing and monitoring <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Geospatial object segmentation, as a significant role in object extraction, can provide semantic and location information for the objects of interest, which belongs to a particular semantic segmentation task with the goal to divide image pixels into two subsets of the foreground objects and the background area. And meanwhile, it needs to further assign a unique semantic label to each pixel in the foreground object area.</p><p>Compared with natural scene, geospatial object segmentation is more challenging in the HSR remote sensing images. There are three reasons at least:</p><p>1) The object always has larger-scale variation in the HSR remote sensing images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>. This causes the multiscale problem, which makes it difficult to locate and recognize the object.</p><p>2) The background is much more complex in the HSR remote sensing images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13]</ref>, which causes serious false alarms due to larger intra-class variance.</p><p>3) The foreground ratio is much less than it in the natural images, as <ref type="figure" target="#fig_0">Fig. 1</ref> shows, which causes foregroundbackground imbalance problem.</p><p>For natural images, the object segmentation task is directly seen as a semantic segmentation task in the computer vision field, the performance of which is mainly limited by the multi-scale problem. Therefore, current state-of-theart general semantic segmentation methods focus on scaleaware <ref type="bibr" target="#b6">[7]</ref> and multi-scale <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44]</ref> modeling. However, for the HSR remote sensing images, false alarms problem and foreground-background imbalance problem are ignored in these general semantic segmentation methods. We argue that this is because these methods are lack of explicit modeling for the foreground. This seriously limits the further improvement of object segmentation in the HSR remote sensing images.</p><p>In this paper, a foreground-aware relation network (FarSeg) is proposed to tackle aforementioned two problems by exploiting explicitly foreground modeling for more robust object segmentation in the HSR remote sensing imagery. We explore two perspectives of explicitly foreground modeling: relation-based and optimization-based foreground modeling, and we further propose two modules in the FarSeg: foreground-scene relation module and foreground-aware optimization. The foreground-scene relation module learns the symbiotic relation between scene and foreground to associate foreground-correlated contexts to enhance the foreground features, thus reducing false alarms. The foreground-aware optimization focus the model on the foreground by suppressing numerous easy examples in the background to alleviate the foreground-background imbalance problem.</p><p>The main contributions of our study are summarized as follows:</p><p>1. A foreground-aware relation network (FarSeg) is proposed for geospatial object segmentation in HSR remote sensing imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To inherit multi-scale context modeling and learn geospatial scene representation, FarSeg builds a foreground branch based on the feature pyramid network (FPN) and a scene embedding branch upon a shared backbone network, namely multi-branch encoder.</p><p>3. To suppress false alarms, F-S relation module leverages the symbiotic relation between geospatial scene and geospatial objects, to associate foregroundcorrelated contexts and enhance the discrimination of foreground features. And meanwhile, the background without any contribution is suppressed by this symbiotic relation, thus suppressing false alarms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>General Semantic Segmentation Traditional methods first extract features for each pixel by the handcrafted feature descriptor. The further promotion of these traditional methods mainly depends on the improvement of handcrafted feature descriptors. However, designing a feature descriptor is time-consuming and the handcrafted feature is not robust due to limitation of prior knowledge of the expert.</p><p>The success of deep learning-based methods lies in solving this problem by learning feature representation from data directly <ref type="bibr" target="#b16">[17]</ref>. Convolutional neural network (CNN), as structured feature representation framework in deep learning, has been explored for semantic segmentation via patch-wise classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>. However, patch-wise fashion limits the spatial context modeling and brings redundant computation on overlapped areas between patches. To solve this problem, fully convolutional network (FCN) <ref type="bibr" target="#b32">[33]</ref> was proposed, which directly outputs the pixelwise prediction from the input with arbitrary size via the in-network upsampling layer. FCN was the first pixels-topixels semantic segmentation method and was end-to-end trained.</p><p>To further exploit spatial context for semantic segmentation, deeplab v1 <ref type="bibr" target="#b3">[4]</ref> utilized atrous convolution to enlarge receptive field of the CNN for wider spatial context modeling. And a dense conditional random field (CRF) was used as a postprocess to smooth the prediction.</p><p>To learn multi-scale feature representation, atrous spatial pyramidal pooling (ASPP) <ref type="bibr" target="#b4">[5]</ref> and pyramid pooling module (PPM) <ref type="bibr" target="#b47">[48]</ref> were proposed. ASPP utilized multiple atrous convolutions with different atrous rate to extract features with the different receptive field, while PPM generated pyramidal feature maps via pyramid pooling <ref type="bibr" target="#b19">[20]</ref>. The image-level features and batch normalization were embedded into ASPP for further improvement of accuracy in deeplab v3 <ref type="bibr" target="#b5">[6]</ref>. DenseASPP <ref type="bibr" target="#b43">[44]</ref> further enhanced multiscale feature representation via densely connected ASPP to make the multi-scale features covering larger and denser scale range. However, these methods failed to extract fine details of the object, such as the edge. U-Net <ref type="bibr" target="#b37">[38]</ref> and SegNet [1] utilized a new "encoderdecoder" network architecture, which reused the shallow features with high spatial resolution to enhance the deep features with strong semantics on spatial detail. RefineNet <ref type="bibr" target="#b29">[30]</ref> proposed a multi-path refinement network to progressively recover the spatial detail of deep features for better accuracy and visual performance. Deeplab v3+ also  adopted "encoder-decoder" framework to further improve performance via a more powerful backbone Xception <ref type="bibr" target="#b9">[10]</ref> and a light-weight decoder to recover the spatial resolution of features with a small overhead. These general semantic segmentation methods mainly focus on multi-scale context modeling, ignoring the special issues in the HSR remote sensing imagery, such as false alarms and foreground-background imbalance. This causes that these methods are lack of explicit modeling for the foreground. Therefore, a foreground-aware method is needed for object segmentation in the HSR remote sensing imagery.</p><formula xml:id="formula_0">i u v ϕ  (a) Multi-branch Encoder (b) Foreground-Scene Relation (c) Light-weight Decoder * 4 up × * 2 up × 4 up × foreground branch ( , ) ϕ ⋅ ⋅ ( ) i θ ψ ⋅ ( ) η ⋅ 4 ( ) θ ψ ⋅ 3 ( ) θ ψ ⋅ 2 ( ) θ ψ ⋅ i v i v  ( , ) ( ) 1+ i i w i u v v e ϕ κ −  step. 1 hard example estimation ( ) i P obj class ⋅ input step. 2 dynamic weighting H W W R × ∈ step. 3 back-propagation input argmax ( ) i P obj class ⋅ (d) Foreground-Aware Optimization</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation in Remote Sensing Community</head><p>There are a lot of applications using semantic segmentation technique in the remote sensing community, such as land use and land cover (LULC) classification <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref>, building extraction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15]</ref>, road extraction <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3]</ref>, vehicle detection <ref type="bibr" target="#b34">[35]</ref>, etc. The main methodologies follow general semantic segmentation, but for special application scenario (e.g. road or building), there were many improved techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref> for its application scenario.</p><p>However, these methods mainly focus on the improvement under the special application scenario, ignoring the consideration of common issues for object segmentation in the HSR remote sensing imagery, such as false alarms problem and foreground-background imbalance problem, especially for large scale HSR remote sensing imagery. Hence, we propose a foreground-aware relation network (FarSeg) to tackle these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Foreground-Aware Relation Network</head><p>To explicit model the foreground for object segmentation in the HSR remote sensing imagery, we propose a foreground-aware relation network (FarSeg), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The proposed FarSeg consists of a variant of feature pyramid network (FPN), foreground-scene (F-S) relation module, light-weight decoder and foreground-aware (F-A) optimization. FPN is responsible for multi-scale object segmentation. In the F-S relation module, we first formulate false alarms problem as a problem of lacking discriminative information in the foreground, and then introduce the latent scene semantics and F-S relation to improve the discrimination of foreground features. The light-weight decoder is simply designed to recover the spatial resolution of semantic features. To make the network focus on foreground during training, the F-A optimization is proposed to alleviate foreground-background imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Branch Encoder</head><p>Multi-branch encoder is made up of a foreground branch and a scene embedding branch. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (a), these branches are built upon a backbone network. In the proposed method, ResNets <ref type="bibr" target="#b20">[21]</ref> are chosen as the backbone network for basic feature extraction. {C i |i = 2, 3, 4, 5} denotes the set of feature maps extracted from ResNets, where the feature map C i has a output stride of 2 i pixels with respect to the input image. Similar to the original FPN, the top-down pathway and lateral connections are used to generated pyramidal feature maps {P i |i = 2, 3, 4, 5} with a same number of channels d. We formulate this procedure as follows:</p><formula xml:id="formula_1">P i = ζ(C i ) + Γ(P i+1 ), i = 2, 3, 4, 5<label>(1)</label></formula><p>where ζ denotes the lateral connection implemented by a learnable 1×1 convolutional layer and Γ denotes a nearest neighbor upsampling with a scale factor of 2. By this topdown pathway and lateral connections, the feature maps can be enhanced with high spatial detail from shallow layers and strong semantics from deep layers, which is helpful to recover the detail of objects and multi-scale context modeling. Apart from the pyramidal feature maps v i , a extra branch is attached on C 5 to generate a geospatial scene feature C 6 via global context aggregation. For simplicity, we use global average pooling as the aggregation function. C 6 is used to model the relation between geospatial scene and foreground, which is illustrated in the Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Foreground-Scene Relation Module</head><p>The background is much more complex in the HSR remote sensing imagery. It means that there is larger intraclass variance in the background, which causes the false alarms problem. To alleviate this problem, foregroundscene (F-S) relation module is proposed to improve the discrimination of foreground features by associating geospatial scene-relevant context. The main idea is shown in <ref type="figure">Fig. 3</ref>. F-S relation module first explicitly models the relation between foreground and geospatial scene and use latent geospatial scene to associate the foreground and relevant context. And then the relation is used to enhance the input feature maps to increase the disparity between foreground features and background features, thereby improving the discrimination of foreground features.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, for the pyramidal feature map v i , F-S relation module will produce a new feature map z i . The feature map z i is obtained by re-encoding v i and then re-weighting it using the relation map r i . The relation map r i is the similarity matrix between geospatial scene representation and foreground representation. To align these two feature representations into a shared manifold R du , there are two projection functions needed to learn for geospatial scene and foreground, respectively.ṽ i is the feature map v i transformed by the scale-aware projection function</p><formula xml:id="formula_2">ψ θi (·) : R d×H×W → R du×H×W , as shown in Eqn. 2. v i = ψ θi (v i )<label>(2)</label></formula><p>where θ i denotes the learnable parameters of ψ θi (·). We adopt a simple form of ψ θi (·) which is just implemented by 1×1 convolutional layer followed by batch normalization and ReLU in order.</p><p>To compute the relation map r i , a 1-D scene embedding vector u ∈ R du is needed to interact with the foreground feature mapsṽ i in the shared manifold. The scene embedding vector u is computed by applying η(·) on C 6 , as shown Eqn. 3.</p><formula xml:id="formula_3">u = η(C 6 )<label>(3)</label></formula><p>where η denotes a projection function for geospatial scene representation and it is implemented by a learnable 1×1 convolutional layer with output channels of d u . The scene embedding vector u is shared for each pyramid because the latent geospatial scene semantics is scale-invariant cross all pyramids. Hence, the relation map r i can be naturally obtained by Eqn. 4.</p><formula xml:id="formula_4">r i = ϕ(u,ṽ i ) = u ṽ i<label>(4)</label></formula><p>where ϕ denotes the similar estimation function and it is implemented by pointwise inner product for simplicity and efficient computational complexity. For each pyramid level, the process detail of the relation modeling is illustrated in <ref type="figure">Fig. 4</ref> and relation enhanced fore-ground feature maps z i is computed as follows:</p><formula xml:id="formula_5">z i = 1 1 + exp(−r i ) · κ wi (v i )<label>(5)</label></formula><p>where κ wi (·) is the encoder with learnable parameters w i for input feature maps v i . The encoder is designed to introduce a extra non-linear unit to avoid feature degradation since the weighting operation is a linear function. Therefore, we adopt a simple form of this encoder, which implemented by a 1×1 convolutional layer followed by batch normalization and ReLU for high efficiency of parameters and computation. The item including r i of Eqn. 5 is used to weight the re-encoded feature maps, which is the normalized relation map using the sigmoid gate function based on a simple self-gating mechanism <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Light-weight decoder</head><p>The light-weight decoder is designed to recover the spatial resolution of relation enhanced semantic feature maps from F-S relation module in a light-weight fashion. The detailed architecture of the light-weight decoder is illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. Given the pyramid feature maps z i ∈ R Cin×H×W from F-S relation module, the upsampled feature maps z i ∈ R Cout×σH×σW is computed via the light-weight decoder. The light-weight decoder is stacked by many upsampling units. The upsampling unit is made up of a channel transformation T (·) and an optional 2× upsampling operation U (·), which only includes T (·) if the scale factor σ = 1. Hence, the light-weight decoder for pyramid level i can be simply formulated as:</p><formula xml:id="formula_6">H W H σ W σ in C out C -1 unit - unit N … N upsampling units 3 3 conv × bn relu 2 bilinear upsample × ( ) T ⋅ ( ) U ⋅</formula><formula xml:id="formula_7">z i =    U • T N (z i ), N &gt; 0, T (z i ), N = 0.<label>(6)</label></formula><p>where N denotes the number of upsampling units and N = i − 2.</p><p>T (·) is implemented by a 3×3 convolutional layer followed by batch normalization and ReLU. U (·) is the bilinear upsampling with a scale factor of 2. The total upsampling scale σ is equal to 2 N because the output stride is 4.</p><p>To aggregate upsampled feature maps from each pyramid, the point-wise mean operation followed by 1×1 convolutional layer is adopted for computation and parameter efficiency. And a 4× bilinear upsampling is used to produce the final class probability map of the same size as the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Foreground-Aware Optimization</head><p>The foreground-background imbalance problem usually causes the fact that background examples dominate the gradients during training. However, only the hard part of the background examples is valuable for optimization in the late period of training, where the hard examples are much less than easy examples in the background. Motivated by this, the foreground-aware optimization is proposed to make the network focus on foreground and hard examples in the background for a balanced optimization. The foregroundaware optimization includes three steps: hard example estimation, dynamic weighting and back-propagation, as shown in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>.</p><p>hard example estimation. This step is used to obtain the weights reflecting the hard degree of examples to adjust the distribution of pixel-wise loss. That the example is harder means that its weight is larger. Motivated by focal loss <ref type="bibr" target="#b30">[31]</ref>, we adopt (1 − p) γ as weight to estimate hard examples, where p ∈ [0, 1] is the predicted probability by the network and γ is the focusing factor. This formulation was used in object detection, but for the pixel-level task with foreground-background imbalance, we only expect to adjust the loss distribution without change of sum for avoiding gradient vanishing. Therefore, we generalize it for object segmentation in the HSR remote sensing imagery by introducing a normalization constant Z that guarantees l(p i , y i ) = 1</p><formula xml:id="formula_8">Z (1 − p i ) γ l(p i , y i ),</formula><p>where l(p i , y i ) denotes the cross entropy loss of i-th pixel computed by predicted probability p i and its ground truth y i . Hence, for the loss of each pixel, it has a weight 1 Z (1 − p i ) γ . dynamic weighting. The hard example estimation relies on the discrimination of the model. However, the discrimination is unconfident in the initial period of training, which makes the hard example estimation unconfident. If this unconfident hard example weights are used, the model training will be unstable, influencing the converged performance. To solve this problem, we propose a dynamic weighting strategy based on an annealing function. We design three annealing functions as the candidates, as <ref type="table" target="#tab_2">Table 1</ref> lists. Given the cross entropy loss l(p i , y i ), the dynamic weighted loss is formulate as:</p><formula xml:id="formula_9">l (p i , y i ) = [ 1 Z (1 − p i ) γ + ζ(t)(1 − 1 Z (1 − p i ) γ )] · l(p i , y i )<label>(7)</label></formula><p>where ζ(·) denotes an annealing function with respect to current training step t and ζ(t) ∈ [0, 1] is a monotonically Implementation detail. The backbone used in FarSeg was ResNet-50 for all the experiments, which was pretrained on ImageNet <ref type="bibr" target="#b11">[12]</ref>. The channels d in FPN was set to 256 and the dimension of shared manifold d u in F-S relation module was set to 256 if not specified. The default focusing factor γ in F-A optimization was 2. For hyperparameters introduced by F-A optimization, annealing step was set to 10k and decay f actor was set to 0.9 for the poly annealing function. For all the experiments, these models were trained for 60k iterations with a "poly" learning rate policy, where the initial learning rate was set to 0.007 and multiplied by (1 − step max step ) power with power = 0.9. We used synchronized SGD over 2 GPUs with a total of 8 images per mini-batch (4 images per GPU), weight decay of 0.0001 and momentum of 0.9. The synchronized batch normalization was used for cross-gpu communication of statistic in the batch normalization layer. For data augmentation, horizontal and vertical flip, rotation of 90 · k (k = 1, 2, 3) degree were adopted during training. For extra data preprocessing, we crop the image into a fixed size of (896, 896) using a sliding window striding 512 pixels.</p><p>Evaluation metric. Following the common practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>, we used the mean intersection over union (mIoU) as the main metric for object segmentation to evaluate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to General methods</head><p>To evaluate the FarSeg, we conduct comprehensive experiments on a larger scale HSR remote sensing images dataset. We compared FarSeg with several CNN-based methods from classical to state-of-the-art, including U-Net <ref type="bibr" target="#b37">[38]</ref>, FCN-8s <ref type="bibr" target="#b32">[33]</ref>, DenseASPP <ref type="bibr" target="#b43">[44]</ref>, Deeplab v3 <ref type="bibr" target="#b5">[6]</ref>, Semantic FPN <ref type="bibr" target="#b26">[27]</ref>, Deeplab v3+ <ref type="bibr" target="#b7">[8]</ref>, RefineNet <ref type="bibr" target="#b29">[30]</ref>, PSPNet <ref type="bibr" target="#b47">[48]</ref>. The quantitative results listed in <ref type="table" target="#tab_3">Table 2</ref> suggest that FarSeg outperforms other methods in HSR scenario. <ref type="figure">Fig. 6</ref> shows the trade-off between speed and accuracy. It indicates that FarSeg achieves a better trade-off between speed and accuracy, which benefits from the light-weight and effective module design. <ref type="figure">Figure 6</ref>. Speed (FPS) versus accuracy (mIoU) on iSAID val set. The radius of circles represents the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Params (M)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct comprehensive experiments to analyze the proposed modules and many important hyper-parameters in FarSeg. The baseline is composed of a FPN and a light-decoder, optimizing cross entropy loss. The mIoU is evaluated on iSAID val set with the same experimental settings if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Foreground-Scene Relation Module</head><p>The effect of F-S relation module. <ref type="table" target="#tab_4">Table 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) and (c)</head><p>show the ablation results of adding F-S relation based on baseline method <ref type="figure">(Table 3 (a)</ref>). F-S relation modules (w/o and w/ scale-aware projection) brings 1.11% and 1.18%  Scale-aware projection for scene embedding. The projection function η is used for geospatial scene representation in F-S relation module. We explore whether the scaleaware projection function η is needed for each pyramid level. The results of <ref type="table" target="#tab_4">Table 3</ref> (b)/(c) and (e)/(f) suggest that scale-aware projection function performs better. With F-A optimization, the gain in mIoU from scale-aware projection is larger. It indicates that geospatial scene representation is related to scale and foregrounds.</p><p>Visual interpretation for F-S relation module. F-S relation module has good visual interpretability, combining with geoscience knowledge. <ref type="figure" target="#fig_4">Fig. 7</ref> shows the visualization of F-S relations in the different pyramid levels. Each pixel represents the relation intensity between the latent geospatial scene and the pixel-self. There are three classical scenarios: airport, harbor, and parking-lot. We can find that different scenarios focus on different objects that are discriminative to this scenario. For example, the harbor mainly focuses on ship and water, while the airport focuses on the airplanes and their contexts. Meanwhile, these relation maps illustrate again that the geospatial scene is related to scale, foreground, and foreground-relative contexts.</p><p>Because we can find that small objects are hot in the relation map with high spatial resolution (OS = 4), such as small vehicle and ship. The large objects are hot in the relation map with lower spatial resolution. However, contexts are not spatial resolution-specific in the relation map. It reveals that the geospatial scene is related to scale-specific foregrounds and scale-agnostic contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Foreground-Aware Optimization</head><p>The effect of F-A optimization.  Due to instability the naive softmax focal loss for object segmentation, the mIoU drops 4.05%. However, when adding the normalization, the performance obtains significant improvement with 2.49% in mIoU. Compared with naive softmax focal loss, it gains 6.54% in mIoU. It suggests that the tuning the loss distribution without change of sum is the key to alleviate foreground-background imbalance problem.</p><p>Annealing function. The annealing function is used in dynamic weighting stage of F-A optimization. It aims to alleviate the training instability due to the wrong hard example estimation in the period of early training. <ref type="table">Table 4</ref> (d), (e), and (f) show the results of applying three proposed annealing functions. We can find that annealing-based dynamic weighting boosts the performance via reducing the wrong hard example estimation in the period of early training. Intuitively, the cosine annealing function obtains the most significant gains of 0.63% in mIoU. Because the cosine annealing function has a slow descent rate at the start and end of training, which can stably adjust the loss distribution for healthy convergence, compared with linear annealing function and polynomial annealing function.</p><p>The choice of the focusing factor γ. The focusing factor γ is introduced to adjust the weight of hard examples. Larger γ, larger weight on hard examples. Following <ref type="bibr" target="#b30">[31]</ref>, we use varying γ to conduct experiments. The results are presented in <ref type="table" target="#tab_6">Table 5</ref>. As γ increase, the performance obtains continually improvement. With γ = 2, F-A optimization yields 3.29% in mIoU improvement over the baseline, achieving the best result of 63.71% in mIoU. However, with γ = 5, the performance drops. The possible reason is that noise labels are wrongly seen as hard examples, as mentioned in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we argue that false alarm and foregroundbackground imbalance problems are the bottlenecks of object segmentation in the HSR remote sensing imagery, while general semantic segmentation methods ignore it. To alleviate these two problems, we propose foreground-aware relation network (FarSeg), which learns foreground-scene relation to enhance the foreground features for less false alarms and trains the network using a foreground-aware optimization in foreground-background balanced fashion. The com-prehensive experimental results show the effectiveness of FarSeg and a better trade-off between speed and accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The main challenges of object segmentation in the HSR remote sensing imagery. (1) larger-scale variation. (2) foregroundbackground imbalance. (3) intra-class variance of background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of FarSeg. (a) Multi-branch Encoder for multi-scale object segmentation. (b) Foreground-scene relation module. (c) Light-weight decoder. (d) Foreground-aware optimization. The yellow dots indicate the relative positions of hard example in the raw image, probability map and estimation surface for a simple demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Concept of F-S relation. The foreground features are associated with relevant context features by their collaborative latent geospatial scene space. Meanwhile, the relevant context features are utilized to enhance the discrimination of the foreground features. The computation detail of relation modeling for the pyramid level i in the F-S relation module. The input and output have the same spatial size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Abstract architecture of the light-weight decoder for each pyramid level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Relation (OS = 4) (d) Relation (OS = 8) (e) Relation (OS = 16) (f) Relation (OS = 32) Visualization of F-S relation heatmap in the different pyramid levels. (a) original images. (b) object segmentation results. (c)-(f) images with F-S relation heatmaps in the different pyramid level. OS denotes "output stride" defined in FPN. For convenient visualization, we resize these relation maps to corresponding image sizes. Legend: Scene 1 (plane, large vehicle, small vehicle), Scene 2 (small vehicle, harbor, ship), Scene 3 (small vehicle, tennis court, baseball diamond), in a row order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Candidates of annealing functions.</figDesc><table><row><cell>Annealing function</cell><cell></cell><cell></cell><cell cols="2">Formula</cell><cell>Hyperparameter</cell></row><row><cell>Linear</cell><cell>ζ(t) = 1 −</cell><cell cols="3">t annealing step</cell><cell>annealing step</cell></row><row><cell>Poly</cell><cell cols="2">ζ(t) = (1 −</cell><cell cols="2">t annealing step ) decay f actor</cell><cell>annealing step, decay f actor</cell></row><row><cell>Cosine</cell><cell cols="3">ζ(t) = 0.5  *  (1 + cos (</cell><cell>t annealing step π)) annealing step</cell></row><row><cell cols="5">decreasing function. By this way, the focus of loss distri-</cell></row><row><cell cols="5">bution can progressively move on hard examples with the</cell></row><row><cell cols="5">increase of the confidence of hard example estimation.</cell></row><row><cell cols="2">4. Experiments</cell><cell></cell><cell></cell></row><row><cell cols="5">4.1. Experimental setting</cell></row><row><cell cols="5">Dataset. iSAID [41] dataset consists of 2,806 HSR re-</cell></row><row><cell cols="5">mote sensing images. These images were collected from</cell></row><row><cell cols="5">multiple sensors and platforms with multiple resolutions.</cell></row></table><note>The original image sizes range from ∼ 800 × 800 pixels to ∼ 4000 × 13000 pixels. The iSAID dataset provides 655,451 instances annotations over 15 categories 1 of the ob- ject, which is the largest dataset for instance segmentation in the HSR remote sensing imagery. The predefined training set contains 1,411 images, while validation (val) set con- tains 458 images and test set has 937 images. In this work, we only use semantic mask annotations for object segmen- tation. And we use the predefined training set to train mod- els and evaluate on the validation set. Because the test set is unavailable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Object segmentation mIoU (%) on iSAID val set. The bold values in each column means the best entries. .80 77.73 86.35 62.08 56.70 36.70 60.59 46.34 35.82 51.21 71.35 72.53 82.03 53.91</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell>mIoU (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IoU per category (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ship</cell><cell>ST</cell><cell>BD</cell><cell>TC</cell><cell>BC</cell><cell cols="2">GTF Bridge</cell><cell>LV</cell><cell>SV</cell><cell>HC</cell><cell>SP</cell><cell>RA</cell><cell>SBF Plane Harbor</cell></row><row><cell>U-Net [38]</cell><cell>-</cell><cell>37.39</cell><cell>49.0</cell><cell>0</cell><cell cols="4">6.51 78.60 22.89 5.52</cell><cell>7.48</cell><cell cols="2">49.89 35.62</cell><cell>0</cell><cell>38.03 46.49 9.67 74.74 45.64</cell></row><row><cell>FCN-8s [33]</cell><cell>VGG-16</cell><cell>41.66</cell><cell cols="6">51.74 22.91 26.44 74.81 30.24 27.85</cell><cell>8.17</cell><cell cols="2">49.35 37.05</cell><cell>0</cell><cell>30.74 51.91 52.07 62.90 42.02</cell></row><row><cell>DenseASPP [44]</cell><cell>DenseNet-121</cell><cell>56.81</cell><cell cols="9">61.15 50.05 67.54 86.09 56.56 52.28 29.61 57.10 38.44</cell><cell>0</cell><cell>43.26 64.80 74.10 78.12 51.09</cell></row><row><cell>Deeplab v3 [6]</cell><cell>ResNet-50</cell><cell>59.05</cell><cell cols="10">59.74 50.49 76.98 84.21 57.92 59.57 32.88 54.80 33.75 31.29 44.74 66.03 72.13 75.84 45.68</cell></row><row><cell cols="2">Semantic FPN [27] ResNet-50</cell><cell>59.31</cell><cell cols="9">63.68 59.49 71.75 86.61 57.78 51.64 33.99 59.15 45.14</cell><cell>0</cell><cell>46.42 68.71 73.58 80.83 51.27</cell></row><row><cell>Deeplab v3+ [8]</cell><cell>ResNet-50</cell><cell>59.33</cell><cell cols="10">59.02 55.15 75.94 84.18 58.52 59.24 32.11 54.54 33.79 31.14 44.24 67.51 73.78 75.70 45.76</cell></row><row><cell>RefineNet [30]</cell><cell>ResNet-50</cell><cell>60.20</cell><cell cols="10">63.80 58.56 72.31 85.28 61.09 52.78 32.63 58.23 42.36 22.98 43.40 65.63 74.42 79.89 51.10</cell></row><row><cell>PSPNet [48]</cell><cell>ResNet-50</cell><cell>60.25</cell><cell>65.2</cell><cell>52.1</cell><cell cols="8">75.7 85.57 61.12 60.15 32.46 58.03 42.96 10.89 46.78 68.6</cell><cell>71.9</cell><cell>79.5</cell><cell>54.26</cell></row><row><cell>FarSeg</cell><cell>ResNet-50</cell><cell>63.71</cell><cell cols="2">65.38 61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Object segmentation mIoU (%) on iSAID val set. Starting from Baseline, the proposed modules are gradually added in the proposed FarSeg for the module analysis.</figDesc><table><row><cell>Method</cell><cell cols="5">F-S Relation Scale-aware Proj. F-A Opt. mIoU(%) ∆#params(M)</cell></row><row><cell>(a) Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.31</cell><cell>0</cell></row><row><cell>(b) Baseline w/ F-S Relation</cell><cell></cell><cell></cell><cell></cell><cell>60.42</cell><cell>1.12</cell></row><row><cell>(c) Baseline w/ F-S Relation and Scale-aware Proj.</cell><cell></cell><cell></cell><cell></cell><cell>60.49</cell><cell>2.89</cell></row><row><cell>(d) Baseline w/ F-A Opt.</cell><cell></cell><cell></cell><cell></cell><cell>61.51</cell><cell>0</cell></row><row><cell>(e) Baseline w/ F-S Relation and F-A Opt.</cell><cell></cell><cell></cell><cell></cell><cell>63.21</cell><cell>1.12</cell></row><row><cell>(f) FarSeg</cell><cell></cell><cell></cell><cell></cell><cell>63.71</cell><cell>2.89</cell></row><row><cell cols="6">Table 4. Foreground-aware optimization module analysis.</cell></row><row><cell>Method</cell><cell cols="5">Normalization Annealing function mIoU(%)</cell></row><row><cell>(a) FarSeg w/o F-A Opt.</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>60.49</cell></row><row><cell>(b) Loss weighted with (1 − p) γ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.44</cell></row><row><cell>(c) + Norm.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.98</cell></row><row><cell>(d) + Norm. + Linear Annealing</cell><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell>63.18</cell></row><row><cell>(e) + Norm. + Poly Annealing</cell><cell></cell><cell></cell><cell>Poly</cell><cell></cell><cell>63.52</cell></row><row><cell>(f) + Norm. + Cosine Annealing</cell><cell></cell><cell></cell><cell>Cosine</cell><cell></cell><cell>63.71</cell></row><row><cell cols="6">performance gains in mIoU, respectively. ∆#params de-</cell></row><row><cell cols="6">notes the extra parameters introduced by the corresponding</cell></row><row><cell cols="6">module. It indicates that F-S relation modules are param-</cell></row><row><cell cols="6">eter efficient with only 2.89 M and 1.12 M, where relative</cell></row><row><cell cols="6">increments of parameters are ∼ 10% and ∼ 4%, respec-</cell></row><row><cell cols="6">tively. This suggests that the performance gain not only</cell></row><row><cell cols="6">comes from the gain of parameters, but also results from F-S</cell></row><row><cell cols="6">relation design of using geospatial scene feature associates</cell></row><row><cell cols="6">the relevant context features to enhance the foreground fea-</cell></row><row><cell>tures.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(d) and (f) show</cell></row></table><note>Normalization. Normalization is designed to only adjust the loss distribution without change of sum for avoiding gra- dient vanishing. Table 4 (c) shows the result of adding nor- malization on the naive softmax focal loss (Table 4 (b)).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>mIoU (%) on iSAID val set using varying γ for F-A optimization. ) 60.42 61.35 62.48 62.99 63.71 62.61</figDesc><table><row><cell>γ</cell><cell>0</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>5</cell></row><row><cell>mIoU (%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The categories are defined as: ship (Ship), storage tank (ST), baseball diamond (BD), tennis court (TC), basketball court (BC), ground field track (GTF), bridge (Bridge), large vehicle (LV), small vehicle (SV), helicopter (HC), swimming pool (SP), roundabout (RA), soccerball field (SBF), plane (Plane), harbor (Harbor).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Roadtracer: Automatic extraction of road networks from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4720" to="4728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved road connectivity by joint learning of orientation and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10385" to="10393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3322" to="3337" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep ship detector in sar images from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4021" to="4039" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiscale object detection in remote sensing imagery with convolutional neural networks. ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="3" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotated rectangles for symbolized building footprint extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dickenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gueguen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="73" to="86" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning. ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="60" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient harmonized singlestage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional recurrent network for road boundary extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9512" to="9521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale and multi-task deep learning framework for automatic road extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vehicle instance segmentation from aerial image and video using a multitask learning residual fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6699" to="6711" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">R 2 -cnn: Fast tiny object detection in large-scale remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5512" to="5524" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Conference on Machine Learning (ICML), number CONF</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The isprs benchmark on urban object classification and 3d building reconstruction. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Breitkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="293" to="298" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes by learning local class interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">isaid: A large-scale dataset for instance segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Building extraction in very high resolution remote sensing imagery using deep learning and guided filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">144</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning building extraction in aerial scenes with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2793" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An object-based convolutional neural network (ocnn) for urban land use classification. Remote sensing of environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Joint deep learning for land cover and land use classification. Remote sensing of environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
