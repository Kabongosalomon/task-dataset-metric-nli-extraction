<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohieddine</forename><surname>Amine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Dghaily</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce SoccerNet, a benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). As such, the dataset is easily scalable. These annotations are manually refined to a one second resolution by anchoring them at a single timestamp following well-defined soccer rules. With an average of one event every 6.9 minutes, this dataset focuses on the problem of localizing very sparse events within long videos. We define the task of spotting as finding the anchors of soccer events in a video. Making use of recent developments in the realm of generic action recognition and detection in video, we provide strong baselines for detecting soccer events. We show that our best model for classifying temporal segments of length one minute reaches a mean Average Precision (mAP) of 67.8%. For the spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances Î´ ranging from 5 to 60 seconds. Our dataset and models are available at https://silviogiancola.github.io/SoccerNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sports is a lucrative sector, with large amounts of money being invested on players and teams. The global sports market is estimated to generate an annual revenue of $91 billion <ref type="bibr" target="#b16">[17]</ref>, whereby the European soccer market contributes about $28.7 billion <ref type="bibr" target="#b17">[18]</ref>, from which $15.6 billion alone come from the Big Five European soccer leagues (EPL, La Liga, Ligue 1, Bundesliga and Serie A) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. After merchandising, TV broadcast rights are the second major revenue stream for a soccer club <ref type="bibr" target="#b20">[21]</ref>. Even though the main scope of soccer broadcast is entertainment, such videos are also used by professionals to generate statistics, analyze strategies, and scout new players. Platforms such as Wyscout <ref type="bibr" target="#b82">[83]</ref>, Reely <ref type="bibr" target="#b58">[59]</ref>, and Stats SPORTVU <ref type="bibr" target="#b71">[72]</ref> have made sports analytics their core business and already pro- vide various products for advanced statistics and highlights.</p><p>In order to get such statistics, professional analysts watch a lot of broadcasts and identify all the events that occur within a game. According to Matteo Campodonico, CEO of Wyscout, a 400 employee company focusing on soccer data analytics <ref type="bibr" target="#b82">[83]</ref>, it takes over 8 hours to provide up to 2000 annotations per game. With more than 30 soccer leagues in Europe, the number of games is very large and requires an army of annotators. Even though Amazon Mechanical Turk (AMT) can provide such workforce, building an annotated dataset of soccer games comes at a significant cost.</p><p>Automated methods for sports video understanding can help in the localization of the salient actions of a game. Several companies such as Reely <ref type="bibr" target="#b58">[59]</ref> are trying to build automated methods to understand sports broadcasts and would benefit from a large-scale annotated dataset for training and evaluation. Many recent methods exist to solve generic human activity localization in video focusing on sports <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>. However, detecting soccer actions is a difficult task due to the sparsity of the events within a video. Soccer broadcast understanding can thus be seen as a sub-problem of video understanding, focusing on a vocabulary of sparse events defined within its own context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (i)</head><p>We propose the task of event spotting within a soccer context. We define events as actions anchored to a single timestamp in a video and, thus, proceed to define and study the task of spotting events within soccer videos (Section 3). (ii) We propose SoccerNet, a scalable dataset for soccer video understanding. It contains 764 hours of video and 6,637 instances split in three classes (Goal, Yellow/Red Card, and Substitution), which makes it the largest localization dataset in term of total duration and number of instances per class (Section 4). (iii) We provide baselines for our dataset in the tasks of video chunk classification and event spotting. Our minute classifier reaches a performance of 67.8% (mAP) and our event spotter an Average-mAP of 49.7% (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This paper relates to the topics of Sports Analytics, Activity Recognition and Action Localization Datasets. We give a brief overview of work relevant to each of these topics and highlight how our paper contributes to each of them.</p><p>Sports Analytics. Many automated sports analytics methods have been developed in the computer vision community to understand sports broadcasts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">58]</ref>. They produce statistics of events within a game by either analyzing camera shots or semantic information. Ekin et al. <ref type="bibr" target="#b23">[24]</ref> present a cornerstone work for game summarization based on camera shot segmentation and classification, followed by Ren et al. <ref type="bibr" target="#b59">[60]</ref> who focus on identifying video production patterns. Huang et al. <ref type="bibr" target="#b34">[35]</ref> analyze semantic information to automatically detect goals, penalties, corner kicks, and card events. Tavassolipour et al. <ref type="bibr" target="#b75">[76]</ref> use Bayesian networks to summarize games by means of semantic analysis.</p><p>More recent work in this category focuses on deep learning pipelines to localize salient actions in soccer videos. Baccouche et al. <ref type="bibr" target="#b5">[6]</ref> use a Bag-of-Words (BOW) approach with SIFT features to extract visual content within a frame. They use such representations to train a Long Short Term Memory (LSTM) network that temporally traverses the video to detect the main actions. Jiang et al. <ref type="bibr" target="#b37">[38]</ref> propose a similar methodology using Convolution Neural Networks (CNN) to extract global video features rather than local descriptors. They also use a play-break structure to generate candidate actions. Tsagkatakis et al. <ref type="bibr" target="#b77">[78]</ref> present a two-stream approach to detect goals, while Homayounfar et al. <ref type="bibr" target="#b32">[33]</ref> recently present a deep method for sports field localization, which is crucial for video registration purposes.</p><p>The main impediment for all these works is the lack of reference datasets/benchmarks that can be used to evaluate their performance at large-scale and standardize their comparison. They all use small and custom-made datasets, which contain a few dozen soccer games at most. We argue that intelligent sports analytics solutions need to be scalable to the size of the problem at hand. Therefore, to serve and support the development of such scalable solutions, we propose a very large soccer-centric dataset that can be easily expanded and enriched with various types of annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity Recognition.</head><p>Activity recognition focuses on understanding videos by either detecting activities or classifying segments of video according to a predefined set of human-centric action classes. A common pipeline consists of proposing temporal segments <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b63">64]</ref>, which are in turn further pruned and classified <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b79">80]</ref>. Common methods for activity classification and detection make use of dense trajectories <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref>, actionness estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b86">87]</ref>, Recurrent Neural Networks (RNN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>, tubelets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b62">63]</ref>, and handcrafted features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b85">86]</ref>.</p><p>In order to recognize or detect activities within a video, a common practice consists of aggregating local features and pooling them, looking for a consensus of characteristics <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b66">67]</ref>. While naive approaches use mean or maximum pooling, more elaborate techniques such as Bag-of-Words (BOW) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b67">68]</ref>, Fisher Vector (FV) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>, and VLAD <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref> look for a structure in a set of features by clustering and learning to pool them in a manner that improves discrimination. Recent works extend those pooling techniques by incorporating them into Deep Neural Network (DNN) architectures, namely NetFV <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b73">74]</ref>, SoftDBOW <ref type="bibr" target="#b56">[57]</ref>, and NetVLAD <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. By looking for correlations between a set of primitive action representations, ActionVLAD <ref type="bibr" target="#b29">[30]</ref> has shown state-of-the-art performance in several activity recognition benchmarks.</p><p>To further improve activity recognition, recent works focused on exploiting context <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50]</ref>, which represent and harness information in both temporal and/or spatial neighborhood, or on attention <ref type="bibr" target="#b50">[51]</ref>, which learns an adaptive confidence score to leverage this surrounding information. In this realm, Caba Heilbron et al. <ref type="bibr" target="#b9">[10]</ref> develop a semantic context encoder that exploits evidence of objects and scenes within video segments to improve activity detection effectiveness and efficiency. Miech et al. <ref type="bibr" target="#b49">[50]</ref>, winners of the first annual Youtube 8M challenge <ref type="bibr" target="#b1">[2]</ref>, show how learnable pooling can produce state-of-the-art recognition performance on a very large benchmark, when recognition is coupled with context gating. More recently, several works use temporal context to localize activities in videos <ref type="bibr" target="#b15">[16]</ref> or to generate proposals <ref type="bibr" target="#b27">[28]</ref>. Furthermore, Nguyen et al. <ref type="bibr" target="#b50">[51]</ref> present a pooling method that uses spatio-temporal attention for enhanced action recognition, while Pei et al. <ref type="bibr" target="#b52">[53]</ref> use temporal attention to gate neighboring observations in a RNN framework. Note that attention is also widely used in video captioning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Activity recognition and detection methods are able to provide good results for these complicated tasks. However, those methods are based on DNNs and require large-scale and rich datasets to learn a model. By proposing a large-scale dataset focusing on event spotting and soccer, we encourage algorithmic development in those directions.</p><p>Datasets. Multiple datasets are available for video understanding, especially for video classification. They include Hollywood2 <ref type="bibr" target="#b46">[47]</ref> and HMDB <ref type="bibr" target="#b44">[45]</ref>, both focusing on movies; MPII Cooking <ref type="bibr" target="#b61">[62]</ref>, focusing on cooking activities; UCF101 <ref type="bibr" target="#b70">[71]</ref>, for classification in the wild; UCF Sports <ref type="bibr" target="#b60">[61]</ref>, Olympics Sports <ref type="bibr" target="#b51">[52]</ref> and Sports-1M <ref type="bibr" target="#b41">[42]</ref>, focusing on sports; Youtube-8M <ref type="bibr" target="#b1">[2]</ref> and Kinetics <ref type="bibr" target="#b42">[43]</ref>, both tackling large scale video classification in the wild. They are widely used in the community but serve the objective of video classification rather than activity localization.</p><p>The number of benchmark datasets focusing on action localization is much smaller. THUMOS14 <ref type="bibr" target="#b38">[39]</ref> is the first reasonably scaled benchmark for the localization task with a dataset of 413 untrimmed videos, totaling 24 hours and 6,363 activities, split into 20 classes. MultiTHU-MOS <ref type="bibr" target="#b84">[85]</ref> is a subset of THUMOS, densely annotated for 65 classes over unconstrained internet videos. Activi-tyNet <ref type="bibr" target="#b11">[12]</ref> tackles the issue of general video understanding using a semantic ontology, proposing challenges in trimmed and untrimmed video classification, activity localization, activity proposals and video captioning. ActivityNet 1.3 provides a dataset of 648 hours of untrimmed videos with 30,791 activity candidates split among 200 classes. It is so far the largest localization benchmark in terms of total duration. Charades <ref type="bibr" target="#b65">[66]</ref> is a recently compiled benchmark for temporal activity segmentation that crowd-sources the video capturing process. After collecting a core set of videos from YouTube, they use AMT to augment their data by recording them at home. This dataset consists of a total of 9,848 videos and 66,500 activities. More recently, Google proposed AVA <ref type="bibr" target="#b30">[31]</ref> as a dataset to tackle dense activity understanding. They provide 57,600 clips of 3 seconds duration taken from featured films, annotated with 210,000 dense spatio-temporal labels across 100 classes, for a total of 48 hours of video. While the main task of AVA is to classify these 3 seconds segments, such dense annotation can also be used for detection.</p><p>Within the multimedia community, TRECVID has been the reference benchmark for over a decade <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b68">69]</ref>. They host a "Multimedia Event Detection" (MED) and a "Surveillance Event Detection" (SED) task every year, using the HAVIC dataset <ref type="bibr" target="#b72">[73]</ref>. These tasks focus on finding all clips in a video collection that contain a given event, with a textual definition, in multimedia and surveillance settings. Also, Ye et al. <ref type="bibr" target="#b83">[84]</ref> propose EventNet, a dataset for event retrieval based on a hierarchical ontology, similar to Activi-tyNet. We argue that these two datasets both focus on largescale information retrieval rather than video understanding.</p><p>We propose SoccerNet, a scalable and soccer-focused dataset for event spotting. It contains 500 games, 764 hours of video and 6,637 instances split in three classes (Goal, <ref type="figure">Figure 2</ref>. Dataset comparison in term of number of instance per class, and total duration. The size of the hexagon shows the density of the event within the video. Our dataset has the largest amount of instances per class and the largest total duration, despite being sparse, which makes the task of localization more difficult.</p><p>Yellow/Red Card, and Substitution), which makes it one of the largest dataset in term of total duration and number of instances per class. With an average of one event every 6.9 minutes, our dataset has a sparse distribution of events in long untrimmed videos, which makes the task of localization more difficult. The annotations are obtained within one minute at no cost by parsing sports websites, and further refined in house to one second resolution. We define our dataset as easily scalable since annotations are obtained for free from online match reports. <ref type="table" target="#tab_0">Table 1</ref> shows a breakdown description and comparison of the datasets available for the problem of action localization. <ref type="figure">Figure 2</ref> shows a graphical comparison between these datasets in terms of the number of instances per class and the total duration of videos they contain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spotting Sparse Events in Soccer</head><p>In this context, we define the concept of events and the task of spotting events within soccer videos.</p><p>Events: Sigurdsson et al. <ref type="bibr" target="#b64">[65]</ref> recently question the concept of temporal boundaries in activities. They re-annotated Charades <ref type="bibr" target="#b65">[66]</ref> and MultiTHUMOS <ref type="bibr" target="#b84">[85]</ref> (using AMT), and conclude that the average agreement with the ground truth is respectively 72.5% and 58.8% tIoU. This clearly indicates that temporal boundaries are ambiguous. However, Sigurdsson et al. <ref type="bibr" target="#b64">[65]</ref> observe that central frames within an activity offer more consensus among annotators.</p><p>Chen et al. <ref type="bibr" target="#b13">[14]</ref> define the concept of action and actionness by underlining 4 necessary aspects that define an action: an agent, an intention, a bodily movement, and a sideeffect. Dai et al. <ref type="bibr" target="#b15">[16]</ref> define an activity as a set of events or actions, with a beginning and an ending time. In our work, we define the concept of event as an action that is anchored in a single time instance, defined within a specific context respecting a specific set of rules. We argue that defining every action with temporal boundaries is ambiguous for multiple reasons:</p><p>1. An action can occur in a glimpse, such as "a man dropped his wallet" or "a man put a letter in the mail". While there are no well-defined boundaries for such actions, a sole instant can readily define these events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">An action can be continuous within a live video, hence</head><p>it is unclear when it starts or stops. For instance, time boundaries in video for actions such as "the night is falling" or "the ice is melting in my glass", rely on a subjective discrimination between measurable quantities such as the illumination level or visual changes in matter state. 3. An action can overlap and conflict with another. Consider a video of a man walking his dog, when he suddenly receives a phone call. It is not clear whether the activity "taking a phone call" actually cancels out the activity "walking a dog", or the activity "walking a dog" should be split into two parts as opposed to one single segment overlapping the "taking a phone call" instance.</p><p>Current benchmarks such as THUMOS14 <ref type="bibr" target="#b38">[39]</ref>, Activi-tyNet <ref type="bibr" target="#b11">[12]</ref>, and Charades <ref type="bibr" target="#b65">[66]</ref> only focus on activities with temporal boundaries and cope with ambiguities by anchoring an activity with a consensus between several annotators. This ambiguity motivates the recently developed AVA <ref type="bibr" target="#b30">[31]</ref> dataset that attempts to tackle the atomic characteristic of actions by providing dense fine-scale annotations within a short time duration (3 seconds).</p><p>In the multimedia community, the concept of events is generally more vague and overlaps with the concept of actions and activities. In the MED task of the TRECVID benchmark <ref type="bibr" target="#b4">[5]</ref>, an event is defined as a kit which consists of a mnemonic title for the event, a textual definition, an evidential description that indicates a non-exhaustive list of textual attributes, and a set of illustrative video examples. They propose a specific set of events, providing a description and defining rules for the start and end times. Such work underlines our hypothesis that events need to be defined with a set of rules and within specific circumstances.</p><p>In the context of live soccer broadcasts, it is unclear when a given action such as "scoring a goal" or "making a foul" starts and stops. For similar reasons, the beginning and end of activities such as "scoring a 3 points shot" or a "slam dunk" in a basketball broadcast are ambiguous. We argue that sports respect well-established rules and define an action vocabulary anchored in a single time instance. In fact, soccer rules provide a strict definition of "goal", "foul", "card", "penalty kick", "corner kick", etc. and also anchor them within a single time. Similarly, Ramanathan et al. <ref type="bibr" target="#b57">[58]</ref> define the action "basket-ball shot" as a 3 seconds activity and its ending time as the moment the ball crosses the basket. Defining starting or stopping anchors around such events or fixing its duration would be considered as subjective and biased by the application.</p><p>Spotting: Rather than identifying the boundaries of an action within a video and looking for similarities within a given temporal Intersection-over-Union (tIoU), we introduce the task of spotting. Spotting consists of finding the anchor time (or spot) that identifies an event. Intuitively, the closer the candidate spot is from the target, the better the spotting is, and its capacity is measured by its distance from the target. Since perfectly spotting a target is intrinsically arduous, we introduce a tolerance Î´ within which a event is considered to be spotted (hit) by a candidate. We believe that event spotting is better defined and easier than detection since it focuses only on identifying the presence of an event within a given tolerance. An iterative process can refine such tolerance at will by using fine localization methods around candidate spots.</p><p>By introducing the task of spotting, we also define the metric to be used for evaluation. First of all, we define a candidate spot as positive if it lands within a tolerance Î´ around the anchor of an event. For each tolerance, we can recast the spotting problem as a general temporal detection problem, where the tIoU threshold used is very small. In that case, we can compute the recall, precision, Average Precision (AP) for each given class, and a mean Average Precision (mAP) across all classes. For general comparison, we also define an Average-mAP over a set of predefined Î´ tolerances, in our case below the minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Collection</head><p>We build our dataset in three steps: (i) we collect videos from online sources; (ii) we synchronize the game and video times by detecting and reading the game clock; and (iii) we parse match reports available in the web to generate temporal aligned annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Collecting Videos</head><p>We compile a set of 500 games from the main European Championships during the last 3 seasons as detailed in <ref type="table" target="#tab_1">Table 2</ref>. Each game is composed of 2 untrimmed videos, one for each half period. The videos come from online providers, in a variety of encodings (MPEG, H264), containers (MKV, MP4, and TS), frame rates (25 to 50 fps), and resolutions (SD to FullHD). The dataset consumes almost 4TB, for a total duration of 764 hours. The games are randomly split into 300, 100, and 100 games for training, validation, and testing ensuring similar distributions of the events between the classes and the datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Game Synchronization with OCR</head><p>The video of the games are untrimmed and contains spurious broadcast content before and after the playing time. Finding a mapping between the game time and the video time is necessary to align the annotations from the web sources to the videos. Soccer games have a continuous game flow, i.e. the clock never stops before the end of a half, hence there is a simple linear relationship (with slope 1) between the video and the game time. Wang et al. <ref type="bibr" target="#b81">[82]</ref> propose a method using the center circle of the field and the sound of the referee whistle to identify the start of the game. We argue that focusing the effort on a single instant is prone to error. In contrast, we focus on detecting the game clock region within multiple video frames and identify the game time through Optical Character Recognition (OCR) at different instants.</p><p>The clock is displayed in most of the frames throughout the video, though its shape and position vary between leagues. We leverage a statistical study of the pixel intensity deviation within a set of N random frames to identify candidates for the clock region. We run the Tesseract OCR Engine <ref type="bibr" target="#b69">[70]</ref> on the candidate clocks and look for a coherent time format for each of the N frames. To cope with eventual misreadings in the clock, we use a RANSAC <ref type="bibr" target="#b26">[27]</ref> approach to estimate the linear relation between the game clock and the video time, enforcing a unitary gradient to our linear model. Our method also checks for the temporal integrity of the video, reporting temporal inconsistencies. To verify the quality of this game-to-video temporal alignment, we manually annotate the start of the game for all 500 games and report an accuracy of 90% for automatically estimating the start of both halves within a tolerance of two seconds, using a set of N = 200 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Collecting Event Annotations</head><p>For our dataset, we obtain event annotations for free by parsing match reports provided by league websites 1 . They summarize the main actions of the game and provide the minute at which the actions occur. We categorize these events into our three main categories: "goals", "cards" and "substitutions". We parse and mine the annotations for all games of the Big Five European leagues (EPL, La Liga, Ligue 1, Bundesliga and Serie A) as well as the Champions League from 2010 to 2017, for a total of 171,778 annotations corresponding to 13,489 games. For sake of storage, we focus on our subset of videos for the 500 games and use only 6,637 events. To resolve these free annotations to one second level, we manually annotate each event within one second resolution by first retrieving its minute annotation and refining it within that minute window. To do so, we define the temporal anchors for our events from their definitions within the rules of soccer. We define a "goal" event as the instant the ball crosses the goal line to end up in the net. We define the "card" event as the instant the referee shows a player a yellow or a red card because of a foul or a misbehaviour. Finally, we define the "substitution" event as the instant a new player enters in the field. We ensure those definition were coherent when annotating the dataset. Apart for the substitutions that occur during half time break, almost all of our instances follow their definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Dataset Scalability</head><p>We believe that scaling our dataset is cheap and easy, since web annotations are freely available with one minute resolution. Algorithm can either use the weakly annotated events within one minute resolution or generate a complete one second resolution annotation which is estimated to take less than 10 minutes per game. We also argue that broadcast providers can easily scale up such datasets by simply providing more videos and richer annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We focus the attention of our experiments on two tasks: event classification for chunks of one minute duration, and event spotting within an entire video. For these tasks, we report and compare the performance metrics for various baseline methods when trained on weakly annotated data (i.e. one minute resolution) and the improvement that is achieved by training on one second resolution annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Video Representation</head><p>Before running any experiments, we extract C3D <ref type="bibr" target="#b76">[77]</ref>, I3D <ref type="bibr" target="#b12">[13]</ref>, and ResNet <ref type="bibr" target="#b31">[32]</ref> features from our videos to be used by our baselines. The videos are trimmed at the game start, resized and cropped at a 224 Ã 224 resolution, and unified at 25fps. Such representation guarantees storage efficiency, fast frame access, and compatible resolution for feature extraction. C3D [77] is a 3D CNN feature extractor that stacks 16 consecutive frames and outputs at the fc7 layer a feature of dimension 4,096. It is pretrained on Sport-1M <ref type="bibr" target="#b41">[42]</ref>. I3D <ref type="bibr" target="#b12">[13]</ref> is based on Inception V1 <ref type="bibr" target="#b74">[75]</ref>, uses 64 consecutive frames, and is pretrained on Kinetics <ref type="bibr" target="#b42">[43]</ref>. In this work, we only extract the RGB features at the PreLogits layer of length 1,024 so to maintain a reasonable computational runtime. They have been shown to produce only meager improvements when flow features are used <ref type="bibr" target="#b12">[13]</ref>.</p><p>ResNet <ref type="bibr" target="#b31">[32]</ref> is a very deep network that outputs a 2,048 feature representation per frame at the fc1000 layer. In particular, we use ResNet-152 pretrained on ImageNet <ref type="bibr" target="#b21">[22]</ref>. Since ResNet-152 applies to single images, it does not intrinsically embed contextual information along the time axis. We use TensorFlow <ref type="bibr" target="#b0">[1]</ref> implementations to extract features every 0.5 second (s). In order to simplify and unify the dimension of those features, we reduce their dimensionality by applying Principal Component Analysis (PCA) on the 5.5M features we extract per model. We reduce C3D, I3D, and ResNet-152 features to a dimension of 512 and respectively maintain 94.3%, 98.2%, and 93.9% of their variance. For the benchmark purpose, we provide the original and cropped versions of the videos, as well as the original and the reduced versions of all the features extracted every 0.5s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Video Chunk Classification</head><p>Similar to the setup in the AVA dataset <ref type="bibr" target="#b30">[31]</ref>, localization can be cast as a classification problem for densely annotated chunks of video, especially since we gather webly annotations. We split our videos into chunks of duration 1 minute, annotated with all events occurring within this minute, gathering respectively 1246, 1558, 960 and 23750 chunks for cards, substitutions, goals and backgrounds for the training dataset, 115 of which having multiple labels. We aggregate the 120 features within a minute as input for different versions of shallow pooling neural networks. By using a sigmoid activation function at the last layer of these networks, we allow for multi-labelling across the candidates. We use an Adam optimizer that minimizes a multi binary cross-entropy loss for all the classes. We used a step decay for the learning rate and an early stopping technique based on the validation set performances. Following best practices in the field, the evaluation metric in this case is mAP (classification) across the three classes on the designated testing set. In what follows, we report strong baseline results using different video features, different pooling techniques, and compare solutions to cope with the imbalanced dataset.</p><p>Learning How to Pool: We investigate the usage of different feature representations and various pooling methods. We propose shallow neural networks that handles the input matrix of dimension 120 Ã 512. We test a mean and a max pooling operation along the aggregation axis that output 512-long features. We use a custom CNN with a kernel of dimension 512Ã20 that traverses the temporal dimension to gather temporal context. Finally, we use implementations of SoftDBOW, NetFV, NetVLAD and NetRVLAD provided by Miech et al. <ref type="bibr" target="#b49">[50]</ref>, who leverage a further contextgating layer. After each of these pooling layer, we stack a fully connected layer with a dropout layer (keep probability 60%) that predicts the labels for the minutes of video and prevent overfitting. <ref type="table" target="#tab_2">Table 3</ref> summarizes a performance comparison between the various pooling methods when applied to the testing set. First of all, we notice similar results across features by using mean and max pooling, that only rely on a single representation of the set of 120 features and not its distribution. Using the custom CNN layer, which is an attempt to gather temporal context, ResNet-152 performs better than C3D which performs better than I3D. We believe that the I3D and C3D already gather temporal information for 64 and 16 frames.</p><p>We can notice that the gap between the features increases by using the pooling methods proposed by Miech et al. <ref type="bibr" target="#b49">[50]</ref>, which is a way to embed context along the temporal dimension. We believe that I3D and C3D features already rely on a temporal characterization within the stack of frames. On the other hand, the ResNet-152 provides a representation that focuses only on the spatial aspect within a frame. We believe that the temporal pooling methods provides more redundant information for I3D and C3D, than for ResNet-152. For this reason, we argue that ResNet-152 features provide better results when coupled with any temporal pooling methods provided by Miech et al. <ref type="bibr" target="#b49">[50]</ref>.</p><p>Focusing on the pooling, VLAD-based methods are at the top of the ranking, followed by the deep versions of the FV and BoW methods. Such improvement is attributed to the efficient clustering for the 120 features learned in NetVLAD <ref type="bibr" target="#b2">[3]</ref> providing state-of-the-art results for action classification <ref type="bibr" target="#b29">[30]</ref>. Note that NetRVLAD performs similarly if not better than NetVLAD by relying only on the average and not the residuals for each clustering, reducing the computational load <ref type="bibr" target="#b49">[50]</ref>. For the rest of the experiment we are relying exclusively on ResNet-152 features.  <ref type="table" target="#tab_2">Table 3</ref>, we use k = 64 clusters, which can be interpreted as the vocabulary of atomic elements that are learned to describe the events. Intuitively, one can expect that a richer and larger vocabulary can enable better overall performance <ref type="bibr" target="#b29">[30]</ref>. We show in <ref type="table" target="#tab_3">Table 4</ref> that this intuition is true within a certain range of values k, beyond which the improvement is negligible and overfitting occurs. The performance of all pooling methods seem to plateau when more than 256 clusters are used for the quantization. The best results are registered when NetVLAD is used with 512 clusters. Nevertheless, the computational complexity increases linearly with the number of clusters, hence computational times grow drastically. Coping with Imbalanced Data: The performance of classifiers are significantly affected when training sets are imbalanced. Due to the sparsity of our events, we have numerous background instances. Here, we present three main techniques to cope with this imbalance. One method focuses on weighting (Weig) the binary cross-entropy with the ratio of negative samples to enforce the learning of the positive examples. Another method applies a random downsampling (Rand) on the highest frequency classes, or by hard negative mining (HNM), i.e. by sampling the examples that are misclassified the most in the previous epoch. The third method uses Data Augmentation (Augm) to bal-ance the classes. In that case, we use the fine annotation of the event and slide the minute window with a stride of 1s within Â±20s of the event spot to sample more video segments for the sparsest event classes. We argue that a chunk of 1 minute within Â±10s around the anchor of the event still contains this event, and the pooling method should be able to identify it. Although, note that our data augmentation requires the data to be finely annotated. <ref type="table" target="#tab_4">Table 5</ref> shows the classification mAP for the testing dataset, training with the previous pooling methods on ResNet features, and using the aforementioned strategies to cope dataset imbalance. We see that weighting slightly improves the metric. Both downsampling methods actually lead to the worst results, because of the reduced amount of data the model has been trained on at each epoch. Using the second resolution annotations to augment the data helps to achieve slightly better classification results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Spotting</head><p>In this section, we discuss the task of event spotting in soccer videos. We use the models trained in the classifier task and apply them in a sliding window fashion on each testing video, with a stride of 1s, thus, leading to a second resolution score along for each event class. We investigate the spotting results of three strong baselines (i) a watershed method to compute segment proposals and use the center time within the segment to define our candidate; (ii) the time index of the maximum value of the watershed segment as our candidate; and (iii) the local maxima along all the video and apply non-maximum-suppression (NMS) within a minute window. The evaluation metric is the mAP with tolerance Î´ as defined for spotting in Section 3, as well as, the Average-mAP expressed as an area under the mAP curve with tolerance ranging from 5 to 60 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Spotting Baselines:</head><p>We investigate the results for event spotting for our best weakly-trained classifier, to leverage the use of webly-parsed annotations, i.e. we train on the imbalanced minute resolution annotated data and do not perform data augmentation. Specifically, we use a NetVLAD model with k = 512 cluters based on ResNet  features and the watershed threshold is set to 50%. <ref type="figure" target="#fig_1">Figure 3a</ref> plots the mAP of each spotting baseline as a function of the tolerance Î´ to the spot. As expected, the mAP decreases with the spot tolerance Î´. Above a tolerance Î´ of 60s, both three baselines plateau at 62.3%. Below 60s, the baseline (ii) and (iii) perform similarly and decrease linearly with the tolerance. On the other hand, baseline (i) decreases more gradually, hence provides a better Average-mAP of 40.6%. Even though the model has been trained using chunks of 1 minute, the method is still able to achieve good spotting results for tolerances below 60s. We argue that our model predicts positively any window that contains an event, creating a plateau.</p><p>Training on Smaller Windows: Here, we train our classifiers from Section 5.2 using a smaller chunk size, ranging from 60 seconds to 5 seconds. We expect these models to perform in a similar fashion, with a drop in performance (mAP) occurring for tolerances below the chunk size. Note that we use finely annotated data to train such classifiers. <ref type="figure" target="#fig_2">Figure 3</ref> depicts the spotting mAP in function of the tolerance Î´ for the models trained on 60, 20 and 5 seconds. They all have similar shape, a metric that plateaus for spotting tolerance Î´ above the chunk video length they have being trained on, and a decreasing metric below such threshold. By using baseline (i) on chunks of 20s we obtain the best Average-mAP of 50% (see <ref type="figure" target="#fig_2">Figure 3b</ref>). Also, a drop in performance occurs with models trained with chunks of 5s (see <ref type="figure" target="#fig_2">Figure 3c</ref>). We believe such gap in performance is related to the amount of context we allow around the event.</p><p>With these experiments, we setup a baseline for the spotting task but the best performance is far from satisfactory. Nevertheless, we see our newly compiled and scalable dataset to be a rich environment for further algorithm development and standardized evaluations; especially when it comes to novel spotting techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>Activity detection is commonly solved by proposing candidates that are further classified. We believe that detection can be solved by spotting a candidate and focusing attention around the spot to localize the activity boundaries.</p><p>In future works, we encourage the usage of RNNs to embed a further temporal aspect that will understand the evolution of the game. We will also include more classes for soccer events to enrich its contents and enable learning potential causal relationships between events. We believe for instance that the event "card" is mostly the result of an event "foul". Also, embedding semantic relationship information from the players, the ball and the field can improve soccer video understanding. Our video also contains an audio track that should be used; visual and audio sentiment analysis could localize the salient moments of the game.</p><p>The match reports from our online provider also includes match commentaries. We collected and will release a total of 506,137 commentaries for the six aforementioned leagues with a one second resolution. We believe such data can be used for captioning events in soccer videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we focus on soccer understanding in TV broadcast videos. We build this work as an attempt to provide a benchmark for soccer analysis, by providing a largescale annotated dataset of soccer game broadcasts. We discussed the concept of event within the soccer context, proposed a definition of "goal", "card" and "substitution" and parse a large amount of annotation from the web. We defined the task of spotting and provide a baseline for it. For the minute classification task, we have shown performance of 67.8% (mAP) using ResNet-152 features and NetVLAD pooling along a 512-long vocabulary and using a coarse annotation. Regarding the spotting task, we have establish an Average-mAP of 49.7% with fine annotation and 40.6% by using only weakly annotated data. We believe that focusing effort on spotting, new algorithms can improve the state-ofthe-art in detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary Material</head><p>We provide further details on the the dataset and further results for the spotting baseline. <ref type="table" target="#tab_5">Table 6</ref> provides more details on the distribution of the events for the training (300 games), validation (100 games) and testing (100 games) sets. We assess that the events are equally distributed along the different sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">PCA reduction</head><p>Reducing the dimension of the frame features reduces the complexity of the successive pooling layers. Nevertheless, the features lose some variance. We ensure in <ref type="figure" target="#fig_3">Figure 4</ref> that the loss in variance is minimal when reducing the dimension to 512 for ResNET, C3D and I3D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Insight for the metrics</head><p>We show in <ref type="figure" target="#fig_4">Figure 5</ref> some insight from the metric we are defining. A candidate spot is considered positive if he lands within a tolerance Î´ of a ground truth spot. In <ref type="figure" target="#fig_4">Figure 5</ref>, candidate A lands within a tolerance Î´, candidate B within a tolerance 3Î´ and candidate C within a tolerance 4Î´, hence considered as positive for tolerances greater or equal to such value, and negative for smaller tolerances. Recall and Precision are defined for a given tolerance Î´, as well as the Average Precision (AP), the mean Average Precision (mAP) along the three classes and the Average mAP along a set of tolerances. Note that the Average mAP can also be estimated through the area under the curve of the mAP in functions of the spotting tolerance (see <ref type="figure" target="#fig_7">Figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Spotting Results</head><p>We show in <ref type="figure" target="#fig_5">Figure 6</ref> the Recall Precision curve for the 3 metrics, using the best result for classification training, i.e. ResNet-152, NetVLAD pooling with k = 64 and segment center spotting baseline. Goal events are the easiest to spot with an AP of 73.0%, then substitutions reach 59.3% and Cards 52.1%. Also, <ref type="figure" target="#fig_7">Figure 7</ref> illustrates the mAP and the Average mAP for different models using different window sizes during training. It shows that the best result is performed by a window size of 20 seconds in classification training. A drop in performances in visible for windows of 5 seconds. Note that such model uses the fine annotation at a one second resolution.   <ref type="figure" target="#fig_9">Figure 8</ref> shows qualitative results for games in the training, validation and testing set. Each tile shows the activation around ground truth events for a given class, and depicts the candidate spot using our best model (ResNet-152, NetVLAD with k = 512) and the center segment (i) spotting baseline. The prediction usually activates for a 60 seconds range around the spot. It validates our hypothesis that any sliding window that contains the ground truth spot activates the prediction for the class. <ref type="figure" target="#fig_10">Figure 9</ref> shows further results with smaller windows sizes in training. As expected, the activation width reduces from 60 seconds to the value of the size of the video chunks used in training.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Qualitative results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of events defined in the context of soccer. From top to bottom: Goal: the instant the ball crosses the goal line. Substitution: the instant a players enters the field to substitute an other player. Card: the instant the referee shows a card to a player.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>Model trained on chunks of 60s (b) Model trained on chunks of 20s (c) Model trained on chunks of 5s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Spotting metric (mAP) in function of the tolerance Î´ for model trained on chunks of size (a) 60s, (b) 20s and (c) 5s. The Average-mAP is estimated through the area under the curve between 5s and 60s for each baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Dimensionality reduction using Principal Component Analysis (PCA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Insight to understand the metrics. Candidate A spots the event within a tolerance of Î´, Candidate B within 3Î´ and Candidate C within 4Î´.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Recall Precision curve for the three classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Model trained on chunks of 50s (b) Model trained on chunks of 40s (c) Model trained on chunks of 30s (d) Model trained on chunks of 20s (e) Model trained on chunks of 10s (f) Model trained on chunks of 5s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Spotting metric (mAP) in function of the tolerance Î´ for model trained on chunks of size (a) 50s, (b) 40s, (c) 30s, (d) 20s, (e) 10s and (f) 5s. The Average-mAP is estimated through the area under the curve between 5s and 60s for each baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Example of games from the training set (b) Example of games from the validation set (c) Example of games from the testing set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results for the Training (a), Validation (b) and Testing (c) examples. The time scale (X axis) is in minute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative results for window size ranging from 60 to 5 s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of benchmark datasets currently tackling the task of action localization.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Context #Video #Instance Duration</cell><cell>Sparsity</cell><cell cols="2">Classes Instance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(hrs)</cell><cell>(event/hr)</cell><cell></cell><cell>per class</cell></row><row><cell>THUMOS'14 [39]</cell><cell>General</cell><cell>413</cell><cell>6363</cell><cell>24</cell><cell>260.4</cell><cell>20</cell><cell>318</cell></row><row><cell>MultiTHUMOS [85]</cell><cell>General</cell><cell>400</cell><cell>38690</cell><cell>30</cell><cell>1289.7</cell><cell>65</cell><cell>595</cell></row><row><cell>ActivityNet [12]</cell><cell>General</cell><cell>19994</cell><cell>30791</cell><cell>648</cell><cell>47.5</cell><cell>200</cell><cell>154</cell></row><row><cell>Charades [66]</cell><cell>General</cell><cell>9848</cell><cell>66500</cell><cell>82</cell><cell>811</cell><cell>157</cell><cell>424</cell></row><row><cell>AVA [31]</cell><cell>Movies</cell><cell>57600</cell><cell>210000</cell><cell>48</cell><cell>4375</cell><cell>80</cell><cell>2625</cell></row><row><cell>Ours</cell><cell>Soccer</cell><cell>1000</cell><cell>6637</cell><cell>764</cell><cell>8.7</cell><cell>3</cell><cell>2212</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of the video collection for our dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Seasons</cell><cell></cell><cell></cell></row><row><cell>League</cell><cell cols="4">14/15 15/16 16/17 Total</cell></row><row><cell>EN -EPL</cell><cell>6</cell><cell>49</cell><cell>40</cell><cell>95</cell></row><row><cell>ES -LaLiga</cell><cell>18</cell><cell>36</cell><cell>63</cell><cell>117</cell></row><row><cell>FR -Ligue 1</cell><cell>1</cell><cell>3</cell><cell>34</cell><cell>38</cell></row><row><cell>DE -BundesLiga</cell><cell>8</cell><cell>18</cell><cell>27</cell><cell>53</cell></row><row><cell>IT -Serie A</cell><cell>11</cell><cell>9</cell><cell>76</cell><cell>96</cell></row><row><cell>EU -Champions</cell><cell>37</cell><cell>45</cell><cell>19</cell><cell>101</cell></row><row><cell>Total</cell><cell>81</cell><cell>160</cell><cell>259</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Classification metric (mAP) for different combinations of frame representations and pooling methods.</figDesc><table><row><cell></cell><cell cols="3">Frame features</cell></row><row><cell>Pooling</cell><cell>I3D</cell><cell>C3D</cell><cell>ResNet</cell></row><row><cell>Mean Pool.</cell><cell>40.8</cell><cell>40.7</cell><cell>40.2</cell></row><row><cell>Max Pool.</cell><cell>50.1</cell><cell>52.4</cell><cell>52.4</cell></row><row><cell>CNN</cell><cell>44.1</cell><cell>47.8</cell><cell>53.5</cell></row><row><cell>SoftDBOW</cell><cell>46.3</cell><cell>56.9</cell><cell>58.9</cell></row><row><cell>NetFV</cell><cell>44.7</cell><cell>59.6</cell><cell>64.4</cell></row><row><cell>NetRVLAD</cell><cell>41.3</cell><cell>59.9</cell><cell>65.9</cell></row><row><cell>NetVLAD</cell><cell>43.4</cell><cell>60.3</cell><cell>65.2</cell></row><row><cell cols="4">For the various pooling methods, the number of clusters</cell></row><row><cell>can be fine-tuned. In</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Classification metric (mAP) for different number of cluster for the pooling methods proposed by Miech et al.<ref type="bibr" target="#b49">[50]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Pooling Methods</cell><cell></cell></row><row><cell>k</cell><cell cols="4">SoftBOW NetFV NetRVLAD NetVLAD</cell></row><row><cell>16</cell><cell>54.9</cell><cell>63.0</cell><cell>64.4</cell><cell>65.2</cell></row><row><cell>32</cell><cell>57.7</cell><cell>64.0</cell><cell>63.8</cell><cell>65.1</cell></row><row><cell>64</cell><cell>58.8</cell><cell>64.1</cell><cell>65.3</cell><cell>65.2</cell></row><row><cell>128</cell><cell>60.6</cell><cell>64.4</cell><cell>67.0</cell><cell>65.6</cell></row><row><cell>256</cell><cell>61.3</cell><cell>63.8</cell><cell>67.7</cell><cell>67.0</cell></row><row><cell>512</cell><cell>62.0</cell><cell>62.1</cell><cell>67.4</cell><cell>67.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Classification metric (mAP) using different solutions to cope with an imbalanced dataset on our pooling methods, using ResNet-152 features.</figDesc><table><row><cell>Imbalance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Details on the events split between Training, Validation and Testing sets.</figDesc><table><row><cell></cell><cell></cell><cell>Events</cell><cell></cell><cell></cell></row><row><cell>Split</cell><cell cols="3">Goals Cards Subs</cell><cell>Total</cell></row><row><cell>Train</cell><cell>961</cell><cell cols="2">1296 1708</cell><cell>3965</cell></row><row><cell>Valid</cell><cell>356</cell><cell>396</cell><cell>562</cell><cell>1314</cell></row><row><cell>Test</cell><cell>326</cell><cell>453</cell><cell>579</cell><cell>1358</cell></row><row><cell>Total</cell><cell>1643</cell><cell cols="2">2145 2849</cell><cell>6637</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We choose www.flashscore.info to get our annotations since they provide a wide number of summaries and have a consistent format across their match reports.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>ManÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 6</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating video search, video event detection, localization, and hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>QuÃ©not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trecvid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID</title>
		<meeting>TRECVID</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action classification in soccer videos with long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="154" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging contextual cues for generating basketball highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="908" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Endto-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07750</idno>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global sports market -total revenue from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/370560/worldwide-sports-market-revenue/.1" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>in billion u.s. dollars</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Market size of the european football market from 2006/07 to 2015/16 (in billion euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261223/european-soccer-market-total-revenue/.1" />
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
	<note>In Statista -The Statistics Portal</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revenue of the biggest (big five*) european soccer leagues from 1996/97 to 2017/18 (in million euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261218/big-five-european-soccer-leagues-revenue/.1" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revenue of the top european soccer leagues (big five*) from 2006/07 to 2017/18 (in billion euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261225/top-european-soccer-leagues-big-five-revenue/.1" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Retrieved October 30</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Top-20 european football clubs breakdown of revenues 2015/16 season (in million euros)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/271636/revenue-distribution-of-top-20-european-soccer-clubs/.1" />
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
	<note>In Statista -The Statistics Portal</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A review of vision-based systems for soccer video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dorazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2911" to="2926" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic soccer video analysis and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="796" to="807" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What will happen next? forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Action-VLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02895</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08421</idno>
		<title level="m">A video dataset of spatio-temporally localized atomic visual actions</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sports field localization via deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic analysis of soccer video using dynamic bayesian network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic soccer video event detection based on a deep neural network combined cnn and rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="490" to="494" />
		</imprint>
	</monogr>
	<note>IEEE 28th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realtime event detection in field sport videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kapela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swietlicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Oconnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision in Sports</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rnn fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="833" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video fill in the blank using lr/rl lstms with spatial-temporal attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on ICMR</title>
		<meeting>the 5th ACM on ICMR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stap: Spatial-temporal attentionaware pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal attention-gated model for robust sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fisher vectors meet neural networks: A hybrid classification architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3743" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reely</surname></persName>
		</author>
		<ptr target="http://www.reely.ai.1" />
		<title level="m">Reely</title>
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Football video segmentation based on video production strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="433" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Action mach a spatiotemporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Amtnet: Action-micro-tube regression by end-to-end trainable deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">What actions are needed for understanding human actions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1470</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Evaluation campaigns and trecvid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th ACM international workshop on Multimedia information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sportvu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sportvu</surname></persName>
		</author>
		<ptr target="https://www.stats.com/sportvu-football.1" />
		<title level="m">SportVU</title>
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Creating havic: Heterogeneous audio visual internet collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caruso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antonishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep fisher kernelsend to end learning of the fisher kernel gmm parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sydorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1402" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Event detection and summarization in soccer videos using bayesian network and copula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tavassolipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karimian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="291" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Goal!! event detection in sports video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsakalides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="15" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Soccer video event annotation by synchronization of attack-defense clips and match reports with coarsegrained time information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1104" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wyscout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wyscout</surname></persName>
		</author>
		<ptr target="http://www.wyscout.com.1" />
		<title level="m">WyScout</title>
		<imprint>
			<date type="published" when="2017-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Eventnet: A large scale structured concept library for complex event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd ACM international conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="471" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1302" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
