<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Ensemble for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-31">31 Aug 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
							<email>wonsik16.kim@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="institution">Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
							<email>bhavya.goyal@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="institution">Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
							<email>kunal.chawla@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="institution">Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="institution">Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
							<email>keunjoo.kwon@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="institution">Samsung Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Ensemble for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-31">31 Aug 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>attention</term>
					<term>ensemble</term>
					<term>deep metric learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep metric learning aims to learn an embedding function, modeled as deep neural network. This embedding function usually puts semantically similar images close while dissimilar images far from each other in the learned embedding space. Recently, ensemble has been applied to deep metric learning to yield state-of-the-art results. As one important aspect of ensemble, the learners should be diverse in their feature embeddings. To this end, we propose an attention-based ensemble, which uses multiple attention masks, so that each learner can attend to different parts of the object. We also propose a divergence loss, which encourages diversity among the learners. The proposed method is applied to the standard benchmarks of deep metric learning and experimental results show that it outperforms the state-of-the-art methods by a significant margin on image retrieval tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep metric learning has been actively researched recently. In deep metric learning, feature embedding function is modeled as a deep neural network. This feature embedding function embeds input images into feature embedding space with a certain desired condition. In this condition, the feature embeddings of similar images are required to be close to each other while those of dissimilar images are required to be far from each other. To satisfy this condition, many loss functions based on the distances between embeddings have been proposed <ref type="bibr">[3, 4, 6, 14, 25, 27-29, 33, 37]</ref>. Deep metric learning has been successfully applied in image retrieval task on popular benchmarks such as CARS-196 <ref type="bibr" target="#b13">[13]</ref>, CUB-200-2011 <ref type="bibr" target="#b35">[35]</ref>, Stanford online products <ref type="bibr" target="#b29">[29]</ref>, and in-shop clothes retrieval <ref type="bibr" target="#b18">[18]</ref> datasets.</p><p>Ensemble is a widely used technique of training multiple learners to get a combined model, which performs better than individual models. For deep metric learning, ensemble concatenates the feature embeddings learned by multiple learners which often leads to better embedding space under given constraints on the distances between image pairs. The keys to success in ensemble are high performance of individual learners as well as diversity among learners. To achieve this objective, different methods have been proposed <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b39">39]</ref>. However, there has not been much research on optimal architecture to yield diversity of feature embeddings in deep metric learning. output1 output2 output3 Our contribution is to propose a novel framework to encourage diversity in feature embeddings. To this end, we design an architecture which has multiple attention modules for multiple learners. By attending to different locations for different learners, diverse feature embedding functions are trained. They are regularized with divergence loss which aims to differentiate the feature embeddings from different learners. Equipped with it, we present M -way attention-based ensemble (ABE-M ) which learns feature embedding with M diverse attention masks. The proposed architecture is represented in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. We compare our model to our M -heads ensemble baseline <ref type="bibr" target="#b16">[16]</ref>, in which different feature embedding functions are trained for different learners ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), and experimentally demonstrate that the proposed ABE-M shows significantly better results with less number of parameters.</p><formula xml:id="formula_0">G 1 G 2 G 3 S input (a) 3-heads ensemble G G output1 output2 A 3 G output3 S input A 2 A 1 shared parameters (b) Attention-based ensemble (ABE-3)</formula><p>residual networks behave like ensembles of relatively shallow networks. Recently the ensemble technique has been applied in deep metric learning as well. Yuan et al . <ref type="bibr" target="#b39">[39]</ref> propose to ensemble a set of models with different complexities in cascaded manner. They train deeply supervised cascaded networks using easier examples through earlier layers of the networks while harder examples are further exploited in later layers. Opitz et al . <ref type="bibr" target="#b22">[22]</ref> use online gradient boosting to train each learner in ensemble. They try to reduce correlation among learners using re-weighting of training samples. Opitz et al . <ref type="bibr" target="#b21">[21]</ref> propose an efficient averaging strategy with a novel DivLoss which encourages diversity of individual learners. Attention mechanism Attention mechanism has been used in various computer vision problems. Earlier researches utilize RNN architectures for attention modeling <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b26">26]</ref>. These RNN based attention models solve classification tasks using object parts detection by sequentially selecting attention regions from images and then learning feature representations for each part. Besides RNN approaches, Liu et al . <ref type="bibr" target="#b17">[17]</ref> propose fully convolutional attention networks, which adopts hard attention from a region generator. And Zhao et al . <ref type="bibr" target="#b40">[40]</ref> propose diversified visual attention networks, which uses different scaling or cropping of input images for different attention masks. However, our ABE-M is able to learn diverse attention masks without relying on a region generator. In addition, ABE-M uses soft attention, therefore, the parameter update is straightforward by backpropagation in a fully gradient-based way while previous approaches in <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b40">40]</ref> use hard attention which requires policy gradient estimation.</p><p>Jaderberg et al . <ref type="bibr" target="#b11">[11]</ref> propose spatial transformer networks which models attention mechanism using parameterized image transformations. Unlike aforementioned approaches, their model is differentiable and thus can be trained in a fully gradient-based way. However, their attention is limited to a set of predefined and parameterized transformations which could not yield arbitrary attention masks.</p><p>3 Attention-based ensemble</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep metric learning</head><p>Let f : X → Y be an isometric embedding function between metric spaces X and Y where X is a N X dimensional metric space with an unknown metric function d X and Y is a N Y dimensional metric space with a known metric function d Y . For example, Y could be a Euclidean space with Euclidean distance or the unit sphere in a Euclidean space with angular distance.</p><p>Our goal is to approximate f with a deep neural network from a dataset D = {(x (1) , x <ref type="bibr" target="#b2">(2)</ref> , d X (x (1) , x (2) ))|x <ref type="bibr" target="#b1">(1)</ref> , x (2) ∈ X } which are samples from X . In case we cannot get the samples of metric d X , we consider the label information from the dataset with labels as the relative constraint of the metric d X . For example, from a dataset D C = {(x, c)|x ∈ X , c ∈ C} where C is the set of labels, for (x i , c i ), (x j , c j ) ∈ D C the contrastive metric constraint could be defined as the following:</p><formula xml:id="formula_1">d X (x i , x j ) = 0, if c i = c j ; d X (x i , x j ) &gt; m c , if c i = c j ,<label>(1)</label></formula><p>where m c is an arbitrary margin. The triplet metric constraint for (x i , c i ), (x j , c j ), (x k , c k ) ∈ D C could be defined as the following:</p><formula xml:id="formula_2">d X (x i , x j ) + m t &lt; d X (x i , x k ), c i = c j and c i = c k ,<label>(2)</label></formula><p>where m t is a margin. Note that these metric constraints are some choices of how to model d X , not those of how to model f .</p><p>An embedding function f is isometric or distance preserving embedding if for every</p><formula xml:id="formula_3">x i , x j ∈ X one has d X (x i , x j ) = d Y (f (x i ), f (x j )</formula><p>). In order to have an isometric embedding function f , we optimize f so that the points embedded into Y produce exactly the same metric or obey the same metric constraint of d X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ensemble for deep metric learning</head><p>A classical ensemble for deep metric learning could be the method to average the metric of multiple embedding functions. We define the ensemble metric function d ensemble for deep metric learning as the following:</p><formula xml:id="formula_4">d ensemble,(f1,...,fM ) (x i , x j ) = 1 M M m=1 d Y (f m (x i ), f m (x j )),<label>(3)</label></formula><p>where f m is an independently trained embedding function and we call it a learner.</p><p>In addition to the classical ensemble, we can consider the ensemble of two-step embedding function. Consider a function s : X → Z which is an isometric embedding function between metric spaces X and Z where X is a N X dimensional metric space with an unknown metric function d X and Z is a N Z dimensional metric space with an unknown metric function d Z . And we consider the isometric embedding g : Z → Y where Y is a N Y dimensional metric space with a known metric function d Y . If we combine them into one function b(x) = g(s(x)), x ∈ X , the combined function is also an isometric embedding b : X → Y between metric spaces X and Y.</p><p>Like the parameter sharing ensemble <ref type="bibr" target="#b16">[16]</ref>, with the independently trained multiple g m and a single s, we can get multiple embedding functions b m : X → Y as the following: b m (x) = g m (s(x)). (4) We are interested in another case where there are multiple embedding functions b m : X → Y with multiple s m and a single g as the following: b m (x) = g(s m (x)).</p><p>Note that a point in X can be embedded into multiple points in Y by multiple learners. In Eq. (5), s m does not have to preserve the label information while it only has to preserve the metric. In other words, a point with a label could be mapped to multiple locations in Z by multiple s m and finally would be mapped to multiple locations in Y. If this were the ensemble of classification models where g approximates the distribution of the labels, all s m should be label preserving functions because the outputs of s m become the inputs of one classification model g.</p><p>For the embedding function of Eq. (5), we want to make s m attends to the diverse aspects of data x in X while maintaining a single embedding function g which disentangles the complex manifold Z into Euclidean space. By exploiting the fact that a point x in X can be mapped to multiple locations in Y, we can encourage each s m to map x into distinctive points z m in Z. Given an isometric embedding g : Z → Y , if we enforce y m in Y mapped from x to be far from each other, z m in Z mapped from x will be far from each other as well. Note that we cannot apply this divergence constraint to z m because metric d z in Z is unknown. We train each b m to be isometric function between X and Y while applying the divergence constraint among y m in Y. If we apply the divergence constraint to classical ensemble models or multihead ensemble models, they do not necessarily induce the diversity because each f m or g m could arbitrarily compose different metric spaces in Y (Refer to experimental results in Sec. 6.2). With the attention-based ensemble, union of metric spaces by multiple s m is mapped by a single embedding function g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention-based ensemble model</head><p>As one implementation of Eq.(5), we propose the attention-based ensemble model which is mainly composed of two parts: feature extraction module F (x) and attention module A(x). For the feature extraction, we assume a general multi-layer perceptron model as the following:</p><formula xml:id="formula_6">F (x) = h l (h l−1 (· · · (h 2 (h 1 (x))))<label>(6)</label></formula><p>We break it into two parts with a branching point at i, S(·) includes h l , h l−1 , . . . , h i+1 , and G(·) includes h i , h i−1 , . . . , h 1 . We call S(·) a spatial feature extractor and G(·) a global feature embedding function with respect to the output of each function. For attention module, we also assume a general multi-layer perceptron model which outputs a three dimensional blob with channel, width, and height as an attention mask. Each element in the attention masks is assumed to have a value from 0 to 1. Given aforementioned two modules, the combined embedding function B m (x) for the learner m is defined as the following:</p><formula xml:id="formula_7">B m (x) = G(S(x) • A m (S(x))),<label>(7)</label></formula><p>where • denotes element-wise product ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Note that, same feature extraction module is shared across different learners while individual learners have their own attention module A m (·). The attention function A m (S(x)) outputs an attention mask with same size as output of S(x).</p><p>This attention mask is applied to the output feature of S(x) with an elementwise product. Attended feature output of S(x) • A m (S(x)) is then fed into global feature embedding function G(·) to generate an embedding feature vector. If all the elements in the attention mask are 1, the model B m (x) is reduced to a conventional multi-layer perceptron model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss</head><p>The loss for training aforementioned attention model is defined as:</p><formula xml:id="formula_8">L({(x i , c i )}) = m L metric,(m) ({(x i , c i )}) + λ div L div ({x i }),<label>(8)</label></formula><p>where {(x i , c i )} is a set of all training samples and labels, L metric,(m) (·) is the loss for the isometric embedding for the m-th learner, L div (·) is regularizing term for diversifying the feature embedding of each learner B m (x) and λ div is the weighting parameter to control the strength of the regularizer. More specifically, divergence loss L div is defined as the following:</p><formula xml:id="formula_9">L div ({x i }) = i p,q max(0, m div − d Y (B p (x i ), B q (x i )) 2 ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">{x i } is set of all training samples, d Y is the metric in Y and m div is a margin. A pair (B p (x i ), B q (x i ))</formula><p>represents feature embeddings of a single image embedded by two different learners. We call it self pair from now on while positive and negative pairs refer to pairs of feature embeddings with same labels and different labels, respectively. The divergence loss encourages each learner to attend to the different part of the input image by increasing the distance between the points embedded by the input image ( <ref type="figure" target="#fig_1">Fig. 2</ref>). Since the learners share the same functional module to extract features, the only differentiating part is the attention module. Note that our proposed loss is not directly applied to the attention masks. In other words, the attention masks among the learners may overlap. And also it is possible to have the attention masks some of which focus on small region while other focus on larger region including small one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We perform all our experiments using GoogLeNet <ref type="bibr" target="#b32">[32]</ref> as the base architecture. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we use the output of max pooling layer following the inception(3b) block as our spatial feature extractor S(·) and remaining network as our global feature embedding function G(·). In our implementation, we simplify attention module A m (·) as A ′ m (C(·)) where C(·) consists of inception(4a) to inception(4e) from GoogLeNet, which is shared among all M learners and A ′ m (·) consists of a convolution layer of 480 kernels of size 1×1 to match the output of S(·) for the element-wise product. This is for efficiency in terms of memory and computation time. Since C(·) is shared across different learners, forward and backward propagation time, memory usage, and number of parameters are decreased compared to having separate A m (·) for each learner (without any shared part). Our preliminary experiments showed no performance drop with this choice of implementation.</p><formula xml:id="formula_11">Spatial Feature Extractor S(·) input image 224x224x3 M Attention masks 14x14x480 ... ... Global Feature Embedding Function G(·)</formula><p>... We study the effects of different branching points and depth of attention module in Sec. 6.3. We use contrastive loss <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6]</ref> as our distance metric loss function which is defined as the following:</p><formula xml:id="formula_12">L metric,(m) ({(x i , c i )}) = 1 N i,j (1 − y i,j )[m c − D 2 m,i,j ] + + y i,j D 2 m,i,j , D m,i,j = d Y (B m (x i ), B m (x j )),<label>(10)</label></formula><p>where {(x i , c i )} is set of all training samples and corresponding labels, N is the number of training sets, y i,j is a binary indicator of whether or not the label c i is equal to c j , d Y is the euclidean distance, [·] + denotes the hinge function max(0, ·) and m c is the margin for contrastive loss. Both of margins m c and m div (in Eq. 8) is set to 1.</p><p>We implement the proposed ABE-M method using caffe <ref type="bibr" target="#b12">[12]</ref> framework. During training, the network is initialized from a pre-trained network on ImageNet ILSVRC dataset <ref type="bibr" target="#b24">[24]</ref>. The final layer of the network and the convolution layer of attention module are randomly initialized as proposed by Glorot et al . <ref type="bibr" target="#b5">[5]</ref>. For optimizer, we use stochastic gradient descent with momentum optimizer with momentum as 0.9, and we select the base learning rate by tuning on validation set of the dataset.</p><p>We follow earlier works <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b38">38]</ref> for preprocessing and unless stated otherwise, we use the input image size of 224×224. All training and testing images are scaled such that their longer side is 256, keeping the aspect ratio fixed, and padding the shorter side to get 256×256 images. During training, we randomly crop images to 224×224 and then randomly flip horizontally. During testing, we use the center crop. We subtract the channel-wise mean of ImageNet dataset from the images. For training and testing images of cropped datasets, we follow the approach in <ref type="bibr" target="#b38">[38]</ref>. For CARS-196 <ref type="bibr" target="#b13">[13]</ref> cropped dataset, 256×256 scaled cropped images are used; while for CUB-200-2011 <ref type="bibr" target="#b35">[35]</ref> cropped dataset, 256×256 scaled cropped images with fixed aspect ratio and shorter side padded are used.</p><p>We run our experiments on nVidia Tesla M40 GPU (24GBs GPU memory), which limits our batch size to 64 for ABE-8 model. Unless stated otherwise, we use the batch size of 64 for our experiments. We sample our mini-batches by first randomly sampling 32 images and then positive pairs for first 16 images and negative pairs for next 16 images, thus making the mini-batch of size 64. Unless mentioned otherwise, we report the results of our method using embedding size of 512. This makes the embedding size for individual learners to be 512/M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We use all commonly used image retrieval task datasets for our experiments and Recall@K metric for our evaluation. During testing, we compute the feature embeddings for all the test images from our network. For every test image, we then retrieve top K similar images from the test set excluding test image itself.</p><p>Recall score for that test image is 1 if at least one image out of K retrieved images has the same label as the test image. We compute the average over whole test set to get Recall@K. We evaluate the model after every 1000 iteration and report the results for the iteration with highest Recall@1.</p><p>We show the effectiveness of the proposed ABE-M method on all the datasets commonly used in image retrieval tasks. We follow same train-test split as <ref type="bibr" target="#b29">[29]</ref> for fair comparison with other works.</p><p>-CARS-196 <ref type="bibr" target="#b13">[13]</ref>  Since CARS-196 and CUB-200-2011 datasets consist of bounding boxes too, we report the results using original images and cropped images both for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison of ABE-M with M -heads</head><p>To show the effectiveness of our ABE-M method, we first compare the performance of ABE-M and M -heads ensemble ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) with varying ensemble embedding sizes (denoted with superscript) on CARS-196 dataset. As show in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_3">Fig. 4</ref>, our method outperforms M -heads ensemble by a significant margin. The number of model parameters for ABE-M is much less compared to ABE-1 contains only one attention module and hence is not an ensemble and does not use divergence loss. ABE-1 performs similar to 1-head. We also report the performance of individual learners of the ensemble. From <ref type="table" target="#tab_1">Table 1</ref>, we can see that the performance of ABE-M 512 ensemble is increasing with increasing M . The performance of individual learners is also increasing with increasing M despite the decrease in embedding size of individual learners (512/M ). The same increase is not seen for the case of M -heads. Further, we can refer to ABE-1 64 , ABE-2 128 , ABE-4 256 and ABE-8 512 , where all individual learners have embedding size 64. We can see a clear increase in recall of individual learners with increasing values of M .   and show the results in <ref type="table" target="#tab_2">Table 2</ref>. As we can see, ABE-M without divergence loss performs similar to its individual learners whereas there is significant gain in ensemble performance of ABE-M compared to its individual learners.</p><p>We also calculate the cosine similarity between positive, negative, and self pairs, and plot in <ref type="figure">Fig. 5</ref>. With divergence loss ( <ref type="figure">Fig. 5(a)</ref>), all learners learn diverse embedding function which leads to decrease in cosine similarity of self pairs. Without divergence loss ( <ref type="figure">Fig. 5(b)</ref>), all learners converge to very similar embedding function so that the cosine similarity of self pairs is close to 1. This could be because all learners end up learning similar attention masks which leads to similar embeddings for all of them.</p><p>We visualize the learned attention masks of ABE-8 on CARS-196 in <ref type="figure" target="#fig_5">Fig. 6</ref>. Due to the space limitation, results from only three learners out of eight and three channels out of 480 are illustrated. The figure shows that different learners are attending to different parts for the same channel. Qualitatively, our proposed loss successfully diversify the attention masks produced by different learners. They are attending to different parts of the car such as upper part, bottom part, roof, tires, lights and so on. In 350th channel, for instance, learner 1 is focusing on bottom part of car, learner 2 on roof and learner 3 on upper part including roof. At the bottom of <ref type="figure" target="#fig_5">Fig. 6</ref>, the mean of the attention masks across all channels shows that the learned embedding function focuses more on object areas than the background.  <ref type="table" target="#tab_3">Table 3</ref>. We can see that the divergence loss does not improve the performance in 8-heads. From <ref type="figure">Fig. 5(c)</ref>, we can notice that cosine similarities of self pairs are close to zero for M -heads. <ref type="figure">Fig. 5(d)</ref> shows that the divergence loss does not affect the cosine similarity of self pairs significantly. As mentioned in Sec. 3.2, we hypothesize this is because each of G m (·) could arbitrarily compose different metric spaces in Y.  Sensitivity to depth of attention module We demonstrate the effect of depth of attention module by changing the number of inception blocks in it. To make sure that we can take the element wise product of the attention mask with the input of attention module, the dimension of attention mask should match the input dimension of attention module. Because of this we remove all the pooling layers in our attention module. <ref type="figure" target="#fig_6">Fig. 7(a)</ref> shows Recall@1 with varying number of inception blocks in attention module starting from 1 (inception(4a)) to 7 (inception(4a) to inception(5b)) in GoogLeNet. We can see that the attention module with 5 inception blocks (inception(4a) to inception(4e)) performs the best.</p><p>Sensitivity to branching point of attention module The branching point of the attention module is where we split the network between spatial feature extractor S(·) and global feature embedding function G(·). To analyze the choice of branching point of the attention module, we keep the number of inception blocks in attention module same (i.e. 5) and change branching points from pool2 to inception(4b). From <ref type="figure" target="#fig_6">Fig. 7(b)</ref>, we see that pool3 performs the best with our architecture. We carry out this experiment with batch size 40 for all the branching points. For ABE-M model, the memory requirement for the G(·) is M times compared to the individual learner. Since early branching point increases the depth of G(·) while decreasing the depth for S(·), it would consequently increase the memory requirement of the whole network. Due to the memory constraints of GPU, we started the experiments from branching points pool2 and adjusted the batch size.</p><p>Sensitivity to λ div <ref type="figure" target="#fig_6">Fig. 7(c)</ref> shows the effect of λ div on Recall@K for ABE-M model. We can see that λ div = 1 performs the best and lower values degrades the performance quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison with state of the art</head><p>We compare the results of our approach with current state-of-the-art techniques. Our model performs the best on all the major benchmarks for image retrieval. <ref type="table" target="#tab_4">Table 4</ref>, 6 and 7 compare the results with previous methods such as Lifted-Struct <ref type="bibr" target="#b29">[29]</ref>, HDC <ref type="bibr" target="#b39">[39]</ref>, Margin † <ref type="bibr" target="#b38">[38]</ref>, BIER <ref type="bibr" target="#b22">[22]</ref>, and A-BIER <ref type="bibr" target="#b22">[22]</ref> on CARS-196 <ref type="bibr" target="#b13">[13]</ref>, CUB-200-2011 <ref type="bibr" target="#b35">[35]</ref>, SOP <ref type="bibr" target="#b29">[29]</ref>, and in-shop clothes retrieval <ref type="bibr" target="#b18">[18]</ref> datasets. Results on the cropped datasets are listed in <ref type="table">Table 5</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present a new framework for ensemble in the domain of deep metric learning. It uses attention-based architecture that attends to parts of the image. We use multiple such attention-based learners for our ensemble. Since ensemble benefits from diverse learners, we further introduce a divergence loss to diversify the feature embeddings learned by each learner. The divergence loss encourages that the attended parts of the image for each learner are different. Experimental results demonstrate that the divergence loss not only increases the performance of ensemble but also increases each individual learners' performance compared to the baseline. We demonstrate that our method outperforms the current state-of-the-art techniques by significant margin on several image retrieval benchmarks including CARS-196 <ref type="bibr" target="#b13">[13]</ref>, CUB-200-2011 <ref type="bibr" target="#b35">[35]</ref>, SOP <ref type="bibr" target="#b29">[29]</ref>, and in-shop clothes retrieval <ref type="bibr" target="#b18">[18]</ref>   In this supplementary material, we consider two additional architecutres for the ablation study: M -heads+att and M -tails. The details of each architectures are following. All experiments in this supplementary material are done on CARS-196 dataset. The models are trained with ensemble embedding size 512 and batch size 64, unless otherwise specified.</p><formula xml:id="formula_13">G 1 G 2 output1 output2 A 3 G 3 output3 S input A 2 A 1 (a) 3-heads+att output1 output2 output3 G G G S 2 input S 3 S 1 shared parameters (b) 3-tails</formula><p>As described in Sec. 3 of the main manuscript, S(·) is the spatial feature extractor, G(·) is the global feature embedding function, and A(·) is the attention module. Our combined embedding function B m (x) for the learner m in ABE-M is defined as the following:</p><formula xml:id="formula_14">B m (x) = G(S(x) • A m (S(x))),<label>(1)</label></formula><p>where • denotes element-wise product.</p><p>-M -heads+att It refers to the M -heads with attention model, which is the ensemble of G m (S(x)•A m (S(x)) ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Here, unlike our ABE-M model, the global feature embedding function is not shared and the model is trained without divergence loss. -M -tails Instead of having multiple heads and a shared tail, we can consider multiple tails and a shared head, which is G(S m (x)) ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Unlike ABE-M , there are no attention modules, and the spatial feature extractor is not shared, but the global feature embedding function is. It is also trained with divergence loss. Due to the memory constraint, this model is trained with a batch size of 32.  <ref type="table" target="#tab_2">Table 2</ref> summarizes the results. In addition, it also presents the results of 8-heads (G m (S(x))) and ABE-8 for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of attention module on M -heads</head><p>To study the contribution of attention module on performance, we propose M -heads with attention model (M -heads+att). This model has multiple heads G m (·) and shared spatial feature extractor S(·), like M -heads model. In addition we attach M different attention models for M different learners, similar to our proposed ABE-M . There are two differences between M -heads+att and ABE-M . Firstly, the global embedding function G(·) is not shared. Second, divergence loss is not applied during training. As discussed in the main manuscript, divergence loss is not required when the global embedding function G m (·) is different for each learner.</p><p>The results show that with attention module, Recall@1 is improved from 76.1% to 79.7% compared to 8-heads. This comparison shows the effect of attention module independent from other factors. Along with the attention module, by sharing the global embedding function G(·) and applying divergence loss, ABE-8 further improves Recall@1 to 85.2%.</p><p>Comparison of M -heads and M -tails The main manuscript considers two different ways of ensembles in regards to two-step embedding function. In twostep embedding function, input is first mapped to an intermediate metric space by S(·) and then mapped to the final embedding space by G(·). The first way of ensemble shares S(·) while having different G m (·) for each learner. The second way shares G(·) while having different S m (·) for each learner. To investigate the effect of two different ways of ensembles on performance, we propose M -tails model as depicted in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>.</p><p>The results demonstrate 8-tails achieves better Recall@1 compared to 8heads. The performance gain from this architecture (+5.0%) is even larger than the case of 8-heads+att (+3.6%). Compared to 8-tails, ABE-8 further improves the performance by using attention module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Difference between M -heads ensemble and attention-based ensemble. Both assume shared parameters for bottom layers (S). (a) In M -heads ensemble, different feature embedding functions are trained for different learners (G1, G2, G3). (b) In attention-based ensemble, single feature embedding function (G) is trained while each learner learns different attention modules (A1, A2, A3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of feature embedding space and divergence loss. Different car brands are represented as different colors: red, green and blue. Feature embeddings of each learner are depicted as a square with different mask patterns. Divergence loss pulls apart the feature embeddings of different learners using same input</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The implementation of attention-based ensemble (ABE-M ) using GoogLeNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Recall@1 comparison with baseline on CARS-196 as a function of (a) number of parameters and (b) flops. Both of ABE-M and M -heads has embedding size of 512 M -heads ensemble as the global feature extractor G(·) is shared among learners. But, ABE-M requires higher flops because of extra computation of attention modules. This difference becomes increasingly insignificant with increasing values of M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6. 2 Fig. 5 .</head><label>25</label><figDesc>Effects of divergence loss ABE-M without divergence loss To analyze the effectiveness of divergence loss in ABE-M , we conduct experiments without divergence loss on CARS-196 Histograms of cosine similarity of positive (blue), negative (red), self (green) pairs trained with different methods. Self pair refers to the pair of feature embeddings from different learners using same image. (a) Attention-based ensemble (ABE-8) using proposed loss, (b) attention-based ensemble (ABE-8) without divergence loss, (c) 8-heads ensemble, (d) 8-heads ensemble with divergence loss.In the case of attentionbased ensemble, divergence loss is necessary for each learner to be trained to produce different features by attending to different locations. Without divergence loss, one can see all learners learn very similar embedding. Meanwhile, in the case of M -heads ensemble, there is no effect of applying divergence loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>with 27th channel of attention masks mean across channels of attention masks masked input images with 118th channel of attention masks masked input images with 350th channel of attention masks input images The attention masks learned by each learner of ABE-8 on CARS-196. Due to the space limitation, results from only three learners out of eight and three channels out of 480 are illustrated. Each column shows the result of different input images. Different learners attend to different parts of the car such as upper part, bottom part, roof, tires, lights and so on Divergence loss in M -heads We show the result of experiments of 8-heads ensemble with divergence loss in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Recall@1 while varying hyperparameters and architectures: (a) number of inception blocks used for attention module A k (·), (b) branching point of attention module, and (c) weight λ div . Here, inception(3a) is abbreviated as in(3a) at a time and keeping others fixed. (More ablation study can be found in the supplementary material.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of additional architectures reported in this supplementary material. (a) 3-heads+att (3-heads with attention module) and (b) 3-tails (ensemble of G(Sm(x)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Recall@K(%) comparison with baseline on CARS-196. Superscript denotes ensemble embedding size</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ensemble</cell><cell></cell><cell></cell><cell cols="2">Individual Learners</cell><cell></cell><cell>params</cell><cell>flops</cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>(×10 7 )</cell><cell>(×10 9 )</cell></row><row><cell>1-head 512</cell><cell cols="4">67.2 77.4 85.3 90.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.65</cell><cell>1.58</cell></row><row><cell cols="5">2-heads 512 73.3 82.5 88.6 93.0</cell><cell cols="4">70.2±.03 79.8±.52 86.7±.01 91.9±.37</cell><cell>1.18</cell><cell>2.25</cell></row><row><cell cols="5">4-heads 512 76.6 84.2 89.3 93.2</cell><cell cols="4">70.4±.80 79.9±.38 86.5±.43 91.4±.42</cell><cell>2.24</cell><cell>3.60</cell></row><row><cell cols="5">8-heads 512 76.1 84.3 90.3 93.9</cell><cell cols="4">68.3±.39 78.5±.39 86.0±.37 91.3±.31</cell><cell>4.36</cell><cell>6.28</cell></row><row><cell>ABE-1 512</cell><cell cols="4">67.3 77.3 85.3 90.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.97</cell><cell>2.21</cell></row><row><cell>ABE-2 512</cell><cell cols="4">76.8 84.9 90.2 94.0</cell><cell cols="4">70.9±.58 80.3±.04 87.1±.07 92.2±.20</cell><cell>0.98</cell><cell>2.96</cell></row><row><cell>ABE-4 512</cell><cell cols="4">82.5 89.1 93.0 95.5</cell><cell cols="4">74.4±.51 83.1±.47 89.1±.34 93.2±.36</cell><cell>1.05</cell><cell>4.46</cell></row><row><cell>ABE-8 512</cell><cell cols="4">85.2 90.5 93.9 96.1</cell><cell cols="4">75.0±.39 83.4±.24 89.2±.31 93.2±.24</cell><cell>1.20</cell><cell>7.46</cell></row><row><cell>ABE-1 64</cell><cell cols="4">65.9 76.5 83.7 89.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.92</cell><cell>2.21</cell></row><row><cell>ABE-2 128</cell><cell cols="4">75.5 84.0 89.4 93.6</cell><cell cols="4">68.6±.38 78.8±.38 85.7±.43 91.3±.16</cell><cell>0.96</cell><cell>2.96</cell></row><row><cell>ABE-4 256</cell><cell cols="4">81.8 88.5 92.4 95.1</cell><cell cols="4">72.3±.68 81.4±.45 87.9±.23 92.3±.13</cell><cell>1.04</cell><cell>4.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Recall@K(%) comparison in ABE-M ensemble without divergence loss L div on CARS-196 0±0.39 83.4±0.24 89.2±0.31 93.2±0.24 ABE-8 512 without L div 69.7 78.8 86.2 91.5 69.5±0.11 78.8±0.14 86.1±0.15 91.5±0.09</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ensemble</cell><cell></cell><cell></cell><cell cols="2">Individual Learners</cell><cell></cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>ABE-8 512</cell><cell cols="4">85.2 90.5 93.9 96.1</cell><cell>75.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Recall@K(%) comparison in M -heads ensemble with divergence loss L div on CARS-196 To analyze the importance of various aspects of our model, we performed experiments on CARS-196 dataset of ABE-8 model, varying a few hyperparameters</figDesc><table><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>8-heads</cell><cell cols="4">76.1 84.3 90.3 93.9</cell></row><row><cell cols="5">8-heads with L div 76.0 84.6 89.7 93.5</cell></row><row><cell>6.3 Ablation study</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Recall@K(%) score on CUB-200-2011 and CARS-196</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell><cell cols="2">CARS-196</cell><cell></cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Recall@K(%) score on Stanford online products dataset (SOP)</figDesc><table><row><cell>K</cell><cell>1</cell><cell>10</cell><cell cols="2">100 1000</cell></row><row><cell>Contrastive 128 [29]</cell><cell cols="4">42.0 58.2 73.8 89.1</cell></row><row><cell cols="5">LiftedStruct 512 [29] 62.1 79.8 91.3 97.4</cell></row><row><cell>N-Pairs 512 [27]</cell><cell cols="4">67.7 83.8 93.0 97.8</cell></row><row><cell>Clustering 64 [28]</cell><cell cols="3">67.0 83.7 93.2</cell><cell>-</cell></row><row><cell cols="2">Proxy NCA † 64 [20] 73.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Margin † 128 [38]</cell><cell cols="4">72.7 86.2 93.8 98.0</cell></row><row><cell>HDC 384 [39]</cell><cell cols="4">69.5 84.4 92.8 97.7</cell></row><row><cell>A-Bier 512 [23]</cell><cell cols="4">74.2 86.9 94.0 97.8</cell></row><row><cell>ABE-2 512</cell><cell cols="4">75.4 88.0 94.7 98.2</cell></row><row><cell>ABE-4 512</cell><cell cols="4">75.9 88.3 94.8 98.2</cell></row><row><cell>ABE-8 512</cell><cell cols="4">76.3 88.4 94.8 98.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Recall@K(%) score on in-shop clothes retrieval dataset</figDesc><table><row><cell>K</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>FasionNet+Joints 4096 [18]</cell><cell cols="6">41.0 64.0 68.0 71.0 73.0 73.5</cell></row><row><cell cols="7">FasionNet+Poselets 4096 [18] 42.0 65.0 70.0 72.0 72.0 75.0</cell></row><row><cell>FasionNet 4096 [18]</cell><cell cols="6">53.0 73.0 76.0 77.0 79.0 80.0</cell></row><row><cell>HDC 384 [39]</cell><cell cols="6">62.1 84.9 89.0 91.2 92.3 93.1</cell></row><row><cell>A-BIER 512 [23]</cell><cell cols="6">83.1 95.1 96.9 97.5 97.8 98.0</cell></row><row><cell>ABE-2 512</cell><cell cols="6">85.2 96.0 97.2 97.8 98.2 98.4</cell></row><row><cell>ABE-4 512</cell><cell cols="6">86.7 96.4 97.6 98.0 98.4 98.6</cell></row><row><cell>ABE-8 512</cell><cell cols="6">87.3 96.7 97.9 98.2 98.5 98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 .</head><label>1</label><figDesc>Recall@K(%) comparison on CARS-196. All presented methods use ensemble embedding size of 512</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ensemble</cell><cell></cell><cell></cell><cell cols="2">Individual Learners</cell><cell></cell><cell>params</cell><cell>flops</cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>(×10 7 )</cell><cell>(×10 9 )</cell></row><row><cell>8-heads</cell><cell cols="4">76.1 84.3 90.3 93.9</cell><cell cols="4">68.3±.39 78.5±.39 86.0±.37 91.3±.31</cell><cell>4.36</cell><cell>6.28</cell></row><row><cell cols="5">8-heads+att 79.7 87.0 91.6 94.8</cell><cell cols="4">69.0±.36 78.9±.27 86.2±.30 91.4±.21</cell><cell>4.96</cell><cell>7.46</cell></row><row><cell>8-tails</cell><cell cols="4">81.1 88.0 92.4 95.4</cell><cell cols="4">71.4±.83 80.9±.55 87.5±.31 92.3±.26</cell><cell>1.08</cell><cell>12.7</cell></row><row><cell>ABE-8</cell><cell cols="4">85.2 90.5 93.9 96.1</cell><cell cols="4">75.0±.39 83.4±.24 89.2±.31 93.2±.24</cell><cell>1.20</cell><cell>7.46</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">† All compared methods use GoogLeNet architecture except Margin which uses ResNet-50 [8] and Proxy-NCA uses IncpeptionBN</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vijaykumarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<title level="m">Why M heads are better than one: Training a diverse ensemble of deep networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient model averaging for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BIER-boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04815</idno>
		<title level="m">Deep metric learning with BIER: Boosting independent embeddings robustly</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representationsworkshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
