<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Quantization Improves GAN Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Feature Quantization Improves GAN Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The instability in GAN training has been a longstanding problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution. In this work, we propose Feature Quantization (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space. Our method can be easily plugged into existing GAN models, with little computational overhead in training. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, including three representative GAN models on 9 benchmarks, achieving new state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> are a powerful class of generative models, successfully applied to a variety of tasks such as image generation <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref>, image-to-image translation <ref type="bibr" target="#b29">(Liu et al., 2017;</ref><ref type="bibr" target="#b52">Zhu et al., 2017;</ref><ref type="bibr" target="#b16">Isola et al., 2017)</ref>, text-to-image generation <ref type="bibr" target="#b47">(Zhang et al., 2017)</ref>, super-resolution <ref type="bibr" target="#b43">(Sønderby et al., 2016)</ref>, domain adaptation <ref type="bibr" target="#b45">(Tzeng et al., 2017)</ref> and sampling from unnormalized distributions .</p><p>Training GANs is a notoriously challenging task, as it involves optimizing a non-convex problem for its Nash equilibrium in a high-dimensional parameter space. In practice, GANs are typically trained via alternatively updating generator and discriminator, using stochastic gradient descent (SGD) based on mini-batches of true/fake data samples. This procedure is often unstable and lacks theoretical guarantees <ref type="bibr" target="#b41">(Salimans et al., 2016)</ref>. Consequently, training may exhibit instability, divergence or mode collapse <ref type="bibr" target="#b32">(Mescheder et al., 2018)</ref>. As a result, many techniques to stabilize GAN training have been proposed <ref type="bibr" target="#b41">(Salimans et al., 2016;</ref><ref type="bibr" target="#b33">Miyato et al., 2018;</ref><ref type="bibr" target="#b19">Karras et al., 2019b)</ref>.</p><p>One possible explanation for the instability is that the learning environment for GANs is non-stationary, and previous models rely heavily on the current mini-batch statistics to match the features across different image regions. Since the mini-batch only provides an estimate, the true underlying distribution can only be learned after passing through a large number of mini-batches. This could prevent adversarial learning on large-scale datasets for a variety of reasons: (i) A small mini-batch may not be able to represent true distribution for large datasets, optimization algorithms may have trouble discovering parameter values that carefully search for continuous features to match fake samples with real samples, and these parameterizations may be brittle and prone to failure when applied to previously unseen images. (ii) Increasing the size of the mini-batch can increase the estimation quality, but doing this also loses the computational efficiency obtained by using SGD. (iii) In particular, the distribution of fake samples shifts as the generator changes during training, making the classification task for discriminator evolve over time <ref type="bibr" target="#b28">Liang et al., 2018;</ref><ref type="bibr" target="#b50">Zhao et al., 2020;</ref><ref type="bibr" target="#b4">Cong et al., 2020)</ref>. In such a non-stationary online environment, discriminator can forget previous tasks if it relies on the statistics from the current single mini-batch, rendering training unstable.</p><p>In this work, we show that GANs benefit from feature quantization (FQ) in the discriminator. A dictionary is first constructed via moving-averaged summary of features in recent training history for both true and fake data samples. This enables building a large and consistent dictionary on-the-fly that facilitates the online fashion of GAN training. Each dictionary item represents a unique feature prototype of similar image regions. By quantizing continuous features in arXiv:2004.02088v2 <ref type="bibr">[cs.</ref>LG] 15 Jul 2020</p><p>Feature Quantization Improves GAN Training Real Fake <ref type="figure">Figure 1</ref>. The proposed FQ-GAN generates images by leveraging quantized features from a dictionary, rather than producing arbitrary features in a continuous space when judged by the discriminator. The odd columns show images of the same class (real on the top row, fake at the bottom row), whose corresponding quantized feature maps are shown in the right even column, respectively. The dictionary items are visualized in 1D as the color-bar using t-SNE <ref type="bibr" target="#b30">(Maaten &amp; Hinton, 2008)</ref>. Image regions with similar semantics utilize the same/similar dictionary items. For example, bird neck is in dark red, sky or clear background is in shallow blue, grass is in orange.</p><p>traditional GANs into these dictionary items, the proposed FQ-GAN forces true and fake images to construct their feature representations from the limited values, when judged by discriminator. This alleviates the poor estimate issue of mini-batches in traditional GANs.</p><p>To better understand what has been learned during the generation process, we visualize the quantized feature maps of the discriminator in FQ-GAN for different images. Some sample images are shown in <ref type="figure">Figure 1</ref>. Image regions with similar semantics utilize the same or similar dictionary items.</p><p>The contributions of this paper are summarized as follows: (i) We propose FQ, a simple yet effective technique that can be added universally to yield better GANs. (ii) The effectiveness of FQ is validated with three GAN models on 10 datasets. Compared with traditional GANs, we show empirically that the proposed FQ-GAN helps training converge faster, and often yields performance improvement by a large margin, measured by generated sample quality. The code is released on Github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries on vanilla GANs</head><p>Consider two general marginal distributions q(x) and p(z) over x ∈ X and z ∈ Z. To generate samples from these random variables, adversarial methods <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> provide a sampling mechanism that only requires gradient backpropagation, without the need to specify the conditional densities. Specifically, instead of sampling directly from the desired conditional distribution, the random variable is generated as a deterministic transformation of an independent noise, e.g., a Gaussian distribution. The sampling procedure for conditionalsx ∼ p θ (x|z) is carried 1 https://github.com/YangNaruto/FQ-GAN out through the following generating process:</p><formula xml:id="formula_0">x = g θ (z), z ∼ p(z),<label>(1)</label></formula><p>where g θ (·) is the generators, specified as neural networks with parameters θ, and p(z) is specified as a simple parametric distribution, e.g., isotropic Gaussian p(z) = N (z; 0, I).</p><p>Note that (1) implies that p θ (x|z) is parameterized by θ, hence the subscripts.</p><p>The goal of GAN <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> is to match the marginal p θ (x) = p θ (x|z)p(z)dz to q(x). Note that q(x) denotes the true distribution of the data, from which we have samples. In order to do the matching, GAN trains a ω-parameterized adversarial discriminator network, f ω (x), to distinguish between samples from p θ (x) and q(x). Formally, the minimax objective of GAN is given by the following expression:</p><formula xml:id="formula_1">min θ max ω L GAN = E x∼q(x) [log σ(f ω (x))]+ Ex ∼p θ (x|z),z∼p(z) [log(1 − σ(f ω (x)))],<label>(2)</label></formula><p>where σ(·) is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pitfall of Continuous Features</head><p>Several works have shown that using feature matching as a training objective of GANs can improve model performance. The basic idea is to embed true/fake distributions in a finite-dimensional continuous feature space, and to match them based on their feature statistics using some divergence metrics. One general form of feature matching is based on Integral probability metric (IPM) <ref type="bibr" target="#b35">(Müller, 1997)</ref>, indexed by the function space F, defined as follows:</p><formula xml:id="formula_2">d F (p, q) = sup f ∈F |Ex ∼p(x) f (x) − E x∼q(x) f (x)|<label>(3)</label></formula><p>The particular function class F determines the probability metric. For example, <ref type="bibr" target="#b34">Mroueh et al. (2017)</ref> proposed     MC-GAN, which utilizes both mean and covariance feature statistics. They further showed that several previous works on GAN can be written within the mean feature matching framework, including Wasserstein GAN (Arjovsky et al., 2017), MMD-GAN <ref type="bibr" target="#b27">(Li et al., 2017b)</ref>, and Improved GAN <ref type="bibr" target="#b41">(Salimans et al., 2016)</ref>.</p><formula xml:id="formula_3">f ! B &lt;</formula><formula xml:id="formula_4">I l G P O E l 5 E N O B E p F g F K 0 U + G F E f B S y z / P h t F e p u j V 3 D r J K v I J U o U C j V / n y + w n L Y q 6 Q S W p M 1 3 N T D H K q U T D J p 2 U / M z y l b E Q H v G u p o j E 3 Q T 4 / e k r O r d I n U a J t K S R z 9 f d E T m N j J n F o O 2 O K Q 7 P s z c T / v G 6 G 0 U 2 Q C 5 V m y B V b L I o y S T A h s w R I X 2 j O U E 4 s o U w L e y t h Q 6 o p Q 5 t T 2 Y b g L b + 8 S l q X N c + t e Q 9 X 1 f p t E U c J T u E M L s C D</formula><formula xml:id="formula_5">I l G P O E l 5 E N O B E p F g F K 0 U + G F E f B S y z / P h t F e p u j V 3 D r J K v I J U o U C j V / n y + w n L Y q 6 Q S W p M 1 3 N T D H K q U T D J p 2 U / M z y l b E Q H v G u p o j E 3 Q T 4 / e k r O r d I n U a J t K S R z 9 f d E T m N j J n F o O 2 O K Q 7 P s z c T / v G 6 G 0 U 2 Q C 5 V m y B V b L I o y S T A h s w R I X 2 j O U E 4 s o U w L e y t h Q 6 o p Q 5 t T 2 Y b g L b + 8 S l q X N c + t e Q 9 X 1 f p t E U c J T u E M L s C D</formula><formula xml:id="formula_6">I l G P O E l 5 E N O B E p F g F K 0 U + G F E f B S y z / P h t F e p u j V 3 D r J K v I J U o U C j V / n y + w n L Y q 6 Q S W p M 1 3 N T D H K q U T D J p 2 U / M z y l b E Q H v G u p o j E 3 Q T 4 / e k r O r d I n U a J t K S R z 9 f d E T m N j J n F o O 2 O K Q 7 P s z c T / v G 6 G 0 U 2 Q C 5 V m y B V b L I o y S T A h s w R I X 2 j O U E 4 s o U w L e y t h Q 6 o p Q 5 t T 2 Y b g L b + 8 S l q X N c + t e Q 9 X 1 f p t E U c J T u E M L s C D</formula><formula xml:id="formula_7">I l G P O E l 5 E N O B E p F g F K 0 U + G F E f B S y z / P h t F e p u j V 3 D r J K v I J U o U C j V / n y + w n L Y q 6 Q S W p M 1 3 N T D H K q U T D J p 2 U / M z y l b E Q H v G u p o j E 3 Q T 4 / e k r O r d I n U a J t K S R z 9 f d E T m N j J n F o O 2 O K Q 7 P s z c T / v G 6 G 0 U 2 Q C 5 V m y B V b L I o y S T A h s w R I X 2 j O U E 4 s o U w L e y t h Q 6 o p Q 5 t T 2 Y b g L b + 8 S l q X N c + t e Q 9 X 1 f p t E U c J T u E M L s C D</formula><p>Though theoretically attractive, these continuous feature matching methods fall short of recent state-of-the-art performance on large datasets <ref type="bibr" target="#b2">(Brock et al., 2018;</ref><ref type="bibr" target="#b18">Karras et al., 2019a)</ref>. We argue there are two issues: (i) Principled methods often require to constrain the discriminator capacity (e.g., weight clipping or gradient penalty) to ensure the boundedness: new architectural adjustments such as a higher number of feature maps are needed to compensate for constraints <ref type="bibr" target="#b34">(Mroueh et al., 2017)</ref>. The question remains what architectural choices can balance the trade-off in practice.</p><p>(ii) More importantly, the direct feature matching scheme in <ref type="formula" target="#formula_2">(3)</ref> is estimated via mini-batch statistics, which can be prohibitively inaccurate on large or complex datasets. An effective alternative to match features at large-scale is required, even if it is indirect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Quantization GANs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From Continuous to Quantized Representations</head><p>Without loss of generality, the discriminator f ω (x) can be rewritten with a function decomposition:</p><formula xml:id="formula_8">f ω (x) = f ω T • f ω B (x),<label>(4)</label></formula><p>where f ω B (x) is the bottom network, whose output feature h ∈ R D is in a D-dimensional continuous space, and used as the input of the top network f ω T (h). Instead of working in a continuous feature space, we propose to quantize features into a discrete space, enabling implicit feature matching.</p><p>Specifically, we consider the discrete feature space as a dictionary E = {e k ∈ R D | k ∈ 1, 2, · · · , K}, where K is the size of the discrete space (i.e., a K-way categorical space), and D is the dimensionality of each dictionary item e k .</p><p>The discrete feature h is then calculated by a nearest neighbour look-up using the shared dictionary:</p><formula xml:id="formula_9">h = f Q (h) = e k , where k = argmin j h − e j 2 ,<label>(5)</label></formula><p>where f Q is a parameter-free look-up function, and h is further sent to the top network. Hence, in contrast to the traditional discriminator in (4), our feature quantization discriminator is:</p><formula xml:id="formula_10">f ω (x) = f ω T • f Q • f ω B (x),<label>(6)</label></formula><p>The overall scheme of FQ-GAN is illustrated in <ref type="figure" target="#fig_4">Figure 2</ref>. FQ-GAN in (6) reduces to the standard GAN model in <ref type="formula" target="#formula_8">(4)</ref> if f Q is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dictionary Learning</head><p>One remaining question is how to construct the dictionary E. Following <ref type="bibr" target="#b37">(Oord et al., 2017)</ref>, we consider a feature quantization loss consisting of two terms specified in <ref type="formula" target="#formula_11">(7)</ref>: (i) The dictionary loss, which only applies to the dictionary items, brings the selected item e close to the output of the bottom network. (ii) The commitment loss encourages the output of the bottom network to stay close to the chosen dictionary item to prevent it from fluctuating too frequently from one code item to another. The operator sg refers to a stop-gradient operation that blocks gradients from flowing into its argument, and β is a weighting hyper-parameter (β = 0.25 in all our experiments):</p><formula xml:id="formula_11">L Q = sg(h) − e k 2 2 dictionary loss +β sg(e k ) − h 2 2 commitment loss ,<label>(7)</label></formula><p>where e k is the nearest dictionary item to h defined in (5).  <ref type="bibr" target="#b28">(Liang et al., 2018)</ref>. In another word, the classification tasks for the discriminator change over time, and recent samples from the generator are more related to current discriminator learning. This inspires us to maintain the dictionary as a queue of features, allowing reusing the encoded features from the preceding mini-batches. The current mini-batch is enqueued to the dictionary, and the oldest mini-batches in the queue are gradually removed. The dictionary always represents a set of prototypes for the recent features, while the extra computation of maintaining this dictionary is manageable. Moreover, removing the features from the oldest mini-batch can be beneficial, because its encoded features are from an early stage of GAN training, and thus the least realistic and consistent with the newest ones.</p><p>Alternatively, one may wonder learning a dictionary using all training data beforehand, and keep the dictionary fixed during GAN training. We note this scheme is not practical in that (i) Modern datasets such as ImageNet are usually very large, learning a dictionary offline is prohibitively computationally expensive. (ii) More importantly, such a dictionary is not representative for fake images at the early of training, rendering it difficult to effectively learn quantized features for fake images.</p><p>Momentum update of dictionary. Specifically, we use the exponential moving average updates to implement the evolving dictionary, as a replacement for the dictionary loss term in <ref type="formula" target="#formula_11">(7)</ref>. For a mini-batch of size n, n k is the number of features that will be quantized to dictionary item e k . The momentum update for e k is:</p><formula xml:id="formula_12">e k ← m k /N k , where m k ← λm k + (1 − λ) n k i=1 h i,k , N k ← λN k + (1 − λ)n k ,<label>(8)</label></formula><p>where λ ∈ (0, 1) is a momentum coefficient. Only the parameters in the bottom network f ω B are updated by backpropagation. The momentum updates above make e k evolve more smoothly. Small λ considers less history. For example, λ = 0 only utilizes the current mini-batch statistics and ignores the entire history, thus (8) reduces to <ref type="formula" target="#formula_11">(7)</ref>. We used the default λ = 0.90 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">FQ-GAN Training</head><p>The overall training objective of the proposed FQ-GAN is:</p><formula xml:id="formula_13">min θ, E max ω L FQ−GAN = L GAN + αL Q ,<label>(9)</label></formula><p>where α is the weight to incorporate the proposed FQ into GANs. The training procedure is detailed in Algorithm 1. In practice, to avoid degeneration, α can be annealed from 0 to 1, and h (instead of h ) can be used to feed to the next layer at the beginning of training. In this case, one may consider FQ regularizes the learned features using clustering. The generator parameter θ and discriminator parameter ω are updated via the regularized GAN objective in (9), while the dictionary items E are updated via (8). FQ-GAN enjoys several favorable properties, explained as follows.</p><p>Scalability. The introduction of a dynamic dictionary decouples the dictionary size from the mini-batch size. The dictionary size can be much larger than a typical mini-batch size, and can be flexibly and independently set as a hyperparameter. The items in the dictionary are progressively replaced. Compared with traditional feature matching methods that only consider the current mini-batch statistics, the proposed FQ-GAN maintains much more representative feature statistics in the dictionary, allowing robust feature matching for large datasets.</p><p>Implicit feature matching. FQ-GAN shares similar spirits of many other regularization techniques for the discriminator in that they reduce the representational power of the discriminator. However, instead of imposing boundness on weights or gradients, FQ-GAN restricts continuous features into a prescribed set of values, i.e., feature centroids. Since both true and fake samples can only choose their representations from the limited dictionary items, FQ-GAN indirectly performs feature matching. This can be illustrated using the visualization example in <ref type="figure" target="#fig_4">Figure 2 (b)</ref>, where true features h and fake featuresh are quantized into the same centroids. Further, the discrete nature improves the possibilities of feature matching, compared to a continuous space.  <ref type="figure">Figure 3</ref>. Illustration of FQ construction in CNNs. In this example, the dictionary has 5 items, and feature map is h ∈ R 5×5×5 . The feature vector at each position is quantized into a dictionary item, e.g., the back-right feature is quantized into a red item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">FQ-GAN for image generation</head><p>FQ is a general method for discriminator design in GANs. We consider image generation tasks in this paper, where the discriminator is often parameterized by convolutional neural networks (CNNs). Each image is represented as a feature map h ∈ R C×L×W in CNNs, where C, L, W is the number of channels as well as the length and width, respectively. We construct a position-wise dictionary, with each item e ∈ R C . At a given position on the feature map, the feature vector characterizes the local image region. It is quantized into its nearest dictionary item for calibration, leading to a new quantized feature map h containing calibrated local feature prototypes. We provide the visual illustration on constructing FQ for CNN-based discriminator in <ref type="figure">Figure 3</ref>. Note that the FQ module can be used in multiple different layers of discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improving GANs</head><p>Training vanilla GANs is difficult: it requires carefully finely-tuned hyper-parameters and network architectures to make it work. Much recent research has accordingly focused on improving its stability, drawing on a growing body of empirical and theoretical insights <ref type="bibr" target="#b36">(Nowozin et al., 2016;</ref><ref type="bibr" target="#b25">Li et al., 2017a;</ref><ref type="bibr" target="#b52">Zhu et al., 2017;</ref><ref type="bibr" target="#b7">Fedus et al., 2017)</ref>. Among them, the three following aspects are related to FQ-GAN.</p><p>Regularized GANs. Various Regularization methods have been proposed, including changing the objective functions to encourage convergence <ref type="bibr" target="#b31">Mao et al., 2017;</ref><ref type="bibr" target="#b32">Mescheder et al., 2018;</ref><ref type="bibr" target="#b22">Kodali et al., 2017;</ref><ref type="bibr" target="#b49">Zhang et al., 2019b)</ref>, and constraining discriminator through gradient penalties <ref type="bibr" target="#b11">(Gulrajani et al., 2017)</ref> or normalization <ref type="bibr" target="#b33">(Miyato et al., 2018)</ref>. They counteract the use of unbounded loss functions and ensure that discriminator provides gradients everywhere to generator. FQ is also realted to variational discriminator bottleneck <ref type="bibr" target="#b38">(Peng et al., 2018)</ref> in the sense that both restrict the feature representation capacity. BigGANs <ref type="bibr" target="#b2">(Brock et al., 2018)</ref> use orthogonal regularization, and achieve state of the art image synthesis performance on ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009)</ref>.</p><p>Network architectures. Recent advances consider architecture designs, such as SA-GAN <ref type="bibr" target="#b48">(Zhang et al., 2019a)</ref>, which adds the self-attention block to capture global structures. Progressive-GAN <ref type="bibr" target="#b17">(Karras et al., 2018)</ref> trains highresolution GANs in the single-class setting by training a single model across a sequence of increasing resolutions. As a new variant, Style-GAN <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref> proposed a generator architecture to separate high-level attributes and stochastic variation, achieving highly varied and highquality human faces.</p><p>Memory-based GANs. <ref type="bibr" target="#b21">Kim et al. (2018)</ref> increased the model complexity via proposing a shared and sophisticated memory module for both generator and discriminator. The generation process is conditioned on samples from the memory. <ref type="bibr" target="#b53">Zhu et al. (2019)</ref> proposed a dynamic memory component for image refinement in the text-to-image task.</p><p>Compared with the above three aspects, FQ-GAN slightly modifies the discriminator architecture by injecting a dictionary-based look-up layer, and thus regularizes the model capacity to encourage easier feature matching. The dictionary in FQ-GAN can be viewed as a much simpler memory module to store feature statistics. Importantly, our FQ-GAN is easier to use and orthogonal to existing GANs, and can be simply employed as a plug-in module to further improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Vector Quantization</head><p>Vector quantization (VQ) <ref type="bibr" target="#b10">(Gray, 1984)</ref> has been used in various settings, including clustering <ref type="bibr" target="#b6">(Equitz, 1989)</ref>, metric learning <ref type="bibr" target="#b42">(Schneider et al., 2009</ref>), etc. The most related work to ours is <ref type="bibr" target="#b37">(Oord et al., 2017)</ref>, where discrete latent representations are proposed for variational auto-encoders to circumvent the issues of "posterior collapse", and show that pairing such quantized representations with an autoregressive prior can generate high-quality images, videos, and speech. Our motivation and scenarios are different from previous VQ works. To the best of our knowledge, this paper presents the first feature quantization work for GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We apply the proposed FQ-GAN method to three stateof-the-art GAN models for a variety of tasks. (i) Big-GAN <ref type="bibr" target="#b2">(Brock et al., 2018)</ref> for image synthesis, especially for ImageNet, representing a generation task for large-scale datasets. (ii) StyleGAN <ref type="bibr" target="#b18">(Karras et al., 2019a;</ref> for face synthesis, representing a generation task for high-resolution images. (iii) U-GAT-IT <ref type="bibr" target="#b20">(Kim et al., 2020)</ref> for an unsupervised image-to-image translation task.  Evaluation metrics. We consider three commonly used evaluation metrics for GANs. (i) Inception Score (IS) (Salimans et al., 2016) measures how realistic the output of the generator is and the intra-class variety, based on how well the image classification model Inception v3 <ref type="bibr" target="#b44">(Szegedy et al., 2016)</ref> classifies them as one of 1, 000 known objects collected in ImageNet-1000. Higher scores mean that the model can generate more distinct images. However, it is not reliable when generated images concentrate to the class centers. (ii) Frchet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref> compares the statistics (mean and variances of Gaussian distributions) between the generated samples and real samples. FID is consistent with increasing disturbances and human judgment. Lower scores indicate that the model can generate higher quality images. (iii) Kernel Inception Distance (KID) <ref type="bibr" target="#b1">(Bińkowski et al., 2018)</ref> improves FID as an unbiased estimator, making it more reliable when there are fewer available test images. We use generated images translated from all test images in the source domain vs. test images in the target domain to compute KID. Lower KID values indicate that images are better translated.</p><p>All the baseline methods are implemented via the official codebases from the authors. Model variants that incorporate the proposed feature quantization technique are named with prefix "FQ". Experiment details are provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">On the impact of hyper-parameters</head><p>We investigate the hyper-parameters of FQ on the CIFAR-100 dataset <ref type="bibr" target="#b23">(Krizhevsky et al., 2009</ref>) . It has 100 classes containing 600 images each, in which there are 500 training images and 100 testing images. Four-layer networks are employed for both the generator and the discriminator. We train the model for 500 epochs, and save a model every 1000 iterations. We take the last 10 checkpoints to report the mean of their performance, measured by FID and IS.</p><p>Dictionary size K. In <ref type="figure" target="#fig_7">Figure 4 (a)</ref>, we show the FQ-GAN performance with various dictionary size K = 2 P . We see that a smaller K yields better performance. Surprisingly, the dictionary with binary values K = 2 (P = 1) provides the best results on this dataset. Larger K is less favorable for two reasons: (1) it can be more memory expensive; <ref type="formula" target="#formula_1">(2)</ref>    <ref type="bibr" target="#b33">(Miyato et al., 2018)</ref>, Repulsive MMD-GAN .</p><p>CIFAR-100 is a more challenging dataset with more finegrained categories, compared to CIFAR-10. The current best classification model achieves 91.3% accuracy on this dataset <ref type="bibr" target="#b15">(Huang et al., 2019)</ref>, suggesting that the class distributions have certain support overlaps. 500 epochs are used. We also integrate FQ into TAC-GAN <ref type="bibr" target="#b8">(Gong et al., 2019)</ref>, which is the current state-of-the-art on CIFAR-100. TAC-GAN improves the intra-class diversity of AC-GAN, thus particularly good at generating images with fine-grained labels. The results are show in    generated image samples to illustrate the improved diversity for each class in <ref type="figure">Figure 9</ref> in Appendix.</p><p>ImageNet-1000 <ref type="bibr" target="#b39">(Russakovsky et al., 2015)</ref> contains around 1.2 million images with 1000 distinct categories. We pre-process images into resolution 64 × 64 and 128 × 128 in our experiments, respectively. 100 epochs are used. The results are show in <ref type="table" target="#tab_2">Table 3</ref>. It shows that FQ improves generation quality for both resolution 64 and 128.</p><p>Computational Cost. To evaluate the computational overhead of FQ, we compare the running time of BigGAN and our FQ-BigGAN variant on three datasets in <ref type="table" target="#tab_4">Table 4</ref>. To finish the same number of training epochs, FQ-GAN takes 1.63%, 3.14%, and 1.23% more time than the original Big-GAN on ImageNet, CIFAR-100 and CIFAR-10, respectively. It means that the additional time cost of FQ is negligible. In practice, FQ-GAN converges faster, as shown in <ref type="figure" target="#fig_8">Figure 5</ref>. It may take less time to reach the same performance.</p><p>How does FQ improve performance? We perform indepth analysis on per-class image generation quality on ImageNet. Two metrics are studied: (i) We extract quantized feature sets from discriminator for both real and fake images per class, and measure their distribution divergence using maximum mean discrepancy (MMD). Lower MMD values indicate better feature matching. (ii) The per-class FID is also computed from the pre-trained inception network, which measures the matching quality that a generated distribution fits the target distribution in each class. Lower FID values indicate better high intra-class diversity. The results are shown in In <ref type="figure" target="#fig_9">Figure 6</ref>. FQ yields significantly lower MMD, and lower FID by a large margin, meaning that FQ can improve feature matching, and intra-class diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">StyleGAN for Face Synthesis</head><p>StyleGAN yields state-of-the-art results in unconditional generative image modeling. StyleGAN <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref> is a new variant of the Progressive GAN <ref type="bibr" target="#b17">(Karras et al., 2018)</ref>, the main difference is the introduction of a latent mapping network in the generator architecture. The very recent version, StyleGAN2 <ref type="bibr" target="#b19">(Karras et al., 2019b)</ref>, simplifies the progressive architecture, and uses a suite of techniques to improve the performance. We apply our FQ to the StyleGAN and StyleGAN2, based on the TensorFlow codes of StyleGAN 3 and StyleGAN2 4 . The Flickr-Faces-HQ (FFHQ) dataset <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref> is used. It consists of 70k high-quality images (1024 × 1024), which endows more variations than the previously widely used CelebA-HQ dataset in terms of accessories, age, ethnicity and image background <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref>. Each model was trained using 25M images by default. For StyleGAN, we consider four resolutions at 32 2 , 64 2 , 128 2 and 1024 2 . The progressive training starts from resolution 8 2 in experiments of resolution 32 2 − 128 2 whereas the initial resolution is 512 2 in the experiment on 1024 2 . The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. The FQ variant improves StyleGAN on all four resolutions. For StyleGAN2, we deploy the model under config-e (Karras et al., 2019b) and use the full resolution FFHQ. The best FID score of FQ-StyleGAN2 is 3.19 which surpasses the reported score 3.31 of StyleGAN2. High-fidelity generated faces from the two models are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Unsupervised Image-to-Image Translation</head><p>The task of unsupervised image translation is becoming increasingly popular, inspired by recent advances in GANs. U-GAT-IT <ref type="bibr" target="#b20">(Kim et al., 2020)</ref> is the latest state-of-the-art. We validate our FQ using their official TensorFlow code-3 https://github.com/NVlabs/stylegan 4 https://github.com/NVlabs/stylegan2 base 5 . Five unpaired image datasets are used for evaluation, including selfie2anime <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>, cat2dog, photo2portrait <ref type="bibr" target="#b24">(Lee et al., 2018)</ref>, horse2zebra and van-gogh2photo . All images are resized to 256 × 256 resolution. Details are given in the Appendix.</p><p>We also compare with several known image translation models, including CycleGAN  and UNIT , which show better performance than MUNIT <ref type="bibr" target="#b29">(Liu et al., 2017)</ref>, DRIT <ref type="bibr" target="#b24">(Lee et al., 2018)</ref> in <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>. Each model is trained for 100 epochs, and we report the best KID values in <ref type="table" target="#tab_6">Table 6</ref>. FQ improves U-GAT-IT on most datasets, and achieves new state-of-theart for image translation. We have also conducted human evaluation on Amazon Mechanical Turk (AMT). Each testing image is judged by 3 users, who are asked to select the best translated image to target domain. We inform to the participants the name of target domain, along with six example images of target domain as visual illustration. An example of user interface is show in <ref type="figure" target="#fig_4">Figure 20</ref> in Appendix. <ref type="table">Table 7</ref> shows the overall percentage that users prefer a particular model. The proposed FQ achieves higher score in human perceptual study (except for comparable results on photo2vangogh), compared to its baseline method. Qualitative comparison in <ref type="figure" target="#fig_10">Figure 7</ref> shows that FQ can produce sharper image regions, this is because the dictionary items that FQ utilizes to construct features are from recent history. More examples on translated images are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose Feature Quantization Generative Adversarial Networks (FQ-GANs), which incorporate a feature quantization module into the discriminator learning of the GAN framework.  <ref type="table">Table 7</ref>. User study on translated image preference between U-GAT-IT and its FQ variant using AMT.</p><p>ages (1024 ⇥ 1024), which provide more variations than the previously widely used CelebA-HQ dataset in terms of accessories, age, ethnicity and image background <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref>. We consider the downsampled versions at 32 ⇥ 32, 64 ⇥ 64 and 128 ⇥ 128. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. The FQ variant improves StyleGAN on three resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Unsupervised Image-to-Image Translation</head><p>The task of unsupervised image translation is becoming increasingly popular, inspired by recent advances in GANs. U-GAT-IT <ref type="bibr" target="#b20">(Kim et al., 2020)</ref> is the latest state-of-the-art. We validate our FQ using their official TensorFlow codebase 3 . Five unpaired image datasets are used for evaluation, including selfie2anime <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>, cat2dog, photo2portrait <ref type="bibr" target="#b24">(Lee et al., 2018)</ref>, horse2zebra and van-gogh2photo . All images are resized to 256 ⇥ 256 resolution. Details are given in the Appendix.</p><p>We also compare with several known image translation models, including CycleGAN  and UNIT , which show better performance than MUNIT <ref type="bibr" target="#b29">(Liu et al., 2017)</ref>, DRIT <ref type="bibr" target="#b24">(Lee et al., 2018)</ref> in <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>. Each model is trained for 100 epochs, and we report the best KID values in <ref type="table" target="#tab_6">Table 6</ref>. FQ improves U-GAT-IT on most datasets, and achieves new state-of-the-art for image translation. We have also conducted human evaluation on Amazon Mechanical Turk. Each testing image is 3 https://github.com/taki0112/UGATIT judged by 3 users, who are asked to select the best translated image to target domain. We inform to the participants the name of target domain, along with six example images of target domain as visual illustration. An example of user interface is show in <ref type="figure" target="#fig_4">Figure 12</ref> in Appendix. <ref type="table">Table 7</ref> shows the overall percentage that users prefer a particular model. The proposed FQ achieves higher score in human perceptual study (except for comparable results on photo2vangogh), compared to its baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose Feature Quantization Generative Adversarial Networks (FQ-GANs), which incorporate a feature quantization module into the discriminator learning of the GAN framework. The FQ module is effective in performing implicit feature matching for large datasets. FQ can be easily used in training many existing GAN models, and improve their performance. It yields improved performance on three canonical tasks, including BigGAN for image generation on ImageNet, StyleGAN for face generation on FFHQ, and unsupervised image-to-image translation. FQ-GAN sets new state-of-the-art performance on most datasets. <ref type="table" target="#tab_6">Table 6</ref>. KID ×100 for different image translation datasets. All numbers except for our FQ variant are from <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>. <ref type="table" target="#tab_2">391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439</ref> Model anime2selfie zebra2horse dog2cat portrait2photo vangogh UNIT 26.32 ± 0.92 14.93 ± 0.75 9.81 ± 0.34 1.42 ± 0.24 9.72 ± CycleGAN 11.84 ± 0.74 8.0 ± 0.66 9.94 ± 0.36 1.82 ± 0.36 4.68 ± U-GAT-IT 11.52 ± 0.57 7.47 ± 0.71 8.15 ± 0.66 1.69 ± 0.53 5.61 ± FQ-U-GAT-IT 10.23 ± 0.40 7.10 ± 0.42 8.90 ± 0.32 0.73 ± 0.16 5.21 ±  <ref type="table">Table 7</ref>. User study on translated image preference between U-GAT-IT and its FQ variant using ages (1024 ⇥ 1024), which provide more variations than the previously widely used CelebA-HQ dataset in terms of accessories, age, ethnicity and image background <ref type="bibr" target="#b18">(Karras et al., 2019a)</ref>. We consider the downsampled versions at 32 ⇥ 32, 64 ⇥ 64 and 128 ⇥ 128. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. The FQ variant improves StyleGAN on three resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Unsupervised Image-to-Image Translation</head><p>The task of unsupervised image translation is becoming increasingly popular, inspired by recent advances in GANs. U-GAT-IT <ref type="bibr" target="#b20">(Kim et al., 2020)</ref> is the latest state-of-the-art. We validate our FQ using their official TensorFlow codebase 3 . Five unpaired image datasets are used for evaluation, including selfie2anime <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>, cat2dog, photo2portrait <ref type="bibr" target="#b24">(Lee et al., 2018)</ref>, horse2zebra and van-gogh2photo . All images are resized to 256 ⇥ 256 resolution. Details are given in the Appendix.</p><p>We also compare with several known image translation models, including CycleGAN  and UNIT , which show better performance than MUNIT <ref type="bibr" target="#b29">(Liu et al., 2017)</ref>, DRIT <ref type="bibr" target="#b24">(Lee et al., 2018)</ref> in <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>. Each model is trained for 100 epochs, and we report the best KID values in <ref type="table" target="#tab_6">Table 6</ref>. FQ improves U-GAT-IT on most datasets, and achieves new state-of-the-art for image translation. We have also conducted human evaluation on Amazon Mechanical Turk. Each testing image is 3 https://github.com/taki0112/UGATIT judged by 3 users, who are asked to sele image to target domain. We inform to name of target domain, along with six target domain as visual illustration. interface is show in <ref type="figure" target="#fig_4">Figure 12</ref> in Appe the overall percentage that users prefe The proposed FQ achieves higher score study (except for comparable results compared to its baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose Feature Qua Adversarial Networks (FQ-GANs), wh ture quantization module into the discr the GAN framework. The FQ module is ing implicit feature matching for large easily used in training many existing G prove their performance. It yields impr three canonical tasks, including BigGA tion on ImageNet, StyleGAN for face g and unsupervised image-to-image trans new state-of-the-art performance on m <ref type="table">Table 7</ref>. User perceptual study on translated image preference (in percentage) between U-GAT-IT and its FQ variant using AMT. In <ref type="figure">Figure 8</ref>, we provide the detailed learning curves under different FQ settings on CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experiment setup</head><p>• CIFAR-10 and CIFAR-100 (32 × 32 ): bs = 64, ch = 64. The architecture is given in <ref type="table">Table 8</ref>. Parameters are set as: bs = 64, G lr = 2e −4 , D lr = 2e −4 , D step = 4, G step = 1. To get the best results shown in <ref type="table" target="#tab_3">Table 2</ref>, we set P = 10, λ = 0.9, α = 1.0 of FQ being added at the layers [0, 1, 2, 3].</p><p>• ImageNet (64 × 64): bs = 512, ch = 64. The architecture is the same as that in Imagenet (128 × 128) when you omit the bottom downsample ResBlock in the discriminator and the top upsample ResBlock in the generator, as shown in <ref type="table">Table.</ref> 10. Parameters are set as: bs = 512, G lr = e −4 , D lr = 4e −4 , D step = 1, G step = 1 with self-attention at resolution 32 × 32. P = 10, λ = 0.7, α = 1.0 of FQ.</p><p>• Imagenet (128 × 128): The architecture is given in <ref type="table">Table.</ref> 10. Due to limited hardware resources, compared with the full-version BigGAN, we did the following modification: bs = 2048 − → bs = 1024, ch = 96 − → ch = 64. P = 10, λ = 0.8, α = 10.0 of FQ.s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Generated image samples</head><p>We show the generated images for CIFAR-100 in <ref type="figure">Figure 9</ref>, and ImageNet in <ref type="figure">Figure 10</ref>. More high-fidelity results are shown in <ref type="figure">Figure 11</ref> and <ref type="figure" target="#fig_4">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. StyleGAN</head><p>The official discriminator architectures used in StyleGAN and StylgeGAN2 are shwon in <ref type="table">Table 9</ref>. To apply the FQ techniqure, we did the following minimal modifications:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FQ-StyleGAN</head><p>In experiments on resolution 32 2 − 128 2 , we put the FQ layer just after Blocks-8 and P = 10, λ = 0.8, α = 1.0 of FQ. In experiments on resolution 1024 2 , the FQ layers were put in Blocks-(16, 32) and P = 7, λ = 0.9, α = 0.25. Randomly selected samples are shown in <ref type="figure">Figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FQ-StylgeGAN2</head><p>We put the FQ layer in Blocks- <ref type="bibr">(16,</ref><ref type="bibr">32)</ref> and P = 7, λ = 0.8, α = 0.25 of FQ. Randomly selected samples are shown in <ref type="figure" target="#fig_7">Figure 14</ref>.</p><p>C. U-GAT-IT C.1. Dataset selfie2anime It is first introduced in <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>. The selfie and anime datasets each contains 3400 training images and 100 testing images.</p><p>horse2zebra and photo2vangogh These datasets are used in . The training dataset size of each class: 1,067 (horse), 1,334 (zebra), 6,287 (photo), and 400 (vangogh). The test datasets consist of 120 (horse), 140 (zebra), 751 (photo), and 400 (vangogh).</p><p>cat2dog and photo2portrait These datasets are used in DRIT <ref type="bibr" target="#b24">(Lee et al., 2018)</ref>. The numbers of data for each class are 871 (cat), 1,364 (dog), 6,452 (photo), and 1,811 (vangogh). Follow <ref type="bibr" target="#b20">(Kim et al., 2020)</ref>, we use 120 (horse), 140 (zebra), 751 (photo), and 400 (vangogh) randomly selected images as test data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Architecture</head><p>In brief, the U-GAT-IT consists of a generator, a global discriminator and a local discriminator for source to target domain translation and vice versa. We only inject our FQ into the global discriminator and keep other parts unchanged. Training settings are the same as U-GAT-IT. The modified global discriminator architecture is shown in <ref type="table">Table 11</ref> and P = 8, λ = 0.8, α = 1.0 of FQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Additional results</head><p>We show more translated images: selfie2anime and anime2selfie in <ref type="figure" target="#fig_8">Figure 15</ref>, cat2dog and dog2cat in <ref type="figure">Figure</ref> 16, photo2portrait and portrait2photo in <ref type="figure" target="#fig_10">Figure 17</ref>, van-gogh2photo and photo2vangogh in <ref type="figure">Figure 18</ref>, horse2zebra and zebra2horse in <ref type="figure">Figure 19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. AMT interface design</head><p>The webpage interface used for human evaluation is shown in <ref type="figure" target="#fig_4">Figure 20</ref>. horse2zebra zebra2horse <ref type="figure">Figure 19</ref>. Visual comparisons on horse2zebra and zebra2horse. First row: input images. Second row: images generated by U-GAT-IT. Third row: images generated by FQ-U-GAT-IT. For the horse2zebra translation, U-GAT-IT tends to focus on the texture of zebra but corrupt most details. On contrast, FQ-U-GAT-IT focuses on the horse itself and protect other details. So, FQ-U-GAT-IT fails in some cases (the 4th column) but owns a low KID value.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 O S 5 I J O V d 3 H o 0 w 0 v Z d G K K H C 0 Q 8 = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y M 1 g E V y U R Q Z e l b l x W s A 9 o Q p h M J + 3 Q e Y S Z i V B C w F 9 x 4 0 I R t 3 6 H O / / G a Z u F t h 6 4 c D j n X u 6 9 J 0 4 Z 1 c b z v p 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / 4 B 4 e d b X M F C Y d L J l U / R h p w q g g H U M N I / 1 U E c R j R n r x 5 H b m 9 x 6 J 0 l S K B z N N S c j R S N C E Y m S s F L k n S Z Q H i s M 8 i J N A c j J C R d Q q I r f u N b w 5 4 C r x S 1 I H J d q R + x U M J c 4 4 E Q Y z p P X A 9 1 I T 5 k g Z i h k p a k G m S Y r w B I 3 I w F K B O N F h P j + / g O d W G c J E K l v C w L n 6 e y J H X O s p j 2 0 n R 2 a s l 7 2 Z + J 8 3 y E x y E + Z U p J k h A i 8 W J R m D R s J Z F n B I F c G G T S 1 B W F F 7 K 8 R j p B A 2 N r G a D c F f f n m V d C 8 b v t f w 7 6 / q z V Y Z R x W c g j N w A X x w D Z r g D r R B B 2 C Q g 2 f w C t 6 c J + f F e X c + F q 0 V p 5 w 5 B n / g f P 4 A V s K V t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 O S 5 I J O V d 3 H o 0 w 0 v Z d G K K H C 0 Q 8 = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y M 1 g E V y U R Q Z e l b l x W s A 9 o Q p h M J + 3 Q e Y S Z i V B C w F 9 x 4 0 I R t 3 6 H O / / G a Z u F t h 6 4 c D j n X u 6 9 J 0 4 Z 1 c b z v p 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / 4 B 4 e d b X M F C Y d L J l U / R h p w q g g H U M N I / 1 U E c R j R n r x 5 H b m 9 x 6 J 0 l S K B z N N S c j R S N C E Y m S s F L k n S Z Q H i s M 8 i J N A c j J C R d Q q I r f u N b w 5 4 C r x S 1 I H J d q R + x U M J c 4 4 E Q Y z p P X A 9 1 I T 5 k g Z i h k p a k G m S Y r w B I 3 I w F K B O N F h P j + / g O d W G c J E K l v C w L n 6 e y J H X O s p j 2 0 n R 2 a s l 7 2 Z + J 8 3 y E x y E + Z U p J k h A i 8 W J R m D R s J Z F n B I F c G G T S 1 B W F F 7 K 8 R j p B A 2 N r G a D c F f f n m V d C 8 b v t f w 7 6 / q z V Y Z R x W c g j N w A X x w D Z r g D r R B B 2 C Q g 2 f w C t 6 c J + f F e X c + F q 0 V p 5 w 5 B n / g f P 4 A V s K V t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 O S 5 I J O V d 3 H o 0 w 0 v Z d G K K H C 0 Q 8 = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y M 1 g E V y U R Q Z e l b l x W s A 9 o Q p h M J + 3 Q e Y S Z i V B C w F 9 x 4 0 I R t 3 6 H O / / G a Z u F t h 6 4 c D j n X u 6 9 J 0 4 Z 1 c b z v p 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / 4 B 4 e d b X M F C Y d L J l U / R h p w q g g H U M N I / 1 U E c R j R n r x 5 H b m 9 x 6 J 0 l S K B z N N S c j R S N C E Y m S s F L k n S Z Q H i s M 8 i J N A c j J C R d Q q I r f u N b w 5 4 C r x S 1 I H J d q R + x U M J c 4 4 E Q Y z p P X A 9 1 I T 5 k g Z i h k p a k G m S Y r w B I 3 I w F K B O N F h P j + / g O d W G c J E K l v C w L n 6 e y J H X O s p j 2 0 n R 2 a s l 7 2 Z + J 8 3 y E x y E + Z U p J k h A i 8 W J R m D R s J Z F n B I F c G G T S 1 B W F F 7 K 8 R j p B A 2 N r G a D c F f f n m V d C 8 b v t f w 7 6 / q z V Y Z R x W c g j N w A X x w D Z r g D r R B B 2 C Q g 2 f w C t 6 c J + f F e X c + F q 0 V p 5 w 5 B n / g f P 4 A V s K V t w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 O S 5 I J O V d 3 H o 0 w 0 v Z d G K K H C 0 Q 8 = " &gt; A A A B / n i c b V D L S s N A F J 3 U V 6 2 v q L h y M 1 g E V y U R Q Z e l b l x W s A 9 o Q p h M J + 3 Q e Y S Z i V B C w F 9 x 4 0 I R t 3 6 H O / / G a Z u F t h 6 4 c D j n X u 6 9 J 0 4 Z 1 c b z v p 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / 4 B 4 e d b X M F C Y d L J l U / R h p w q g g H U M N I / 1 U E c R j R n r x 5 H b m 9 x 6 J 0 l S K B z N N S c j R S N C E Y m S s F L k n S Z Q H i s M 8 i J N A c j J C R d Q q I r f u N b w 5 4 C r x S 1 I H J d q R + x U M J c 4 4 E Q Y z p P X A 9 1 I T 5 k g Z i h k p a k G m S Y r w B I 3 I w F K B O N F h P j + / g O d W G c J E K l v C w L n 6 e y J H X O s p j 2 0 n R 2 a s l 7 2 Z + J 8 3 y E x y E + Z U p J k h A i 8 W J R m D R s J Z F n B I F c G G T S 1 B W F F 7 K 8 R j p B A 2 N r G a D c F f f n m V d C 8 b v t f w 7 6 / q z V Y Z R x W c g j N w A X x w D Z r g D r R B B 2 C Q g 2 f w C t 6 c J + f F e X c + F q 0 V p 5 w 5 B n / g f P 4 A V s K V t w = = &lt; / l a t e x i t &gt; f Q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; g ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o T k 3 s y V 7 K E m g 2 J 7 C l H g n B M c f K i c = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y 2 a 7 a Z d u N m F 3 o p T Q / + H F g y J e / S / e / D d u 2 x y 0 9 c H A 4 7 0Z Z u a F q R Q G X f f b W V l d W 9 / Y L G 2 V t 3 d 2 9 / Y r B 4 c t k 2 S a 8 S Z L Z K I 7 I T V c C s W b K F D y T q o 5 j U P J 2 + H o Z u q 3 H 7 k 2 I l H 3 O E 5 5 E N O B E p F g F K 3 0 M O j l f h g R H 4 c c 6 a R X q b o 1 d w a y T L y C V K F A o 1 f 5 8 v s J y 2 K u k E l q T N d z U w x y q l E w y S d l P z M 8 p W x E B 7 x r q a I x N 0 E + u 3 p C T q 3 S J 1 G i b S k k M / X 3 R E 5 j Y 8 Z x a D t j i k O z 6 E 3 F / 7 x u h t F V k A u V Z s g V m y + K M k k w I d M I S F 9 o z l C O L a F M C 3 s r Y U O q K U M b V N m G 4 C 2 + v E xa 5 z X P r X l 3 F 9 X 6 d R F H C Y 7 h B M 7 A g 0 u o w y 0 0 o A k M N D z D K 7 w 5 T 8 6 L 8 + 5 8 z F t X n G L m C P 7 A + f w B d 4 W S e g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o T k 3 s y V 7 K E m g 2 J 7 C l H g n B M c f K i c = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y 2 a 7 a Z d u N m F 3 o p T Q / + H F g y J e / S / e / D d u 2 x y 0 9 c H A 4 7 0Z Z u a F q R Q G X f f b W V l d W 9 / Y L G 2 V t 3 d 2 9 / Y r B 4 c t k 2 S a 8 S Z L Z K I 7 I T V c C s W b K F D y T q o 5 j U P J 2 + H o Z u q 3 H 7 k 2 I l H 3 O E 5 5 E N O B E p F g F K 3 0 M O j l f h g R H 4 c c 6 a R X q b o 1 d w a y T L y C V K F A o 1 f 5 8 v s J y 2 K u k E l q T N d z U w x y q l E w y S d l P z M 8 p W x E B 7 x r q a I x N 0 E + u 3 p C T q 3 S J 1 G i b S k k M / X 3 R E 5 j Y 8 Z x a D t j i k O z 6 E 3 F / 7 x u h t F V k A u V Z s g V m y + K M k k w I d M I S F 9 o z l C O L a F M C 3 s r Y U O q K U M b V N m G 4 C 2 + v E xa 5 z X P r X l 3 F 9 X 6 d R F H C Y 7 h B M 7 A g 0 u o w y 0 0 o A k M N D z D K 7 w 5 T 8 6 L 8 + 5 8 z F t X n G L m C P 7 A + f w B d 4 W S e g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o T k 3 s y V 7 K E m g 2 J 7 C l H g n B M c f K i c = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y 2 a 7 a Z d u N m F 3 o p T Q / + H F g y J e / S / e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V l d W 9 / Y L G 2 V t 3 d 2 9 / Y r B 4 c t k 2 S a 8 S Z L Z K I 7 I T V c C s W b K F D y T q o 5 j U P J 2 + H o Z u q 3 H 7 k 2 I l H 3 O E 5 5 E N O B E p F g F K 3 0 M O j l f h g R H 4 c c 6 a R X q b o 1 d w a y T L y C V K F A o 1 f 5 8 v s J y 2 K u k E l q T N d z U w x y q l E w y S d l P z M 8 p W x E B 7 x r q a I x N 0 E + u 3 p C T q 3 S J 1 G i b S k k M / X 3 R E 5 j Y 8 Z x a D t j i k O z 6 E 3 F / 7 x u h t F V k A u V Z s g V m y + K M k k w I d M I S F 9 o z l C O L a F M C 3 s r Y U O q K U M b V N m G 4 C 2 + v E x a 5 z X P r X l 3 F 9 X 6 d R F H C Y 7 h B M 7 A g 0 u o w y 0 0 o A k M N D z D K 7 w 5 T 8 6 L 8 + 5 8 z F t X n G L m C P 7 A + f w B d 4 W S e g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o T k 3 s y V 7 K E m g 2 J 7 C l H g n B M c f K i c = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y 2 a 7 a Z d u N m F 3 o p T Q / + H F g y J e / S / e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u a F q R Q G X f f b W V l d W 9 / Y L G 2 V t 3 d 2 9 / Y r B 4 c t k 2 S a 8 S Z L Z K I 7 I T V c C s W b K F D y T q o 5 j U P J 2 + H o Z u q 3 H 7 k 2 I l H 3 O E 5 5 E N O B E p F g F K 3 0 M O j l f h g R H 4 c c 6 a R X q b o 1 d w a y T L y C V K F A o 1 f 5 8 v s J y 2 K u k E l q T N d z U w x y q l E w y S d l P z M 8 p W x E B 7 x r q a I x N 0 E + u 3 p C T q 3 S J 1 G i b S k k M / X 3 R E 5 j Y 8 Z x a D t j i k O z 6 E 3 F / 7 x u h t F V k A u V Z s g V m y + K M k k w I d M I S F 9 o z l C O L a F M C 3 s r Y U O q K U M b V N m G 4 C 2 + v E x a 5 z X P r X l 3 F 9 X 6 d R F H C Y 7 h B M 7 A g 0 u o w y 0 0 o A k M N D z D K 7 w 5 T 8 6 L 8 + 5 8 z F t X n G L m C P 7 A + f w B d 4 W S e g = = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 9 O S Z x j M a 7 G e D u 3 3 7 s W P J f l g E v w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D b b T b t 0 s w m 7 E 6 G G / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H J t R K w e c J p w P 6 I j J U L B K F q p k / W D k D z N B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M G Z p x B U y S Y 3 p e W 6 C f k Y 1 C i b 5 r N J P D U 8 o m 9 A R 7 1 m q a M S N n 8 3 P n Z E z q w x J G G t b C s l c / T 2 R 0 c i Y a R T Y z o j i 2 C x 7 u f i f 1 0 s x v P Y z o Z I U u W K L R W E q C c Y k / 5 0 M h e Y M 5 d Q S y r S w t x I 2 p p o y t A l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D z I U j 3 Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 9 O S Z x j M a 7 G e D u 3 3 7 s W P J f l g E v w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D b b T b t 0 s w m 7 E 6 G G / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H J t R K w e c J p w P 6 I j J U L B K F q p k / W D k D z N B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M G Z p x B U y S Y 3 p e W 6 C f k Y 1 C i b 5 r N J P D U 8 o m 9 A R 7 1 m q a M S N n 8 3 P n Z E z q w x J G G t b C s l c / T 2 R 0 c i Y a R T Y z o j i 2 C x 7 u f i f 1 0 s x v P Y z o Z I U u W K L R W E q C c Y k / 5 0 M h e Y M 5 d Q S y r S w t x I 2 p p o y t A l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D z I U j 3 Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 9 O S Z x j M a 7 G e D u 3 3 7 s W P J f l g E v w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D b b T b t 0 s w m 7 E 6 G G / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H J t R K w e c J p w P 6 I j J U L B K F q p k / W D k D z N B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M G Z p x B U y S Y 3 p e W 6 C f k Y 1 C i b 5 r N J P D U 8 o m 9 A R 7 1 m q a M S N n 8 3 P n Z E z q w x J G G t b C s l c / T 2 R 0 c i Y a R T Y z o j i 2 C x 7 u f i f 1 0 s x v P Y z o Z I U u W K L R W E q C c Y k / 5 0 M h e Y M 5 d Q S y r S w t x I 2 p p o y t A l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D z I U j 3 Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 9 O S Z x j M a 7 G e D u 3 3 7 s W P J f l g E v w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D b b T b t 0 s w m 7 E 6 G G / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H J t R K w e c J p w P 6 I j J U L B K F q p k / W D k D z N B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M G Z p x B U y S Y 3 p e W 6 C f k Y 1 C i b 5 r N J P D U 8 o m 9 A R 7 1 m q a M S N n 8 3 P n Z E z q w x J G G t b C s l c / T 2 R 0 c i Y a R T Y z o j i 2 C x 7 u f i f 1 0 s x v P Y z o Z I U u W K L R W E q C c Y k / 5 0 M h e Y M 5 d Q S y r S w t x I 2 p p o y t A l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D z I U j 3 Y = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " l h c D Q a 9 n 3 U P d 8 b 3 + e m / s w K O 8 s D k = " &gt; A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O y S 5 i d n U 2 G z D 6 c 6 Q 2 G J d / h x Y M i X v 0 Y b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l p 1 J o t O 1 v q 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 / a O s k U 4 y 2 W y E R 1 f a q 5 F D F v o U D J u 6 n i N P I l 7 / i j 2 5 n f G X O l R R I / 4 C T l X k Q H s Q g F o 2 g k z 0 U h A 5 6 7 f k i e p v 1 q z a 7 b c 5 B V 4 h S k B g W a / e q X G y Q s i 3 i M T F K t e 4 6 d o p d T h Y J J P q 2 4 m e Y p Z S M 6 4 D 1 D Y x p x 7 e X z o 6 f k z C g B C R N l K k Y y V 3 9 P 5 D T S e h L 5 p j O i O N T L 3 k z 8 z + t l G F 5 7 u Y j T D H n M F o v C T B J M y C w B E g j F G c q J I Z Q p Y W 4 l b E g V Z W h y q p g Q n O W X V 0 n 7 o u 7 Y d e f + s t a 4 K e I o w w m c w j k 4 c A U N u I M m t I D B I z z D K 7 x Z Y + v F e r c + F q 0 l q 5 g 5 h j + w P n 8 A 0 t G S H g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h c D Q a 9 n 3 U P d 8 b 3 + e m / s w K O 8 s D k = " &gt; A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O y S 5 i d n U 2 G z D 6 c 6 Q 2 G J d / h x Y M i X v 0 Y b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l p 1 J o t O 1 v q 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 / a O s k U 4 y 2 W y E R 1 f a q 5 F D F v o U D J u 6 n i N P I l 7 / i j 2 5 n f G X O l R R I / 4 C T l X k Q H s Q g F o 2 g k z 0 U h A 5 6 7 f k i e p v 1 q z a 7 b c 5 B V 4 h S k B g W a / e q X G y Q s i 3 i M T F K t e 4 6 d o p d T h Y J J P q 2 4 m e Y p Z S M 6 4 D 1 D Y x p x 7 e X z o 6 f k z C g B C R N l K k Y y V 3 9 P 5 D T S e h L 5 p j O i O N T L 3 k z 8 z + t l G F 5 7 u Y j T D H n M F o v C T B J M y C w B E g j F G c q J I Z Q p Y W 4 l b E g V Z W h y q p g Q n O W X V 0 n 7 o u 7 Y d e f + s t a 4 K e I o w w m c w j k 4 c A U N u I M m t I D B I z z D K 7 x Z Y + v F e r c + F q 0 l q 5 g 5 h j + w P n 8 A 0 t G S H g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h c D Q a 9 n 3 U P d 8 b 3 + e m / s w K O 8 s D k = " &gt; A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O y S 5 i d n U 2 G z D 6 c 6 Q 2 G J d / h x Y M i X v 0 Y b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l p 1 J o t O 1 v q 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 / a O s k U 4 y 2 W y E R 1 f a q 5 F D F v o U D J u 6 n i N P I l 7 / i j 2 5 n f G X O l R R I / 4 C T l X k Q H s Q g F o 2 g k z 0 U h A 5 6 7 f k i e p v 1 q z a 7 b c 5 B V 4 h S k B g W a / e q X G y Q s i 3 i M T F K t e 4 6 d o p d T h Y J J P q 2 4 m e Y p Z S M 6 4 D 1 D Y x p x 7 e X z o 6 f k z C g B C R N l K k Y y V 3 9 P 5 D T S e h L 5 p j O i O N T L 3 k z 8 z + t l G F 5 7 u Y j T D H n M F o v C T B J M y C w B E g j F G c q J I Z Q p Y W 4 l b E g V Z W h y q p g Q n O W X V 0 n 7 o u 7 Y d e f + s t a 4 K e I o w w m c w j k 4 c A U N u I M m t I D B I z z D K 7 x Z Y + v F e r c + F q 0 l q 5 g 5 h j + w P n 8 A 0 t G S H g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l h c D Q a 9 n 3 U P d 8 b 3 + e m / s w K O 8 s D k = " &gt; A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O y S 5 i d n U 2 G z D 6 c 6 Q 2 G J d / h x Y M i X v 0 Y b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l p 1 J o t O 1 v q 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 / a O s k U 4 y 2 W y E R 1 f a q 5 F D F v o U D J u 6 n i N P I l 7 / i j 2 5 n f G X O l R R I / 4 C T l X k Q H s Q g F o 2 g k z 0 U h A 5 6 7 f k i e p v 1 q z a 7 b c 5 B V 4 h S k B g W a / e q X G y Q s i 3 i M T F K t e 4 6 d o p d T h Y J J P q 2 4 m e Y p Z S M 6 4 D 1 D Y x p x 7 e X z o 6 f k z C g B C R N l K k Y y V 3 9 P 5 D T S e h L 5 p j O i O N T L 3 k z 8 z + t l G F 5 7 u Y j T D H n M F o v C T B J M y C w B E g j F G c q J I Z Q p Y W 4 l b E g V Z W h y q p g Q n O W X V 0 n 7 o u 7 Y d e f + s t a 4 K e I o w w m c w j k 4 c A U N u I M m t I D B I z z D K 7 x Z Y + v F e r c + F q 0 l q 5 g 5 h j + w P n 8 A 0 t G S H g = = &lt; / l a t e x i t &gt; Fake True h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R Q N D T D B 3 s 8 I D 5 s 5 Z a 6 X G x m G k J n E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V T F t o Q 9 l s N + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 w 8 j M h 5 U a 2 7 d X Y C s E 6 8 g N S j Q G l S / + s O E Z T F X y C Q 1 p u e 5 K Q Y 5 1 S i Y 5 L N K P z M 8 p W x C R 7 x n q a I x N 0 G + O H Z G L q w y J F G i b S k k C / X 3 R E 5 j Y 6 Z x a D t j i m O z 6 s 3 F / 7 x e h t F N k A u V Z s g V W y 6 K M k k w I f P P y V B o z l B O L a F M C 3 s r Y W O q K U O b T 8 W G 4 K 2 + v E 7 a V 3 X P r X s P 1 7 X m b R F H G c 7 g H C 7 B g w Y 0 4 R 5 a 4 A M D A c / w C m + O c l 6 c d + d j 2 V p y i p l T + A P n 8 w d Q V I 5 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R Q N D T D B 3 s 8 I D 5 s 5 Z a 6 X G x m G k J n E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V T F t o Q 9 l s N + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 w 8 j M h 5 U a 2 7 d X Y C s E 6 8 g N S j Q G l S / + s O E Z T F X y C Q 1 p u e 5 K Q Y 5 1 S i Y 5 L N K P z M 8 p W x C R 7 x n q a I x N 0 G + O H Z G L q w y J F G i b S k k C / X 3 R E 5 j Y 6 Z x a D t j i m O z 6 s 3 F / 7 x e h t F N k A u V Z s g V W y 6 K M k k w I f P P y V B o z l B O L a F M C 3 s r Y W O q K U O b T 8 W G 4 K 2 + v E 7 a V 3 X P r X s P 1 7 X m b R F H G c 7 g H C 7 B g w Y 0 4 R 5 a 4 A M D A c / w C m + O c l 6 c d + d j 2 V p y i p l T + A P n 8 w d Q V I 5 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R Q N D T D B 3 s 8 I D 5 s 5 Z a 6 X G x m G k J n E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V T F t o Q 9 l s N + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 w 8 j M h 5 U a 2 7 d X Y C s E 6 8 g N S j Q G l S / + s O E Z T F X y C Q 1 p u e 5 K Q Y 5 1 S i Y 5 L N K P z M 8 p W x C R 7 x n q a I x N 0 G + O H Z G L q w y J F G i b S k k C / X 3 R E 5 j Y 6 Z x a D t j i m O z 6 s 3 F / 7 x e h t F N k A u V Z s g V W y 6 K M k k w I f P P y V B o z l B O L a F M C 3 s r Y W O q K U O b T 8 W G 4 K 2 + v E 7 a V 3 X P r X s P 1 7 X m b R F H G c 7 g H C 7 B g w Y 0 4 R 5 a 4 A M D A c / w C m + O c l 6 c d + d j 2 V p y i p l T + A P n 8 w d Q V I 5 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R Q N D T D B 3 s 8 I D 5 s 5 Z a 6 X G x m G k J n E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V T F t o Q 9 l s N + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 w 8 j M h 5 U a 2 7 d X Y C s E 6 8 g N S j Q G l S / + s O E Z T F X y C Q 1 p u e 5 K Q Y 5 1 S i Y 5 L N K P z M 8 p W x C R 7 x n q a I x N 0 G + O H Z G L q w y J F G i b S k k C / X 3 R E 5 j Y 6 Z x a D t j i m O z 6 s 3 F / 7 x e h t F N k A u V Z s g V W y 6 K M k k w I f P P y V B o z l B O L a F M C 3 s r Y W O q K U O b T 8 W G 4 K 2 + v E 7 a V 3 X P r X s P 1 7 X m b R F H G c 7 g H C 7 B g w Y 0 4 R 5 a 4 A M D A c / w C m + O c l 6 c d + d j 2 V p y i p l T + A P n 8 w d Q V I 5 Y &lt; / l a t e x i t &gt;h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q F H q H 2 v Q 9 s B r w T y S A i T D c j r s / X w = " &gt; A A A B 9 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y m a 7 a Z d u N n F 3 U i i h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U y k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P R 3 c x v j 7 k 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a 6 j D P T S g C Q y e 4 B l e 4 c 0 Z O y / O u / O x a F 1 z i p k T + A P n 8 w e 3 u Z I O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q F H q H 2 v Q 9 s B r w T y S A i T D c j r s / X w = " &gt; A A A B 9 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y m a 7 a Z d u N n F 3 U i i h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U y k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P R 3 c x v j 7 k 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a 6 j D P T S g C Q y e 4 B l e 4 c 0 Z O y / O u / O x a F 1 z i p k T + A P n 8 w e 3 u Z I O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q F H q H 2 v Q 9 s B r w T y S A i T D c j r s / X w = " &gt; A A A B 9 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y m a 7 a Z d u N n F 3 U i i h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U y k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P R 3 c x v j 7 k 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 6 j D P T S g C Q y e 4 B l e 4 c 0 Z O y / O u / O x a F 1 z i p k T + A P n 8 w e 3 u Z I O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q F H q H 2 v Q 9 s B r w T y S A i T D c j r s / X w = " &gt; A A A B 9 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 8 V j B f k A T y m a 7 a Z d u N n F 3 U i i h v 8 O L B 0 W 8 + m O 8 + W / c t j l o 6 4 O B x 3 s z z M w L U y k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P R 3 c x v j 7 k 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>a 6 j D P T S g C Q y e 4 B l e 4 c 0 Z O y / O u / O x a F 1 z i p k T + A P n 8 w e 3 u Z I O &lt; / l a t e x i t &gt; e k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; (a) FQ-GAN architecture (b) Dictionary look-up Illustration of FQ-GAN: (a) The neural network architecture. A feature quantization (i.e., dictionary look-up) step fQ is injected into the discriminator of the standard GANs. (b) A visualization example of dictionary E and the look-up procedure. Each circle " " indicates a quantization centroid. The true sample features h (" ") and fake sample featuresh (" ") are quantized into their nearest centroids e k (represented in the same color in this example), and thus performing implicit feature matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Feature</head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e PI = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 3 T x 4 Z R C b h 2 A u G j 4 E D D C 8 I 7 c e P I = " &gt; A A A B 8 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z E O S E G Y n v c m Q m d l l Z l Y I S 7 7 C i w d F v P o 5 3 v w b J 8 k e N L G g o a j q p r s r T A Q 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m j j V D B s s F r F u h 9 S g 4 A o b l l u B 7 U Q j l a H A V j i + n f m t J 9 S G x + r B T h L s S T p U P O K M W i c 9 Z t 0 w I j j t j / v l i l / 1 5 y C r J M h J B X L U + + W v 7 i B m q U R l m a D G d A I / s b 2 M a s u Z w G m p m x p M K B v T I X Y c V V S i 6 W X z g 6 f k z C k D E s X a l b J k r v 6 e y K g 0 Z i J D 1 y m p H Z l l b y b + 5 3 V S G 1 3 3 M q 6 S 1 K J i i 0 V R K o i N y e x 7 M u A a m R U T R y j T 3 N 1 K 2 I h q y q z L q O R C C J Z f X i X N i 2 r g V 4 P 7 y 0 r t J o + j C C d w C u c Q w B X U 4 A 7 q 0 A A G E p 7 h F d 4 8 7 b 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w + T 5 Z A / &lt; / l a t e x i t &gt; f Q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 2 f n w p W d C y U s m q / m d u f I + a d A V d 4 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j 0 4 j E B 8 5 A k h N l J b z J k Z n a Z m R X C k q / w 4 k E R r 3 6 O N / / G S b I H T S x o K K q 6 6 e 4 K E 8 G N 9 f 1 v b 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q h g 0 W i 1 i 3 Q 2 p Q c I U N y 6 3 A d q K R y l B g K x z f z f z W E 2 r D Y / V g J w n 2 J B 0 q H n F G r Z M e o 3 7 W 1 Z L U p / 1 S 2 a / 4 c 5 B V E u S k D D l q / d J X d x C z V K K y T F B j O o G f 2 F 5 G t e V M 4 L T Y T Q 0 m l I 3 p E D u O K i r R 9 L L 5 w V N y 7 p Q B i W L t S l k y V 3 9 P Z F Q a M 5 G h 6 5 T U j s y y N x P / 8 z q p j W 5 6 G V d J a l G x x a I o F c T G Z P Y 9 G X C N z I q J I 5 R p 7 m 4 l b E Q 1 Z d Z l V H Q h B M s v r 5 L m Z S X w K 0 H 9 q l y 9 z e M o w C m c w Q U E c A 1 V u I c a N I C B h G d 4 h T d P e y / e u / e x a F 3 z 8 p k T + A P v 8 w e R R p A 9 &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Ablation studies on the impact of hyper-parameters. The image generation quality is measured with FID ↓ and IS ↑. (a) Dictionary size K = 2 P . (b) The positions to apply FQ to discriminator, layer ID is shown on the horizontal axis. (c) The decay hyper-parameter λ in dictionary update. (d) The weight α to incorporate FQ, the dashed horizon lines are standard GAN baseline α = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Learning curves on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Comparison on per-class metrics for ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison. The 1st, 2nd and 3rd shows source and the translated images using U-GAT-IT and FQ, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FeatureFigure 8 .Figure 15 .Figure 16 .Figure 17 .FeatureFigure 18 .</head><label>815161718</label><figDesc>Ablation studies on the impact of hyper-parameters. The image generation quality is measured with FID ↓ and IS ↑. (a) selfie2animeanime2selfie Visual comparisons on selfie2anime and anime2selfie. First row: input images. Second row: images generate by U-GAT-IT. Third row: images generated by FQ-U-GAT-IT.cat2dogdog2cat Visual comparisons on cat2dog and dog2cat. First row: input images. Second row: images generated by U-GAT-IT. Third row: images generated by FQ-U-GAT-IT.photo2portraitportrait2photo Visual comparisons on photo2portrait and portrait2photo. First row: input images. Second row: images generated by U-GAT-IT. Third row: images generated by FQ-U-GAT-IT. Visual comparisons on vangogh2photo and photo2vangogh. First row: input images. Second row: images generated by U-GAT-IT. Third row: images generated by FQ-U-GAT-IT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Randomly initializing the parameters of generator g θ , discriminator f ω , and dictionary E for a number of training iterations do # Produce a minibatch of true and fake samples</figDesc><table><row><cell>Feature Quantization Improves GAN Training</cell></row><row><cell>Algorithm 1 Feature Quantization GAN</cell></row><row><cell>Require: Sample z ∼ p(z) and true samples x ∼ q(x); Forward z to generate fake samplesx = g θ (z);</cell></row><row><cell># Feature quantization &amp; Dictionary learning</cell></row><row><cell>Forward samples {x,x} using (6), and produce h; Feature quantization using (5); Momentum update of dictionary E using (8);</cell></row><row><cell># Update discriminator Compute gradient ∂LFQGAN ∂ω Update ω via gradient ascent; of (9);</cell></row><row><cell># Update generator Compute gradient ∂LFQGAN ∂θ Update θ via gradient descent; of (9); end for</cell></row><row><cell>A dynamic &amp; consistent dictionary The evolution of the generator during GAN training poses a continual learning problem for the discriminator</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison on ImageNet-1000 for two resolutions. Both models were trained for 256K iterations if not diverge early. The top and bottom block shows the best results within half and full of the entire training procedure, respectively. ‡ from (Gong et al., 2019), † from (Brock et al., 2018), we cannot reproduce it using their codebase, as the training diverges early. except for adding FQ layers. Best scores (FID* / IS*) and averaged scores (FID / IS) are reported. Standard deviations are computed over five random initializations and their average are reported from the best in each run.</figDesc><table><row><cell>CIFAR-10 (Krizhevsky et al., 2009) consists of 60K im-ages at resolution 32×32 in 10 classes; 50K for training and 10K for testing. 500 epochs are used. The results are show in Table 1. The FQ module improves BigGAN. FQ-BigGAN also outperforms other strong existing GAN models, includ-ing spectral normalization (SN) GAN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Experimental settings to achieve these results are provided in A.2. The proposed FQ can improve both TAC-GAN and BigGAN. In particular, FQ significantly improves BigGAN on CIFAR-100 dataset. This is because FQ can increase intra-class diversity, as the dictionary items store a longer distribution history. We show</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Training time comparison of before and after adding FQ module. TITAN XP GPUs are used in these experiments. We train ImageNet (64 × 64) for 256k iterations on 2 GPUs, and CIFAR-100 and CIFAR-10 on 1 GPU for 10k iterations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>StyleGAN: Best FID-50k scores in FFHQ at different resolutions.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Feature Quantization Improves GAN Training</cell></row><row><cell>Resolution</cell><cell>32 2</cell><cell cols="2">64 2 128 2 1024 2</cell></row><row><cell cols="3">StyleGAN FQ-StyleGAN 3.01 4.36 5.98 3.28 4.82 6.33</cell><cell>5.24 4.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>± 0.59 10.44 ± 0.67 8.15 ± 0.48 1.20 ± 0.31 4.26 ± 0.29 CycleGAN 13.08 ± 0.49 8.05 ± 0.72 8.92 ± 0.69 1.84 ± 0.34 5.46 ± 0.33 U-GAT-IT 11.61 ± 0.57 7.06 ± 0.8 7.07 ± 0.65 1.79 ± 0.34 4.28 ± 0.33 FQ-U-GAT-IT 11.40 ± 0.28 2.93 ± 0.36 6.44 ± 0.35 1.09 ± 0.17 6.54 ± 0.18 KID ⇥100 for different image translation datasets. All numbers except for our FQ variant are from<ref type="bibr" target="#b20">(Kim et al., 2020)</ref>.</figDesc><table><row><cell>The FQ module is effective in per-forming implicit feature matching for large datasets. FQ can be easily used in training many existing GAN models, and improve their performance. It yields improved performance on three canonical tasks, including BigGAN for image gen-eration on ImageNet, StyleGAN and StyleGAN2 for face generation on FFHQ, and unsupervised image-to-image translation. FQ-GAN sets new state-of-the-art performance on most datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>KID ⇥100 for different image translation datasets. All numbers except for our FQ variant are from(Kim    </figDesc><table><row><cell>Model</cell><cell>baseline</cell><cell>FQ</cell></row><row><cell>selfie2anime horse2zebra cat2dog photo2portrait photo2vangogh</cell><cell>44.7 36.2 34.0 42.5 48.8</cell><cell>55.3 63.8 66.0 57.5 51.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ajbrock/BigGAN-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/taki0112/UGATIT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors gratefully acknowledge Yanwu Xu for preparing the TAC-GAN codebase, and Yulai Cong for proofreading the draft. We are also grateful to the entire Philly Team inside Miscrosoft for providing our computing platform.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Which player/layer to add FQ? The proposed FQ module can be plugged into either a generator or a discriminator. We found that the performance does not change much when used in the generator. For example, FID is 9.01 ±.44 and 8.96 ±.26 before and after adding FQ, respectively. For the discriminator, we place FQ at different positions of the network, and show the results in <ref type="figure">Figure 4</ref> (b). Multiple FQ layers can generally outperform a single FQ layer.</p><p>Momentum decay λ. Note that λ determines how much recent history to incorporate when constructing the dictionary. Larger values consider more history. Our experimental results in <ref type="figure">Figure 4</ref> (c) show that λ = 0.9 is a sweet point to balance the current and historical statistics.</p><p>FQ weight α. The impact of weighting hyper-parameter α for FQ in (9) is studied in <ref type="figure">Figure 4</ref> (d). Adding FQ can immediately improve the baseline by a large margin. Larger α can further decrease FID while keeping IS values almost unchanged. We used α = 1 for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">BigGAN for Image Generation</head><p>BigGAN <ref type="bibr" target="#b2">(Brock et al., 2018)</ref> holds the state-of-the-art on the task of class-conditional image synthesis, which benefits from scaling up model size and batch size. Our implementation of BigGAN is based upon BigGAN-PyTorch 2 . We use the same architecture and experimental settings as BigGAN,  Feature Quantization Improves GAN Training </p><p>ReLU, Global sum pooling  ResBlock up 16ch − → 8ch</p><p>ResBlock up 8ch − → 4ch</p><p>ResBlock up 4ch − → 2ch</p><p>Non-Local Block (64 × 64)</p><p>Non-Local Block (64 × 64)</p><p>ResBlock down 8ch − → 16ch</p><p>ResBlock down 16ch − → 16ch</p><p>ResBlock 16ch − → 16ch</p><p>ReLU, Global sum pooling </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gan memory with no forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11810</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new vector quantization clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Equitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Many paths to equilibrium: Gans do not need to decrease a divergence at every step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08446</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twin auxilary classifiers GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Assp Magazine</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Feature Quantization Improves GAN Training</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of styleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">U-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Memorization precedes generation: Learning unsupervised GANs with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stability</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ALICE: Towards understanding adversarial learning for joint distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial learning of a sampler based on an unnormalized distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generative adversarial network training is a continual learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11083</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised imageto-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08398</idno>
		<title level="m">Mean and covariance feature matching gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00821</idno>
		<title level="m">Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Feature Quantization Improves GAN Training</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Distance learning in discriminative vector quantization. Neural computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Amortised MAP inference for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving MMD-GAN training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halgamuge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<title level="m">Selfattention generative adversarial networks. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12027</idno>
		<title level="m">Consistency regularization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On leveraging pretrained gans for limited-data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11810</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic memory generative adversarial networks for textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dm-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
