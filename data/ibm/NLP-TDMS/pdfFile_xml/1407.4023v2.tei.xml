<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aggregate Channel Features for Multi-view Face Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Biometrics and Security Research</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Pattern Recognition Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>jjyan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Biometrics and Security Research</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Pattern Recognition Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<email>zlei@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Biometrics and Security Research</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Pattern Recognition Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Biometrics and Security Research</orgName>
								<orgName type="institution" key="instit1">National Laboratory of Pattern Recognition Institute of Automation</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aggregate Channel Features for Multi-view Face Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multiscale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in Viola-Jones framework, the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB testsets, while runs at 42 FPS on VGA images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction. In the past decade, the most influential work should be the face detection framework proposed by Viola and Jones <ref type="bibr" target="#b21">[22]</ref>. The Viola-Jones (abbreviated as VJ below) framework uses rectangular Haar-like features and learns the hypothesis using Adaboost algorithm. Combined with the attentional cascade structure, the VJ detector achieved real-time face detection at that time. Despite the great success of the VJ detector, the performance is still far from satisfactory due to the large appearance variance of faces in unconstrained settings. To handle faces in the wild, many subsequences of VJ framework merged. These methods mainly get the performance gains in two aspects, more complicated features <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> and (or) more powerful learning algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25]</ref>. As the combination of boosting and cascade has been proven to be quite effective in face detection, the bottleneck lies in the feature representation since complicated features adopted in the above literatures bring about limited performance gains at the cost of large computation cost.</p><p>Lately in another domain of pedestrian detection, a family of channel features has achieved record performances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. Channel features compute registered maps of the original images like gradients and histograms of oriented gradients and then extract features on these extended channels. The classifier learning process follows the VJ framework pipeline. In this paper, we adopt a variant of channel features called aggregate channel features <ref type="bibr" target="#b4">[5]</ref>, which are extracted directly as pixel values on subsampled channels. Channel extension offers rich representation capacity, while simple feature form guarantees fast computation. With these two superiorities, the aggregate channel features break through the bottleneck in VJ framework and have the potential to make great advance in face detection.</p><p>As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper, we not only just adopt the aggregate channel features in face detection, but also try to explore the full potential of this novel representation. To do so, we make a deep and all-round investigation into the specific feature parameters concerning channel types, feature pool size, subsampling method, feature scale and so on, which gives insights into the feature design and hopefully provides helpful guidelines for practitioners. Through the deep exploration, we find that: 1) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale; 2) different combinations of channel types impact the performance greatly, while for face detection the color channel in LUV space, plus gradient magnitude channel and gradient histograms channels in RGB space show best result; 3) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Although multi-view detection could effectively deal with diverse poses, additional issues come up as how to merge detections output by separately trained subview detectors, and how to deal with the offsets of location and scale between output detections and ground-truth. We solve these problems by carefully designed post-processing including score re-ranking, detection merging and bounding box adjustment.</p><p>The detailed experimental exploration of aggregate channel features, along with our improvements on multiview detection, leads to large performance gain in face detection in the wild. On two challenging face databases, AFW and FDDB, the proposed multi-view face detector shows competitive performance against state-of-the-art detectors in both detection accuracy and speed.</p><p>The remaining parts of this paper are organized as follows. Section 2 revisits related work in face detection. Section 3 describes how we build the face detector using aggregate channel features. Section 4 addresses problems concerning multi-view face detection. Experimental results on AFW and FDDB are shown in section 5 and we conclude the paper in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Face detection has drawn much attention since the early time of computer vision. Although many solutions had been put forward, it was not until Viola and Jones <ref type="bibr" target="#b21">[22]</ref> proposed their milestone work that face detection saw surprising progress in the past decades. The VJ face detector features in three aspects: fast feature computation via integral image representation, classifier learning using Adaboost, and the attentional cascade structure. One main drawback of the VJ framework is that the features have limited repre-sentation capacity, while the feature pool size is quite large to compensate for that. Typically, in a 24 × 24 detection window, the number of Haar-like features is 160,000 <ref type="bibr" target="#b21">[22]</ref>. To address the problem, efforts are made in two directions. Some focus on more complicated features like HoG <ref type="bibr" target="#b25">[26]</ref>, SURF <ref type="bibr" target="#b12">[13]</ref>. Some aim to speed up the feature selection in a heuristic way <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref>. However, the problem hasn't been solved perfectly. In this paper, we mainly focus on the feature representation part and make a deep exploration into it, which is complementary to existing work on the learning algorithm and classifier structure in the VJ framework.</p><p>Recently channel features have been proposed and shown record performance in pedestrian detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. Due to the channel extension to diverse types like gradients and local histograms, the features show richer representation capacity for classification. However, the features are extracted as rectangular sums at various locations and scales which we believe leads to a redundant feature pool. During preparation of this paper, Mathias et al. <ref type="bibr" target="#b15">[16]</ref> independently discover the effectiveness of integral channel features in face detection domain. In this paper, we adopt a novel variant of channel features called aggregate channel features, which extract features directly as pixel values in extended channels without computing rectangular sums at various locations and scales. The feature has powerful representation capacity and the feature pool size is only several thousands. Through careful design in section 3 and implementation of multi-view detection in section 4, the aggregate channel features based detector achieves state-of-theart performance on challenging databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed face detector</head><p>In this section, we make a full exploration of the aggregate channel features in the context of face detection. We first give a brief introduction of the feature itself, including its computation, properties and advantages over traditional Haar-like features used in VJ framework. Then the detailed experimental investigation is described in two parts, feature design and training design. Before that, some guidelines concerning how we conduct the investigation are demonstrated. Each design part is divided into several separate experiments ended with a summary explaining the specific parameters used in our proposed face detector. Note that each experiment focuses on only one parameter and the others remain constant. Through the well-designed experiments, the proposed face detector based on aggregate channel features is built step by step. Issues concerning the implementation of multi-view face detection which further improves the performance are discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature description</head><p>Channel extension: The basic structure of the aggregate channel features is channel. The application of channel has a long history since digital images were invented. The most common type of channel should be the color channels of the image, with Gray-scale and RGB being typical ones. Besides color channels, many different channel types have been invented to encode different types of information for more difficult problems. Generally, channels can be defined as a registered map of the original image, whose pixels are computed from corresponding patches of original pixels <ref type="bibr" target="#b5">[6]</ref>. Different channels can be computed with linear or non-linear transformation of the original image. To allow for sliding window detection, the transformations are constrained to be translationally invariant.</p><p>Feature computation: Based on the definition of channels, the computation of aggregate channel features is quite simple. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, given a color image, all defined channels are computed and subsampled by a preset factor. The aggregate pixels in all subsampled channels are then vectorized into a pixel look-up table. Note that an optional smoothing procedure can be done on each channel with a binomial filter both before computation and after subsampling.</p><p>Classifier learning: The learning process is quite simple. Two changes are made compared with VJ framework. First is that weak classifier is changed from decision stump to depth-2 decision tree. The more complex weak classifier shows stronger ability in seeking the discriminant intra and inter channel correlations for classification <ref type="bibr" target="#b14">[15]</ref>. Second difference is that soft-cascade <ref type="bibr" target="#b0">[1]</ref> structure is used. Unlike the attentional cascade structure in VJ framework which has several cascade stages, a single-stage classifier is trained on the whole training data and a threshold is then set after each weak classifier picked by Adaboost. These two changes lead to more efficient training and detection.</p><p>Overall superiority: Compared with traditional Haarlike features used in VJ framework, aggregate channel features have the following differences and advantages: 1) The image channels are extended to more types in order to encode diverse information like color, gradients, local histograms and so on, therefore possess richer representation capacity. 2) Features are extracted directly as pixel values on downsampled channels rather than computing rectangu-lar sums with various locations and scales using integral images, leading to a faster feature computation and smaller feature pool size for boosting learning. With the help of cascade structure, detection speed is accelerated more. 3) Due to its structure consistence with the overall image, when coupled with boosting method, the boosted classifier naturally encodes structured pattern information from large training data (see <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration), which gives more accurate localization of faces in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Investigation guidelines</head><p>All investigations are trained on the AFLW face database 1 <ref type="bibr" target="#b9">[10]</ref> and tested on the Annotated Faces in the Wild (AFW) testset 2 . To make it clear, there are in total 36, 112 positive samples and 108, 336 negative samples selected from AFLW which are kept constant in all investigations. Testset contains 205 natural images with faces that vary a lot in pose, appearance and illumination.</p><p>To alleviate the ground-truth offset caused by different annotation styles <ref type="figure">(Figure 4</ref>) in training and testing set and make the evaluation more comparable, a lower Jaccard index 3 with threshold 0.3 is adopted in comparative evaluation. Practically the lower threshold won't cause errors being mistakenly corrected. Note that in final evaluation of the proposed face detector (section 5), the AFW testset, together with another face benchmark FDDB database, are used as testbed and the evaluation metric follows the database protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature design</head><p>To fully exploit the power of aggregate channel features in face detection domain, a deep investigation into the design of the feature is done mainly on channel types, window size, subsampling method and feature scale. Results of comparative experiments are shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>Channel types: Three types of channels are used, which are color channel (Gray-scale, RGB, HSV and LUV), gradient magnitude, and gradient histograms. The computation of the latter two channel types could be seen as a generalized version of HoG features. Specifically, gradient magnitude is the biggest response on all three color channels, and oriented gradient histograms follow the idea of HoG in that: 1) rectangular cell size in HoG equals the subsampling factor in aggregated channel features; 2) each orientation bin results in one feature channel (6 orientation bins are used in this paper). <ref type="figure" target="#fig_4">Figure 6</ref> (a)˜(c) show how much each of these three types alone contributes to the performance of face detection. It can be seen that the gradient histograms contribute most to the performance among all three channel types. <ref type="figure" target="#fig_4">Figure 6</ref> (d) shows the performances of combinations of these three types computed on different color channels.</p><p>Detection window size: Detection window size is the scale to which we resize all face and non-face samples and then train our detector. Larger window size includes more pixels in feature pool and thus may improve the face detection performance. On the other hand, too large window will miss some small faces and diminish the detection efficiency. <ref type="figure" target="#fig_4">Figure 6</ref> (e) shows comparison of window size ranging from 32 to 112 with a stride of 16 pixels.</p><p>Subsampling: The factor for subsampling can be regarded as the perceptive scale for that it controls the scale at which the aggregation is done. Changing the factor from large to small leads to the feature representation shifting from coarse to fine and the feature pool size getting bigger. Experiments on different subsampling factors are shown in <ref type="figure" target="#fig_4">Figure 6</ref> (f). In original aggregate channel features, the way to do subsampling is average pooling. Following the idea in Convolutional Neural Networks, another two ways of subsampling, max pooling and stochastic pooling <ref type="bibr" target="#b23">[24]</ref> are tested in <ref type="figure" target="#fig_4">Figure 6</ref> (g).</p><p>Smoothing: As described in feature description, both pre and post smoothing is done in default setting of aggregate channel features. A binomial filter with a radius of 1 is used for smoothing. The smoothing procedure also has a great influence on the scale of the feature representation. Concretely, pre-smoothing determines how far the local neighborhood is in which local correlations are encoded before channel computation, while post-smoothing determines the neighborhood size in which the computed channel features are integrated with each other. In <ref type="bibr" target="#b5">[6]</ref>, the former corresponds to the 'local scale' of the feature, while the latter represents the 'integration scale'. We vary the filter radius used in pre and post smoothing and find that both using a radius of 1 gets the best results. <ref type="figure" target="#fig_4">Figure 6</ref> (h)˜(i) present the comparative results.</p><p>Multi-scale: In aggregate channel features, although hidden information at different scale could be extracted at a cost of more weak classifiers, it would be better to make the integrated channel features multi-scaled and thus make themselves more discriminant. Therefore the same or better classification performance can be achieved with fewer weak classifiers. In this part, we implement three multiscale version of aggregate channel features in the aforementioned three kinds of scale, perceptive scale (subsampling), local scale (pre-smoothing) and integration scale (post-smoothing) and compare their performaces. See results in <ref type="figure" target="#fig_4">Figure 6</ref> (j)˜(l).</p><p>Summary: The color channel, gradient magnitude and gradient histograms prove themselves a good match in aggregate channel features. However, different choices of color channel used and on which gradients are computed have a great impact on performance. According to the ex-periments, LUV channel and gradient magnitude and 6-bin histograms computed on RGB color space (in total 10 channels) are the best choice for face detection.</p><p>Larger detection window size generally gets better performance, but will miss many small faces in testing and lead to inefficient detection. In this work, we set the size to 80 × 80 as its optimal performance.</p><p>A subsampling factor of 4 is most reasonable according to the experiments, while different pooling methods show small differences. However, max pooling and stochastic pooling are much slower than average pooling, therefore the average pooling becomes the best match for the sake of efficiency. In this way, the resulting feature pool size of our face detector is (80/4) × (80/4) × 10 = 4000, considerably smaller than that in VJ framework.</p><p>As for multi-scale version of aggregate channel features, multi-local-scale with an additional scale of radius 2 shows the best performance. The probable reason is that pre-smoothing controls the local scale of the neighborhood feature correlations and therefore matches the intuition inside multi-scale best. Compared with other fine-tuning, the multi-scale version has a notable performance gain for that it makes up for the scale uniformity caused by subsampling to some extent. One main drawback is that it doubles the feature pool size and as a result slows down the detection speed somewhat. Based on the trade-off, we implement two face detectors with different scale settings, one is singlescaled with faster speed and the other is multi-scaled with better accuracy. We evaluate and discuss the performances of these two versions in detail in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training design</head><p>Besides careful design of the aggregate channel features, experiments on the training process which is similar to that in VJ framework are also carried out. The differences are that the weak classifier is changed into depth-2 decision tree and soft-cascade <ref type="bibr" target="#b0">[1]</ref> structure is used. Details of the training design are as follows.</p><p>Number of weak classifiers: Given a feature pool size of 4, 000, we vary the number of weak classifiers contained in the soft-cascade. In <ref type="figure" target="#fig_2">Figure 3</ref> performances of various numbers of weak classifiers ranging from 32 to 8192 are displayed, which shows that apparently more classifiers generate better performance, and when the number gets larger the performance begins to saturate. Since more classifiers slow down the detection speed, there's a tradeoff between accuracy and speed. Searching for the saturate point as the optimal is significant during training in such framework.</p><p>Training data: Empirically, more training data will get better performance given powerful representation capacity. In this case, AFLW database is used as the only positive training data. However, as images in AFLW database are very salient and the background has very less variance, negative samples cropped from the AFLW database can't represent the real world scenario well, which limits the face detection performance in the wild. In this part, we further use PASCAL VOC 2007 database and randomly crop windows from images without person as the new negative samples. Experiments show that the new training data containing cluttered background significantly improve the performance with 4.1%.</p><p>Summary: Based on observations above, we choose 2048 as the number of weak classifiers contained in the soft cascade. As each weak classifier is a depth-2 decision tree, it takes only two comparing operations to apply a weak classifier, which is quite fast. During training, as negative data is large, we adopt a standard Bootstrap procedure to sample hard negative samples from PASCAL VOC 2007 in the implementation of the proposed face detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-view detection</head><p>Human faces in real world usually have highly varied poses. In AFLW database, the human pose is divided into three aspects: 1 in-plane rotation 'roll' and 2 out-of-plane rotations 'yaw' and 'pitch'. Because of this large variance in face pose, it is difficult to train a single view face detector to handle all the poses effectively. A multi-view detection is further examined in this part. Due to the adoption of soft-cascade structure, a multi-view version of face detector won't cause too much computation burden. Typically, we divide the out-of-plane rotation yaw into different views and let the classifier itself tolerate the pose variance in the other two types of rotations.</p><p>Adopting multi-view detection also brings about many troublesome issues. If handled improperly, the performance will differ greatly. First, detectors of different view will each produce a set of candidate positive windows followed with a set of confidence scores. For application purpose, we need to merge these detections from different views and also remove duplicated windows. A typical approach is Non-Maximum Suppression (NMS) <ref type="bibr" target="#b2">[3]</ref>. An issue rises on how to compare confidence scores from different classifiers and how to do window merging in the trade-off between high <ref type="figure">Figure 4</ref>. Illustration of different ground-truth annotation styles in databases, the view partition and symmetric detection adjustment. Rectangles with red and green color correspond to detections before and after adjustment. precision rate and high detection rate. Second, as for detection evaluation, usually the overlap of bounding boxes is used as the criterion. However, annotations in different data sets may not have a consistent style <ref type="figure">(Figure 4 (a)</ref>). This diversity suffers more in profile faces. Since our face detector is trained and tested on different data sets, this issue impacts the performance a lot. Third, detectors of different views need to be trained with different samples separately. How to divide the views therefore becomes another concerning problem. In this section, we address the above three issues successfully by careful designs and therefore fully exploit the advantage of multi-view detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">View partition</head><p>In the scenario of detecting faces in the wild, pose variation caused by yaw is usually severer than pitch and roll. Therefore we divide the faces in AFLW database according to yaw angle. We have 6 subviews which are horizontally symmetric (see <ref type="figure">Figure.</ref> 4 (b)) because we flip each image in the training set. Specifically, there are 6630, 8446, 9610, 9610, 8446, 6630 images in views from 1 to 6. Benefitting from the symmetry of our model, we can only train three subview detectors of the right side for simplicity, and use these trained right-side detectors to generate the left-side detectors. Detections of all six detectors are then merged to get the final detections. Though multi-view detection significantly improves the detection performance (especially the recall rate), the post-processing of detections from different detectors becomes a trouble. If handled improperly, the performance degrades a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Post-processing</head><p>Difficulties in the post-processing of multi-view detection mainly reflect on the following aspects: 1) different score distributions and; 2) different bounding box styles.</p><p>Concretely, as each subview detector is trained separately, their output confidence scores usually have different distributions. What's more, due to the annotation rule in the AFLW database that the face's nose is approximately at the center location of the bounding box ground-truth, as the subview changes, the bounding box shifts. This bounding box offset causes difficulty both in detection merging and final evaluation using Jaccard index metric. To solve these annoying issues and make the best use of multi-view detection, we introduce the following methods for postprocessing.</p><p>Score re-ranking: We propose the following three kinds of score re-ranking: 1) normalizing scores of different views to [0, 1]; 2) defining a new score that has uniform distribution and; 3) taking overlapping detections into consideration. N ormalization: After training a classifier, calculate the output range of the classifier and use the range to do normalization later so that output score has a range of [0, 1]. N ewScore: Originally, each weak classifier in the softcascade owns a score and final score is the sum of all scores. Instead, we use the number of weak classifier that the image patch passed positively as the new score. Therefore the upper limit of the new score is 2048 in our case. OverlapRerank: Given an image, multiple detections from multi-view detectors exist each with a score. For each detection, we first calculate the number of overlapped detection it has (overlap threshold is 0.65) and then multiply score of each detection with a factor of its overlapping number ranking 1 . Sumof Overlap: Instead of using overlapping as a multiply factor, here we use the sum of overlapped detections' scores as the current detection's new score.</p><p>Detection merging: Apart from the Greedy * version of Non Maximum Suppression <ref type="bibr" target="#b5">[6]</ref>, we also use the detection combination introduced in <ref type="bibr" target="#b19">[20]</ref>. It averages the locations of overlapped detections rather than suppresses them.</p><p>Detection adjustment: As shown in <ref type="figure">Figure 4</ref> (a), different databases have different annotation styles of groundtruth. Specifically, AFLW has square annotations with nose located approximately at the center. AFW uses tight rectangular bounding boxes as annotations with the eye-brow being the approximate upper bound. FDDB uses elliptical annotations bounding the whole head. As our detector is trained on AFLW and tested on AFW and FDDB, there exist offsets in both detection position and scale. According to observations, the offsets vary as face pose changes. Therefore we adopt a view-specific detection adjustment to alleviate the offsets. Note that the adjustment is constant for all images and faces in the same database, see <ref type="figure">Figure 4</ref> (b) for 1 A toy example: Det1: score: 10, nOverlap: 10; Det2: score: 9, nOverlap: 20; Det3: score: 5, nOverlap: 5. After score re-ranking: Det1: score: 10 × 2 3 = 6.67; Det2: score: 9 × 3 3 = 9; Det3: score: 5 × </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>details.</head><p>Summary: According to experimental results <ref type="table">(Table 1)</ref>, OverlapRerank seems to be the best score re-ranking method. The underlying reason may be that true positives usually have many overlapped detections, while the false positives would only get a few responses. Therefore leveraging this overlapping information in score re-ranking can reduce many false positives. However, in practice, overlap related methods and detection combination both cost much time to process, which is infeasible in a large majority of applications. We finally adopt N ormalization score reranking combined with Greedy * Non Maximum Suppression for the sake of detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we compare our method with state-ofthe-art methods on AFW and FDDB databases which contain challenging faces in the wild. In AFW, we compare with three commercial systems (Google Picasa, Face.com and Face++) and five academic methods (Shen et al. <ref type="bibr" target="#b20">[21]</ref>, Zhu et al. <ref type="bibr" target="#b26">[27]</ref>, DPM <ref type="bibr" target="#b6">[7]</ref>, multiHOG <ref type="bibr" target="#b26">[27]</ref> and Kalal et al. <ref type="bibr" target="#b8">[9]</ref>). In FDDB, we compare with one commercial system (Olaworks) and six academic methods (Yan et al. <ref type="bibr" target="#b22">[23]</ref>, Boosted Exemplar et al. <ref type="bibr" target="#b11">[12]</ref>, SURF multiview <ref type="bibr" target="#b12">[13]</ref>, PEP-Adapt <ref type="bibr" target="#b10">[11]</ref>, XZJY <ref type="bibr" target="#b20">[21]</ref> and Zhu et al. <ref type="bibr" target="#b26">[27]</ref>) listed on FDDB results page 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on benchmark face database</head><p>As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, in AFW, our multi-scale detector achieves an ap value of 96.8%, outperforming other academic methods by a large margin. When it comes to commercial systems, ours is better than Face.com and almost equal to Face++ and Google Picasa. Note that most of our false positives on AFW database are faces that haven't been annotated (small, seriously occluded or artificial faces like mask and cartoon character).</p><p>When evaluated on FDDB database, we follow the evaluation protocol in <ref type="bibr" target="#b7">[8]</ref> and report the average discrete and continuous ROC of the ten subfolders. For equality, we fix the number of false positives to 284 (equivalent to an average of 1 False Positive Per Image) and compare the true positive rate. In discrete score where evaluation metric is the same as in AFW, our detector achieves 83.7%, which is a little better than Yan et al. <ref type="bibr" target="#b22">[23]</ref>. Note that the groundtruth in FDDB are elliptical faces, therefore the evaluation metric of an overlap ratio bigger than 0.5 cannot reveal the true performance of the proposed detector well. When using continuous score which takes the overlap ratio as the score, our method gets 61.9% true positive rate at 1 FPPI for multiscale version, surpassing other methods which output rectangular detections by a notable margin (the Yan et al. detector outputs the same elliptical detections as the groundtruth, therefore having advantages with this metric). Our detector using single-scale features performs a little worse with the benefit of faster detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussion</head><p>Training efficiency: We implement the method with Piotr's MATLAB toolbox <ref type="bibr" target="#b3">[4]</ref> on a PC with Intel Core i7-3770 CPU and 16GB RAM. With 21, 328 positive images and 5, 771 negative images in total 6 views, the training process takes about 5.3 mins for a single-scale subview detector containing 2048 weak classifiers and 10.2 mins for multi-scale version. Note that we use much fewer training data than SURF multiview <ref type="bibr" target="#b12">[13]</ref> whilst still outperforming their performance.</p><p>Comparative results: When inspecting detections of the proposed face detector and other algorithms on the testsets, some patterns can be found to explain why our detector outperforms others. One evident strength lies in detecting faces with extreme poses. Because we adopt multi-view detection and train each subview detector separately, our detector handles pose variations very well. Second is the outstanding illumination invariance of our detector, which is mainly owing to the extension of channel types to LUV color space and gradient-related channels.</p><p>Detection speed: Due to the simple form of aggregate channel features and fast computation of feature pyramid <ref type="bibr" target="#b4">[5]</ref>, detection is quite efficient. For full yaw pose face detection in VGA image, the proposed detector using single-scale features runs at 20 FPS on a single thread and 62 FPS if 4 threads are used. If only frontal faces are concerned, the detector runs at 34 FPS and 95 FPS after parallelization. When it comes to the proposed detector using multi-scale features, the above four indices reduce to 15, 42, 21 and 55 FPS. Considering the large performance gain and similar speed, the proposed method can replace Viola-Jones detector for face detection in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>A novel feature representation called aggregate channel features possesses the merits of fast feature extraction and powerful representation capacity. In this paper, we successfully apply the feature representation to face detection domain through a deep investigation into the feature design, and propose a multi-scale version of feature which further enriches the representation capacity. Combined with our efforts into solving issues concerning multi-view detection, the proposed multi-view face detector shows state-of-the-art performance in both effectiveness and efficiency on faces in the wild. The proposed method appeals to real world application demands and has the potential to be embedded into low power devices. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An intuitive visualization of our multi-view face detector using aggregate channel features. The area with warmer color indicates more attention paid to by the detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Work-flow of proposed face detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of different numbers of weak classifier in the soft cascade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Experimental results on AFW and FDDB database. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Results of comparative experiments in feature design. Best viewed in color.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://lrs.icg.tugraz.at/research/aflw/ 2 http://www.ics.uci.edu/˜xzhu/face/<ref type="bibr" target="#b2">3</ref> The Jaccard index is defined as the size of the intersection divided by the size of the union of the sample sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://vis-www.cs.umass.edu/fddb/results.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the Chinese National Natural Science Foundation Projects #61105023, #61103156, #61105037, #61203267, #61375037, National Science and Technology Support Program Project #2013BAK02B01, Chinese Academy of Sciences Project No. KGZD-EW-102-2, and AuthenMetric R&amp;D Funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Piotr&apos;s image and video matlab toolbox (pmt)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<ptr target="http://vision.ucsd.edu/˜pdollar/toolbox/doc/index.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Software available at</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted sampling for largescale boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic elastic part model for unsupervised face detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient boosted exemplar-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint haar-like features for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast training and selection of haar features using statistics in boosting-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast polygonal integration and its application in extending haar-like features to improve object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229v4</idno>
		<title level="m">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting and aligning faces by image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557v1</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple-instance pruning for learning efficient cascade detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
