<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-10">10 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
								<address>
									<postCode>4489-105-221</postCode>
									<settlement>Endo, Fujisawa</settlement>
									<region>Kanagawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi</addrLine>
									<settlement>Chiyoda, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Keio University</orgName>
								<address>
									<postCode>5322</postCode>
									<settlement>Endo, Fujisawa</settlement>
									<region>Kanagawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>8916-5</postCode>
									<settlement>Takayama, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi</addrLine>
									<settlement>Chiyoda, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
							<email>takefuji@sfc.keio.ac.jp</email>
							<affiliation key="aff3">
								<orgName type="institution">Keio University</orgName>
								<address>
									<postCode>5322</postCode>
									<settlement>Endo, Fujisawa</settlement>
									<region>Kanagawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-10">10 Jun 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-theart accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Disambiguation (NED) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g., Wikipedia). NED has lately been extensively studied <ref type="bibr" target="#b5">(Cucerzan, 2007;</ref><ref type="bibr">Mihalcea and Csomai, 2007;</ref><ref type="bibr">Milne and Witten, 2008b;</ref><ref type="bibr" target="#b27">Ratinov et al., 2011</ref>) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population <ref type="bibr" target="#b18">(McNamee and Dang, 2009;</ref><ref type="bibr" target="#b14">Ji et al., 2010)</ref>, and semantic search <ref type="bibr" target="#b0">(Blanco et al., 2015)</ref>. We use Wikipedia as KB in this paper.</p><p>The main difficulty in NED is ambiguity in the meaning of entity mentions. For example, the mention "Washington" in a document can refer to various entities, such as the state, or the capital of the US, the actor Denzel Washington, the first US president George Washington, and so on. In order to resolve these ambiguous mentions into references to the correct entities, early approaches focused on modeling textual context, such as the similarity between contextual words and encyclopedic descriptions of a candidate entity <ref type="bibr" target="#b2">(Bunescu and Pasca, 2006;</ref><ref type="bibr">Mihalcea and Csomai, 2007)</ref>. Most state-of-theart methods use more sophisticated global approaches, where all mentions in a document are simultaneously disambiguated based on global coherence among disambiguation decisions. Word embedding methods are also becoming increasingly popular <ref type="bibr" target="#b21">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b25">Pennington et al., 2014)</ref>. These involve learning continuous vector representations of words from large, unstructured text corpora. The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low-dimensional vector space.</p><p>In this paper, we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space. In this model, similar words and entities are placed close to one another in a vector space. Hence, we can measure the similarity between any pair of items (i.e., words, entities, and a word and an entity) by simply computing their cosine similarity. This enables us to easily measure the contextual information for NED, such as the similarity between a context word and a candidate entity, and the relatedness of entities required to model coherence.</p><p>Our model is based on the skip-gram model <ref type="bibr" target="#b21">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013b)</ref>, a recently proposed embedding model that learns to predict each context word given the target word. Our model consists of the following three models based on the skip-gram model: 1) the conventional skip-gram model that learns to predict neighboring words given the target word in text corpora, 2) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB, and 3) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB. By jointly optimizing these models, our method simultaneously learns the embedding of words and entities.</p><p>Based on our proposed embedding, we also develop a straightforward NED method that computes two contexts using the proposed embedding: textual context similarity, and coherence. Textual context similarity is measured according to vector similarity between an entity and words in a document. Coherence is measured based on the relatedness between the target entity and other entities in a document. Our NED method combines these contexts with several standard features (e.g., prior probability) using supervised machine learning.</p><p>We tested the proposed method using two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset. Experimental results revealed that our method outperforms state-of-the-art methods on both datasets by significant margins. Moreover, we conducted experiments to separately assess the quality of the vector representation of entities using an entity relatedness dataset, and discovered that our method successfully learns the quality representations of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Joint Embedding of Words and Entities</head><p>In this section, we first describe the conventional skip-gram model for learning word embedding. We then explain our method to construct an embedding that jointly maps words and entities into the same continuous d-dimensional vector space. We extend the skip-gram model by adding the KB graph model and the anchor context model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skip-gram Model for Word Similarity</head><p>The training objective of the skip-gram model is to find word representations that are useful to predict context words given the target word. Formally, given a sequence of T words w 1 , w 2 , ..., w T , the model aims to maximize the following objective function:</p><formula xml:id="formula_0">L w = T t=1 −c≤j≤c,j =0 log P (w t+j |w t ) (1)</formula><p>where c is the size of the context window, w t denotes the target word, and w t+j is its context word. The conditional probability P (w t+j |w t ) is computed using the following softmax function:</p><formula xml:id="formula_1">P (w t+j |w t ) = exp(V wt ⊤ U w t+j ) w∈W exp(V wt ⊤ U w )<label>(2)</label></formula><p>where W is a set containing all words in the vocabulary, and V w ∈ R d and U w ∈ R d denote the vectors of word w in matrices V and U, respectively.</p><p>The skip-gram model is trained to optimize the above function L w , and V are used as the resulting vector representations of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extending the Skip-gram Model</head><p>We extend the skip-gram model to learn the vector representations of entities. We expand matrices V and U to include the vectors of entities V e ∈ R d and U e ∈ R d in addition to the vectors for words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">KB Graph Model</head><p>We use an internal link structure in KB to enable the model to learn the relatedness between pairs of entities. Wikipedia Link-based Measure (WLM) <ref type="bibr">(Milne and Witten, 2008a</ref>) is a method to measure entity relatedness based on its link structure. It has been used as a standard method to compute the relatedness of entities for modeling coherence in past NED studies. The relatedness between two entities is computed using the following function:</p><formula xml:id="formula_2">W LM (e 1 , e 2 ) = 1 − log max(|Ce 1 |,|Ce 2 |)−log |Ce 1 ∩Ce 2 | log |E|−log min(|Ce 1 |,|Ce 2 |)<label>(3)</label></formula><p>where E is the set of all entities in KB and C e is the set of entities with a link to an entity e. Intuitively, WLM assumes that entities with similar incoming links are related. Despite its simplicity, WLM yields state-of-the-art performance <ref type="bibr" target="#b10">(Hoffart et al., 2012)</ref>. Inspired by WLM, the KB graph model simply learns to place entities with similar incoming links near one another in the vector space. We formalize this as the following objective function:</p><formula xml:id="formula_3">L e = e i ∈E eo∈Ce i ,e i =eo log P (e o |e i )<label>(4)</label></formula><p>We compute the conditional probability P (e o |e i ) using the following softmax function:</p><formula xml:id="formula_4">P (e o |e i ) = exp(V e i ⊤ U eo ) e∈E exp(V e i ⊤ U e )<label>(5)</label></formula><p>We train the model to predict the incoming links C e given an entity e. Therefore, C e plays a similar role to context words in the skip-gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Anchor Context Model</head><p>If we add only the KB graph model to the skipgram model, the vectors of words and entities do not interact, and can be placed in different subspaces of the vector space. To address this issue, we introduce the anchor context model to place similar words and entities near one another in the vector space.</p><p>The idea underlying this model is to leverage KB anchors and their context words to train the model. As mentioned in Section 1, we use Wikipedia as a KB. It contains many internal anchors that can be safely treated as unambiguous occurrences of referent KB entities. By using these anchors, we can easily obtain many occurrences of entities and their corresponding context words directly from the KB.</p><p>As in the skip-gram model, we simply train the model to predict the context words of an entity pointed to by the target anchor. The objective function is as follows:</p><formula xml:id="formula_5">L a = (e i ,Q)∈A wo∈Q log P (w o |e i )<label>(6)</label></formula><p>where A denotes a set of anchors in the KB, each of which contains a pair of a referent entity e i and a set of its context words Q. Here, Q contains the previous c words and the next c words. Note that |A| equals the number of internal anchors in the KB. As in past models, the conditional probability P (w o |e i ) is computed using the softmax function:</p><formula xml:id="formula_6">P (w o |e i ) = exp(V e i ⊤ U wo ) w∈W exp(V e i ⊤ U w )<label>(7)</label></formula><p>Using the proposed model, we align the vector representations of words and entities by placing words and entities with similar context words close to one another in the vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Considering the three model components mentioned above, we propose the following objective function by linearly combining the above objective functions:</p><formula xml:id="formula_7">L = L w + L e + L a<label>(8)</label></formula><p>The training of the model is intended to maximize the above function, and the resulting matrix V is used to embed words and entities.</p><p>One of the problems in training our model is that the normalizers contained in the softmax functions P (w t+j |w t ), P (e o |e i ), and P (w o |e i ) are computationally very expensive because they involve summation over all words W or entities E. To address this problem, we use negative sampling (NEG) <ref type="bibr" target="#b22">(Mikolov et al., 2013b)</ref> to convert original objective functions into computationally feasible ones. NEG is defined by the following objective function:</p><formula xml:id="formula_8">log σ(V wt ⊤ U wt+j ) + g i=1 E wi∼Pneg (w) log σ(−V wt ⊤ U wi ) (9)</formula><p>where σ(x) = 1/(1 + exp(−x)) and g is the number of negative samples. We replace the log P (w t+j |w t ) term in Eq. (1) with the above objective function. Consequently, the objective function is transformed from that in Eq.</p><p>(1) to a simple objective function of the binary classification to distinguish the observed word w t from words drawn from noise distribution P neg (w). We also replace log P (e o |e i ) in Eq. (4) and log P (w o |e i ) in Eq. (6) in the same manner. Note that NEG takes a negative distribution P neg (w) as a free parameter. Following <ref type="bibr" target="#b22">(Mikolov et al., 2013b)</ref>, we use the unigram distribution of words (U (w)) raised to the 3/4 th power (i.e., U (w) 3/4 /Z, where Z is a normalization constant) in the skip-gram model and the anchor context model. In the KB graph model, we use a uniform distribution over KB entities E as the negative distribution.</p><p>We use Wikipedia to train all the above models. Optimization is carried out simultaneously to maximize the transformed objective function by iterating over Wikipedia pages several times. We use stochastic gradient descent (SGD) for the optimization. The optimization is performed using a multiprocess-based implementation of our model using Python, Cython, and NumPy configured with OpenBLAS with storing matrices V and U in the shared memory. To improve speed, we decide not to introduce locks to the shared matrices. Embedding</p><p>In this section, we explain our NED method using our proposed embedding. Let us formally define the task. Given a set of entity mentions M = {m 1 , m 2 , ..., m N } in a document d with an entity set E = {e 1 , e 2 , ..., e K } in the KB, the task is defined as resolving mentions (e.g., "Washington") into their referent entities (e.g., Washington D.C.).</p><p>We introduce two measures that have been frequently observed in past NED studies: entity prior P (e) and prior probability P (e|m). We define entity prior P (e) = |A e, * |/|A * , * | where A * , * denotes all anchors in the KB and A e, * is the set of anchors that point to entity e. Prior probability is defined as P (e|m) = |A e,m |/|A * ,m | where A * ,m represents all anchors with the same surface as mention m in KB and A e,m is a subset of A * ,m that points to entity e. We separate the NED task into two sub-tasks: candidate generation and mention disambiguation. In candidate generation, candidates of referent entities are generated for each mention. Details of candidate generation are provided in Section 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mention Disambiguation</head><p>Given a document d and mention m with its candidate referent entities {e 1 , e 2 , ..., e k } generated in the candidate generation step, the task is to disambiguate mention m by selecting the most relevant entity from the candidate entities.</p><p>The key to improving the performance of this task is to effectively model the context. We propose two novel methods to model the context using the proposed embedding. Further, we combine these two models with several standard NED features using supervised machine learning described in 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Modeling Textual Context</head><p>Textual context is designed based on the assumption that an entity is more likely to appear if the context of a given mention is similar to that of the entity.</p><p>We propose a method to measure the similarity between textual context and entity using the proposed embedding by first deriving the vector representation of the context and then computing the similarity between the context and the entity using cosine similarity. To derive the vector of context, we average the vectors of context words:</p><formula xml:id="formula_9">v cw = 1 |W cm | w∈Wc m v w<label>(10)</label></formula><p>where W cm is a set of the context words of mention m and v w ∈ V denotes the vector representation of word w. We use all noun words in document d as context words. 1 Moreover, we ignore a context word if the surface of mention m contains it.</p><p>We then measure the similarity between candidate entity and the derived textual context by using cosine similarity between v cw and the vector of entity v e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Modeling Coherence</head><p>It has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED. However, this is a chickenand-egg problem because the assignment of entities to mentions, which is required to measure coherence, is not possible prior to performing NED.</p><p>Similar to past work <ref type="bibr" target="#b27">(Ratinov et al., 2011)</ref>, we address this problem by employing a simple twostep approach: we first train the machine learning model using the coherence score among unambiguous mentions 2 , in addition to other features, and then retrain the model using the coherence score among the predicted entity assignments instead.</p><p>To estimate coherence, we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity e. Note that context entities are unambiguous entities in the first step, and predicted entities are used instead in the second step.</p><p>To derive the vector representation of context entities, we average their vector representations:</p><formula xml:id="formula_10">v ce = 1 |E cm | e * ∈Ec m v e *<label>(11)</label></formula><p>where E cm denotes the set of context entities described above.</p><p>To estimate the coherence score, we again use cosine similarity between the vector of entity v e and that of context entities v ce .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Learning to Rank</head><p>To combine the proposed contextual information described above with standard NED features, we employ a method of supervised machine learning to rank the candidate entities given mention m and document d.</p><p>In particular, we use Gradient Boosted Regression Trees (GBRT) <ref type="bibr" target="#b6">(Friedman, 2001)</ref>, a stateof-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here <ref type="bibr" target="#b19">(Meij et al., 2012)</ref>. GBRT consists of an ensemble of regression trees, and predicts a relevance score given an instance. We use the GBRT implementation in scikit-learn 3 and the logistic loss is used as the loss function. The main parameters of GBRT are the number of iterations η, the learning rate β, and the maximum depth of the decision trees ξ.</p><p>With regard to the features of machine learning, we first use prior probability (P (e|m)) and entity prior (P (e)). Further, we include a feature representing the maximum prior probability of the candidate entity e of all mentions in the document. We also add the number of entity candidates for mention m as a feature. The above set of four features is called base features in the rest of the paper.</p><p>We also use several string similarity features used in past work on NED <ref type="bibr" target="#b19">(Meij et al., 2012)</ref>. These features aim to capture the similarity between the title of entity e and the surface of mention m, and consist of the edit distance, whether the title of entity e exactly equals or contains the surface of mention m, and whether the title of entity e starts or ends with the surface of mention m.</p><p>Finally, we include contextual features measured using the proposed embedding. We use cosine similarity between the candidate entity and the textual context (see Section 3.1.1), and similarity between an entity and contextual entities (see Section 3.1.2). Furthermore, we include the rank of entity e among candidate entities of mention m, sorted according to these two similarity scores in descending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the setup and results of our experiments. In addition to experiments on the NED task, we separately assessed the quality of pairwise entity relatedness in order to test the 3 http://scikit-learn.org/ effectiveness of our method in capturing pairwise similarity between pairs of entities. We first describe the details of the training of the embedding and then present the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training for the Proposed Embedding</head><p>To train the proposed embedding, we used the December 2014 version of the Wikipedia dump 4 . We first removed the pages for navigation, maintenance, and discussion, and used the remaining 4.9 million pages. We parsed the Wikipedia pages and extracted text and anchors from each page. We further tokenized the text using the Apache OpenNLP tokenizer. We also filtered out rare words that appeared fewer than five times in the corpus. We thus obtained approximately 2 billion tokens and 73 million anchors. The total number of words and entities in the embedding were approximately 2.1 million and 5 million, respectively. Consequently, the number of rows of matrices V and U were 7.1 million. The number of dimensions d of the embedding was set to 500. Following <ref type="bibr" target="#b22">(Mikolov et al., 2013b)</ref>, we also used learning rate α = 0.025 which linearly decreased with the iterations of the Wikipedia dump. Regarding the other parameters, we set the size of the context window c = 10 and the negative samples g = 30. The model was trained online by iterating over pages in the Wikipedia dump 10 times. The training lasted approximately five days using a server with a 40-core CPU on Amazon EC2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Relatedness</head><p>To test the quality of the vector representation of entities, we conducted an experiment using a dataset for entity relatedness created by Ceccarelli et al. <ref type="bibr" target="#b3">(Ceccarelli et al., 2013)</ref>. The dataset consists of training, test, and validation sets, and we only use the test set. The test set contains 3,314 entities, where each entity has 91 candidate entities with gold-standard labels indicating whether the two entities are related. Following , we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) <ref type="bibr" target="#b13">(Järvelin and Kekäläinen, 2002)</ref> and mean average precision (MAP) <ref type="bibr" target="#b17">(Manning et al., 2008)</ref>. We adopted WLM as baseline. <ref type="table" target="#tab_0">Table 1</ref> shows the results. The score for WLM was obtained from Huang et al. . Our method clearly outperformed WLM. The results show that our method accurately captures pairwise entity relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Named Entity Disambiguation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Setup</head><p>We now explain our experimental setup for the NED task. We tested the performance of our proposed method on two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset. The details of these datasets are provided below. Moreover, as with the corpus used in the embedding, we used the December 2014 version of the Wikipedia dump as the referent KB, and to derive the prior probability as well as the entity prior.</p><p>To find the best parameters for our machine learning model, we ran a parameter search on the CoNLL development set. We used η = 10, 000 trees, and tested all combinations of the learning rate β = {0.01, 0.02, 0.03, 0.05} and the maximum depth of the decision trees ξ = {3, 4, 5}. We computed their accuracy on the dataset, and found that the parameters did not significantly affect performance (1.0% at most). We used β = 0.02 and ξ = 4 which yielded the best performance.</p><p>CoNLL The CoNLL dataset is a popular NED dataset constructed by Hoffart et al. <ref type="bibr" target="#b10">(Hoffart et al., 2011)</ref>. The dataset is based on NER data from the CoNLL 2003 shared task, and consists of training, development, and test sets, containing 946, 216, and 231 documents, respectively. We trained our machine learning model using the training set and reported its performance using the test set. We also used the development set for the parameter tuning described above. Following <ref type="bibr" target="#b10">(Hoffart et al., 2011)</ref>, we only used 27,816 mentions with valid entries in the KB and reported the standard micro-(aggregates over all mentions) and macro-(aggregates over all documents) accuracies of the top-ranked candidate entities to assess disambiguation perfor-mance. For candidate generation, we use the following two resources: 1) a public dataset recently built by <ref type="bibr" target="#b26">Pershina et al. (Pershina et al., 2015)</ref> (denoted by PPRforNED) for the sake of compatibility with their state-of-the-art results, and 2) a dictionary built using a standard YAGO means relation dataset <ref type="bibr">(Hoffart et al., 2011) (denoted by YAGO)</ref>. Moreover, we used PPRforNED for the parameter tuning of the machine learning model and for error analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAC 2010</head><p>The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC) 5 <ref type="bibr" target="#b14">(Ji et al., 2010)</ref>. The dataset is based on news articles from various agencies and Web log data, and consists of a training and a test set containing 1,043 and 1,013 documents, respectively. Following past work <ref type="bibr" target="#b9">(He et al., 2013;</ref><ref type="bibr" target="#b4">Chisholm and Hachey, 2015)</ref>, we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities. We trained our model using the training set and assessed its performance using the test set. Consequently, we evaluated our model on 1,020 mentions contained in the test set. For candidate generation, we used a dictionary that was directly built from the Wikipedia dump mentioned previously. Similar to past work, we retrieved possible mention surfaces of an entity from (1) the title of the entity, (2) the title of another entity redirecting to the entity, and (3) the names of anchors that point to the entity. We retained the top 50 candidates through their entity priors for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with State-of-the-art Methods</head><p>We compared our method with the following recently proposed state-of-the-art methods:</p><p>• Hoffart et al. <ref type="figure">(Hoffart et al., 2011)</ref> is a graphbased approach that finds a dense subgraph of entities in a document to address NED.</p><p>• He et al. <ref type="bibr" target="#b9">(He et al., 2013)</ref> uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED.</p><p>• Chisholm and Hachey <ref type="bibr" target="#b4">(Chisholm and Hachey, 2015)</ref> uses a Wikilinks dataset <ref type="bibr" target="#b28">(Singh et al., 2012)</ref> to improve the performance of NED.   <ref type="bibr" target="#b10">Hoffart et al., 2011</ref> 82.5 81.7 - <ref type="bibr" target="#b9">He et al., 2013</ref> 85.6 84.0 81.0 <ref type="bibr" target="#b4">Chisholm &amp; Hachey, 2015</ref> 88.7 -80.7 <ref type="bibr" target="#b26">Pershina et al., 2015</ref> 91.8 89.9 - <ref type="table">Table 3</ref>: Accuracy scores of the proposed method and the state-of-the-art methods.</p><p>• Pershina et al. <ref type="bibr" target="#b26">(Pershina et al., 2015)</ref> improved NED by modeling coherence using the personalized page rank algorithm, and achieved the best-known accuracy on the CoNLL dataset. <ref type="table" target="#tab_2">Table 2</ref> shows the experimental results of our proposed method. Our method successfully achieved enhanced performance on both the CoNLL and the TAC 2010 datasets. Moreover, we found that the choice of candidate generation method considerably affected performance on the CoNLL dataset. Further, <ref type="table">Table 3</ref> shows the experimental results of our proposed method as well as those of stateof-the-art methods. Our method outperformed all the state-of-the-art methods on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Feature Study</head><p>We conducted a feature study on our method. We began with base features, added various features to our system incrementally, and reported their impact on performance. We then introduced our twostep approach to achieve the final results. <ref type="table" target="#tab_4">Table 4</ref> shows the results. Surprisingly, we attained results comparable with those of some state-of-the-art methods on the both datasets by only using base features. Adding string similarity features slightly further improved performance.</p><p>We observed significant improvement when adding textual context features based on our proposed embedding. Our method outperformed  some state-of-the-art methods without using coherence. Further, coherence based on unambiguous entity mentions and our two-step approach significantly improved performance on the CoNLL dataset. However, it did not contribute to performance on the TAC 2010 dataset. This was because of the significant difference in the density of entity mentions between the datasets. The CoNLL dataset contains approximately 20 entity mentions per document, but the TAC 2010 only contains approximately one mention per document which is unarguably insufficient to model coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Error Analysis</head><p>We also conducted an error analysis on the CoNLL test set with candidate generation using PPRforNED dataset. We observed that approximately 48.6% errors were caused by metonymy mentions <ref type="bibr" target="#b16">(Ling et al., 2015)</ref> (i.e., mentions with more than one plausible annotation). In particular, our NED method often erred when an incorrect entity was highly popular and exactly matched the mention surface (e.g., "South Africa" referring to the entity South Africa national rugby union team rather than the entity South Africa). This makes sense because our machine learning model uses the popularity statistics of the KB (i.e., prior probability and entity prior), and the string similarity between the title of the entity and the mention surface. This problem is discussed further in <ref type="bibr" target="#b16">(Ling et al., 2015)</ref>.</p><p>Furthermore, because our method depends on the presence of KB anchors in order to learn entity representation, it arguably fails to learn satisfactory representations of tail entities (i.e., entities rarely referred to by anchors), thus resulting in disambiguation errors. We discovered that nearly 9.6% errors were due to referent entities with less than 10 inbound KB anchors, and 4.5% involved entities with no inbound KB anchor. These errors might be addressed using KB data other than KB anchors, such as the description of the entities and the KB categories in order to avoid dependence on the KB anchors. This remains part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early NED methods addressed the problem as a well-studied word sense disambiguation problem <ref type="bibr">(Mihalcea and Csomai, 2007)</ref>. These methods primarily focused on modeling the similarity of textual (local) context. Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document <ref type="bibr" target="#b5">(Cucerzan, 2007;</ref><ref type="bibr">Milne and Witten, 2008b;</ref><ref type="bibr" target="#b10">Hoffart et al., 2011;</ref><ref type="bibr" target="#b27">Ratinov et al., 2011)</ref>. These approaches have also been called collective or global approaches in the literature.</p><p>Learning the representations of entities for NED has been addressed in past literature. <ref type="bibr" target="#b8">Guo and Barbosa (Guo and Barbosa, 2014)</ref> used random walks on KB graphs to construct vector representations of entities and documents to address NED. <ref type="bibr" target="#b0">Blanco et al. (Blanco et al., 2015)</ref> proposed a method to map entities into the word embedding (i.e., Word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013b)</ref>) space using entity descriptions in the KB and applied it for NED. He et al. <ref type="bibr" target="#b9">(He et al., 2013)</ref> used deep neural networks to compute representations of entities and contexts of mentions directly from the KB. Similarly, Sun et al.  proposed a method based on deep neural networks to model representations of mentions, contexts of mentions, and entities. Huang et al.  also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method (i.e., WLM) for NED. Further, Hu et al. <ref type="bibr" target="#b11">(Hu et al., 2015)</ref> used hierarchical information in the KB to build entity embedding and applied it to model coherence. Unlike these methods, our proposed approach involves jointly learning vector representations of entities as well as words, hence enabling the accurate computation of the semantic similarity among its items to model both the textual context and coherence.</p><p>Moreover, Yaghoobzadeh and Schutze (Yaghoobzadeh and Schütze, 2015) addressed an entity typing task by building an embedding of words and entities on a corpus with annotated entities (i.e., FACC1 <ref type="bibr" target="#b7">(Gabrilovich et al., 2013)</ref>) using the skip-gram model. Compared to our method, in addition to the significant difference between their task and NED, their embedding does not incorporate the link graph data of KB, which is known to be highly important for NED.</p><p>Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published <ref type="bibr" target="#b1">(Bordes et al., 2011;</ref><ref type="bibr" target="#b29">Socher et al., 2013;</ref>. These methods focus on learning vector representations of entities to primarily address the link prediction task that aims to predict a new fact based on existing facts in KB. Particularly, Wang et al. <ref type="bibr" target="#b31">(Wang et al., 2014)</ref> have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task, which is somewhat analogous to our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed an embedding method to jointly map words and entities into the same continuous vector space. Our method enables us to effectively model both textual and global contexts. Further, armed with these context models, our NED method outperforms state-of-the-art NED methods.</p><p>In future work, we intend to improve our model by leveraging relevant knowledge, such as relations in a knowledge graph (e.g., Freebase). We would also like to seek applications of our proposed embedding other than NED.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of the entity relatedness task.</figDesc><table><row><cell></cell><cell>NDCG@1</cell><cell>NDCG@5</cell><cell>NDCG@10</cell><cell>MAP</cell></row><row><cell>Our Method</cell><cell>0.59</cell><cell>0.56</cell><cell>0.59</cell><cell>0.52</cell></row><row><cell>WLM</cell><cell>0.54</cell><cell>0.52</cell><cell>0.55</cell><cell>0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Experimental results of our proposed</cell></row><row><cell>NED method.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CoNLL</cell><cell>CoNLL</cell><cell>TAC10</cell></row><row><cell></cell><cell>(Micro)</cell><cell>(Macro)</cell><cell>(Micro)</cell></row><row><cell>Our Method</cell><cell>93.1</cell><cell>92.6</cell><cell>85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of our feature study.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used Apache OpenNLP tagger to detect nouns. https://opennlp.apache.org/ 2 We consider that mention m unambiguously refers to entity e if its prior probability P (e|m) is greater than 0.95.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The dump was retrieved from Wikimedia Downloads. http://dumps.wikimedia.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.nist.gov/tac/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and Space-Efficient Entity Linking for Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Structured Embeddings of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 25th Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Encyclopedic Knowledge for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Bunescu and Pasca2006</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Relatedness Measures for Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ceccarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entity Disambiguation with Web Links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-Scale Named Entity Disambiguation Based on Wikipedia Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Greedy Function Approximation: A Gradient Boosting Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 2013-06-26</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gabrilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Format version 1, Correction level 0</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity Linking with a Unified Semantic Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web (WWW)</title>
		<meeting>the 23rd International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1305" to="1310" />
		</imprint>
	</monogr>
	<note>Guo and Barbosa2014</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Entity Representation for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">KORE: Keyphrase Overlap Relatedness for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<editor>Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum</editor>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entity Hierarchy Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>abs/1504.0</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Järvelin and Kekäläinen2002</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2010 Knowledge Base Population Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Text Analytics Conference</title>
		<meeting>eeding of Text Analytics Conference</meeting>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design Challenges for Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2009 Knowledge Base Population Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Text Analysis Conference</title>
		<meeting>eeding of Text Analysis Conference</meeting>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>McNamee and Dang2009</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adding Semantics to Microblog Posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Meij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the Fifth ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wikify!: Linking Documents to Encyclopedic Knowledge</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the Sixteenth ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
	<note>Rada Mihalcea and Andras Csomai</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witten2008a] David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI)</title>
		<meeting>the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Witten</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witten2008b] David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 17th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>eeding of the 17th ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
	<note>Witten</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized Page Rank for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pershina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ratinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Singh et al.2012</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
	<note>Sun et al.2015</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge Graph and Text Jointly Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Corpus-level fine-grained entity typing using contextual information</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
