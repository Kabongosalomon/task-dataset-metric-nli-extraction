<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Image Reconstruction using Deep Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
							<email>razvan@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Moyer</surname></persName>
							<email>dmoyer@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Golland</surname></persName>
							<email>polina@csail.mit.edu</email>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Image Reconstruction using Deep Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><p>Input Output Input Output superresolution inpainting <ref type="figure">Figure 1</ref>: Given a generator pre-trained on an image set (human faces, x-rays, brain MRIs), we perform various reconstructions such as super-resolution and inpainting. Our method does not require any additional corruption-specific training, as it couples a dataset-specific generator G, pre-trained on clean images, with a forward corruption process f , such as downsampling or cropping. We demonstrate our results based on the state-of-the-art unconditional generator StyleGAN2 [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Machine learning models are commonly trained endto-end and in a supervised setting, using paired (input, output) data. Classical examples include recent superresolution methods that train on pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches require re-training every time there is a distribution shift in the inputs (e.g., night images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for building powerful image priors, which enable application of Bayes' theorem for many downstream reconstruction tasks. Our method, called Bayesian Reconstruction through Generative Models (BRGM), uses a single pre-trained generator model to solve different image restoration tasks, i.e., super-resolution and in-painting, by combining it with different forward corruption models. We demonstrate BRGM on three large, yet diverse, datasets that enable us to build powerful priors: (i) 60,000 images from the Flick Faces High Quality dataset <ref type="bibr" target="#b21">[22]</ref> (ii) 240,000 chest X-rays from MIMIC III <ref type="bibr" target="#b19">[20]</ref> and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans <ref type="bibr" target="#b10">[11]</ref>. Across all three datasets and without any dataset-specific hyperparameter tuning, our approach yields state-of-the-art performance on super-resolution, particularly at low-resolution levels, as well as inpainting, compared to state-of-the-art methods that are specific to each reconstruction task. Our source code and all pre-trained models are available online:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While end-to-end supervised machine learning is currently the most popular paradigm in the research community, it suffers from several problems. First, distribution shifts in the inputs often require re-training, as well as the effort of collecting an updated dataset that accounts for the distribution shifts. In some settings, such shifts can occur often (hospital scanners are often upgraded) and even continuously (population is slowly aging due to improved healthcare). Secondly, due to the combinatorial effect in the number of potential inputs and outputs, large numbers of inputspecific and output-specific end-to-end models need to be trained and maintained. For example, consider a set of N types of images (inputs) from different hospital scanners and M diseases (outputs). With end-to-end training, the total number of models required to map one input to another output is N × M . However, if we instead use compositionality, such as that based on an intermediate representation, the number of models required to map every input to every output is just N + M . Third, current state-of-the-art machine learning (ML) models often require prohibitive computational resources, which are only available in a select number of companies and research centers. Therefore, the ability to leverage pre-trained models for solving downstream prediction or reconstruction tasks becomes crucial. For example, instead of training a method to perform super-resolution on human faces, one can use a pre-trained face generator combined with a downsampling corruption model to create a super-resolution method that does not require any further training or fine-tuning.</p><p>In order to create ML models that are robust to distribution shifts and are easy to train, we aim to mimic the data generating process, an approach that has been proposed in causal modelling <ref type="bibr" target="#b38">[39]</ref>. Unlike classical causal modelling that aims to uncover the structure of dependencies in the data, we are interested in causal modelling for robustness, in particular for handling distribution shifts, which leads to better performance in out-of-distribution tasks. To this end, it is of crucial importance to introduce causal inductive biases in the models. Among the many benefits of such causal inductive biases is the independence of mechanisms <ref type="bibr" target="#b38">[39]</ref>. Consider the graphical model from <ref type="figure">Fig. 2b</ref>. The fact that the downstream model is independent of the upstream model is of crucial importance, because it implies that any upstream changes (either in the input variables or the upstream model) do not require changing the downstream model.</p><p>Deep generative models have recently obtained state-ofthe-art results in simulating high-quality images from a variety of computer vision datasets. Generative Adversarial Networks such as StyleGAN2 <ref type="bibr" target="#b22">[23]</ref>, StyleGAN-ADA <ref type="bibr" target="#b20">[21]</ref> have been demonstrated for unconditional image generation, while BigGAN has shown impressive performance in class-conditional image generation <ref type="bibr" target="#b7">[8]</ref>. Similarly, Variational Autoencoder-based methods such as VQ-VAE <ref type="bibr" target="#b46">[47]</ref> and β-VAE <ref type="bibr" target="#b17">[18]</ref> have also been competitive in several image generation tasks. A different line of research in deep generative models is represented by auto-regressive models such as PixelCNN <ref type="bibr" target="#b45">[46]</ref> and PixelRNN <ref type="bibr" target="#b36">[37]</ref>. While they obtain accurate likelihood on test data, they usually suffer from slow image generation due to the sequential nature of the sampling process. Yet another line of research is represented by invertible flow models such as NeuralODE <ref type="bibr" target="#b8">[9]</ref>, Glow <ref type="bibr" target="#b24">[25]</ref> and RealNVP <ref type="bibr" target="#b11">[12]</ref>. While these models are able to generate high-quality images that are similar to a given dataset, they are not directly applicable for solving more complex tasks, such as image reconstruction or manipulation. Other recent work in generative modelling performs structured image formation <ref type="bibr" target="#b27">[28]</ref> for improved generalisation.</p><p>A particularly important application domain for generative models are inverse problems, which aim to reconstruct an image that has undergone a corruption process such as blurring or motion. Prior work has focused on regularizing the inversion process using smoothness priors <ref type="bibr" target="#b42">[43]</ref> or sparsity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> . However, such priors often result in blurry images, and do not enable hallucination of features given prior data from a distribution of clean images. More recent deep-learning approaches such as AUTOMAP <ref type="bibr" target="#b51">[52]</ref> solve this using training data made of pairs of (low-resolution, high-resolution) images, but any change in either the corruption process or the image dataset require re-training or fine-tuning.</p><p>In addition to the fact that state-of-the-art deep learningbased image reconstruction methods for super-resolution <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b47">48]</ref> or inpainting <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref> are both dataset-specific and corruption-specific, requiring re-training for every distribution shift, they also compute the loss in the highresolution/inpainted space. More precisely, given pairs (I i , I i CLN ) of noisy/cropped and clean images, they usually minimize E L m(I i ), I i CLN , for a given model m and loss function L. This has fundamental implications, as pixelwise or perceptual losses in the high-resolution space have the so-called blurriness effect <ref type="bibr" target="#b26">[27]</ref>: since there exist multiple high-resolution images I CLN that map to a low-resolution image I, the loss function minimizes the average of all such solutions, resulting in a blurry image. Some authors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> have addressed this through adversarial losses, which forces the model to output one potential solution on the manifold of possible images instead of the average of all solutions. However, even with adversarial losses, it is not clear which solution image is retrieved at inference time, and neither is it possible to sample multiple solutions at inference time in a Bayesian setting, without further modifications <ref type="bibr" target="#b15">[16]</ref>.</p><p>The natural approach to overcome the averaging of all possible solutions to ill-posed problems is to then estimate all potential solutions, or a distribution of solutions, for which a Bayesian framework is a natural choice. There is a rich literature of such Bayesian methods in the computer vision literature for image reconstruction tasks. <ref type="bibr" target="#b40">[41]</ref> learned rich Markov Random Field priors over images, and used Bayesian inference for denoising and inpainting. <ref type="bibr" target="#b6">[7]</ref> used Bayes' rule to compute a posterior distribution of illuminants for color constancy. <ref type="bibr" target="#b29">[30]</ref> performed inpainting using an MRF model that leverages global statistics, and performed inference using loopy belief propagation. <ref type="bibr" target="#b14">[15]</ref> used Bayesian inference to quantify the distribution of scene parameters and light direction given an input image. <ref type="bibr" target="#b41">[42]</ref> used Bayesian inference using a prior on the histogram of a band-pass filter output. <ref type="bibr" target="#b43">[44]</ref> performed super-resolution using a Bayesian MAP framework, by setting the likelihood as the marginalisation over the high-dimensional image, and used a Gaussian process prior over images.</p><p>In this work, we revisit the classical Bayesian method for image reconstruction, this time using state-of-the-art deep generative networks. Given a noisy image I to be restored, we pass a latent vector w through a pre-trained generator model G to first generate a potential clean reconstruction G(w), and then pass it through a given forward corruption model f to simulate a corrupted image I COR = f • G(w). I COR is then compared to the input image I according to a distance metric L of our choice -our aim is thus to minimize the loss function L(f • G(w), I). In contrast to previous supervised learning approaches that model in the anti-causal Low Res.</p><p>High Res.  <ref type="figure">Figure 2</ref>: (a) Classical deep-learning methods for image reconstruction learn to invert specific corruption models such as downsampling with a specific kernel or in-painting with rectangular masks. (b) Our generative approach can handle any arbitrary corruption process, such as downsampling or inpainting with arbitrary mask, by optimizing for it at test time. Given a latent vector w, we use generator G to generate clean images G(w), followed by a corruption model f 1 to generate a corrupted image f 1 •G(w). Given an input image I, one could find the latent w * that generated the input image using w * = arg min w ||f 1 • G(w) − I|| 2 2 . In practice, this leads to unsuitable solutions, so we optimize a more complex loss function that also contains priors on w as well as a perceptual loss, which is derived as the maximum a-posteriori (MAP) estimate of a Bayesian model we propose. The same process can be repeated for other corruption processes (f 2 , f 3 ) such as masking, motion, to-grayscale in order to achieve inpainting, motion-correction or colorization, as well as for specific parametrisation of a process (e.g., super-resolution with different kernels or factors). direction by starting with the corrupted image and generating the restored image, our approach is causal and follows closely the data generating process. By design, it can account for distribution shifts in either the image dataset or the corruption process by updating G or f respectively. We demonstrate BRGM on three different datasets: (1) 60,000 images of human faces from the Flickr Faces High Quality (FFHQ) dataset <ref type="bibr" target="#b21">[22]</ref>, (2) ≈ 240,000 chest X-ray images from MIMIC III <ref type="bibr" target="#b19">[20]</ref>, and (3) 7,329 brain MRI slices from a collection of 5 neuroimaging datasets <ref type="bibr" target="#b10">[11]</ref>. We evaluate our model on unseen test images against previous state-of-theart approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, where our model performs favorably on all three datasets, without any dataset-specific fine-tuning. The contributions of our work are:</p><p>• We demonstrate a framework of creating powerful baselines for various inverse problems, by combining different prior generative models with various corruption processes.</p><p>• We demonstrate our method on a variety of crossvalidated datasets (FFHQ, X-rays, brain MRIs), with a combination of several corruption processes, each allowing us to perform different image restoration tasks, such as super-resolution and in-painting.</p><p>• We evaluate our method against state-of-the-art superresolution and inpainting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>The closest work to ours is PULSE <ref type="bibr" target="#b35">[36]</ref>, which uses a similar generative causal model, yet only for super-resolution with a fixed forward corruption model. In this work, we generalise PULSE by enabling any corruption model (e.g., not just downsampling with bicubic interpolation). Another similar work is Image2StyleGAN <ref type="bibr" target="#b0">[1]</ref>, although it only focused on finding the latent StyleGAN vector that generates a given image, which has not been corrupted. The more updated Image2StyleGAN++ <ref type="bibr" target="#b1">[2]</ref> demonstrates image manipulations using the recovered latent variables, such as blending of two images, as well as inpainting. Compared to Im-age2StyleGAN++, we place our work in a Bayesian setting, and derive our loss function from the Bayesian MAP estimate. In addition, Image2StyleGAN++ <ref type="bibr" target="#b1">[2]</ref> was evaluated on a limited test set of 10 frontalized images, it is difficult to tell how well the method generalizes to large and diverse sets of images. We present comprehensive, 10-fold cross-validated results on FFHQ and two medical datasets, showing the ability of our method to generalise to other data distributions. Yet other approaches attempt to estimate encoders that map the input images directly into the latent space <ref type="bibr" target="#b39">[40]</ref> in an endto-end framework. However, such approaches are not robust to distributional shifts and the optimized embedder is specific to the latent space of a particular StyleGAN -in contrast, our corruption models are the same across all datasets.</p><p>Similar approaches have also been discussed in inverse problems research. Deep Bayesian Inversion <ref type="bibr" target="#b2">[3]</ref> is a related framework, and the actual implementation is based on a supervised learning approximation, requiring re-training for any distribution shift. Noise2Noise <ref type="bibr" target="#b28">[29]</ref> is a framework that enables reconstruction without any examples of clean images and without requiring and explicit corruption model. However, it does not build a prior over clean images, thus</p><formula xml:id="formula_0">I COR = f • G(w) I I CLN = G(w) w µ σ κ</formula><p>σpixel σperc G f <ref type="figure">Figure 3</ref>: Graphical model of our method. In gray shade are known observations or parameters: the input corrupted image I, the parameters µ, σ and κ defining the prior on w and σ pixel , σ percept , the parameters defining the noise model over I. Unknown latent variables (in white), that need to be estimated, are w, the latent vectors of StyleGAN, I CLN , the clean image, and I COR , the corrupted image simulated through the pipeline. Transformation G is modelled by the StyleGAN2 generator, while f by a known corruption model (e.g. downsampling with known kernel).</p><p>rendering it unsuitable for severely ill-posed problems (e.g. inpainting with large masks), that require inferring a distribution of plausible reconstructions. AmbientGAN <ref type="bibr" target="#b5">[6]</ref> also builds a generative GAN model of clean images given noisy observations only, for a specified corruption model. However, as opposed to our method, their focus is on training a new model able to generate clean data -instead, we show that one can obtain a state-of-the-art model for image reconstruction without any training, by using a pre-trained generator and only optimizing the corruption model at inference time. A mathematical analysis of compressed sensing with generative models has also been performed by <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>An overview of our method is given in <ref type="figure">Fig. 2</ref>. We assume a given generator G can model the distribution of clean images in a given dataset (e.g., human faces), then we use a pre-defined forward model f that corrupts the clean image. Given a corrupted input image I, we compute the optimal latent w that minimizes L(f • G(w), I). The graphical model is given in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Reconstruction using Bayes' theorem</head><p>Bayes' theorem offers the optimal solution to a general set of image reconstruction problems. Given an input corrupted image I, we aim to reconstruct the clean image I CLN .</p><p>In practice, there could be a distribution p(I CLN |I) of such clean images given a particular input image I, which is estimated using Bayes' theorem:</p><formula xml:id="formula_1">p(I CLN |I) ∝ p(I CLN )p(I|I CLN )<label>(1)</label></formula><p>The prior term p(I CLN ) describes the manifold of clean images, restricting the possible reconstructions I CLN to "realistic" images. In our context, the likelihood term p(I|I CLN ) describes the corruption process f , which takes a clean image and outputs a corrupted image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The image prior term</head><p>In our framework, the prior model p(I CLN ) has been trained a-priori, before the corruption task was known, hence satisfying the principle of independent mechanisms from causal modelling <ref type="bibr" target="#b38">[39]</ref>. We instantiate I CLN = G(w), where w = [w 1 , . . . , w 18 ] ∈ R 512×18 is the latent vector of StyleGAN2 (18 layers in total, one vector w i per layer), G : R 512×18 → R n G ×n G is the deterministic function given by the StyleGAN2 synthesis network, and n G ×n G is the output resolution of StyleGAN, in our case 1024x1024 (FFHQ, X-Rays) or 256x256 (brains). However, our framework is not specific to StyleGAN2: any generator function that has a low-dimensional space, such as that given by a VAE <ref type="bibr" target="#b23">[24]</ref>, can be used, as long as one can flow gradients through the model.</p><p>We use the change of variable formula to write the probability density function over clean images:</p><formula xml:id="formula_2">p(I CLN ) := p(G(w)) = p(w) det ∂G(w) ∂w −1<label>(2)</label></formula><p>While the traditional change of variables formula assumes that the function G is invertible, it can be extended to noninvertible 1 mappings <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. In addition, we assume that the jacobian ∂G(w) ∂w is constant for all w, in order to simplify the calculations later on.</p><p>We now seek to instantiate p(w). Since the latent space of StyleGAN2 consists of many vectors w = [w 1 , . . . , w L ], where L = 18 (one for each layer), we need to set meaningful priors for them. While StyleGAN2 assumed that all vectors w i are equal, we slightly relax that assumption but set two priors: (i) a cosine similarity prior that ensures they are roughly colinear, and (ii) another prior N (w i |µ, σ 2 ) that ensures they lie in the same region as the latents used during training. We use the following distribution for p(w):</p><formula xml:id="formula_3">p(w) = i N w i µ, σ 2 i,j M cos −1 w i w T j |w i ||w j | 0, κ<label>(3)</label></formula><p>where cos −1 wiw T j |wi||wj | is the angle between vectors w i and w j , and M(.|0, κ) is the von Mises distribution with mean zero and scale parameter κ which ensures that vectors w i are aligned. This distribution is analogous to a Gaussian distribution over angles in [0, 2π]. We compute µ and σ as the mean and standard deviation of 10,000 latent variables passed through the mapping network, like the original Style-GAN2 inversion <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The image likelihood term</head><p>We instantiate the likelihood term p(I|I CLN ) with a potentially probabilistic forward corruption process  We can write the likelihood model as instantiated through f as:</p><formula xml:id="formula_4">p(I|I CLN ) = p(I|G(w)) = p(I|f • G(w))|J f (G(w)) | −1 (4) where J f (G(w)) = ∂f •G(w) ∂G(w)</formula><p>is the Jacobian matrix of f evaluated at G(w), and is again assumed constant. For the noise model in p(I|f •G(w)), we consider two types of noise distributions: pixelwise independent Gaussian noise, as well as "perceptual noise", i.e. independent Gaussian noise in the perceptual VGG embedding space. This gives us the following model <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_5">p(I|f • G(w)) = N (I|f • G(w), σ 2 pixel I n 2 f ) N (φ(I)|φ • f • G(w), σ 2 percept I n 2 φ )<label>(5)</label></formula><p>2 Since in our experiments θ is fixed, we drop the notation of θ in subsequent derivations. <ref type="bibr" target="#b2">3</ref> We note that the formulation in Eq. 5 is equivalent to p(I|I CLN ) = </p><formula xml:id="formula_6">N I φ(I) f • G(w) φ • f • G(w) , σ 2 pixel I n 2 f 0 0 σ 2 percept I n 2 φ where φ : R n f ×n f → R n φ ×n φ is the VGG network,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Image restoration as Bayesian MAP estimate</head><p>Using Eq. (1), the restoration of the optimal clean image I * CLN given a noisy input image I can be performed through the Bayesian maximum a-posteriori (MAP) estimate:</p><formula xml:id="formula_7">I * CLN = arg max I CLN p(I CLN |I) = arg max I CLN p(I CLN )p(I|I CLN )<label>(6)</label></formula><p>We now instantiate the prior p(I CLN ) and the likelihood p(I|I CLN ) with formulas from Eq. 3 and Eq. 5, and recast the problem as an optimisation over w. We now aim to find the optimal w * which maximises:</p><formula xml:id="formula_8">w * = arg max w p(w)p(I|w) = i N (w i |µ, σ 2 ) i,j M(cos −1 w i w T j |w i ||w j | |0, κ) N (I|f • G(w), σ 2 pixel I n 2 f ) N (φ(I)|φ • f • G(w), σ 2 percept I n 2 φ )<label>(7)</label></formula><p>Given the Bayesian MAP solution w * , the clean image is returned as I * CLN = G(w * ). Eq. 7 can be simpified to the following loss function (see Supplementary section A for full derivation): </p><formula xml:id="formula_9">w i − µ σ i 2 Lw −2κ i,j w i w T j |w i ||w j | L colin + σ −2 pixel I − f • G(w) 2 2 L pixel + σ −2 percept I − φ • f • G(w) 2 2 Lpercept<label>(8)</label></formula><p>which can be succintly written as a weighted sum of four loss terms:</p><formula xml:id="formula_10">w * = arg min w L w + λ c L colin + λ x L pixel + λ p L percept (9)</formula><p>where L w is the prior loss over w, L colin is the colinearity loss on w, L pixel is the pixelwise loss on the image reconstruction, and L percept is the perceptual loss, λ c = −2κ, λ pixel = σ −2 pixel and λ percept = σ −2 percept .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Model Optimisation</head><p>We optimise the loss in Eq. 9 using Adam with learning rate of 0.001, while fixing λ c , λ x and λ p a-priori. On our datasets, we found the following values to give good results: λ c = 0.03, λ pixel = 10 −5 , λ percept = 0.01. <ref type="figure" target="#fig_0">In Fig 4,</ref> we show image super-resolution and inpainting starting from the original StyleGAN2 inversion, and gradually modifying the loss function and optimisation until we arrive at our proposed solution. The original StyleGAN2 inversion results in line artifacts for super-resolution, while for inpainting it cannot reconstruct well. After removing the optimisation of noise layers from the original StyleGAN2 inversion <ref type="bibr" target="#b22">[23]</ref> and switching to W + , the image quality improves for superresolution, while for inpainting the existing image is recovered well, but the reconstructed part gets even worse. More improvements are observed by adding the pixelwise L 2 loss, mostly because the perceptual loss only operates at 256x256 resolution. Adding the prior on w and the cosine loss yields a smoother reconstruction with less artifacts, especially for inpainting. While Image2StyleGAN <ref type="bibr" target="#b0">[1]</ref>showed that any image can be "recovered" with the StyleGAN2, we note this is not possible in our case for image reconstruction, as we cannot fit the noise variables and w i latent variables belonging the high-resolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Model training and evaluation</head><p>We train our model on data from three datasets: (i) 70,000 images from FFHQ <ref type="bibr" target="#b21">[22]</ref> at 1024x1024 resolution, 240,000 frontal-view chest X-ray image from MIMIC III <ref type="bibr" target="#b19">[20]</ref> at 1024x1024 resolution, as well as 7,329 middle coronal 2D slices from a collection of 5 brain datasets: ADNI <ref type="bibr" target="#b18">[19]</ref>, OA-SIS <ref type="bibr" target="#b33">[34]</ref>, PPMI <ref type="bibr" target="#b34">[35]</ref>, AIBL <ref type="bibr" target="#b12">[13]</ref> and ABIDE <ref type="bibr" target="#b16">[17]</ref>. All brain images were pre-registered rigidly. For all experiments, we trained the generator, in our case StyleGAN2, on 90% of the data, and left the remaining 10% for testing. We did not use the pre-trained StyleGAN2 on FFHQ as it was trained on the full FFHQ. Training was performed on 4 Titan-Xp GPUs using StyleGAN2 config-e, and was performed for 20,000,000 images shown to the discriminator (20,000 kimg), which took almost 2 weeks on our hardware.</p><p>For super-resolution, we compared our approach to PULSE <ref type="bibr" target="#b35">[36]</ref>, ESRGAN <ref type="bibr" target="#b47">[48]</ref> and SRFBN <ref type="bibr" target="#b30">[31]</ref>, while for inpainting, we compared to SN-PatchGAN <ref type="bibr" target="#b49">[50]</ref>. For these methods, we downloaded the pre-trained models. For PULSE <ref type="bibr" target="#b35">[36]</ref>, we could only apply it on FFHQ, as we were unable to re-train StyleGAN2 in Pytorch instead of Tensor-  <ref type="figure">Figure 6</ref>: Qualitative evaluation on medical datasets at different resolutions. The left column shows input images, while the right column shows the true high-quality images. Our method shows improved quality of reconstructions across all resolution levels and datasets. We used the exact same setup as in FFHQ in <ref type="figure">Fig. 5</ref>, without any dataset-specific parameter tuning.</p><p>flow, which we used in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training StyleGAN2</head><p>In <ref type="figure">Supplementary Fig. 9</ref>, we show uncurated images generated by the cross-validated StyleGAN2 trained on our medical datasets, along with a few real examples. For the highresolution X-rays, we notice that the image quality is very good, although some artifacts are still present: some text tags are not properly generated, some bones and rib contours are wiggly, and the shoulder bones show less contrast. For the brain dataset, we do not notice any clear artifacts, although we did not assess distributional preservation of regional volumes as in <ref type="bibr" target="#b44">[45]</ref>. For the cross-validated FFHQ model, we obtained an FID of 4.01, around 0.7 points higher than the best result of 3.31 reported for config-e [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Super-resolution</head><p>We then ran our framework for super-resolution at different levels, and compared it to previous state-of-the-art models at different resolution levels in <ref type="figure">Fig. 5 and Fig. 6</ref>. On all three datasets, our method performs considerably better than other methods, in particular at lower input resolutions:</p><p>ESRGAN yields jittery artifacts, SRFBN gives smoothed-out results, while PULSE generates very high-resolution images that don't match the true image, likely due to the hard projection of their optimized latent to S d−1 , the unit sphere in d-dimensions, as opposed to a soft prior term such as L w in our case. Moreover, as opposed to ESRGAN and SRFBN, both our model as well as PULSE can perform more than x4 super-resolution, going up to 1024x1024. Without changing any hyper-parameters, we find the same results on the other two medical datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inpainting</head><p>In <ref type="figure" target="#fig_2">Fig. 7</ref>, we show results of our method on in-painting with arbitrary as well as rectangular masks, and compare it to the leading inpainting model SN-PatchGAN <ref type="bibr" target="#b49">[50]</ref>. Our method produces considerably better results than SN-PatchGAN. In particular, SN-PatchGAN lacks high-level semantics in the reconstruction, and cannot handle large masks. For example, in the first figure, when the mother is cropped out, Sn-PatchGAN is unable to reconstruct the ear. Our method on the other hand is able to reconstruct the ear and the jawline, albeit there is considerable room for further improvement. One reason for the lower performance of SN-PatchGAN could be that it was trained on CelebA, which has lower variation than FFHQ. In <ref type="figure">Supplementary Figs. 10,  11</ref> and 12, we show further inpainting examples with our method as well as SN-PatchGAN <ref type="bibr" target="#b49">[50]</ref>, on all three datasets, and for different types of arbitrary masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Quantitative evaluation</head><p>In <ref type="table">Table 1</ref>, we show quantitative evaluation of superresolution on 100 unseen images at different resolution levels. At low 16x16 input resolutions, our method outperforms all other super-resolution methods consistently on all three datasets. However, at resolutions of 32x32 and higher, SRFBN <ref type="bibr" target="#b30">[31]</ref> achieves the lowest LPIPS <ref type="bibr" target="#b50">[51]</ref> and root mean squared error (RMSE), albeit the qualitative results from this method showed that the reconstructions are overly smooth, lacking detail. The performance degradation of our model is likely because the StyleGAN2 generator G cannot easily generate these unseen images at high resolutions, although this is expected to change in the near future given the fastpaced improvements in such generator models. In addition, as pointed out by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51]</ref>, measures such as RMSE and PSNR are not good for estimating perceptual quality.</p><p>To account for human perceptual quality, we performed a forced-choice pairwise comparison test, which has been shown to be most sensitive and simple for users to perform <ref type="bibr" target="#b32">[33]</ref>. Twenty raters were each shown 100 test pairs of the true image and the four reconstructed images by each algorithm, and raters were asked to choose the best reconstruction (see supplementary section A.1 for more information on the design). We opted for this paired test instead of the mean   <ref type="table">Table 3</ref>: Evaluation of inpainting on 100 unseen images. opinion score (MOS) because it also accounts for fidelity of the reconstruction to the true image. This is important in our setup, because a method such as PULSE can reconstruct high-resolution faces that are nonetheless of a different person (see <ref type="figure">Fig. 5</ref>). In <ref type="table" target="#tab_5">Table 2</ref>, the results of the test confirm that out method is the best at low 16 2 resolutions, with lower performance at resolutions of 64 2 and above. In Supplementary <ref type="table" target="#tab_7">Table 4</ref>, we additionally show PSNR, SSIM and MAE scores, which show a similar behavior to RMSE, as they perform pixelwise comparisons.</p><p>For quantitative evaluation on inpainting, we generated 7 masks similar to the setup of <ref type="bibr" target="#b1">[2]</ref>, and applied them in cyclical order to 100 unseen images from the test sets of each dataset. In <ref type="table">Table 3</ref>, we show that our method consistently outperforms SN-PatchGAN <ref type="bibr" target="#b49">[50]</ref> on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Method limitations</head><p>In <ref type="figure" target="#fig_3">Fig. 8 (top)</ref>, we show an example failure cases on the super-resolution task. The reason for the failure is likely due to the limited generalisation abilities of the StyleGAN2 generator to such unseen images. We particularly note that, as opposed to the simple inversion of Image2StyleGAN <ref type="bibr" target="#b0">[1]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Reconstruction True Input Downsampled Rec. Difference which relies on latent variables at high resolution to recover the fine details, we cannot optimize these high-resolution latent variables, thus having to rely on the proper ability of StyleGAN2 to extrapolate from lower-level latent variables. Another limitation of our method is the inconsistency between the downsampled input image and the givne input image, which we exemplify in <ref type="figure" target="#fig_3">Fig. 8 (bottom)</ref>. As shown in <ref type="figure" target="#fig_0">Supplementary Figs 14 and 15</ref>, these occur for higher resolution input images. We attribute this again to the limited generalisation of the generator to these unseen images. The same inconsistency also applies to in-painting, as shown in <ref type="figure" target="#fig_0">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We proposed a simple framework for performing different reconstruction tasks using powerful generators such as StyleGAN2. We demonstrated our method on two reconstruction tasks, and on three distinct datasets, including two challenging medical datasets. We obtain better results than SOTA models in super-resolution for low-resolution inputs, as well as inpainting, confirmed through both qualitative and quantitative evaluations. Future work can focus on jointly optimizing the parameters of the corruption models, as well as analyzing other corruption models. photo upsampling via latent space exploration of generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2437-2445, 2020. <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13</ref> κ, and σ 2 pixel I n 2 f and σ 2 percept I n 2 φ are identity matrices scaled by variance terms.</p><p>The Bayesian MAP estimate is the vector w * that maximises Eq. 10, and provides the most likely vector w that could've generated input image I:</p><formula xml:id="formula_11">w * = arg max w p(w)p(I|w) = arg max w i N (w i |µ, σ 2 ) i,j M(cos −1 w i w T j |w i ||w j | |0, κ) N (I|f • G(w), σ 2 pixel I n 2 f ) N (φ(I)|φ • f • G(w), σ 2 percept I n 2 φ )<label>(11)</label></formula><p>Since logarithm is a strictly monotonic function that won't change the output of the arg max w operator, we take the logarithm to simplify Eq. 11 to:</p><formula xml:id="formula_12">w * = arg max w i log N (w i |µ, σ 2 )+ i,j log M(cos −1 w i w T j |w i ||w j | |0, κ)+ log N (I|f • G(w), σ 2 pixel I n 2 f )+ log N (φ(I)|φ • f • G(w), σ 2 percept I n 2 φ )<label>(12)</label></formula><p>We expand the probability density functions of each distribution to get:</p><formula xml:id="formula_13">w * = arg max w i C 1 − (w i − µ) 2 2σ 2 i + i,j C 2 + κcos(cos −1 w i w T j |w i ||w j | ) + C 1 − 1 2 (I − f • G(w)) T (σ −2 pixel I n 2 f )(I − f • G(w)) + C 2 − 1 2 (I − φ • f • G(w)) T (σ −2 percept I n 2 φ ) (I − φ • f • G(w)) (13) where C 1 = log (2πσ 2 i ) − 1 2 , C 2 = log (−2πI 0 (κ)), C 3 = log ((2π) n 2 |σ pixel I n 2 f |) − 1 2 and C 4 = log ((2π) m 2 |σ percept I n 2 φ |) − 1 2</formula><p>are constants with respect to w, so we can ignore them.</p><p>We remove the constants, multiply by (-2), which requires switching to the arg min operator, to get:</p><formula xml:id="formula_14">w * = arg min w i w i − µ σ i 2 − 2κ i,j w i w T j |w i ||w j | + σ −2 pixel I − f • G(w) 2 2 + σ −2 percept I − φ • f • G(w) 2 2<label>(14)</label></formula><p>This is equivalent to Eq. 8, which finishes our proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Evaluation through human raters</head><p>To evaluate human perceptual quality, we performed a forced-choice pairwise comparison test as shown in <ref type="figure" target="#fig_5">Fig. 13</ref>. Each rater is shown a true, high-quality image on the left, and four potential reconstructions they have to choose from. For each input resolution level (16 2 , . . . , 128 2 ), we ran the human evaluation on 20 raters using 100 pairs of 5 images each (total of 500 images per experiment shown to each rater). We launched all human evaluations on Amazon Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Method inconsistency</head><p>One caveat of our method is that it can create reconstructions that are inconsistent with the input data. We highlight this in <ref type="figure" target="#fig_0">Figs. 14 and 15</ref>. This is because our method relies on the ability of a pre-trained generator to generate any potential realistic image as input. In addition to that, in Eq. (9), our method optimises the pixelwise and perceptual loss terms between the input image and the downsampled reconstruction. As we show in <ref type="figure" target="#fig_0">Figs. 14 and 15</ref>, while there are little differences between the input and the reconstruction at low 16x16 resolutions, at higher 128x128 resolutions, these differences become larger and more noticeable. Another aspect that affects contributes to this issue is the extra prior term L cosine , which is however required to ensure better reconstructions (see <ref type="figure" target="#fig_0">Fig 4)</ref>. Nevertheless, we believe that the inconsistency is fundamentally caused by limitations of the generator G, that will be solved in the near future with better generator models that offer improved generalisability to unseen images.</p><p>Real Generated (FID: 9.2) Real Generated (FID: 7.3) <ref type="figure">Figure 9</ref>: Uncurated images generated by our StyleGAN2 generator trained on the chest X-ray dataset (MIMIC III) (top) and the brain dataset (bottom). Left images are random examples of real images from the actual datasets, while the right-side images are generated. The image quality is relatively good, albeit some anatomical artifacts are still observed, such as incomplete labels, wiggly bones or discontinuous wires.     <ref type="figure" target="#fig_0">Figure 14</ref>: Inconsistency of our method on FFHQ, across different resolution levels, using uncurated example pictures. The reconstructions by our method are downsampled to match the input resolution (middle column). For higher resolution inputs (128x128), the method cannot accurately reconstruct the input image, likely because the generator has limited generalisability to such unseen faces from FFHQ (our method was trained not on the entire FFHQ, but on a training subset). The difference maps, representing x3 scaled mean absolute errros, show that certain regions in particular are not well reconstructed, such as the hair of the girl on the right. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Reconstructions as the loss function evolves from the original StyleGAN2 inversion to our proposed method. Top row shows super resolution, while bottom row shows inpainting. We start from (a) the original StyleGAN2 inversion, and (b) remove noise optimisation, (c) extend optimisation to full W + space, (d) add pixelwise L 2 term, (e) add prior on w latent variables and (f) add colinear loss term for w. f (I CLN ; θ), parameterized by θ 2 . In this work, we present two types of corruption processes f as follows: • Super-resolution: f SR is defined as the forward operator that performs downsampling parameterized by a given kernel k. For a high-resolution image I CLN , this produces a low-resolution (corrupted) image I COR = (I CLN k) ↓ s , where denotes convolution and ↓ s denotes downsampling operator by a factor s. The parameters of this process are θ = {k, s} • In-painting with arbitrary mask: f IN is implemented as an operator that performs pixelwise multiplication with a mask M . For a given clean image I CLN and a 2D binary mask M , it produces a cropped-out (corrupted) image I COR = I CLN M , where is the Hadamard product. The parameters of this corruption process are θ = {M } where M ∈ {0, 1} H×W , where H and W are the height and width of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>σ 2 pixel I n 2 f 2 fand I n 2 φ are of dimensions n 2 f × n 2 f and n 2 φ</head><label>2222</label><figDesc>and σ 2 percept I n 2 φ are diagonal covariance matrices, n f × n f and n φ × n φ are the resolutions of the corrupted images f •G(w) as well as perceptual embeddings φ•f •G(w). Note that, in Eq. 5, images I, φ(I), f •G(w) and φ•f •G(w) are flattened to 1D vectors, while covariance matrices I n × n 2 φ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between our method and SN-PatchGAN [50]on inpainting. SN-PatchGAN fails on large masks, while our method can recover high-level structure, such as the ear and jawline of the little boy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>(top row) Failure case of our method. (bottom row) Inconsistency of our method between input image and the downsampled reconstruction. The right side difference image (in L 1 ), between the input and the downsampled reconstruction, shows inconsistent regions in white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :Figure 11 :Figure 12 :</head><label>101112</label><figDesc>Uncurated inpainting examples on the FFHQ dataset. Uncurated inpainting examples on the Chest X-ray dataset. Mask SN-PatchGAN [50] BRGM Uncurated inpainting examples on the brain dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 :</head><label>13</label><figDesc>Setup of our human study, using a forced-choice pairwise comparison design. Each rater is shown a true, high-quality imge on the left, and four potential reconstructions (A-D) by different algorithms. They have to select which reconstruction best resembled the HQ image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 :</head><label>15</label><figDesc>Inconsistency of our method on the medical datasets, using uncurated examples. Same setup as inFig. 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>FFHQ 16 2 0.24/25.66 0.29/27.14 0.35/29.32 0.33/22.07 FFHQ 32 2 0.30/18.93 0.48/42.97 0.29/23.02 0.23/12.73 FFHQ 64 2 0.36/16.07 0.53/41.31 0.26/18.37 0.23/9.40 FFHQ 128 2 0.34/15.84 0.57/34.89 0.15/15.84 0.09/7.55 X-ray 16 2 0.18/11.61</figDesc><table><row><cell>Dataset</cell><cell cols="4">BRGM PULSE [36] ESRGAN [48] SRFBN [31]</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>0.32/14.67</cell><cell>0.37/12.28</cell></row><row><cell cols="2">X-ray 32 2 0.23/10.47</cell><cell>-</cell><cell>0.32/12.56</cell><cell>0.21/6.84</cell></row><row><cell cols="2">X-ray 64 2 0.31/10.58</cell><cell>-</cell><cell>0.30/8.67</cell><cell>0.22/5.32</cell></row><row><cell cols="2">X-ray 128 2 0.27/10.53</cell><cell>-</cell><cell>0.20/7.19</cell><cell>0.07/4.33</cell></row><row><cell cols="2">Brains 16 2 0.12/12.42</cell><cell>-</cell><cell>0.34/22.81</cell><cell>0.33/12.57</cell></row><row><cell cols="2">Brains 32 2 0.17/11.08</cell><cell>-</cell><cell>0.31/14.16</cell><cell>0.18/6.80</cell></row><row><cell cols="5">Table 1: Evaluation of (x4) super-resolution at different input</cell></row><row><cell cols="5">resolution levels (16 2 − 64 2 ). Reported are LPIPS/RMSE</cell></row><row><cell cols="3">scores -lower scores are better.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">BRGM PULSE [36] ESRGAN [48] SRFBN [31]</cell></row><row><cell>FFHQ 16 2</cell><cell>0.42</cell><cell>0.32</cell><cell>0.11</cell><cell>0.15</cell></row><row><cell>FFHQ 32 2</cell><cell>0.39</cell><cell>0.02</cell><cell>0.12</cell><cell>0.47</cell></row><row><cell>FFHQ 64 2</cell><cell>0.14</cell><cell>0.08</cell><cell>0.32</cell><cell>0.45</cell></row><row><cell cols="2">FFHQ 128 2 0.14</cell><cell>0.10</cell><cell>0.39</cell><cell>0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Proportion of votes for the best super-resolution reconstruction in the forced-choice pairwise comparison test. Our method was voted best in particular at low input resolutions.</figDesc><table><row><cell></cell><cell>BRGM</cell><cell>SN-PatchGAN [50]</cell></row><row><cell cols="3">Dataset LPIPS RMSE PSNR SSIM LPIPS RMSE PSNR SSIM</cell></row><row><cell cols="3">FFHQ 0.19 24.28 21.33 0.84 0.24 30.75 19.67 0.82</cell></row><row><cell>X-ray</cell><cell cols="2">0.13 13.55 27.47 0.91 0.20 27.80 22.02 0.86</cell></row><row><cell cols="3">Brains 0.09 8.65 30.94 0.88 0.22 24.74 21.47 0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SSIM ↑ MAE ↓ PSNR ↑ SSIM ↑ MAE ↓ PSNR ↑ SSIM ↑ MAE ↓ PSNR ↑ SSIM ↑ MAE ↓</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>BRGM</cell><cell></cell><cell></cell><cell>PULSE [36]</cell><cell></cell><cell cols="3">ESRGAN [48]</cell><cell cols="2">SRFBN [31]</cell><cell></cell></row><row><cell cols="2">PSNR ↑ FFHQ 16 2 20.13</cell><cell>0.74</cell><cell>17.46</cell><cell>19.51</cell><cell>0.68</cell><cell>19.20</cell><cell>18.91</cell><cell>0.69</cell><cell>20.01</cell><cell>21.43</cell><cell>0.76</cell><cell>15.01</cell></row><row><cell>FFHQ 32 2</cell><cell>22.74</cell><cell>0.74</cell><cell>12.52</cell><cell>15.37</cell><cell>0.35</cell><cell>33.13</cell><cell>21.10</cell><cell>0.72</cell><cell>14.53</cell><cell>26.28</cell><cell>0.89</cell><cell>7.46</cell></row><row><cell>FFHQ 64 2</cell><cell>24.16</cell><cell>0.70</cell><cell>10.63</cell><cell>15.74</cell><cell>0.37</cell><cell>31.54</cell><cell>23.14</cell><cell>0.72</cell><cell>10.94</cell><cell>28.96</cell><cell>0.90</cell><cell>5.21</cell></row><row><cell cols="2">FFHQ 128 2 24.29</cell><cell>0.65</cell><cell>10.53</cell><cell>17.23</cell><cell>0.45</cell><cell>25.87</cell><cell>24.53</cell><cell>0.70</cell><cell>9.20</cell><cell>30.98</cell><cell>0.90</cell><cell>4.06</cell></row><row><cell>X-ray 16 2</cell><cell>27.14</cell><cell>0.91</cell><cell>7.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.17</cell><cell>0.87</cell><cell>10.14</cell><cell>26.88</cell><cell>0.92</cell><cell>7.63</cell></row><row><cell>X-ray 32 2</cell><cell>27.84</cell><cell>0.84</cell><cell>6.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.44</cell><cell>0.81</cell><cell>8.36</cell><cell>31.80</cell><cell>0.95</cell><cell>3.71</cell></row><row><cell>X-ray 64 2</cell><cell>27.62</cell><cell>0.79</cell><cell>6.63</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.47</cell><cell>0.87</cell><cell>5.33</cell><cell>33.91</cell><cell>0.95</cell><cell>2.47</cell></row><row><cell cols="2">X-ray 128 2 27.34</cell><cell>0.78</cell><cell>6.74</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.78</cell><cell>0.87</cell><cell>4.28</cell><cell>35.59</cell><cell>0.96</cell><cell>1.77</cell></row><row><cell>Brains 16 2</cell><cell>26.33</cell><cell>0.84</cell><cell>7.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.06</cell><cell>0.60</cell><cell>14.27</cell><cell>26.21</cell><cell>0.77</cell><cell>8.62</cell></row><row><cell>Brains 32 2</cell><cell>27.30</cell><cell>0.81</cell><cell>6.54</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.23</cell><cell>0.78</cell><cell>8.35</cell><cell>31.60</cell><cell>0.93</cell><cell>3.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Additional PSNR, SSIM and MAE scores for the super-resolution evaluation.</figDesc><table><row><cell>13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The supplementary section of<ref type="bibr" target="#b9">[10]</ref> presents an excellent introduction to the generalised change of variable theorem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The von-Mises distribution is the analogous of the Gaussian distribution over angles [0 − 2π]. M(.|µ, κ) is analogous to N (.|µ, σ), where κ −1 = σ 2</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of loss function for the Bayesian MAP estimate</head><p>We assume w = [w 1 , . . . , w 18 ] ∈ R 512×18 is the Style-GAN2 latent vector, I ∈ R n×n is the corrupted input image, G : R 512×18 → R n G ×n G is the StyleGAN2 generator network function, f : R n G ×n G → R n f ×n f is the corruption function, and φ : R n f ×n f → R n φ ×n φ is a function describing the perceptual network. n G × n G , n f × n f and n φ × n φ are the resolutions of the clean image G(w), corrupted image f • G(w) and of the perceptual embedding φ • f • G(w). The full Bayesian likelihood of our model, taken from Eq. 7, is:</p><p>where µ ∈ R, σ ∈ R are means and standard deviations of the prior on w i , the i-th element of w, M(.|0, κ) is the von Mises distribution <ref type="bibr" target="#b3">4</ref> with mean zero and scale parameter</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Im-age2stylegan++: How to edit the embedded images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8296" to="8305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Öktem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05910</idno>
		<title level="m">Deep bayesian inversion</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. K-Svd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<title level="m">Compressed sensing using generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ambientgan: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">ICLR</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian color constancy. JOSA A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1393" to="1411" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimal achievable sufficient statistic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günther</forename><surname>Koliander</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1465" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anatomical priors in convolutional networks for unsupervised biomedical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert R</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The australian imaging, biomarkers and lifestyle (AIBL) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of Alzheimer&apos;s disease. International psychogeriatrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kathryn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><forename type="middle">I</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><forename type="middle">De</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">T</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Lautenschlager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maruff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="672" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bound optimization approach to wavelet-based image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert D</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">782</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The generic viewpoint assumption in a framework for visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">6471</biblScope>
			<biblScope unit="page" from="542" to="545" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Yarin Gal. Uncertainty in deep learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Identification of autism spectrum disorder using deep learning and the ABIDE dataset. NeuroImage: Clinical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anibal</forename><surname>Sólon Heinsfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">Rosa</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Craddock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Buchweitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Meneguzzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford R Jack</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Borowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><forename type="middle">J</forename><surname>Britson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Whitwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chadwick</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stylebased generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Geometric integration theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold R Parks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Leeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07796</idno>
		<title level="m">Structural autoencoders improve representations for generation and transfer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04189</idno>
		<title level="m">Noise2noise: Learning image restoration without clean data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparison of four subjective methods for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafał</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radosław</forename><surname>Tomaszewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2478" to="2491" />
		</imprint>
		<respStmt>
			<orgName>Wiley Online Library</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Fotenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">L</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2677" to="2684" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Parkinson&apos;s progression markers initiative (PPMI)-establishing a PD biomarker cohort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohini</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Siderowf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Lasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Caspell-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Simuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><forename type="middle">M</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Q</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of clinical and translational neurology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1460" to="1477" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PULSE: Self-supervised</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Elements of causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951,2020.3</idno>
		<title level="m">Encoding in style: a stylegan encoder for imageto-image translation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploiting the sparse derivative prior for super-resolution and image demosaicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marshall F Tappen Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the solution of illposed problems and the method of regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Nikolaevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tikhonov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady Akademii Nauk</title>
		<imprint>
			<date type="published" when="1963" />
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="501" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1303" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petru-Daniel</forename><surname>Tudosiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parashkev</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05692</idno>
		<title level="m">Neuromorphologicaly-preserving volumetric data encoding using VQ-VAE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew S</forename><surname>Bruce R Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7697</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
