<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Pooling for Graph Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
						</author>
						<title level="a" type="main">Relational Pooling for Graph Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Applications with relational graph data, such as molecule classification, social and biological network prediction, first order logic, and natural language understanding, require an effective representation of graph structures and their attributes. While representation learning for graph data has made tremendous progress in recent years, current schemes are unable to produce so-called most-powerful representations that can provably distinguish all distinct graphs up to graph isomorphisms. Consider for instance the broad class of Weisfeiler-Lehman (WL) based Graph Neural Networks (WL-GNNs) <ref type="bibr" target="#b17">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b30">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b20">Gilmer et al., 2017;</ref><ref type="bibr" target="#b21">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b57">Velickovic et al., 2018;</ref><ref type="bibr" target="#b39">Monti et al., 2017;</ref><ref type="bibr" target="#b63">Ying et al., 2018;</ref><ref type="bibr" target="#b62">Xu et al., 2019;</ref><ref type="bibr" target="#b40">Morris et al., 2019)</ref>. These are unable to distinguish pairs of nonisomorphic graphs on which the standard WL isomorphism heuristic fails <ref type="bibr" target="#b10">(Cai et al., 1992;</ref><ref type="bibr" target="#b62">Xu et al., 2019;</ref><ref type="bibr" target="#b40">Morris et al., 2019)</ref>. As graph neural networks (GNNs) are applied to increasingly more challeng- ing problems, having a most-powerful framework for graph representation learning would be a key development in geometric deep learning .</p><p>In this work we introduce Relational Pooling (RP), a novel framework with maximal representation power for any graph input. In RP, we specify an idealized most-powerful representation for graphs and a framework for tractably approximating this ideal. The ideal representation can distinguish pairs of nonisomorphic graphs even when the WL isomorphism test fails, which motivates a straightforward procedure using approximate RP -we call this RP-GNNfor making GNNs more powerful.</p><p>A key inductive bias for graph representations is invariance to permutations of the adjacency matrix (graph isomorphisms), see <ref type="bibr" target="#b0">Aldous (1981)</ref>; <ref type="bibr" target="#b15">Diaconis &amp; Janson (2008)</ref>; <ref type="bibr" target="#b45">Orbanz &amp; Roy (2015)</ref>. Our work differs in its focus on learning representations of finite but variable-size graphs. In particular, given a finite but arbitrary-sized graph G potentially endowed with vertex or edge features, RP outputs a representation f (G) ∈ R d h , d h &gt; 0 , that is invariant to graph isomorphisms. RP can learn representations for each vertex in a graph, though to simplify the exposition, we focus on learning one representation of the entire graph.</p><p>Contributions. We make the following contributions: (1) We introduce Relational Pooling (RP), a novel framework for graph representation that can be combined with any existing neural network architecture, including ones not generally associated with graphs such as Recurrent Neural Networks (RNNs). (2) We prove that RP has maximal representation power for graphs and show that combining WL-GNNs with RP can increase their representation power. In our experiments, we classify graphs that cannot be distinguished by a state-of-the-art WL-GNN <ref type="bibr" target="#b62">(Xu et al., 2019)</ref>. <ref type="formula" target="#formula_4">(3)</ref> We introduce approximation approaches that make RP computationally tractable. We demonstrate empirically that these still lead to strong performance and can be used with RP-GNN to speed up graph classification when compared to traditional WL-GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relational Pooling</head><p>Notation. We consider graphs endowed with vertex and edge features. That is, let G = (V, E, X (v) , X (e) ) be a graph with vertices V , edges E ⊆ V × V , vertex fea-arXiv:1903.02541v2 <ref type="bibr">[cs.</ref>LG] 14 May 2019 tures stored in a |V | × d v matrix X <ref type="bibr">(v)</ref> , and edge features stored in a |V | × |V | × d e tensor X <ref type="bibr">(e)</ref> . W.l.o.g, we let V := {1, . . . , n}, choosing some arbitrary ordering of the vertices. Unlike the vertex features X <ref type="bibr">(v)</ref> , these vertex labels do not represent any meaningful information about the vertices, and learned graph representations should not depend upon the choice of ordering. Formally, there always exists a bijection on V (called a permutation or isomorphism) between orderings so we desire permutationinvariant, or equivalently, isomorphic-invariant functions.</p><p>In this work, we encode G by two data structures: (1) a |V | × |V | × (1 + d e ) tensor that combines G's adjacency matrix with its edge features and (2) a |V | × d v matrix representing node features X <ref type="bibr">(v)</ref> . The tensor is defined as A v,u,· = 1 (v,u)∈E X (e) v,u for v, u ∈ V where [· ·] denotes concatenation along the 3rd mode of the tensor, 1 <ref type="bibr">(·)</ref> denotes the indicator function, and X (e) v,u denotes the feature vector of edge (v, u) by a slight abuse of notation. A permutation is bijection π : V → V from the label set V to itself. If vertices are relabeled by a permutation π, we represent the new adjacency tensor by A π,π , where (A π,π ) π(i),π(j),k = A i,j,k ∀i, j ∈ V , k ∈{1, . . . , 1 + d e }; the index k over edge features is not permuted. Similarly, the vertex features are represented by X</p><formula xml:id="formula_0">(v) π where (X (v) π ) π(i),l = X (v)</formula><p>i,l , ∀i ∈ V and l ∈ {1, . . . , d v }. The Supplementary Material shows a concrete example and <ref type="bibr" target="#b28">Kearnes et al. (2016)</ref> use a similar representation.</p><p>For bipartite graphs (e.g., consumers × products), V is partitioned by V <ref type="bibr">(r)</ref> and V (c) and a separate permutation function can be defined on each. Their encoding is similar to the above and we define RP for the two different cases below. Joint RP. Inspired by joint exchangeability <ref type="bibr" target="#b0">(Aldous, 1981;</ref><ref type="bibr" target="#b15">Diaconis &amp; Janson, 2008;</ref><ref type="bibr" target="#b45">Orbanz &amp; Roy, 2015)</ref>, we define a joint RP permutation-invariant function of nonbipartite graphs, whether directed or undirected, as</p><formula xml:id="formula_1">f (G) = 1 |V |! π∈Π |V | f (A π,π , X (v) π ),<label>(1)</label></formula><p>where Π |V | is the set of all distinct permutations of V and f is an arbitrary (possibly permutation-sensitive) function of the graph with codomain R d h . Following <ref type="bibr" target="#b41">Murphy et al. (2019)</ref>, we use the notation · to denote permutationinvariant function. Since Equation 1 averages over all permutations of the labels V , f is a permutation-invariant function and can theoretically represent any such function g (consider f = g). We can compose f with another function ρ (outside the summation) to capture additional signal in the graph. This can give a maximally expressive, albeit intractable, graph representation (Theorem 2.1). We later discuss tractable approximations for f and neural network architectures for f .</p><p>Separate RP. RP for bipartite graphs is motivated by separate exchangeability <ref type="bibr" target="#b15">(Diaconis &amp; Janson, 2008;</ref><ref type="bibr" target="#b45">Orbanz &amp; Roy, 2015)</ref> and is defined as</p><formula xml:id="formula_2">f (G) = C π∈Π |V (r) | σ∈Π |V (c) | f A π,σ , X (r,v) π , X (c,v) σ<label>(2)</label></formula><p>where C = (|V (r) |!|V (c) |!) −1 and π, σ are permutations of V (r) , V (c) , respectively. Results that apply to joint RP apply to separate RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Representation Power of RP</head><p>Functions f should be expressive enough to learn distinct representations of nonisomorphic graphs or graphs with distinct features. We say f (G) is most-powerful or mostexpressive when f (G) = f (G ) iff G and G are isomorphic and have the same vertex/edge features up to permutation. If f is not most-powerful, a downstream function ρ may struggle to predict different classes for nonisormorphic graphs. Theorem 2.1. If node and edge attributes come from a finite set, then the representation f (G) in Equation 1 is the most expressive representation of G, provided f is sufficiently expressive (e.g., a universal approximator).</p><p>All proofs are shown in the Supplementary Material. This result provides a key insight into RP; one can focus on building expressive functions f that need not be permutation-invariant as the summation over permutations assures that permutation-invariance is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Network Architectures</head><p>Since f may be permutation sensitive, RP allows one to use a wide range of neural network architectures.</p><p>RNNs, MLPs. A valid architecture is to vectorize the graph (concatenating node and edge features, as illustrated in the Supplementary Material) and learn f over the resulting sequence. f can be an RNN, like an LSTM <ref type="bibr" target="#b24">(Hochreiter &amp; Schmidhuber, 1997)</ref> or GRU <ref type="bibr" target="#b11">(Cho et al., 2014)</ref>, or a feedforward neural network (multilayer perceptron, MLP) with padding if different graphs have different sizes. Concretely,</p><formula xml:id="formula_3">f (G) = 1 |V |! π∈Π |V | f vec(A π,π , X (v) π ) .</formula><p>CNNs. Convolutional neural networks (CNNs) can also be directly applied over the tensor A π,π and combined with the node features X</p><formula xml:id="formula_4">(v) π , as in f (G)= 1 |V |! π∈Π |V | MLP CNN(A π,π ) MLP(X (v) π ) ,<label>(3)</label></formula><p>where CNN denotes a 2D <ref type="bibr" target="#b32">(LeCun et al., 1989;</ref><ref type="bibr" target="#b31">Krizhevsky et al., 2012)</ref> if there are no edge features and a 3D CNN <ref type="bibr" target="#b27">(Ji et al., 2013)</ref> if there are edge features, [· ·] is a concatenation of the representations, and MLP is a multilayer perceptron. Multi-resolution 3D convolutions <ref type="bibr" target="#b46">(Qi et al., 2016)</ref> can be used to map variable-sized graphs into the same sized representation for downstream layers.</p><p>GNNs. The function f can also be a graph neural network (GNN), a broad class of models that use the graph G itself to define the computation graph. These are permutationinvariant by design but we will show that their integration into RP can (1) make them more powerful and (2) speed up their computation via theoretically sound approximations. The GNNs we consider follow a message-passing <ref type="bibr" target="#b20">(Gilmer et al., 2017)</ref> scheme defined by the recursion</p><formula xml:id="formula_5">h (l) u = φ (l) h (l−1) u , JP (h (l−1) v ) v∈N (u) ,<label>(4)</label></formula><p>where φ (l) is a learnable function with distinct weights at each layer 1 ≤ l ≤ L of the computation graph, JP is a general (learnable) permutation-invariant function <ref type="bibr" target="#b41">(Murphy et al., 2019)</ref>, N (u) is the set of neighbors of u ∈ V , and h</p><formula xml:id="formula_6">(l) u ∈ R d (l)</formula><p>h is a vector describing the embedding of node u at layer l. h (0) u is the feature vector of node u, (X (v) ) u,· or can be assigned a constant c if u has no features. Under this framework, node embeddings can be used directly to predict node-level targets, or all node embeddings can be aggregated (via a learnable function) to form an embedding h G used for graph-wide tasks.</p><p>There are several variations of Equation 4 in the literature. <ref type="bibr" target="#b17">Duvenaud et al. (2015)</ref> proposed using embeddings from all layers l ∈ {1, 2, . . . , L} for graph classification. <ref type="bibr" target="#b21">Hamilton et al. (2017a)</ref> used a similar framework for node classification and link prediction tasks, using the embedding at the last layer, while <ref type="bibr" target="#b61">Xu et al. (2018)</ref> extend <ref type="bibr" target="#b21">Hamilton et al. (2017a)</ref> to once again use embeddings at all layers for node and link prediction tasks. Other improvements include attention <ref type="bibr" target="#b57">(Velickovic et al., 2018)</ref>. This approach can be derived from spectral graph convolutions (e.g., <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>). More GNNs are discussed in Section 3.</p><p>Recently, <ref type="bibr" target="#b62">Xu et al. (2019)</ref>; <ref type="bibr" target="#b40">Morris et al. (2019)</ref> showed that these architectures are at most as powerful as the Weisfeiler-Lehman (WL) algorithm for testing graph isomorphism <ref type="bibr" target="#b59">(Weisfeiler &amp; Lehman, 1968)</ref>, which itself effectively follows a message-passing scheme. Accordingly, we will broadly refer to models defined by Equation 4 as WL-GNNs. <ref type="bibr" target="#b62">Xu et al. (2019)</ref> proposes a WL-GNN called Graph Isomorphism Network (GIN) which is as powerful as the WL test in graphs with discrete features.</p><p>Can a WL-GNN be more powerful than the WL test? WL-GNNs inherit a shortcoming from the WL test (Cai  <ref type="bibr" target="#b10">et al., 1992;</ref><ref type="bibr" target="#b2">Arvind et al., 2017;</ref><ref type="bibr" target="#b18">Fürer, 2017;</ref><ref type="bibr" target="#b40">Morris et al., 2019)</ref>; node representations h (l) u do not encode whether two nodes have the same neighbor or distinct neighbors with the same features, limiting their ability to learn an expressive representation of the entire graph. Consider a task where graphs represent molecules, where node features indicate atom type and edges denote the presence or absence of bonds. Here, the first WL-GNN layer cannot distinguish that two (say) carbon atoms have a bond with the same carbon atom or a bond to two distinct carbon atoms. Successive layers of the WL-GNN update node representations and the hope is that nodes eventually get unique representations (up to isomorphisms), and thus allow the WL-GNN to detect whether two nodes have the same neighbor based on the representations of their neighbors. However, if there are too few WL-GNN layers or complex cycles in the graph, the graph and its nodes will not be adequately represented.</p><p>To better understand this challenge, consider the extreme case illustrated by the two graphs in <ref type="figure">Figure 1</ref>. These are cycle graphs with M = 11 nodes where nodes that are R ∈ {2, 3} 'hops' around the circle are connected by an edge. These highly symmetric graphs, which are special cases of circulant graphs <ref type="bibr" target="#b58">(Vilfred, 2004)</ref> are formally defined in Definition 2.1 but the key point is that the WL test, and thus WL-GNNs, cannot distinguish these two nonisomorphic graphs.</p><p>Definition 2.1: [Circulant Skip Links (CSL) graphs] Let R and M be co-prime natural numbers 1 such that R &lt; M − 1. G skip (M, R) denotes an undirected 4-regular graph with vertices {0, 1, . . . , M − 1} whose edges form a cycle and have skip links. That is, for the cycle, {j, j + 1} ∈ E for j ∈ {0, . . . , M − 2} and {M − 1, 0} ∈ E. For the skip links, recursively define the sequence s 1 = 0, s i+1 = (s i + R) mod M and let {s i , s i+1 } ∈ E for any i ∈ N. ♦ We will use RP to help WL-GNNs overcome this shortcoming. Let f be a WL-GNN that we make permutation sensitive by assigning each node an identifier that depends on π. Permutation sensitive IDs prevent the RP sum from collapsing to just one term but more importantly help distinguish neighbors that otherwise appear identical. In particular, given any π ∈ Π |V | , we append to the rows of X (v) π one-hot encodings of the row number before computing f . We can represent this by an augmented vertex attribute ma-1 Two numbers are co-primes if their only common factor is 1. trix X (v) π I |V | for every π ∈ Π |V | , where I |V | is a |V | × |V | identity matrix and [B C] concatenates the columns of matrices B and C. RP-GNN is then given by</p><formula xml:id="formula_7">f (G) = 1 |V |! π∈Π |V | f A π,π , X (v) π I |V | (5) = 1 |V |! π∈Π |V | f A, X (v) (I |V | ) π ,</formula><p>where the second holds since f a GNN and thus invariant to permutations of the adjacency matrix. The following theorem shows that f (G) in Equation 5 is strictly more expressive than the original WL-GNN; it can distinguish all nodes and graphs that WL-GNN can in addition to graphs that the original WL-GNN cannot. Theorem 2.2. The RP-GNN in Equation 5 is strictly more expressive than the original WL-GNN. Specifically, if f is a GIN <ref type="bibr" target="#b62">(Xu et al., 2019)</ref> and the graph has discrete attributes, its RP-GNN is more powerful than the WL test.</p><p>Equation 5 is computationally expensive but can be made tractable while retaining expressive power over standard GNNs. While all approximations discussed in Section 2.3 for RP in general are applicable to RP-GNN, a specific strategy is to assign permutation-sensitive node IDs in a clever way. In particular, if vertex features are available, we only need to assign enough IDs to make all vertices unique and thereby reduce the number of permutations we need to evaluate. For example, in the molecule CH 2 O 2 , if we create node features with one-hot IDs (C, 0, 1),(H, 0, 1),(H, 1, 0),(O, 0, 1),(O, 1, 0), then we need only consider 1!· 2! · 2! = 4 permutations. For unattributed graphs, we assign i mod m to node i; setting m=1 reduces to a GNN and m=|V | is the most expressive. More examples are in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">RP Tractability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">TRACTABILITY VIA CANONICAL ORIENTATIONS</head><p>Equation 1 is intractable as written and calls for approximations. The most direct approximation is to compose a permutation-sensitive f with a canonical orientation function that re-orders A such that CANONICAL(A, X (v) )= CANONICAL(A π,π , X (v) π ), ∀π ∈ Π |V | . For instance, vertices can be sorted by centrality scores with some tiebreaking scheme <ref type="bibr" target="#b38">(Montavon et al., 2012;</ref><ref type="bibr" target="#b43">Niepert et al., 2016)</ref>. This causes the sum over all permutations to collapse to just an evaluation of f • CANONICAL. Essentially, this introduces a fixed component into the permutation-invariant function f with only the second stage learned from data. This simplifying approximation to the original problem is however only useful if CANONICAL is related to the true function, and can otherwise result in poor representations <ref type="bibr" target="#b41">(Murphy et al., 2019)</ref>.</p><p>A more flexible approach collapses the set of all permutations into a smaller set of equivalent permutations which we denote as poly-canonical orientation. Depth-First Search (DFS) and Breadth-First Search (BFS) serve as two examples. In a DFS, the nodes of the adjacency matrix/tensor A π,π are ordered from 1 to |V | according to the order they are visited by a DFS starting at π(1). Thus, if G is a length-three path and we consider permutation functions defined (elementwise) as π(1, 2, 3) = (1, 2, 3), π (1, 2, 3) = (1, 3, 2), DFS or BFS would see respectively 1 2 3 and 1 3 2 (where vertices are numbered by permuted indices), start at π(1)=1 and result in the same 'leftto-right' orientation for both permutations. In disconnected graphs, the search starts at the first node of each connected component. Learning orientations from data is a discrete optimization problem left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">TRACTABILITY VIA π-SGD</head><p>A simple approach for making RP tractable is to sample random permutations during training. This offers the computational savings of a single canonical ordering but circumvents the need to learn a good canonical ordering for a given task. This approach is only approximately invariant, a tradeoff we make for the increased power of RP.</p><p>For simplicity, we analyze a supervised graph classification setting with a single sampled permutation, but this can be easily extended to sampling multiple permutations and unsupervised settings. Further, we focus on joint invariance but the formulation is similar for separate invariance. Consider N training data examples D ≡ {(G(1), y(1)), . . . , (G(N ), y(N ))}, where y(i) ∈ Y is the target output and graph G(i) its corresponding graph input. For a parameterized function f with parameters W ,</p><formula xml:id="formula_8">f (G(i); W ) = 1 |V (i)|! π∈Π |V (i)| f (A π,π (i), X (v) π (i); W ),</formula><p>our (original) goal is to minimize the empirical loss</p><formula xml:id="formula_9">L(D; W ) = N i=1 L y(i) , f (G(i); W ) ,<label>(6)</label></formula><p>where L is a convex loss function of f (·; ·) such as crossentropy or square loss. For each graph G(i), we sample a permutation s i ∼ Unif(Π |V (i)| ) and replace the sum in <ref type="figure">Equation 1</ref> with the estimatê</p><formula xml:id="formula_10">f (G(i); W ) = f (A si,si (i), X (v) si (i); W ).<label>(7)</label></formula><p>For separate invariance, we would sample a distinct permutation for each set of vertices. The estimator in Equation <ref type="formula" target="#formula_10">7</ref> is unbiased:</p><formula xml:id="formula_11">E si [f (G si,si (i); W )] = f (G(i); W ),</formula><p>where G si,si is shorthand for a graph that has been permuted by s i . However, this is no longer true when f is chained with a nonlinear loss L:</p><formula xml:id="formula_12">E si [L(y(i),f (G si,si (i); W ))] = L(y(i), E si [f (G si,si (i); W )])</formula><p>. Nevertheless, as we will soon justify, we follow <ref type="bibr" target="#b41">Murphy et al. (2019)</ref> and use this estimate in our optimization.</p><formula xml:id="formula_13">Definition 2.2: [π-SGD for RP] Let B t = {(G(1), y(1)), . . . , (G(B), y(B))} be a mini-batch i.i.d. sampled uniformly from the training data D at step t.</formula><p>To train RP with π-SGD, we follow the stochastic gradient descent update</p><formula xml:id="formula_14">W t = W t−1 − η t Z t ,<label>(8)</label></formula><formula xml:id="formula_15">where Z t = 1 B B i=1 ∇ W L y(i),f (G(i); W t−1 ) is the random gradient with the random permutations {s i } B i=1 , (sampled independently s i ∼ Unif(Π |V (i)| ) for all graphs G(i) in batch B t ), and the learning rate is η t ∈ (0, 1) s.t. lim t→∞ η t = 0, ∞ t=1 η t = ∞, and ∞ t=1 η 2 t &lt; ∞. ♦</formula><p>Effectively, this is a Robbins-Monro stochastic approximation algorithm of gradient descent <ref type="bibr" target="#b49">(Robbins &amp; Monro, 1951;</ref><ref type="bibr" target="#b7">Bottou, 2012)</ref> and optimizes the modified objective</p><formula xml:id="formula_16">J(D; W )= 1 N N i=1 E si L y(i),f (G si,si (i); W ) = 1 N N i=1 1 |V (i)|! π∈Π |V (i)| L y(i),f (G π,π (i); W ) . (9)</formula><p>Observe that the expectation over permutations is now outside the loss function (recall f (G(i); W ) in in Equation 6 is an expectation). The loss in Equation 9 is also permutation-invariant, but π-SGD yields a result sensitive to the random input permutations presented to the algorithm. Further, unless the function f itself is permutationinvariant (f = f ), the optima of J are different from those of the original objective function L. Instead, if L is convex in f (·; ·), J is an upper bound to L via Jensen's inequality, and minimizing this bound forms a tractable surrogate to the original objective in Equation 6.</p><p>The following convergence result follows from the π-SGD formulation of <ref type="bibr" target="#b41">Murphy et al. (2019)</ref>. Proposition 2.1. π-SGD stochastic optimization enjoys properties of almost sure convergence to optimal W under conditions similar to SGD (listed in Supplementary). Remark 2.1. Given fixed point W of the π-SGD optimization and a new graph G at test time, we may ex- <ref type="bibr" target="#b41">Murphy et al. (2019)</ref> propose k-ary pooling whereby the computational complexity of summing over all permutations of an input sequence is reduced by considering only permutations of subsequences of size k. Inspired by this, we propose k-ary Relational Pooling which operates on k-node induced subgraphs of G, which corresponds to patches of size k × k × (d e + 1) of A and k rows of X <ref type="bibr">(v)</ref> . Formally, we define k-ary RP in joint RP by</p><formula xml:id="formula_17">actly compute E s [f (G s,s ; W )] = f (G; W ) or esti- mate it with 1 m m j=1 f (G sj ,sj ; W ), where s 1 . . . ,s m i.i.d. ∼ Unif Π |V | . 2.3.3. TRACTABILITY VIA k-ARY DEPENDENCIES</formula><formula xml:id="formula_18">f (k) (G; W ) = 1 |V |! π∈Π |V | f A π,π [1:k, 1:k, :], X (v) π [1:k,:]; W ,<label>(10)</label></formula><p>where A[·, ·, ·] denotes access to elements in the first, second, and third modes of A; a : b denotes selecting elements corresponding to indices from a to b inclusive; and ":" by itself denotes all elements along a mode. Thus, we permute the adjacency tensor and select fibers along the third mode from the upper left k × k × (d e + 1) subtensor of A as well as the vertex attributes from the first k rows of X</p><formula xml:id="formula_19">(v)</formula><p>π . An illustration is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The graph on the right is numbered by its 'original' node indices and we assume that it has no vertex features and onedimensional edge features. This 'original' graph would be represented by a 5 × 5 × 2 tensor A where, for all pairs of vertices, the front slice holds adjacency matrix information and the back slice holds edge feature information (not shown). Given the permutation function π † ∈ Π |V | defined as π † (1, 2, 3, 4, 5) = (3, 4, 1, 2, 5), the permuted A π † ,π † is shown on the left. Its entries show elements from A shuffled appropriately by π † . For k = 3 RP, we select the upperleft 3×3 region from A π † ,π † , shaded in red, and pass this to f . This is repeated for all permutations of the vertices. For separate RP, the formulation is similar but we can select k 1 and k 2 nodes from V (r) and V (c) , respectively.</p><p>In practice, the relevant k-node induced subgraphs can be selected without first permuting the entire tensor A and matrix X <ref type="bibr">(v)</ref> . Instead, we enumerate all subsets of size k from index set V and use those to index A and X (v) .</p><p>More generally, we have the following conclusion: Proposition 2.2. The RP in Equation 10 requires summing over all k-node induced subgraphs of G, thus saving computation when k &lt; |V |, reducing the number of terms in the sum from |V |! to |V |! (|V |−k)! .</p><p>Fewer computations are needed if f is made permutationinvariant over its input k-node induced subgraph. We now show that the expressiveness of k-ary RP increases with k.</p><formula xml:id="formula_20">Proposition 2.3. f (k)</formula><p>becomes strictly more expressive as k increases. That is, for any k∈ N, define F k as the set of all permutation-invariant graph functions that can be represented by RP with k-ary dependencies. Then,</p><formula xml:id="formula_21">F k−1 ⊂ F k . A (3,3,2) A (3,4,2) A (3,1,2) A (3,2,2) A (3,5,2)</formula><p>A (4,3,2) A (4,4,2) A (4,1,2) A (4,2,2) A (4,5,2)</p><formula xml:id="formula_22">A (1,3,2) A (1,4,2) A (1,1,2) A (1,2,2) A (1,5,2) A (2,3,2) A (2,4,2) A (2,1,2) A (2,2,2) A (2,5,2)</formula><p>A (5,3,2) A (5,4,2) A (5,1,2) A (5,2,2) A <ref type="bibr">(5,</ref><ref type="bibr">5,</ref><ref type="bibr">2)</ref> A (3,3,1) A (3,4,1) A (3,1,1) A (3,2,1) A (3,5,1) A (4,3,1) A (4,4,1) A (4,1,1) A (4,2,1) A (4,5,1)</p><formula xml:id="formula_23">A (1,3,1) A (1,4,1) A (1,1,1) A (1,2,1) A (1,5,1) A (2,3,1) A (2,4,1) A (2,1,1) A (2,2,1) A (2,5,1) A (5,3,1) A (5,4,1) A (5,1,1) A (5,2,1) A (5,5,1) Adjacency tensor A π † ,π † where π † (1, 2, 3, 4, 5) = (3, 4, 1, 2, 5) (elementwise): the top-left 3 × 3 × 2 subtensor is passed to f (3)</formula><p>. 1 2 3 4 5 An example five-node graph encoded by A. We select a 3-node induced subgraph, corresponding to the top-left of A π † ,π † indicated by shaded nodes and thickened edges. Further computational savings. The number of k-node induced subgraphs can be very large for even moderatesized graphs. The following yield additional savings.</p><p>Ignoring some subgraphs: We can encode task-and modelspecific knowledge by ignoring certain k-sized induced subgraphs, which amounts to fixing f to 0 for these graphs. For example, in most applications the graph structure -and not the node features alone -is important so we may ignore subgraphs of k isolated vertices. Such decisions can yield substantial computational savings in sparse graphs.</p><p>Use of π-SGD: We can combine the k-ary approximation with other strategies like π-SGD and poly-canonical orientations. For instance, a forward pass can consist of sampling a random starting vertex and running a BFS until a k-node induced subgraph is selected. Combining π-SGD and k-ary RP can speed up GNNs but will not provide unbiased estimates of the loss calculated with the entire graph. Future work could explore using the MCMC finite-sample unbiased estimator of <ref type="bibr" target="#b56">Teixeira et al. (2018)</ref> with RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Our Relational Pooling framework leverages insights from Janossy Pooling <ref type="bibr" target="#b41">(Murphy et al., 2019)</ref>, which learns expressive permutation-invariant functions over sequences by approximating an average over permutation-sensitive functions with tractability strategies. The present work raises novel applications -like RP-GNN -that arise when pooling over permutation-sensitive functions of graphs.</p><p>Graph Neural Networks (GNNs) and Graph Convolutional Networks (GCNs) form an increasingly popular class of methods <ref type="bibr" target="#b52">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b9">Bruna et al., 2014;</ref><ref type="bibr" target="#b17">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b43">Niepert et al., 2016;</ref><ref type="bibr" target="#b3">Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b30">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b20">Gilmer et al., 2017;</ref><ref type="bibr" target="#b39">Monti et al., 2017;</ref><ref type="bibr" target="#b14">Defferrard et al., 2016;</ref><ref type="bibr" target="#b21">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b57">Velickovic et al., 2018;</ref><ref type="bibr" target="#b33">Lee et al., 2018;</ref><ref type="bibr" target="#b62">Xu et al., 2019)</ref>. Applications include chemistry, where molecules are represented as graphs and we seek to predict chemical prop-erties like toxicity <ref type="bibr" target="#b17">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b20">Gilmer et al., 2017;</ref><ref type="bibr" target="#b33">Lee et al., 2018;</ref><ref type="bibr" target="#b60">Wu et al., 2018;</ref><ref type="bibr" target="#b51">Sanchez-Lengeling &amp; Aspuru-Guzik, 2018)</ref> and document classification on a citations network <ref type="bibr" target="#b22">(Hamilton et al., 2017b)</ref>; and many others (cf. <ref type="bibr" target="#b4">Battaglia et al. (2018)</ref>).</p><p>Recently, <ref type="bibr" target="#b62">Xu et al. (2019)</ref> and <ref type="bibr" target="#b40">Morris et al. (2019)</ref> show that such GNNs are at most as powerful as the standard Weisfeiler-Lehman algorithm (also known as color refinement or naive vertex classification <ref type="bibr" target="#b59">(Weisfeiler &amp; Lehman, 1968;</ref><ref type="bibr" target="#b2">Arvind et al., 2017;</ref><ref type="bibr" target="#b18">Fürer, 2017)</ref>) for graph isomorphism testing, and can fail to distinguish between certain classes of graphs <ref type="bibr" target="#b10">(Cai et al., 1992;</ref><ref type="bibr" target="#b2">Arvind et al., 2017;</ref><ref type="bibr" target="#b18">Fürer, 2017)</ref>. In Section 4, we demonstrate this phenomenon and provide empirical evidence that RP can correct some of these shortcomings. Higher-order (k-th order) versions of the WL test (WL[k]) exist and operate on tuples of size k from V rather than on one vertex at a time <ref type="bibr" target="#b18">(Fürer, 2017)</ref>. Increasing k increases the capacity of WL[k] to distinguish nonisomorphic graphs, which can be exploited to build more powerful GNNs <ref type="bibr" target="#b40">(Morris et al., 2019)</ref>. <ref type="bibr" target="#b37">Meng et al. (2018)</ref>, introduce a WL[k]-type representation to predict high-order dynamics in temporal graphs. Using GNNs based on WL[k] may be able to give better f functions for RP but we focused on providing a representation for more expressive than WL[1] procedures.</p><p>In another direction, WL is used to construct graph kernels <ref type="bibr" target="#b54">(Shervashidze et al., 2009;</ref>. CNNs have also been used with graph kernels <ref type="bibr" target="#b44">(Nikolentzos et al., 2018)</ref> and some GCNs can be seen as CNNs applied to single canonical orderings <ref type="bibr" target="#b43">(Niepert et al., 2016;</ref><ref type="bibr" target="#b14">Defferrard et al., 2016)</ref>. RP provides a framework for stochastic optimization over all or poly-canonical orderings. Another line of work derives bases for permutation-invariant functions of graphs and propose learning the coefficients of basis elements from data <ref type="bibr" target="#b34">(Maron et al., 2018;</ref><ref type="bibr" target="#b23">Hartford et al., 2018)</ref>.</p><p>In parallel, Bloem-Reddy &amp; Teh (2019) generalized permutation-invariant functions to group-action invariant functions and discuss connections to exchangeable prob-ability distributions <ref type="bibr" target="#b13">(De Finetti, 1937;</ref><ref type="bibr" target="#b15">Diaconis &amp; Janson, 2008;</ref><ref type="bibr" target="#b0">Aldous, 1981)</ref>.Their theory uses a checkerboard function <ref type="bibr" target="#b45">(Orbanz &amp; Roy, 2015)</ref> and the left-order canonical orientation of <ref type="bibr" target="#b19">Ghahramani &amp; Griffiths (2006)</ref> to orient graphs but it will fail in some cases unless graph isomorphism can be solved in polynomial time. Also, as discussed, there is no guarantee that a hand-picked canonical orientation will perform well on all tasks. On the tractability side, <ref type="bibr" target="#b42">Niepert &amp; Van den Broeck (2014)</ref> shows that exchangeabilty assumptions in probabilistic graphical models provide a form of k-ary tractability and <ref type="bibr" target="#b12">Cohen &amp; Welling (2016)</ref>; <ref type="bibr" target="#b48">Ravanbakhsh et al. (2017)</ref> use symmetries to reduce sample complexity and save on computation. Another development explores the universality properties of invariance-preserving neural networks and concludes some architectures are computationally intractable <ref type="bibr" target="#b35">(Maron et al., 2019)</ref>. Closer to RP, <ref type="bibr" target="#b38">Montavon et al. (2012)</ref> discusses random permutations but RP provides a more comprehensive framework with theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our first experiment shows that RP-GNN is more expressive than WL-GNN. The second evaluates RP and its approximations on molecular data. Our code is on GitHub 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Testing RP-GNN vs WL-GNN</head><p>Here we perform experiments over the CSL graphs from <ref type="figure">Figure 1</ref>. We demonstrate empirically that WL-GNNs are limited in their power to represent them and that RP can be used to overcome this limitation. Our experiments compare the RP-GNN of Equation 5 using the Graph Isomorphism Network (GIN) architecture <ref type="bibr" target="#b62">(Xu et al., 2019)</ref> as f against the original GIN architecture. We choose GIN as it is arguably the most powerful WL-GNN architecture.</p><p>For the CSL graphs, the "skip length" R effectively defines an isomorphism class in the sense that predicting R is tantamount to classifying a graph into its isomorphism class for a fixed number of vertices M . We are interested in predicting R as an assessment of RP's ability to exploit graph structure. We do not claim to tackle the graph isomorphism problem as we use approximate learning (π-SGD for RP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RP-GIN. GIN follows the recursion of Equation 4</head><p>, replacing JP with summation and defining φ (l) as a function that sums its arguments and feeds them through an MLP:</p><formula xml:id="formula_24">h (l) u = MLP (l) (1 + (l) )h (l−1) u + v∈N (u) h (l−1) v ,</formula><p>for l = 1, . . . , L, where { (l) } L l=1 can be treated as hyperparameters or learned parameters (we train ). This recur- <ref type="table">Table 1</ref>: RP-GNN outperforms WL-GNN in 10-class classification task. Summary of validation-set accuracy (%). u at each given l, then concatenating the results, as proposed by <ref type="bibr" target="#b62">Xu et al. (2019)</ref>. When applying GIN directly on our CSL graphs, we assign a constant vertex attribute to all vertices in keeping with the traditional WL algorithm, as the graph is unattributed. Recall that RP-GIN assigns one-hot node IDs and passes the augmented graph to GIN ( f ) (Equation 5). We cannot assign IDs with standard GIN as doing so renders it permutation-sensitive. Further implementation and training details are in the Supplementary Material.</p><p>Classifying skip lengths. We create a dataset of graphs from <ref type="bibr">3, 4, 5, 6, 9, 11, 12, 13, 16}</ref> and predict R as a discrete response. Note M = 41 is the smallest such that 10 nonisomorphic G skip (M, R) can be formed; ∃R 1 = R 2 such that G skip (M, R 1 ) and G skip (M, R 2 ) are isomorphic. For all 10 classes, we form 15 adjacency matrices by first constructing A (R) according to Definition 2.1 and then 14 more as A (R) π,π for 14 distinct permutations π. This gives a dataset of 150 graphs. We evaluate GIN and RP-GIN with five-fold cross validation -with balanced classes on both training and validation -on this task.</p><formula xml:id="formula_25">G skip (41, R) R where R ∈ {2,</formula><p>The validation-set accuracies for both models are shown in <ref type="table">Table 1</ref> and <ref type="figure">Figure 3</ref> in the Supplementary Material. Since GIN learns the same representation for all graphs, it predicts the same class for all graphs in the validation fold, and therefore achieves random-guessing performance of 10% accuracy. In comparison, RP-GIN yields substantially stronger performance on all folds, demonstrating that RP-GNNs are more powerful than their WL-GNN and serving as empirical validation of Theorem 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Predicting Molecular Properties</head><p>Deep learning for chemical applications learns functions on graph representations of molecules and has a rich literature <ref type="bibr" target="#b17">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b28">Kearnes et al., 2016;</ref><ref type="bibr" target="#b20">Gilmer et al., 2017)</ref>. This domain provides challenging tasks on which to evaluate RP, while in other applications, different GNN models of varying sophistication often achieve similar performance <ref type="bibr" target="#b53">(Shchur et al., 2018;</ref><ref type="bibr" target="#b41">Murphy et al., 2019;</ref><ref type="bibr" target="#b62">Xu et al., 2019)</ref>. We chose datasets from the Molecu-leNet project <ref type="bibr" target="#b60">(Wu et al., 2018</ref>) -which collects chemical datasets and collates the performance of various models -that yield classification tasks and on which graph-based methods achieved superior performance 3 . In particular, we chose MUV <ref type="bibr" target="#b50">(Rohrer &amp; Baumann, 2009</ref>), HIV, and Tox21 <ref type="bibr" target="#b36">(Mayr et al., 2016;</ref><ref type="bibr" target="#b26">Huang et al., 2016)</ref>, which contain measurements on a molecule's biological activity, ability to inhibit HIV, and qualitative toxicity, respectively.</p><p>We processed datasets with DeepChem <ref type="bibr" target="#b47">(Ramsundar et al., 2019)</ref> and evaluated models with ROC-AUC per the MoleculeNet project. Molecules are encoded as graphs with 75-and 14-dimensional node and edge features. Table 3 (in Supplementary) provides more detail.</p><p>We use the best-performing graph model reported by MoleculeNet as f to evaluate k-ary RP and to explore whether RP-GNN can make it more powerful. This is a model inspired by the GNN in <ref type="bibr" target="#b17">Duvenaud et al. (2015)</ref>, implemented in DeepChem by Altae-Tran et al. <ref type="formula" target="#formula_1">(2017)</ref>, which we refer to as the <ref type="bibr">'Duvenaud et al.'</ref> model. This model is specialized for molecules; it trains a distinct weight matrix for each possible vertex degree at each layer, which would be infeasible in other domains. One might ask whether RP-GNN can add any power to this state-of-the-art model, which we will explore here. We evaluated GIN <ref type="bibr" target="#b62">(Xu et al., 2019)</ref>  . We evaluate f on the entire graph but make RP-Duvenaud tractable by training with π-SGD. At inference time, we sample 20 permutations (see Remark 2.1). Additionally, we assign just enough one-hot IDs to make atoms of the same type have unique IDs (as discussed in Section 2.2). To quantify variability, we train over 20 random data splits.</p><p>The results shown in <ref type="table" target="#tab_1">Table 2</ref> suggest that RP-Duvenaud is more powerful than the baseline on the HIV task and similar in performance on the others. While we bear in mind the over-confidence in the variability estimates <ref type="bibr" target="#b5">(Bengio &amp; Grandvalet, 2004)</ref>, this provides support of our theory. k-ary RP experiments Next we empirically assess the tradeoffs involved in the k-ary dependency models -evaluating f on k-node induced subgraphs -discussed in Section 2.3.3. Propositions 2.3 and 2.2 show that expressive power and computation decrease with k. Here, f is a 'Duvenaud et al. model' that operates on induced subgraphs of size k = 10, 20, 30, 40, 50 (the percentages of molecules with more than k atoms in each dataset are shown in the Supplementary Material). We train using π-SGD (20 inference-time samples) and evaluate using five random train/val/test splits.  Results are shown in <ref type="table" target="#tab_1">Table 2</ref> and Figures 4, 5, and 6 in the Supplementary Material. With the Tox21 dataset, we see a steady increase in predictive performance and computation as k increases. For instance, k-ary with k = 10 is 25% faster than the baseline with mean AUC 0.687 (0.005 sd) and with k = 20 being 10% faster with AUC 0.755 (0.003 sd), where (sd) indicates the standard deviation over 5 bootstrapped runs. Results level off around k = 30. For the other datasets, neither predictive performance nor computation vary significantly with k. Overall, the molecules are quite small and we do not expect dramatic speed-ups with smaller k, but this enables comparing between using the entire graph and its k-sized induced subgraphs.</p><p>RP with CNNs and RNNs. RP permits using neural networks for f . We explored RNNs and CNNs and report the results in <ref type="table" target="#tab_1">Table 2</ref>. Specific details are discussed in the Supplementary Material. The RNN achieves reasonable performance on Tox21 and underperforms on the other tasks. The CNN underperforms on all tasks. Future work is needed to determine tasks where these approaches are better suited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we proposed the Relational Pooling (RP) framework for graph classification and regression. RP gives ideal most-powerful, though intractable, graph representations. We proposed several approaches to tractably approximate this ideal and showed theoretically and empirically that RP can make WL-GNNs more expressive than the WL test. RP permits neural networks like RNNs and CNNs to be brought to such problems. Our experiments evaluate RP on a number of datasets and show how our framework can be used to improve properties of state-ofthe-art methods. Future directions for theoretical study include improving our understanding of the tradeoff between representation power and computational cost of our tractability strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of Relational Pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tensor Representation and vec Operation on Graphs</head><p>We briefly provide a concrete example of the representation of graphs and the operation vec(G). Consider a graph with three vertices, one edge attribute at each edge, and two vertex attributes at each vertex. The connectivity structure and edge attributes are represented by the 3 × 3 × 2 adjacency tensor A where A (i,j,1) denotes the value of the graph's adjacency matrix and A (i,j,2) denotes the value of the additional edge attribute, i, j ∈ V = {1, 2, 3}.</p><formula xml:id="formula_26">A (1,1,2) A (1,2,2) A (1,3,2) A (2,1,2) A (2,2,2) A (2,3,2) A (3,1,2) A (3,2,2) A (3,3,2) A (1,1,1) A (1,2,1) A (1,3,1) A (2,1,1) A (2,2,1) A (2,3,1) A (3,1,1) A (3,2,1) A (3,3,1)</formula><p>Observe that the possibility of attributed self-loops is contemplated but this representation is applicable both to graphs that have self-loops and those that do not. The vertex attributes are represented in a matrix</p><formula xml:id="formula_27">X (v) = X1,1 X1,2 X2,1 X2,2 X3,1 X3,2 .</formula><p>A simple vec operation is shown below. The modeler is free to make modifications such as applying an MLP to the vertex attributes before concatenating with the edge attributes. Representing G by A and X (v) , vec(G) = A (1,1,1) , A (1,1,2) , A (1,2,1) , A (1,2,2) , A (1,3,1) , A (1,3,2) , X 1,1 , X 1,2 , A (2,1,1) , A (2,1,2) , A (2,2,1) , A (2,2,2) , A (2,3,1) , A (2,3,2) , X 2,1 , X 2,2 , A (3,1,1) , A (3,1,2) , A (3,2,1) , A (3,2,2) , A (3,3,1) , A (3,3,2) , X 3,1 , X 3,2 .</p><p>Starting with the first vertex, each edge attribute (including the edge indicator) is listed, then the vertex attributes are added before doing the same with subsequent vertices. The vectorization method for k-ary type models is similar, except that we apply vec on induced subgraphs of size k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More on Permutation-Invariance, WL-GNN Models, and Unique Identifiers</head><p>Here we elaborate on the addition of unique identifiers to graphs and implications for WL-GNN models. For simplicity, we consider undirected graphs with vertex attributes but no edge attributes, allowing us to simplify our notation to an adjacency matrix A and vertex attribute matrix X. We also consider an oversimplified model with just one GNN layer (L = 1), the following aggregation scheme</p><formula xml:id="formula_28">h u = x u + v∈N (u)</formula><p>x v , ∀u ∈ V, and the following read-out function to yield a graph representation</p><formula xml:id="formula_29">h G = v∈V h v .</formula><p>This can be expressed as h G = 1 T (A + I |V | )X for adjacency matrix A, vertex attribute matrix X, identity matrix I |V | , and where 1 T is a row vector of ones.</p><p>For instance, we may observe the following graph with endowed vertex attributes. The numbers indicate vertex features, not labels. .</p><p>Here, 1 T (A + I |V | )X = 41. Equivalently, we might have chosen to represent this graph as A π,π = 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 , X π = 6 2 5 1</p><p>Here we swapped the third and fourth column of X and the third and fourth row and column of A. Yet again, 1 T (A π,π + I |V | )X π = 41, as desired for isomorphicinvariant functions. We have chosen to assign scalar vertex attributes, but the invariance to permutation holds for vector vertex attributes. Now, we propose assigning unique one-hot IDs after constructing the adjacency matrix, which corresponds to the following representations A = 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 , X I |V | = 6 1 0 0 0 2 0 1 0 0 1 0 0 1 0 5 0 0 0 1 A π,π = 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 , X π I |V | = 6 1 0 0 0 2 0 1 0 0 5 0 0 1 0 1 0 0 0 1 (recall that [· ·] denotes concatenation). Note that we effectively assign identifiers after constructing A and X (and similarly for A π,π , X π ), so that the latter four columns of X I |V | and X π I |V | are the same. Now, 1 T (A + I |V | ) X I |V | = (41, 2, 3, 3, 4) yet 1 T (A π,π + I |V | ) X π I |V | = <ref type="figure" target="#fig_1">(41, 2, 3, 4, 3)</ref>. This permutation sensitivity in the presence of unique IDs holds for more general WL-GNNs and not just the one considered here. Often h G is fed forward through a linear or more complex layer to obtain the final graph-level prediction and this layer is usually permutation sensitive. Thus, we apply RP to GNNs with unique IDs to guarantee permutation invariance; meanwhile, the intuition for using unique IDs is to better distinguish vertices and thus create a more powerful representation for the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. An alternative approach to RP-GNN models</head><p>Next we present an equivalent but alternative representation of RP-GNN models <ref type="bibr">(Equation 5</ref>) that may be simpler to implement in practice and provide an example. In the previous section, we described permuting the adjacency tensor and matrix of endowed vertex attributes, leaving the matrix of identifiers unchanged. Alternatively, with f modeled as an isomorphic-invariant Graph Neural Network, one may leave the former two unchanged and instead permute the matrix of identifiers. Thus, the alternative model becomes <ref type="formula" target="#formula_1">(11)</ref> where (I |V | ) π denotes a permutation of the rows of the identity matrix. The more tractable version discussed previously of assigning a one-hot encoding of the id i mod m to node i ∈ V , for some m ∈ {1, 2, . . . , |V |} is still applicable. In this case, we replace (I |V | ) π with a |V | × m matrix of m-bit one-hot identifiers, appropriately permuted by π.</p><formula xml:id="formula_30">f (G) = 1 |V |! π∈Π |V | f A, X (v) (I |V | ) π ,</formula><p>For example, consider again the graph defined by adjacency matrix A and vertex features X given above. To evaluate f (A π,π , X π ) when the permutation is given by π <ref type="figure" target="#fig_1">(1, 2, 3, 4) = (2, 1, 3, 4)</ref>, we could forward A = 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 , X (I |V | ) π = 6 0 1 0 0 2 1 0 0 0 1 0 0 1 0 5 0 0 0 1 through our model of f . Previously the first row of X I |V | was (6, 1, 0, 0, 0) whereas after permutation by π it became (6, 0, 1, 0, 0), and so on. Both formulations discussed in this section were used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2.1</head><p>Proof. Let Ω be a finite set of graphs G = (V, E, X (v) , X (e) ) = (A, X <ref type="bibr">(v)</ref> ) that includes all graph topologies for any given (arbitrarily large but finite) graph order, as well as the associated vertex and edge attributes from a finite set. Note that isomorphic graphs G and G π,π are considered distinct elements in Ω (G π,π denotes a permutation of A and X <ref type="bibr">(v)</ref> ). If G = (A, X (v) ), let G(G) = {(A π,π , X (v) π ) : π ∈ Π |V | } denote the set of graphs that are isomorphic to G and have the same vertex and edge attribute matrices up to permutation. Consider a classification/prediction task where G ∈ Ω is assigned a target value t(G) from a collection of |Ω| possible values, such that t(G) = t(G ) <ref type="figure">iff G ∈ G(G)</ref>. Clearly, this is the most general classification task. Moreover, by replacing the target value t(G) with a probability p(G) (measure), the above task also encompasses generative tasks over Ω. All we need to show is that f of Equation 1 is sufficiently expressive for the above task.</p><p>We now consider a permutation-sensitive function f that assigns a distinct one-hot encoding to each distinct input graph G = (A, X <ref type="bibr">(v)</ref> ). This f can be approximated arbitrarily well by a sufficiently expressive neural network (operating on the vector representation of the input) as these are known to be universal approximators <ref type="bibr" target="#b25">(Hornik et al., 1989)</ref>. Now, letting G ∈ Ω be arbitrary, for all G ∈ G(G ), we have</p><formula xml:id="formula_31">f (G) = 1 |V |! π∈Π |V | f (A π,π , X (v) π ) = 1 |V |! (A ,X (v) )∈G(G) f (A , X (v) ) = 1 |V |! (A ,X (v) )∈G(G ) f (A , X (v) ) = 1 |V |! f (G ),</formula><p>thus f (G ) is the unique fingerprint of the set G(G ). Then, all we need is a function ρ(·) that takes the representation f (G ) and assigns the unique target value t(G ), satisfying the desired condition and proving that RP has maximal representation power over Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2.2</head><p>Preliminaries. For an n × d B matrix B and an n × d C matrix C, write D = [B C] to denote their concatenation to form an n×(d B +d C ) matrix D. Recall that RP-GNN adds a one-hot encoding of the node id to the node features. This node id is defined as its position in the adjacency matrix or tensor A π,π for a permutation π. Let I |V | be a |V | × |V | identity matrix representing the one-hot encoding vectors of node IDs 1 to |V |. We let α denote a maximally powerful WL-GNN, that is, a deep-enough WL-GNN satisfying the conditions of Theorem 3 in <ref type="bibr" target="#b62">Xu et al. (2019)</ref>. That is, the multiset functions for vertex aggregation and the graph-level readout are both injective over discrete node attributes. In accordance with <ref type="bibr" target="#b62">Xu et al. (2019)</ref>, we focus on graphs whose features live in a countable space. Finally, we denote multisets by {{. . .}}.</p><p>Proof. We need to show that RP-GNN is strictly more expressive than any WL-GNN. More specifically, we will show that RP-GNN (1) maps isomorphic graphs to the same graph embedding, (2) maps nonisomorphic graphs to distinct embeddings whenever a WL-GNN does, and <ref type="formula" target="#formula_4">(3)</ref> can map pairs of nonisomorphic graphs to distinct graph embeddings even when a most-powerful WL-GNN maps them to the same embedding. Again, in the context of the proof, when we say two graphs are 'isomorphic' it is understood that they have the same topology and the same vertex/edge features up to permutation.We suppose that all pairs of graphs have the same number of vertices, denoted by n; if they have different numbers of vertices it is trivial to show that both RP-GNN and WL-GNN can represent them differently.</p><p>(</p><formula xml:id="formula_32">1) Assume G 1 = (A 1 , X (v) 1 ) and G 2 = (A 2 , X<label>(v)</label></formula><p>2 ) are isomorphic graphs with the same features (up to permutation). Let (X (v) j ) π I n be the new (RP-GNN) features for some permutation π ∈ Π n of graph G j for j ∈ {1, 2}. Since G 1 is isomorphic to G 2 , there exists a π ∈ Π n such that A 1 = (A 2 ) π ,π and X (v) 1</p><formula xml:id="formula_33">I n = (X (v) 2 ) π I n . Thus f (G 1 ) = 1 n! π∈Πn α (A 1 ) π,π , (X (v) 1 ) π I n = 1 n! π ∈Π n α (A 2 ) π ,π , (X (v) 2 ) π I n = f (G 2 ),</formula><p>where we define Π n = {π • π : π ∈ Π n } and observe that Π n = Π n . Thus, no pairs of isomorphic graphs will be mapped to different representations by an RP-GNN f .</p><p>(2) Assume now that G 1 = (A 1 , X</p><p>1 ) and G 2 = (A 2 , X</p><p>2 ) are nonisomorphic graphs with discrete attributes successfully deemed nonisomorphic by the WL test. Theorem 3 of <ref type="bibr" target="#b62">Xu et al. (2019)</ref> implies</p><formula xml:id="formula_36">α(A 1 , X (v) 1 ) = α(A 2 , X (v)</formula><p>2 ). Next, we can always construct an α which has the same weights for the affine transformation over the endowed attributes as α but zero weights for the affine transformation over the RP-specific identifiers I n such that α A π,π , (X (v) ) π I n = α A π,π , X (v) π for any G = (A, X (v) ). Note that α is isomorphicinvariant since α is by construction; indeed, α ignores its permutation-sensitive part. Thus,</p><formula xml:id="formula_37">f (G 1 ) = 1 n! π∈Πn α (A 1 ) π,π , (X (v) 1 ) π I n = α A 1 , X (v) 1 = α A 2 , X (v) 2 = 1 n! π∈Πn α (A 2 ) π,π , (X (v) 2 ) π I n = f (G 2 ).</formula><p>Therefore, RP-GNN can map graphs that WL-GNNs can distinguish to different representations, completing our proof of part (2).</p><p>(3) Finally, we construct an example to show that RP-GNN is more expressive than WL-GNN. Consider the circulant graphs with different skip links in <ref type="figure">Figure 1</ref>. We show that these two (pairwise nonisomorphic) graphs can have different representations by RP-GNN but cannot be represented as distinct by WL-GNN. Let G 1 = (A 1 , X (v) ) denote the graph G skip (M = 11, R = 2) and G 2 = (A 2 , X (v) ) denote the graph G skip (M = 11, R = 3), where X (v) = c1, a vector of c ∈ R. It is not hard to show that WL-GNN cannot give different representations to G 1 and G 2 , as the WL test fails in these graphs <ref type="bibr" target="#b2">(Arvind et al., 2017;</ref><ref type="bibr" target="#b10">Cai et al., 1992;</ref><ref type="bibr" target="#b18">Fürer, 2017)</ref> and the most powerful WL-GNN is just as powerful as the WL test <ref type="bibr" target="#b62">(Xu et al., 2019)</ref>.</p><p>To show that RP-GNN is capable of giving different representations, we first show that for any given permutation π of G 1 , there is no permutation π of G 2 such that α((A 1 ) π,π , (X</p><formula xml:id="formula_38">(v) 1 ) π I n ) = α((A 2 ) π ,π , (X (v)</formula><p>2 ) π I n ) (note that α is a mostpowerful GNN and thus not a constant function). In this part of the proof, for simplicity and without loss of generality, consider π a permutation such that the vertices are numbered sequentially from 1, 2, . . . , n clockwise around the circle in <ref type="figure">Figure 1</ref>. Then, node 3 in G 1 has neighbors N 3 = {1, 2, 4, 5} and node 4 has neighbors N 4 = {2, 3, 5, 6}, with intersection N 3 ∩ N 4 = {2, 5}. However, in G 2 , no two nodes share two neighbors. Therefore, the multisets of all neighborhood attribute sequences for both permuted graphs -denoted in (G l ) π,π as (h (0) l,u ) u∈Nv v∈V l , for l ∈ {1, 2}, where the h (0) terms include the rows of I n -will be distinct. Thus a most powerful α with the recursion in Equation 4 will map them to distinct collections of vertex embeddings. Thus, the graph embeddings h G l , obtained by applying an injective read-out function to h</p><p>(1) l,v v∈V l will be distinct: h (G1)π,π = h (G2) π ,π , as desired.</p><p>As no representation of G 2 can match any representation of G 1 , we can find a function g(·) that, when composed with α, ensures that the sum in <ref type="figure">Equation 1</ref> gives different values for G 1 and G 2 by Lemma 5 of <ref type="bibr" target="#b62">Xu et al. (2019</ref><ref type="bibr">) (or Theorem 2 of Zaheer et al. (2017</ref>). Since we can always redefine α = g • α and α is still a WL-GNN, we conclude our proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2.1</head><p>The following proposition regarding the convergence of π-SGD was stated in the paper: Proposition 2.1. π-SGD stochastic optimization enjoys properties of almost sure convergence to optimal W under conditions similar to SGD (listed in <ref type="figure">Supplementary)</ref>.</p><p>Here we list the relevant conditions. <ref type="bibr" target="#b41">Murphy et al. (2019)</ref> point out that π-SGD can be characterized by the work of <ref type="bibr" target="#b64">Younes (1999)</ref>; <ref type="bibr" target="#b65">Yuille (2005)</ref> and is a a familiar application of stochastic approximation algorithms already used in training neural networks.</p><p>In particular, the following assumptions are made:</p><p>1. There exists a constant M &gt; 0 such that for all W ,</p><formula xml:id="formula_39">−G T t W ≤ M W − W 2 2 ,</formula><p>where G t is the true gradient for the full batch over all permutations and W is an optimum.</p><p>2. there exists a constant δ &gt; 0 such that for all W ,</p><formula xml:id="formula_40">E t [ Z t 2 2 ] ≤ δ 2 (1 + W t − W t 2 2 ),</formula><p>where Z t is the random gradient of the loss w.r.t. weights at step t and the expectation is taken with respect to all the data prior to step t.</p><p>If these assumptions are satisfied, then π-SGD (as with SGD) converges to a fixed point with probability one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 2.2</head><p>We restate the proposition for completeness. Proposition. The RP in Equation 10 requires summing over all k-node induced subgraphs of G, thus saving computation when k &lt; |V |, reducing the number of terms in the sum from |V |! to |V |! (|V |−k)! .</p><p>Proof. k-ary RP needs to iterate over the k-node induced subgraphs of G ( |V | k subgraphs), but for each subgraph there are k! different ways to order its nodes, resulting in</p><formula xml:id="formula_41">|V |! (|V |−k)! evaluations of f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 2.3</head><p>We restate the proposition for completeness.</p><p>Proposition. f <ref type="bibr">(k)</ref> becomes strictly more expressive as k increases. That is, for any k ∈ N, define F k as the set of all permutation-invariant graph functions that can be represented by RP with k-ary dependencies. Then, F k−1 is a proper subset of F k . Thus, RP with k-ary dependencies can express any RP function with (k −1)-ary dependencies, but the converse does not hold. is arbitrary, we conclude F k−1 ⊂ F k .</p><formula xml:id="formula_42">(F k ⊂ F k−1 ): The case where k = 1 is trivial, so assume k &gt; 1. We will demonstrate ∃f (k) ∈ F k such that f (k) ∈ F k−1 . Let f (k) and f (k−1)</formula><p>be associated with f , respectively. Task. Consider the task of representing the class of circle graphs with skip links shown in <ref type="figure">Figure 1</ref>. Let G k ∈ G skip (M k , k) and G k+1 ∈ G skip (M k , k + 1) where M k is any prime number satisfying M k &gt; 2(k − 1)(k + 1). That is, G k and G k+1 are circulant skip length graphs with the same number of vertices and skip lengths of k and k + 1, respectively. Note that M k &gt; k + 1 is prime and thus it is co-prime with both k and k + 1; further, M k − 1 &gt; k + 1 so the conditions for creating the CSL graph in Definition 2.1 are indeed satisfied. To complete the proof, we need to</p><formula xml:id="formula_43">show that (1) there is a f (k) capable of distinguishing G k from G k+1 but (2) no such f (k−1) exists. Denote G R = (A R , X (v) ), R ∈ {k, k +1}, where X (v) = c1</formula><p>for some c ∈ R for both graphs, as there are no vertex features, and where A R represents an adjacency matrix for G R (there are no edge features).</p><p>(1) A k-ary f <ref type="bibr">(k)</ref> that can distinguish between G k and G k+1 . We will define f <ref type="bibr">(k)</ref> in terms of a composition with a canonical orientation. In particular, we only allow the orientation of A (and X (v) ) that arises from first generating an edgelist by the scheme described in Definition 2.1 and then constructing the adjacency matrix from it in the usual way. For either graph, we define f (k) ((A R ) π,π [1 : k, 1 :</p><formula xml:id="formula_44">k, :], X (v)</formula><p>π [1 : k, :]) = 0 for all permutations that do not yield this 'canonical' adjacency matrix.</p><p>Under this orientation, the k × k submatrix A k [1 : k, 1 : k] of G k will have more nonzero elements than that of G k+1 , A k+1 [1 : k, 1 : k]. The relevant induced subgraph of size k in G k will include a pair of vertices that are k 'hops' away which will thus be connected by an edge, whereas in G k+1 , the skip length is too long so its induced subgraphs of size k will have fewer edges. Therefore, it suffices to let f <ref type="bibr">(k)</ref> count the number of nonzero elements in the (properly oriented) submatrices presented to it.</p><p>(2) No (k − 1)-ary f <ref type="bibr">(k−1)</ref> can distinguish between G k and G k+1 . We will show that the induced subgraphs of size k − 1 are "the same" in both G k and G k+1 , which will imply that no satisfactory f <ref type="bibr">(k−1)</ref> can be constructed. In particular, if we denote by L k−1 (G k ) the multiset of induced subgraphs of size k − 1 in G k and by L k−1 (G k+1 ) the multiset of induced subgraphs of size k − 1 in G k+1 , it can be shown that L k−1 (G k ) and L k−1 (G k+1 ) are equivalent in the following sense. There exists a bijection φ between these finite multisets such that for every</p><formula xml:id="formula_45">H ∈ L k−1 (G k ), φ(H) ∈ L k−1 (G k+1 ) is isomorphic to H.</formula><p>For example, the multisets (with arbitrarily labeled vertices) 1 2 3 , 2 3 4 , 5 6 7 and { 1 3 5 , 2 5 7 , 5 7 9</p><p>have an isomorphism-preserving bijection between them and will thus be considered equal.</p><p>In the interest of brevity, what follows is a sketch. We first observe that we only need to consider the multisets of induced subgraphs that include vertex 0 ∈ V = {0, 1, . . . , M k − 1} due to the vertex transitivity of the CSL graphs. Next, we observe that we only need to consider the multisets of 'maximally connected' induced subgraphs of size k − 1. By 'maximally connected', we mean an induced subgraph of size k − 1 such that no more edges from G R , R ∈ {k, k +1}, can be added without adding a k th vertex to the induced subgraph. Indeed, once we show that a bijection exists between maximally connected induced subgraphs of size k−1, it follows that such a bijection exists for any connected induced subgraphs of size k − 1 since these can be formed by deleting any edge that does not render the induced subgraph disconnected. Then, viewing disconnected graphs as the disjoint union of connected components, a similar argument to the one applied for connected induced subgraphs can be used to complete the argument for any possible induced subgraph of size k − 1.</p><p>We can construct all such maximally connected subgraphs including 0 ∈ V in both G R for R ∈ {k, k + 1} by form-ing recursive sequences on the integers {0, 1, . . . , M k − 1} with addition mod M k (see for instance Definition 2.1); the key difference in these sequences is whether R = k or R = k + 1 can be added to or subtracted from the previous value in the sequence. We will call the former a k-sequence and the latter a (k + 1)-sequence. In either case, distinct sequences may result in the same induced subgraphs but we can simply take one representative from each equivalence class.</p><p>Importantly, these sequences can be constructed in a way that abstracts from either underlying graph G k or G k+1 . Due to our choice of M k , the recursive sequences never 'wrap around' the graph and can be informally thought of as a recursive sequence on the integers of a bounded interval with 0 in the middle rather than a circle with skip links. In particular, we can define recursive sequences on the set of integers {−(k + 1)(k − 1), . . . , (k + 1)(k − 1)} with regular addition to construct the same induced subgraphs (-1 corresponds to vertex (M k − 1) ∈ V and so on, in either case). Then it becomes clear that there is an isomorphism-preserving bijection between the induced subgraphs formed by recursive (k + 1)-sequences and ksequences on this bounded interval of integers; any sequence defined in terms of adding or subtracting k + 1 can be replaced by one that adds or subtracts k (and vice versa), which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Details of the Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Relational Pooling and Graph Structure Representation on CSL Graphs</head><p>Our GIN architecture uses five layers of recursion, where every MLP (l) has two hidden layers with 16 neurons in the hidden layers. The graph embedding is mapped to the output through a final linear layer softmax(h T G W). (l) is treated as a learnable parameter. With standard GIN, since the vertex attributes are not one-hot encoded (they are constants), we first apply an MLP embedding before computing the first update recursion (as in <ref type="bibr" target="#b62">Xu et al. (2019)</ref>). Since RP-GIN utilizes one-hot IDs, we do not need an MLP embedding in the first update. For these experiments, we assign one-hot encoding of i mod 10 for i ∈ {1, 2 . . . , |V | = 41} -rather than completely unique IDs -which facilitates learning. We train with π-SGD, applying one random permutation of each adjacency matrix at each epoch. For inference, we average the score over five random permutations of each graph, as in Remark 2.1. <ref type="figure">Figure 3</ref> shows the stronger performance of RP-GIN on this task.</p><p>Both models are trained for 1000 epochs using ADAM <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref> for optimization.  <ref type="figure">Figure 3</ref>: RP-GNN is more powerful than WL-GNN in a challenging 10-class classification task. For the cross-validation, we use five random initializations at each fold. The folds are such that the classes are balanced in both training and validation.</p><p>Models are trained on CPUs but on machines with multiple CPUs; PyTorch inherently multithreads the execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Predicting Molecular Properties</head><p>Here we provide additional details on the molecular experiments.</p><p>(1) For the models based on Graph Convolution <ref type="bibr" target="#b17">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b1">Altae-Tran et al., 2017)</ref>, we extend the architecture provided from DeepChem and the MoleculeNet project. Following them, the learning rate was set to 0.003, we trained with mini-batches of size 96, and used the Adagrad optimizer <ref type="bibr" target="#b16">(Duchi et al., 2011)</ref>. Models were trained for 100 epochs. Training was performed on 48 CPUs using the inherent multithreading of DeepChem.</p><p>Note that we re-trained DeepChem models using this fewer number of epochs to make results comparable. That being said, many models reached optimal performance before the last epoch; we use the model with best validation-set performance for test-set prediction.</p><p>(2) For the so-called RNN and CNN models, all MLPs have one hidden layer with 100 neurons. We used the Adam optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref>, again training all models with mini-batches of size 96 and 50 epochs. We performed a hyperparameter line search over the learning rate, with values in {0.003, 0.001, 0.01, 0.03, 0.1, 0.3}. Training was performed on GeForce GTX 1080 Ti GPUs. To model the RNN, we use an LSTM with 100 neurons and use the long term memory as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RP-Duvenaud</head><p>When we train RP-Duvenaud, we follow any training particulars as in the DeepChem implementation. For instance, DeepChem's implementation computes a weighted loss which penalizes misclassification differently depending on the task, and they compute an overall performance metric by taking the mean of the AUC across all tasks (see <ref type="table" target="#tab_2">Table 3</ref>). One difference is that the DeepChem recommends either metrics PRC-AUC or ROC-AUC and splits "random" or "scaffold" depending on the dataset under consideration. Since ROC-AUC and random splits were the most commonly used among the three datasets we chose, we decided -before training any models -to use random splits and ROC-AUC for every dataset for simplicity. We also note that the authors of MoleculeNet report ROC-AUC scores on all three datasets. Regarding the sizes of the train/validation/test splits, we used the default values provided by DeepChem.</p><p>We implement the model that assigns unique IDs to atoms by first finding the molecule with the most atoms across training, validation, and test sets, and then appending a feature vector of that size to the endowed vertex attributes. That is, if the largest molecule has A atoms, we concatenate a vector of length A of one-hot IDs to the existing vertex attributes (for every vertex in each molecule).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs and RNNs</head><p>We explore k = 20-ary RP with f as a CNN, learned with π-SGD. At each forward step, we run a DFS from a different randomly-selected vertex to obtain a 20 × 20 × 14 subtensor of A (there are 14 edge features), which we feed through two iterations of conv → ReLU → MaxPool to obtain a representation h A of A. The corresponding vertex attributes are fed through an MLP and concatenated with h A to obtain a representation h G of the graph which in turn is fed through an MLP to obtain the predicted class (see also Equation 3). Zero padding was used to account for the variable-sized molecules. Twenty initial vertices for the DFS (i.e. random permutations) were sampled at inference time. <ref type="table" target="#tab_1">Table 2</ref> shows that the CNN f underperforms in all tasks.</p><p>We also consider RP with an RNN as f learned with π-SGD, starting with a DFS to yield a |V | × |V | × 14 subtensor. For f , we treat the edge features of a given vertex as a sequence: for vertex v, we apply an LSTM to the sequence (A v,1,· , A v,2,· , . . . , A v,|V |,· ) and extract the long-term state. We also take the vertex attributes and pass them through an MLP. The long term state and output of the MLP are concatenated, ultimately forming a representation for every vertex (and its neighborhood) which we view as a second sequence. We apply a second LSTM and again extract the long term state, which can be denoted h G , the embedding of the graph. Last, h G is forwarded through an MLP yielding a class prediction. Twenty starting vertices (i.e. permutations) were sampled at inference time. Variability was quantified with 5 random train/val/test splits for both neural network based models. Interestingly, <ref type="table" target="#tab_1">Table 2</ref> shows that the RP-RNN approach performs reasonably well in the Tox21 dataset, while underperforming in other datasets. Future work is needed to determine those tasks for which the RNN and CNN approaches are better suited.</p><p>π-SGD To train with π-SGD, we sample a different random permutation of the graph at each forward pass. In the case of RP-Duvenaud, this involves assigning permutationdependent unique IDs at each forward step (as in Equation 5). In our implementation, we achieve this by building a new DeepChem object for the molecule at each forward pass. This operation is expensive but we did not consider refined code optimizations for this work. In general, with properly optimized code, sampling permutations need not be as expensive and allows for a tractable and theoretically justified procedure. Looking ahead to the test data in order to find the largest molecule in test and validation corresponds to using domain knowledge and the modeling choice that the resulting model will only work on molecules with at most A atoms. It is not hard to construct a similar model that does not rely on this lookahead mechanism, such as assigning a one-hot encoding of i mod A observed where i ∈ {1, 2, . . . , |V |} and A observed is the largest molecule observed in the model building phase.</p><p>Molecule dataset details Details on the molecular datasets are shown in <ref type="table" target="#tab_2">Table 3</ref>. The observant reader may notice that we report a different number of Tox21 molecules than in <ref type="bibr" target="#b60">Wu et al. (2018)</ref>. This resulted from simultaneously finding (1) that the validation and testing Python objects for Tox21 were empty when we first loaded them in the early stage of development and (2) comments in the DeepChem source code that led us to believe that this was expected behavior. We thus split up the 'training' dataset rather than using the provided splits. This treatment was the same for all models, making the comparison fair.</p><p>The number of molecules in each dataset with greater than k = 10, 20, 30, 40, 50 molecules are 98.18%, 63.71%, 22.30%, 7.71%, 3.59% for HIV; 99.93%, 75.33%, 12.30%, 0.03%, 0.00% for MUV; and 78.07%, 33.63%, 10.39%, 3.90%, 1.97% for Tox21. : Training time and model performance of k-ary models for the Tox21 task. Test-set AUC was computed for five different random splits of train/validation/test: we show the mean ± one standard deviation. We also show the speedup factor for training: time to train on the full graph divided by time for k-ary model. Training was performed on 48 CPUs, making use of PyTorch's inherent multithreading. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>G</head><label></label><figDesc>skip (11, 2) G skip (11, 3)Figure 1: The WL test incorrectly deems these isomorphic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of a k-ary (k = 3) RP on a 5-node graph with one-dimensional edge attributes (d e = 1) and no vertex attributes. The graph is encoded as a 5×5× 2 tensor A. k-ary RP selects the top-left k×k corner of a permuted tensor A π,π .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-level representations that can be mapped to a graph-level representation by summing across h(l)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>but it was unable to outperform 'Duvenaud et al'. Model architectures, hyperparameters, and training procedures are detailed in the Supplementary Material. RP-GNN We compare the performance of the 'Duvenaud et al.' baseline to RP-Duvenaud, wherein the 'Duvenaud et al.' GNN is used as f in Equation 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Duvenaud et al. 0.818 (0.022) 0.768 (0.014) 0.778 (0.007) k = 40 Duvenaud et al. 0.807 (0.025) 0.776 (0.032) 0.783 (0.007) k = 30 Duvenaud et al. 0.829 (0.024) 0.776 (0.030) 0.775 (0.011) k = 20 Duvenaud et al. 0.813 (0.017) 0.777 (0.041) 0.755 (0.003) k = 10 Duvenaud et al. 0.812 (0.035) 0.773 (0.045) 0.687 (0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Proof. (F k−1 ⊂ F k ): Consider an arbitrary element f (k−1) ∈ F k−1 , and write f (A[1 : (k − 1), 1 : (k − 1), : ], X (v) [1 : (k − 1), :]; W ) for its associated permutationsensitive RP function. Also consider f (k) ∈ F k and let f be its associated permutation-sensitive RP function. For any tensor A and attribute matrix X (v) , we can define f (A[1 : k, 1 : k, :], X (v) [1 : k, :]; W ) = f (A[1 : (k − 1), 1 : (k −1), :], X (v) [1:(k −1), :]; W ). Thus, f (k−1) ∈ F k and because f (k−1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 4: Training time and model performance of k-ary models for the Tox21 task. Test-set AUC was computed for five different random splits of train/validation/test: we show the mean ± one standard deviation. We also show the speedup factor for training: time to train on the full graph divided by time for k-ary model. Training was performed on 48 CPUs, making use of PyTorch's inherent multithreading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Training time and model performance of k-ary models for the HIV task. Training time and model performance of k-ary models for the MUV task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Statistics, and 2 Department of Computer Science, Purdue University, West Lafayette, Indiana, USA. Correspondence to: Ryan L. Murphy &lt;murph213@purdue.edu&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of RP-GNN and k-ary RP where f is the 'Duvenaud et al.' GNN or a neural-network. We show mean (standard deviation) ROC-AUC across multiple random train/val/test splits. DFS indicates Depth-First Search poly-canonical orientation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Datasets used in our experiments.</figDesc><table><row><cell cols="3">Data Set Number of Compounds Number of Tasks</cell></row><row><cell>HIV</cell><cell>41,127</cell><cell>1</cell></row><row><cell>MUV</cell><cell>93,087</cell><cell>17</cell></row><row><cell>Tox 21</cell><cell>6,284</cell><cell>12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/PurdueMINDS/RelationalPooling</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">moleculenet.ai/latest-results, (Dec. 2018)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was sponsored in part by the ARO, under the U.S. Army Research Laboratory contract number W911NF-09-2-0053, the Purdue Integrative Data Science Initiative and the Purdue Research foundation, the DOD through SERC under contract number HQ0034-13-D-0004 RT #206, and the National Science Foundation under contract numbers IIS-1816499 and DMS-1812197.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representations for partially exchangeable arrays of random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Aldous</surname></persName>
		</author>
		<idno>0047259X. doi: 10.1016/ 0047-259X</idno>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="90099" to="90102" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low data drug discovery with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph isomorphism, color refinement, and compactness. computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="627" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No unbiased estimator of the variance of k-fold cross-validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1089" to="1105" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Probabilistic symmetry and invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06082</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>1053-5888. doi: 10.1109/ MSP.2017.2693418</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7974879/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">La prévision: ses lois logiques, ses sources subjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Finetti</surname></persName>
		</author>
		<idno>53-118</idno>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;institut Henri Poincaré</title>
		<editor>H. E. Kyburg and H.E. Smokler</editor>
		<imprint>
			<date type="published" when="1937" />
			<biblScope unit="page" from="1" to="68" />
		</imprint>
	</monogr>
	<note>Studies in Subjective Probability</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph limits and exchangeable random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
		<idno>1542-7951. doi: 10.1080/ 15427951.2008.10129166</idno>
		<ptr target="http://arxiv.org/abs/0712.2749" />
	</analytic>
	<monogr>
		<title level="j">Rend. di Mat. e delle sue Appl. Ser. VII</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the combinatorial power of the Weisfeiler-Lehman algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithms and Complexity</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infinite latent feature models and the Indian buffet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/gilmer17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep models of interactions across sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02879</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tox21challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakamuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shahane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossoshek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simeonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">A Method for Stochastic Optimization. International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<title level="m">variant and equivariant graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09342</idno>
		<title level="m">On the universality of invariant networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeptox: toxicity prediction using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Subgraph pattern neural networks for high-order graph evolution prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning invariant representations of molecules for atomization energy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fazli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ziehe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman Go</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neural</surname></persName>
		</author>
		<title level="m">Higher-order Graph Neural Networks. Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Janossy pooling: Learning deep permutationinvariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJluy2RcFm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tractability through exchangeability: A new perspective on efficient probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kernel graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-P</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian models of graphs, arrays and other exchangeable random structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="437" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<ptr target="https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837" />
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poczos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2892" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Maximum unbiased validation (muv) data sets for virtual screening based on pubchem bioactivity data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Inverse molecular design using machine learning: Generative models for matter engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">6400</biblScope>
			<biblScope unit="page" from="360" to="365" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph pattern mining and learning through user-defined relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meira</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1266" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On circulant graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vilfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sethuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Theory and its Applications</title>
		<editor>R. J.</editor>
		<imprint>
			<publisher>Narosa Publishing House</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="34" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.03536" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastics: An International Journal of Probability and Stochastic Processes</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="177" to="228" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The convergence of contrastive divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
