<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint extraction of entities and relations aims to detect entity pairs along with their relations using a single model. Prior work typically solves this task in the extract-then-classify or unified labeling manner. However, these methods either suffer from the redundant entity pairs, or ignore the important inner structure in the process of extracting entities and relations. To address these limitations, in this paper, we first decompose the joint extraction task into two interrelated subtasks, namely HE extraction and TER extraction. The former subtask is to distinguish all head-entities that may be involved with target relations, and the latter is to identify corresponding tail-entities and relations for each extracted head-entity. Next, these two subtasks are further deconstructed into several sequence labeling problems based on our proposed span-based tagging scheme, which are conveniently solved by a hierarchical boundary tagger and a multi-span decoding algorithm. Owing to the reasonable decomposition strategy, our model can fully capture the semantic interdependency between different steps, as well as reduce noise from irrelevant entity pairs. Experimental results show that our method outperforms previous work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new state-of-the-art on three public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Extracting pairs of entities with relations from unstructured text is an essential step in automatic knowledge base construction, and an ideal extraction system should be be capable of extracting overlapping relations (i.e., multiple relations share a common entity) <ref type="bibr" target="#b25">[26]</ref>. Traditional pipelined approaches first recognize entities, then choose a relation for every possible pair of extracted entities. Such framework makes the task easy to conduct, but ignoring the underlying interactions between these two subtasks <ref type="bibr" target="#b10">[11]</ref>. One improved way is to train them jointly by parameter sharing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>. Although showing promising results, these extract-then-classify approaches still require explicit separate components for entity extraction and relation classification. As a result, their relation classifiers may be misled by the redundant entity pairs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>, since N entities will lead to roughly N 2 pairs, and most of which are in the NA (non-relation) class.</p><p>Rather than extracting entities and relations separately, Zheng et al. <ref type="bibr" target="#b28">[29]</ref> proposed a unified tagging scheme to transform joint extrac-tion to a sequence labeling problem with a kind of multi-part tags. However, this model lacks the elegance to identify overlapping relations, which may lead to poor recall when processing a sentence with overlapping relations. As the improvement, Dai et al. <ref type="bibr" target="#b2">[3]</ref> presented PA-LSTM which tags entity and relation labels simultaneously according to each query word position, and achieves state-ofthe-art performance. Nevertheless, these models always ignore the inner structure such as dependency included in the head entity, tail entity and relation due to the unified labeling-once process. As is well known, a tail-entity and a relation should be depended on a specific head-entity. In other words, if one model cannot fully perceive the semantics of head-entity, it will be unreliable to extract the corresponding tail entities and relations. For a complex NLP task, it is very common to decompose the task into easier modules or processes, and a reasonable design is quite crucial to help one model make further progress <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. In this paper, through analysis of the two kinds of methods above, we exploit the inner structure of joint extraction and propose a novel decomposition strategy, which hierarchically decomposes the task into several sequence labeling problems with partial labels capturing different aspects of the final task (see <ref type="figure">Figure 1</ref>). Starting with a sentence, we first judiciously distinguish all the candidate head-entities that may be involved with target relations, then label corresponding tail-entities and relations for each extracted head-entity. We call the former subtask as Head-Entity (HE) extraction, and the later as Tail-Entity and Relation (TER) extraction. Such extract-then-label (ETL) paradigm can be understood by decomposing the joint probability of triplet extraction into conditional probability p(h, r, t|S) = p(h|S)p(r, t|h, S), where (h, r, t) is a triplet in sentence S. In this manner, our TER extractor is able to take the semantic and position information of the given head-entity into account when tagging tail-entities and relations, and naturally, one head-entity can interact with multiple tail-entities to form overlapping relations. Besides, compared with the extract-then-classify methods, our paradigm no longer extracts all entities at the first step, only head-entities that are likely to participate in target triplets are identified, thus alleviating the impact of redundant entity pairs.</p><p>Next, inspired by extractive question answering which identifies answer span by predicting its start and end indices <ref type="bibr" target="#b18">[19]</ref>, we further decompose HE and TER extraction with a span-based tagging scheme. Specifically, for HE extraction, entity type is labeled at the the start and end positions of each head-entity. For TER extraction, we annotate the relation types at the start and end positions of all the tail-entities which have relationship to a given head-entity. To enhance the association between boundary positions, we present a hierarchical boundary tagger, which labels the start and end posi-  <ref type="figure">Figure 1</ref>. An example of our tagging scheme. PER is short for entity type PERSON, LOC is short for LOCATION, PO is short for relation type President of, BI is short for Born in, and LI is short for Located in.</p><p>tions separately in a cascade structure and decode them together by a multi-span decoding algorithm. By this means, HE and TER extraction can be modeled in the unified span-based extraction framework, differentiated only by their prior knowledge and output label set. Overall, for a sentence with m head-entities, the entire task is deconstructed into 2 + 2m sequence labeling subtasks, the first 2 for HE tagging and the other 2m for TER. Intuitively, the individual subtasks are significantly easy to learn compared with the whole extraction task, suggesting that by trained cooperatively with shared underlying representations, they can constrain the learning problem and achieve a better overall outcome.</p><p>We evaluate our method on three public datasets: NYT-single, NYT-multi and WebNLG. Experimental results show that the proposed method significantly outperforms previous work on normal, overlapping and multiple relation extraction, increasing the SOTA F1 score to 59.0% (+5.2%), 78.0% (+5.9%) and 83.1% (+21.5%), respectively. Further analysis confirms the effectiveness and rationality of our decomposition strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>In this section, we first introduce our tagging scheme, based on which the joint extraction task is transformed into several sequence labeling problems. Then we detail the hierarchical boundary tagger, which is the basic labeling module in our method. Finally, we move on to the entire extraction system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tagging Scheme</head><p>Let us consider the head-entity (HE) extraction first. As discussed in the previous section, it is decomposed into two sequence labeling subtasks. The first subtask mainly focuses on identifying the start position of one head-entity. One token is labeled as the corresponding entity type if it is the start word, otherwise it is assigned the label "O" (Outside). In contrast, the second subtask aims to identify the end position of one head-entity and has a similar labeling process except that the entity type is labeled for the token which is the end word.</p><p>For each identified head-entity, TER extraction is also decomposed into two sequence labeling subtasks which make use span boundaries to extract tail-entities and predict relations simultaneously. The first sequence labeling subtask mainly labels the relation type for the token which is the start word of the tail-entity, while the second subtask tags the end word. <ref type="figure">Figure 1</ref> illustrates an example of our tagging scheme, in which the words "United", "States", "Trump", "Queens", "New" and "City" are all related to final extraction results, thus they are labelled with special tags. For example, the word "Trump" is the first and also the last word of head-entity "Trump", so the tags are both PER-SON in the start and end tag sequences when tagging HE. For TER extraction, when the given head-entity is "Trump", there are two tail-entities involved in with wanted relations, i.e., ("Trump", President Of, "United States") and ("Trump", Born In, "New York City"), so "United" and "New" are labeled as President Of and Born In respectively in the start tag sequences. Similarly, we can obtain end tag sequences that "States" and "City" are marked. Beyond that, the other words irrelevant to the final result are labeled as "O".</p><p>Note that our tagging scheme is quite different from PA-LSTM <ref type="bibr" target="#b2">[3]</ref>. For an n-word sentence, PA-LSTM builds n different tag sequences according to different query position while our model tags the same sentence for 2 + 2 × m times to recognize all overlapping relations, where m is the number of head-entities and m &lt;&lt; n. This means our model is more time-saving and efficient. Besides, it uses "BIES" signs to indicate the position of tokens in the entity while we only predict the start and end positions without loss of the ability to extract multi-word entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Boundary Tagger</head><p>According to our tagging scheme, we utilize a unified architecture to extract HE and TER. In this paper, we wrap such extractor into a general module named hierarchical boundary tagger (abbreviated as HBT). For the sake of generality, we do not distinguish between head and tail-entity, and they are collectively referred to as targets in this subsection. Formally, the probability of extracting a target t with label l (entity type for head-entity or relation type for tail-entity) from sentence S is universally modeled as:  <ref type="figure">Figure 2</ref>. An illustration of our model. The left panel is an overview of our joint extraction system, and the right panel shows the detailed structure of our sequence tagger HBT. Here, "Queens" is extracted by the HE extractor, then its hidden state in the shared encoder is marked as the yellow box and entered into the TER extractor as prior knowledge.</p><formula xml:id="formula_0">p(t, l|S) = p(s l t |S)p(e l t |s l t , S)<label>(1)</label></formula><p>where s l t is the start index of t with label l and e l t is the end index. Such decomposition indicates that there is a natural order among the tasks: predicting end positions may benefit from the prediction results of start positions, which motivates us to employ a hierarchical tagging structure. As shown in the right panel of <ref type="figure">Figure 2</ref>, we associate each layer with one task and take the tagging results as well as hidden states from the low-level task as input to the high-level. In this work, we choose BiLSTM <ref type="bibr" target="#b5">[6]</ref> as the basic encoder. Formally, the label of word xi when tagging the start position is predicted as Eq. 4.</p><formula xml:id="formula_1">h sta i = BiLSTMsta([hi; ai]) (2) P (y sta i ) = Softmax(W sta · h sta i + b sta ) (3) sta tag(xi) = arg max k P (y sta i = k)<label>(4)</label></formula><p>where hi denotes token representation and ai is an auxiliary vector. For HE extraction, ai is a global representation learned from the entire sentence. It is beneficial to make more accurate predictions from a global perspective. For TER extraction, ai is the concatenation of a global representation and a head-entity-related vector to indicate the position and semantic information of the given head-entity. Here we adopt BiLSTMsta to fuse hi with ai into a single vector h sta i . Analogously, xi's end tag can be calculated by Eq. 6.</p><formula xml:id="formula_2">h end i = BiLSTM end ([h sta i ; ai; p se i ]) (5) P (y end i ) = Softmax(W end · h end i + b end ) (6) end tag(xi) = arg max k P (y end i = k)<label>(7)</label></formula><p>The difference between Eq. 2-4 and Eq. 5-7 is twofold. Firstly, we replace hi in Eq. 2 with h sta i to make model aware of the hidden states of start positions when predicting end positions. Secondly, inspired by the position encoding vectors used in <ref type="bibr" target="#b23">[24]</ref>, we feed the position embedding p se i to the BiLSTM end layer as its additional input. p se i can be obtained by looking up p se i in a trainable position embedding matrix, where</p><formula xml:id="formula_3">p se i = i − s * , if s * exists C, otherwise<label>(8)</label></formula><p>Here s * is the nearest start position before current index, and p se i is the relative distance between xi and s * . When there is no start position before xi, s * will not exist, then p s i is assigned as a constant C that is normally set to the maximum sentence length. In this way, we explicitly limit the length of the extracted entity and teach model that the end position is impossible to be in front of the start position. To prevent error propagation, we use the gold p se (distance to the correct nearest start position) during training process.</p><p>We define the training loss (to be minimized) of HBT as the sum of the negative log probabilities of the true start and end tags by the predicted distributions:</p><formula xml:id="formula_4">LHBT = − 1 n n i=1 (log P (y sta i =ŷ sta i ) + log P (y end i =ŷ end i )) (9)</formula><p>whereŷ sta i andŷ end i are the true start and end tags of the i-th word, respectively, and n is the length of the input sentence.</p><p>At inference time, to adapt to the multi-target extraction task, we propose a multi-span decoding algorithm, as shown in Algorithm 1. For each input sentence S, we first initialize several variables (Lines 1-4) to assist with the decoding: (1) n is defined as the length of S. (2) R is initialized as an empty set to record extracted targets and type tags. (3) s * is introduced to hold the nearest start position before current index. (4) p se is initialized as a list of length n with default value C to save the position sequence [p se 1 , · · · , p se n ]. Next, we obtain the start tag sequence by Eq. 4 (Line 5) and compute p se i for each token by Eq. 8 (Lines 6-10). On the basis of p se , we can get p se by looking up position embedding matrix (Line 11) . Then the tag sequence of end position can be computed by Eq. 7 (Line 12). Now, all preparations necessary are in place, we start to decoding sta tag(S) and end tag(S). We first traverse sta tag(S) to find the start position of a target (Line 13). If the tag of current index is not "O", it denotes that this position may be a start word (Line 14), then we will traverse end tag(S) from this index to search for a end position (Line 15). The matching criterion is that if the tag of the end position is identical to the start position (Line 16), the words between the two indices are considered to be a candidate target (Line 17), and the label of start position (or end position) is deemed as the tag of this target (Line 18). The extracted target along with its tag is then added to the set R (Line 19), and the search in end tag(S) is terminated to continue to traverse sta tag(S) to find the next start position (Line 20). Once all the indices in sta tag(S) are iterated, this decoding function ends by returning the recordset R (Line 21).</p><p>Algorithm 1 Multi-span decoding Input: S, C S denotes the input sentence C is a predefined distance constant Output: {(ej, tagj)} m j=1 , ej denotes the j-th extracted target and tagj is the type tag 1: Define n ← Sentence Length 2: Initialize R ← {} 3: Initialize s * ← 0 4: Initialize p se as a list of length n with default value C 5: Obtain sta tag(S) by Eq. 4 <ref type="bibr">6:</ref> for idx ← 1 to n do <ref type="bibr">7:</ref> if sta tag (S)[idx] = "O" then 8:</p><formula xml:id="formula_5">s * ← idx 9:</formula><p>if s * &gt; 0 then 10:</p><formula xml:id="formula_6">p se [idx] ← idx − s * 11:</formula><p>Obtain p se by transforming p se into matrix <ref type="bibr">12:</ref> Obtain end tag(S) by Eq. 7 <ref type="bibr">13:</ref> for idxs ← 1 to n do <ref type="bibr">14:</ref> if sta tag(S)[idxs] = "O" then <ref type="bibr">15:</ref> for idxe ← idxs to n do <ref type="bibr">16:</ref> if end tag(S) <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EXTRACTION SYSTEM</head><p>With the span-based tagging scheme and the hierarchical boundary tagger, we propose an end-to-end neural architecture ( <ref type="figure">Figure 2)</ref> to extract entities and overlapping relations jointly, which first encodes the sentence with a shared BiLSTM encoder. Then, a HE extractor is built to extract head entities. For each extracted head entity, the TER extractor is triggered with this head-entity's semantic and position information to detect corresponding tail-entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Shared Encoder</head><p>Given sentence S = {x1, · · · , xn}, we utilize a BiLSTM layer to incorporate information from both forward and backward directions:</p><formula xml:id="formula_7">hi = BiLSTM sha (xi)<label>(10)</label></formula><p>where hi is the hidden state at position i, and xi is the word representation of xi which contains pre-trained embeddings and characterbased word representations generated by running a CNN on the character sequence of xi. Following <ref type="bibr" target="#b3">[4]</ref>, we also employ part-of-speech (POS) embedding to enrich xi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">HE Extractor</head><p>HE extractor aims to distinguish candidate head-entities and exclude irrelevant ones. We first concatenate hi and g to get the feature vector xi = [hi; g], where g is a global contextual representation computed by max pooling over all hidden states. Actually, g works as the ai for each token in Eq. 2. Moreover, we use HHE = {x1, · · · ,xn} to denote all the word representations for HE extraction and subsequently feed HHE into one HBT to extract head-entities:</p><formula xml:id="formula_8">RHE = HBTHE(HHE)<label>(11)</label></formula><p>where RHE = {(hj, type h j )} m j=1 contains all the head-entities and corresponding entity type tags in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">TER Extractor</head><p>Similar to HE extractor, TER extractor also uses the basic representation hi and global vector g as input features. However, simply concatenating hi and g is not enough for detecting tail-entities and relations with the specific head-entity. The key information required to perform TER extraction includes: (1) the words inside the tail-entity;</p><p>(2) the depended head-entity; (3) the context that indicates the relationship; (4) the distance between tail-entity and head-entity. Under these considerations, we propose the position-aware, head-entityaware and context-aware representationxi. Given a head-entity h, we definexi as follows:</p><formula xml:id="formula_9">x i = [hi; g; h h ; p ht i ]<label>(12)</label></formula><p>where h h = [hs h ; he h ] denotes the representation of head-entity h, in which hs h and he h are the hidden states at the start and end indices of h respectively. p ht i is the position embedding to encode the the relative distance from xi to h. Obviously, [g; h h ; p ht i ] is the auxiliary feature vector for TER extraction as ai in Eq. 2.</p><p>Formally, we take HTER = {x1, · · · ,xn} as input to one HBT, and the output RTER = {(to, relo)} z o=1 , in which to is the o-th extracted tail-entity and relo is its relation tag with the given head-entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTER = HBTTER(HTER)</head><p>Then we can assemble triplets by combining h and each (to, relo) to form {(h, relo, to)} z o=1 , which contains all triplets with headentity h in sentence S 5 . It is worth noting that at the training time, h is the gold head-entity, while at the inference time we select headentity one by one from RHE to complete the extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Training of Joint Extractor</head><p>Two learning signals are provided to train the model: LHE for HE extraction and LTER for TER extraction, both are formulated as Eq.9. To share input utterance across tasks and train them jointly, for each training instance, we randomly select one head-entity from gold head-entity set as the specified input of the TER extractor. We can also repeat each sentence many times to ensure all triplets are utilized, but the experimental results show that this is not beneficial. Finally, the joint loss is given by:</p><formula xml:id="formula_11">L = LHE + LTER<label>(14)</label></formula><p>Then, the model is trained with stochastic gradient descent. Optimizing Eq.14 enables the extraction of head-entity, tail-entity, and relation to be mutually influenced, such that, errors in each component can be constrained by the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Datasets</head><p>Following popular choices and previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>, We conduct experiments on three benchmark datasets: (1) NYT-  <ref type="bibr" target="#b4">[5]</ref> for Natural Language Generation task. We use the dataset pre-processed by Zeng et al <ref type="bibr" target="#b25">[26]</ref> and the train set contains 5019 sentences, the test set contains 703 sentences and the validation set contains 500 sentences. Statistics of the datasets are shown in <ref type="table" target="#tab_3">Table 1</ref>.</p><p>Besides, as suggested in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, we also divided the test set into three categories: Normal, SingleEntityOverlap (SEO), and Entity-PairOverlap (EPO) to verify the effectiveness on extracting overlapping relations. Specifically, a sentence belongs to Normal class if none of its triplets has overlapping entities. If the entity pairs of two triplets are identical but the relations are different, the sentence will be added to the EPO set. A sentence belongs to SEO class if some of its triplets have an overlapped entity and these triplets dont have any overlapped entity pair. Note that a sentence in the EPO set may contain multiple Normal and SEO triplets. We discuss the result for different categories in the detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation</head><p>We follow the evaluation metrics in previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. A triplet is marked correct if and only if its relation type and two corresponding entities are all correct, where the entity is considered correct if the head and tail offsets are both correct. We adopt the standard micro Precision, Recall and F1 score to evaluate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Implementation Details</head><p>We use the 300 dimension Glove <ref type="bibr" target="#b15">[16]</ref> to initialize word embeddings. The POS, character and position embeddings are randomly initialized with 30 dimensions. The window size of CNN for characterbased word representations is set to 3, and the number of filters is 50. For Bi-LSTM encoder, the hidden vector length is set to 100. Parameter optimization is performed using Adam <ref type="bibr" target="#b8">[9]</ref> with learning rate 0.001 and batch size 64. Dropout is applied to word embeddings and hidden states with a rate of 0.4. To prevent the gradient explosion problem, we set gradient clip-norm as 5. All the hyper-parameters are tuned on the validation set. We run 5 times for each experiment then report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Comparison Models</head><p>For comparison, we employ the following models as baselines: <ref type="bibr" target="#b0">(1)</ref> Cotype <ref type="bibr" target="#b16">[17]</ref> learns jointly the representations of entity mentions, relation mentions and type labels; (2) NovelTagging <ref type="bibr" target="#b28">[29]</ref> is the first proposed unified sequence tagger which predicts both entity type and relation class for each word; (3) MultiDecoder <ref type="bibr" target="#b25">[26]</ref> considers relation extraction as a sequence-to-sequence problem and uses dynamic decoders to extract relation triplets; (4) MultiHead <ref type="bibr" target="#b1">[2]</ref> first identifies all candidate entities, then perform relation extraction by identifying multiple relations for each entity, these two tasks are trained jointly; (5) PA-LSTM <ref type="bibr" target="#b2">[3]</ref> is the current best unified labeling method, which tags entity and relation labels simultaneously according to a query word position and achieves the recent state-of-the-art results on the NYT-single dataset; <ref type="bibr" target="#b5">(6)</ref> GraphRel <ref type="bibr" target="#b3">[4]</ref> is the latest extrat-thenclassify method, which first employs GCNs to extract hidden features, then predicts relations for all word pairs of an entity mention pair extracted by a sequence tagger; <ref type="bibr" target="#b6">(7)</ref> OrderRL <ref type="bibr" target="#b24">[25]</ref> is the state-ofthe-art method on the NYT-multi and WebNLG datasets, which applies the reinforcement learning into a sequence-to-sequence model to generate multiple triplets.</p><p>We call our proposed extract-then-label method with span-based scheme as ETL-Span. In addition, to access the performance influence of span-based scheme, we also implement another competitive baseline by replacing our tagger with widely used BiLSTM-CRF without any change in the input features (xi andxi), and utilize BIES-based scheme accordingly, which associates each type tag (entity type or relation type) with four position tags to indicate entity positions and types simultaneously, denoted as ETL-BIES. <ref type="table">Table 2</ref> reports the results of our models against other baseline methods. It can be seen that our method, ETL-Span, significantly outperforms all other methods and achieves the state-of-the-art F1 score on all three datasets. Over the latest extract-then-classify method GraphRel, ETL-Span achieves substantial improvements of 16.1% and 40.2% in F1 score on the NYT-multi and WebNLG datasets respectively. We attribute the performance gain to two design choices: (1) the integration of tail-entity and relation extraction as it captures the interdependency between entity recognition and relation classification; (2) the exclusion of redundant (non-relation) entity pairs by the judicious recognition of head-entities which are likely to take part in some relations. For the NYT-single dataset, ETL-Span improves by a relative margin of 5.2% against the strong baseline PA-LSTM. We consider that it is because (1) we decompose the difficult joint extraction task into several more manageable subtasks and handle them in a mutually enhancing way; (2) our TER extractor effectively captures the semantic and position information of the depended headentity, while PA-LSTM detects tail-entities and relations relying on a single query word. In addition, we find that the results of our model are better than sequence-to-sequence methods like MultiDecoder and OrderRL, it is likely due to the innate restrictions on RNN unrolling, the capacity of generating triplets is limited <ref type="bibr" target="#b3">[4]</ref>. Beyond that, we notice that the Precision of our model drops compared with NovelTagging on the NYT-single dataset. One possible reason is that many overlapping relations are not annotated in the manually labeled test data. Following PA-LSTM <ref type="bibr" target="#b2">[3]</ref>, we add some gold triplets into NYTsingle test set and further achieve a large improvement of 12.5% in F1 score and 18.7% in Precision compared with the results in <ref type="table">Table  2</ref>. Overall, these results indicate that our extraction paradigm which first extracts head-entity then labels corresponding tail-entity and relation can better capture the relational information in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results and Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Main Results</head><p>We also observe that ETL-Span performs remarkably better than ETL-BIES, we guess it is because ETL-BIES must do additional work to learn the semantics of the BIES tags, while in ETL-Span, the entity position is naturally encoded by the set of type labels, thus reducing the tag space of each functional tagger. Another advantage of span-based tagging is that it avoids the computing overhead of CRF, as shown in <ref type="table">Table 3</ref>, ETL-Span accelerates the decoding speed of ETL-BIES by up to 3.7 times. The main reason is that decoding the best chain of labels with CRF requires a significant amount of computing resources especially when the tag space is huge (e.g., on WebNLG with 246 relations and 989 tags). Besides, ETL-Span only takes about 1/4 time per batch and 1/5 GPU memory compared with ETL-BIES during training, which further verdicts the superiority of our span-based scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Ablation Study</head><p>To demonstrate the effectiveness of each component, we remove one particular component at a time to understand its impact on the performance. Concretely, we investigated character embedding, position embedding p ht , hierarchical tagging (by tagging boundary positions at the outmost BiLSTM layer), head-entity type tagging (by tagging 0/1 instead of entity types in the HE extractor) and joint learning (by training HE extractor and TER extractor separately without parameter sharing). From these ablations shown in <ref type="table">Table 4</ref>, we find that: (1) Consistent with PA-LSTM <ref type="bibr" target="#b2">[3]</ref>, the character-level representations are helpful to capture the morphological information and deal with OOV words.</p><p>(2) When we remove p ht , the score drops by 3.8%, which indicates that it is vital to let tail-entity extractor aware of position information of the given head-entity to filter out irrelevant entities by implicit distance constraint. (3) Removing the hierarchical tagging structure hurts the result by 2.5% F1 score, which indicates that predicting end positions benefits from the prediction results of start positions. (4) By predicting entity type in the HE extractor, we can implicitly incorporate type information into head-entity representation, which is beneficial to the subsequent TER tagging. (5) Compared with the pipelined manner, joint learning framework brings a remarkable improvement (5.3%) in F1 score, which demonstrates that our HE extractor and TER extractor actually work in the mutual promotion way, and again confirms the effectiveness and rationality of our decomposition strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Analysis on Different Sentence Types</head><p>To verify the ability of our model in handling the overlapping problem, following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, we conduct further experiments on the NYTmulti test set. The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Among the compared baselines, GraphRel and OrderRL are the latest two models with the capacity to handle the EPO triplets. For this purpose, GraphRel predicts relations for all word pairs, in this case, its relation classifier will be overwhelmed by the superfluous candidates.</p><p>OrderRL utilizes a sequence-to-sequence model to decode overlapping relations but can decode only the first word of multi-word entity, while ours can detect the whole. Readers may have noticed that ETL-Span cannot solve the problem of entity pair overlapping. Nevertheless, ETL-Span still surpasses baselines in all categories. Specifically, ETL-Span outperforms OrderRL by 6.1% on the Normal class, 6.9% on the SEO class, and 0.6% on the EPO class. In fact, even on the EPO set, there are still a significant amount of triplets where entity pairs don't overlap. The most common triplets in the real-life corpus are those of Normal and SEO class and our substantial surpass on these two categories masks our shortcomings on the EPO class. We leave the identification of EPO triplets for future work. We also compare the results given different numbers of triplets in a sentence, and sentences in the NYT-multi test set are divided into 5 subclasses, each class contains sentences that has 1,2,3,4 or ≥ 5 triplets. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, ETL-Span outperforms the baselines under all numbers of triplets in a sentence. When the sentence only contains one triplet, ETL-Span yields a 8.8% improvement in comparison with OrderRL. When there are multiple triplets in a sentence, ETL-Span still outperforms GraphRel and OrderRL significantly. These observations demonstrate that our extraction paradigm is effective to handle the multiple relation extraction task.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>There have been extensive studies for entity relation extraction task. Traditional pipelined methods divide this task into two separate subtasks: first extract the token spans in the text to detect entity mentions, and then discover the relational structures between entity mentions <ref type="bibr">[?]</ref>. Entity recognization has traditionally been solved as a sequence labeling problem, and most recent work leverages a LSTM-CRF architecture <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28]</ref>. Relation extraction is normally treated as a problem of multi-label classification [?]. Zeng et al. <ref type="bibr" target="#b23">[24]</ref> employed a deep convolutional neural network for extracting lexical and sentence level features. Zhou et al. <ref type="bibr" target="#b29">[30]</ref> combined attention mechanisms with BiLSTM to reduce intra-sentence noise. Yu et al. <ref type="bibr" target="#b22">[23]</ref> proposed to learn the latent relational expressions based on the segment attention layer for relation extraction. However, all these methods require preprocessing step such as NER and ignore interactions between entity recognization and relation extraction, therefore may suffer from error propagation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>To address the above limitation, a variety of joint learning methods were proposed [?, ?, ?, ?, ?]. Kate et al. <ref type="bibr" target="#b7">[8]</ref> presented a card-pyramid graph to represent entities and their relations in a sentence. Miwa et al. <ref type="bibr" target="#b14">[15]</ref> introduced a simple and flexible table representation of entities and relations. However, these models need complicated process of feature engineering, which requires much manual efforts and domain expertise. Recently, several end-to-end neural architectures are applied to joint relation extraction. Sun et al. <ref type="bibr" target="#b20">[21]</ref> optimized a global loss function to jointly train entity recognition model and relation classification model under the framework work of Minimum Risk Training. Bekoulis et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> first recognized the entities, then they formulated the relation extraction task as a multi-head selection problem. For each entity, they calculated the score between it and every other entities for a given relation. Tan et al. <ref type="bibr" target="#b21">[22]</ref> first identified all candidate entities, then performed relation extraction via ranking with translation mechanism. Sun et al. <ref type="bibr" target="#b19">[20]</ref> developed an entityrelation bipartite graph to perform joint inference on entity types and relation types. Fu et al. <ref type="bibr" target="#b3">[4]</ref> utilized graph convolutional network to extract overlapping relations by splitting entity mention pairs into several word pairs and considering all pairs for prediction. Nevertheless, these extract-then-classify methods still require explicit separate components for entity extraction and relation classification, and the relation classifier may be overwhelmed by the redundant extracted entity pairs. Another line of work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> directly generated triplets one by one by a sequence-to-sequence model but fail to extract an entity that has multiple words. Zheng et al. <ref type="bibr" target="#b28">[29]</ref> proposed a unified tagging model which utilizes a special tagging scheme to convert joint extraction task to a sequence tagging problem. However, their model cannot recognize overlapping relations in the sentence. As the improvement, Dai et al. <ref type="bibr" target="#b2">[3]</ref> proposed to extract overlapping triplets by tagging one n-word sentence for n times. Unfortunately, due to the labeling-once process, this kind of unified labeling methods cannot fully exploit the inter-dependency between entities and relations.</p><p>In this paper, we design a novel joint extraction paradigm which first extracts head-entities and then labels tail-entities and relations for each head-entity. In essence, it bridges the gap between extractthen-classify and unified labeling approaches. More specifically, when compared with the extract-then-classify methods, our extractthen-label paradigm no longer extracts all entities at the first step, only head-entities that are likely to participate in target triplets are identified, thus alleviating the impact of redundant entity pairs. Owing to the reasonable decomposition strategy, our model can better capture the correlations between head-entities and tail-entities than unified labeling approaches, thus resulting in a better joint extraction performance. Besides, our span-based tagging scheme is inspired by recent advances in machine reading comprehension <ref type="bibr" target="#b18">[19]</ref>, which derived the answer by predicting its start and the end indices in the paragraph. Hu et al. <ref type="bibr" target="#b6">[7]</ref> also applied this sort of architecture to opendomain aspect extraction and achieved great success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we present an end-to-end sequence labeling framework for joint extraction of entities and relations based on a novel decomposition strategy. Experimental results show that the functional decomposition of the original task simplifies the learning process and leads to a better overall learning outcome, achieving a new state-ofthe-art on three public datasets. Further analysis demonstrates the ability of our model in handling normal, overlapping and multiple relation extraction. In the future, we would like to explore similar decomposition strategy in other information extraction tasks, such as event extraction and aspect extraction. The source code of this paper can be obtained from https://github.com/yubowen-ph/JointER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>F1 score by overlapping category on the NYT-multi test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>F1 score by sentence triplet count on the NYT-multi test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1909.04273v3 [cs.CL] 19 Feb 2020</figDesc><table><row><cell>Sentence:</cell><cell cols="3">The United States</cell><cell>President</cell><cell>Trump</cell><cell>was</cell><cell>born</cell><cell>in</cell><cell>the</cell><cell>borough</cell><cell>of</cell><cell>Queens</cell><cell>in</cell><cell cols="2">New York</cell><cell>City</cell><cell>.</cell></row><row><cell>HE Tagging:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Start Tags:</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>PER</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>LOC</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell>End Tags:</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>PER</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>LOC</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trump</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Queens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TER Tagging: (for Trump):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Start Tags:</cell><cell>O</cell><cell>PO</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>BI</cell><cell>O</cell><cell>BI</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell>End Tags:</cell><cell>O</cell><cell>O</cell><cell>PO</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>BI</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>BI</cell><cell>O</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(Trump, Born_in, Queens)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">(Trump, President_of, United States)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(Trump, Born_in, New York City)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TER Tagging: (for Queens):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Start Tags:</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>LI</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell>End Tags:</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>LI</cell><cell>O</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(Queens, Located_in, New York City)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>idxe] = sta tag(S)[idxs] then</figDesc><table><row><cell>17:</cell><cell>e ← S[idxs : idxe]</cell></row><row><cell>18:</cell><cell>tag ←end tag(S)[idxe]</cell></row><row><cell>19:</cell><cell>R ← R ∪ {(e, tag)}</cell></row><row><cell>20:</cell><cell>Break</cell></row><row><cell>21: return R</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">NYT-single NYT-multi WebNLG</cell></row><row><cell># Relation types</cell><cell>24</cell><cell>24</cell><cell>246</cell></row><row><cell># Training sentences</cell><cell>66,335</cell><cell>56,195</cell><cell>5,019</cell></row><row><cell># Test sentences</cell><cell>395</cell><cell>5,000</cell><cell>703</cell></row><row><cell cols="4">single is sampled from the New York Times corpus [18] and pub-</cell></row><row><cell cols="4">lished by Ren et al [17]. The training data is automatically labeled us-</cell></row><row><cell cols="4">ing distant supervision, while 395 sentences are annotated manually</cell></row><row><cell cols="4">as test data, most of which have single triplet in each sentence. (2)</cell></row><row><cell cols="4">NYT-multi is published by Zeng et al. [26] for testing overlapping</cell></row><row><cell cols="4">relation extraction, they selected 5000 sentences from NYT-single as</cell></row><row><cell cols="4">the test set, 5000 sentences as the validation set and the rest 56195</cell></row><row><cell cols="4">sentences are used as training set. (3) WebNLG is proposed by Claire</cell></row><row><cell>et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .Table 4 .</head><label>24</label><figDesc>Main results on three benchmark datasets. Bold marks highest number among all models. ‡ marks results quoted directly from the original papers. † marks results reported in<ref type="bibr" target="#b2">[3]</ref> and<ref type="bibr" target="#b25">[26]</ref>. * marks results produced with offcial implementation. An ablation study of ETL-Span on the NYT-multi dev set.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">NYT-single Precision Recall</cell><cell>F1</cell><cell cols="2">NYT-multi Precision Recall</cell><cell>F1</cell><cell cols="2">WebNLG Precision Recall</cell><cell>F1</cell></row><row><cell cols="2">CoType  ‡ [17]</cell><cell>42.3%</cell><cell cols="2">51.1% 46.3%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">NovelTagging  † [29]</cell><cell>61.5%</cell><cell cols="2">41.4% 49.5%</cell><cell>32.8%</cell><cell cols="2">30.6% 31.7%</cell><cell>52.5%</cell><cell>19.3% 28.3%</cell></row><row><cell cols="2">MultiDecoder  ‡ [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.0%</cell><cell cols="2">56.6% 58.7%</cell><cell>37.7%</cell><cell>36.4% 37.1%</cell></row><row><cell cols="2">MultiHead  *  [2]</cell><cell>51.5%</cell><cell cols="2">52.8% 52.1%</cell><cell>60.7%</cell><cell cols="2">58.6% 59.6%</cell><cell>57.5%</cell><cell>54.1% 55.7%</cell></row><row><cell cols="2">PA-LSTM  ‡ [3]</cell><cell>49.4%</cell><cell cols="2">59.1% 53.8%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">GraphRel  ‡ [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.9%</cell><cell cols="2">60.0% 61.9%</cell><cell>44.7%</cell><cell>41.1% 42.9%</cell></row><row><cell cols="2">OrderRL  ‡ [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.9%</cell><cell cols="2">67.2% 72.1%</cell><cell>63.3%</cell><cell>59.9% 61.6%</cell></row><row><cell cols="2">ETL-BIES</cell><cell>51.1%</cell><cell cols="2">64.6% 57.2%</cell><cell>84.4%</cell><cell cols="2">71.5% 77.4%</cell><cell>83.5%</cell><cell>81.1% 82.3%</cell></row><row><cell>ETL-Span</cell><cell></cell><cell>53.8%</cell><cell cols="2">65.1% 59.0%</cell><cell>85.5%</cell><cell cols="2">71.7% 78.0%</cell><cell>84.3 %</cell><cell>82.0% 83.1%</cell></row><row><cell cols="6">Table 3. Comparison of test-time speed. Bat/s refers to the number of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">batches can be processed per second.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">NYT-single NYT-multi</cell><cell cols="2">WebNLG</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ETL-BIES</cell><cell>10.9 Bat/s</cell><cell>11.2 Bat/s</cell><cell cols="2">6.3 Bat/s</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ETL-Span</cell><cell>26.1 Bat/s</cell><cell>25.6 Bat/s</cell><cell cols="2">23.5 Bat/s</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ETL-Span</cell><cell></cell><cell>86.5%</cell><cell cols="2">73.5% 79.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Char embedding</cell><cell>83.1%</cell><cell>71.2%</cell><cell>76.7%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Position embedding p ht</cell><cell>81.9%</cell><cell>70.3%</cell><cell>75.7%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Hierarchical tagging</cell><cell>84.6%</cell><cell>70.7%</cell><cell>77.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-Head-entity type tagging</cell><cell>85.8%</cell><cell>72.2%</cell><cell>78.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Joint learning</cell><cell></cell><cell>80.4%</cell><cell>68.9%</cell><cell>74.2%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Institute of Information Engineering, Chinese Academy of Sciences, Bejing, China. Emails: {yubowen, zhangzhenyu1996, shuxiaobo, liutingwen, wangyubin}@iie.ac.cn. * Corresponding Author. 2 School of Cyber Security, University of Chinese Academy of Sciences, Bejing, China. 3 Xiaomi AI Lab, Xiaomi Inc., China. Email: wangbin11@xiaomi.com. 4 Key Laboratory of Computational Linguistics, Peking University, MOE, China. Email: lisujian@pku.edu.cn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that type h is not included in the final output of our extraction system. However, we claim that by predicting entity types, we can implicitly incorporate type information into head-entity representation, which is beneficial to the subsequent TER tagging as our experiment reveals.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>The authors thank Jianlin Su from Zhuiyi Tech Co. Ltd.for his helpful discussions. The work presented in this paper is supported by the National <ref type="figure">Key</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multihead selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using position-attentive sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open-domain targeted sentiment analysis via span-based extraction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Lv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03820</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with taskaware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Porous lattice-based transformer encoder for chinese ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Mengge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Tingwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Erli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Quangang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02733</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting entities and relations with joint minimum risk training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jointly extracting multiple triplets with multilayer translation constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond word attention: using segment attention in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quangang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5401" to="5407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning the extraction order of multiple relational facts in a sentence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sentiment tagging with partial labels using modular architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00534</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chinese ner using lattice lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
