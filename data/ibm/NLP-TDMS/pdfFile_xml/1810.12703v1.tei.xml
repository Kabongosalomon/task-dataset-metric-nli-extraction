<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marie</surname></persName>
							<email>bmarie@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
							<email>atsushi.fujita@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that rely on monolingual corpora only. In this work, we propose to define unsupervised NMT (UNMT) as NMT trained with the supervision of synthetic bilingual data. Our approach straightforwardly enables the use of state-of-the-art architectures proposed for supervised NMT by replacing human-made bilingual data with synthetic bilingual data for training. We propose to initialize the training of UNMT with synthetic bilingual data generated by unsupervised statistical machine translation (USMT). The UNMT system is then incrementally improved using back-translation. Our preliminary experiments show that our approach achieves a new state-of-the-art for unsupervised machine translation on the WMT16 German-English news translation task, for both translation directions. arXiv:1810.12703v1 [cs.CL] 30 Oct 2018 11  We considered both the sentence pairs used to initialize UNMT and all the sentence pairs generated by each iteration of UNMT in the set of sentence pairs to filter. 12  We used α = 0.5 in our experiments. 13 http://www.statmt.org/wmt18/ translation-task.html 14  We escaped special characters but did not use the option for "aggressive" tokenization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine translation (MT) systems usually require a large amount of bilingual data, produced by humans, as supervision for training. However, finding such data remains challenging for most language pairs, as it may not exist or may be too costly to manually produce.</p><p>In contrast, a large amount of monolingual data can be easily collected for many languages, for instance from the Web. 1 Previous work proposed many ways for taking advantage of the monolingual data in order to improve translation models trained on bilingual data. These methods usually exploit existing accurate translation models and have shown to be useful especially when targeting 1 See for instance the Common Crawl project: http:// commoncrawl.org/ low-resource language pairs and domains. However, they usually fail when the available bilingual data is too noisy or too small to train useful translation models. In such scenarios, the use of pivot languages or unsupervised machine translation are possible alternatives.</p><p>Recent work has shown remarkable results in training MT systems using only monolingual data in the source and target languages. Unsupervised statistical (USMT) and neural (UNMT) machine translation have been proposed <ref type="bibr" target="#b1">(Artetxe et al., 2018b;</ref><ref type="bibr" target="#b17">Lample et al., 2018b)</ref>. State-of-theart USMT <ref type="bibr" target="#b1">(Artetxe et al., 2018b;</ref><ref type="bibr" target="#b17">Lample et al., 2018b</ref>) uses a phrase table induced from source and target phrases, extracted from the monolingual data, paired and scored using bilingual word, or n-gram, embeddings trained without supervision. This phrase table is plugged in a standard phrasebased SMT framework that is used to translate target monolingual data into the source language, i.e., performing a so-called back-translation. The translated target sentences and their translations in the source language are paired to form synthetic parallel data and to train a source-to-target USMT system. This back-translation/re-training step is repeated for several iterations to refine the translation model of the system. <ref type="bibr">2</ref> On the other hand, state-of-the-art UNMT <ref type="bibr" target="#b17">(Lample et al., 2018b)</ref> uses bilingual sub-word embeddings. They are trained on the concatenation of source and target monolingual data in which tokens have been segmented into sub-word units using, for instance, byte-pairencoding (BPE) <ref type="bibr" target="#b23">(Sennrich et al., 2016b)</ref>. This method can learn bilingual embeddings if the source and target languages have in common some sub-word units. The sub-word embeddings are then used to initialize the lookup tables in the encoder and decoder of the UNMT system. Follow-2 Previous work did not address the issue of convergence and rather fixed the number of iterations to perform for these refinement steps.</p><p>ing this initialization step, UNMT mainly relies on denoising autoencoder as language model during training and on latent representation shared across the source and target languages for the encoder and the decoder.</p><p>While the primary target of USMT and UNMT is low-resource language pairs, their possible applications for these language pairs remain challenging, especially for distant languages, <ref type="bibr">3</ref> and have yet to be demonstrated. On the other hand, unsupervised MT achieves impressive results on resource-rich language pairs, with recent and quick progresses, suggesting that it may become competitive, or more likely complementary, to supervised MT in the near future.</p><p>In this preliminary work, we propose a new approach for unsupervised MT to further reduce the gap between supervised and unsupervised MT. Our approach exploits a new framework in which UNMT is bootstrapped by USMT and uses only synthetic parallel data as supervision for training. The main outcomes of our work are as follows:</p><p>• We propose a simplified USMT framework.</p><p>It is easier to set up and train. We also show that using back-translation to train USMT is not suitable and underperform.</p><p>• We propose to use supervised NMT framework for the unsupervised NMT scenarios by simply replacing true parallel data with synthetic parallel data generated by USMT. This strategy enables the use of well-established NMT architectures with all their features, without assuming any relatedness between source and target languages in contrast to previous work.</p><p>• We empirically show that our framework leads to significantly better UNMT than USMT on the WMT16 German-English news translation task, for both translation directions.</p><p>2 What is truly unsupervised in this paper?</p><p>Since the term "unsupervised" may be misleading, we present in this section what aspects of this work are truly unsupervised.</p><p>As previous work, we define "unsupervised MT" as MT that does not use human-made translation pairs as bilingual data for training. Nonetheless, MT still needs some supervision for training. Our approach uses as supervision synthetic bilingual data generated from monolingual data.</p><p>"Unsupervised" qualifies only the training of MT systems on bilingual parallel data of which at least one side is synthetic. For tuning, it is arguably unsupervised in some of our experiments or supervised using a small set of human-made bilingual sentence pairs. We discuss "unsupervised tuning" in Section 3.2. For evaluation, it is fully supervised, as in previous work, since we use a human-made test set to evaluate the translation quality.</p><p>Even if our systems are trained without humanmade bilingual data, we can still argue that the monolingual corpora used to generate synthetic parallel data have been produced by humans. Source and target monolingual corpora in our experiments (see Section 5.1) could include some comparable parts. Moreover, we cannot ensure that they do not contain any human-made translations from which our systems can take advantage during training. Finally, we use SMT and NMT architectures, set and use their hyper-parameters (for instance, the default parameters of the Transformer model) in our framework that have already shown to give good results in supervised MT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simplified USMT</head><p>Our USMT framework is based on the same architecture proposed by previous work <ref type="bibr" target="#b1">(Artetxe et al., 2018b;</ref><ref type="bibr" target="#b17">Lample et al., 2018b)</ref>: a phrase table is induced from monolingual data and used to compose the initial USMT system that is then refined iteratively using synthetic parallel data. We propose the following improvements and discussions to simplify the framework and make it faster with lighter models (see also <ref type="figure" target="#fig_0">Figure 1</ref>):</p><p>• Section 3.1: we propose several modifications to rely more on compositional phrases and to simplify the phrase table induction compared to the method proposed by <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> • Section 3.2: we discuss the feasibility of unsupervised tuning.</p><p>• Section 3.3: we propose to replace the backtranslation in the refinement steps with for-  ward translation to improve translation quality and to remove the need of simultaneously training models for both translation directions.</p><p>• Section 3.4: we propose to prune the phrase table to speed up the generation of synthetic parallel data during the refinement steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase table induction</head><p>As proposed by <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> and <ref type="bibr" target="#b17">Lample et al. (2018b)</ref>, the first step of our approach for USMT is an unsupervised phrase table induction that only takes as inputs a set of source phrases, a set of target phrases, and their respective embeddings, as illustrated by <ref type="figure">Figure 2</ref>. <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> regarded the most frequent unigrams, bigrams, and trigrams in the monolingual data as phrases. The embedding of each n-gram is computed with a generalization of the skipgram algorithm <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref>. Then, source and target n-gram embedding spaces are aligned in the same bilingual embedding space without supervision <ref type="bibr" target="#b0">(Artetxe et al., 2018a)</ref>. Lample et al. (2018b)'s method also works at n-gram level, but computes phrase embeddings as proposed by <ref type="bibr" target="#b32">Zhao et al. (2015)</ref>: performing the element-wise addition of the embeddings of the We choose to build USMT with an alternative method for phrase table induction. We adopt the method proposed by <ref type="bibr" target="#b18">Marie and Fujita (2018)</ref>, except that we remove the supervision using a bilingual word lexicon. First, phrases are collected using the following equation <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref>:</p><formula xml:id="formula_0">score(w i w j ) = freq(w i w j ) − δ freq(w i ) × freq(w j ) ,<label>(1)</label></formula><p>where w i and w j are two consecutive tokens or phrases in the monolingual data, freq(·) the frequency of the given token or phrase, and δ a discounting coefficient for preventing the retrieval of phrases composed of very infrequent tokens. Consecutive tokens/phrases having a higher score than a pre-defined threshold are regarded as new phrases, 4 and a new pass is performed to obtain longer phrases. The iteration results in the collection of much longer and meaningful phrases, i.e., not only very frequent sequences of grammatical words, rather than only short n-grams. In our experiments, we perform 6 iterations to collect phrases of up to 6 tokens. 5 Equation <ref type="formula" target="#formula_0">(1)</ref> was originally proposed to identify non-compositional phrases. However, we choose to enforce the collection of more compositional phrases with a low δ 6 for the following reasons:</p><p>• very few phrases are actually noncompositional in standard SMT systems <ref type="bibr" target="#b30">(Zens et al., 2012)</ref>,</p><p>• most of them are not very frequent, and</p><p>• useful representation of compositional phrases can easily be obtained compositionally <ref type="bibr" target="#b32">(Zhao et al., 2015)</ref>.</p><p>To obtain the pairs of source and target phrases that populate the induced phrase </p><formula xml:id="formula_1">p(t j |s i ) = exp (β cos(emb(t j ), emb(s i ))) k exp (β cos(emb(t k ), emb(s i ))) ,</formula><p>(2) where t j is the j-th phrase in the target phrase list and s i the i-th phrase in the source phrase list, β a parameter to tune the peakiness of the distribution 8 <ref type="bibr" target="#b24">(Smith et al., 2017)</ref>, and emb(·) a function returning the bilingual embedding of a given phrase.</p><p>In this work, for a reasonably fast computation, we retained only the 300k most frequent phrases in each language and retained for each of them the 300-best target phrases according to Equation <ref type="formula">(2)</ref>.</p><p>Standard phrase-based SMT uses the following four translation probabilities for each phrase pair. is usually used as the maximum length in most state-of-theart SMT frameworks. <ref type="bibr">6</ref> We set δ = 10 in all our experiments. <ref type="bibr">7</ref> We could not obtain results similar to the results reported in <ref type="bibr" target="#b17">Lample et al. (2018b)</ref> (the second version of their arXiv paper) by using their Equation (3) with β = 30 as they proposed. We have confirmed through personal communications with the authors that Equation (2), as we wrote, with β = 30, generates the expected results. We did not use the Equation computing φ in <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref>, since it produces negative value as a probability when cosine similarity is negative. <ref type="bibr">8</ref> We set β = 30 since it is the default value proposed in the code released by Smith et al. <ref type="formula" target="#formula_0">(2017)</ref>: https://github.com/Babylonpartners/ fastText_multilingual These probabilities, except (a), need to be computed only for the 300-best target phrases for each source phrase that are already determined using (a). (b) is given by switching s i and t j in Equation (2). To compute lexical translation probabilities, (c) and (d), given the significant filtering of candidate target phrases, we can adopt a more costly but better similarity score. In this work, we compute them using word embeddings as proposed by <ref type="bibr" target="#b26">Song and Roth (2015)</ref>:</p><formula xml:id="formula_2">lex(t j |s i ) = 1 L L l=1 K max k=1 p(t k j |s l i )<label>(3)</label></formula><p>where K and L are the number of words in t j and s i , respectively, and p(t k j |s l i ) the translation probability of the k-th target word t k j of t j given the l-th source word s l i of s i given by Equation <ref type="formula">(2)</ref>. This phrase-level lexical translation probability is computed for both translation directions. Note that, unlike <ref type="bibr" target="#b26">Song and Roth (2015)</ref> and Kajiwara and Komachi (2016), we do not use a threshold value under which p(t k j |s l i ) is ignored, since it would require some supervised fine-tuning to be set according to the translation task. In practice, even without this threshold value, our preliminary experiments showed significant improvements of translation quality by incorporating lex(t j |s i ) and lex(s i |t j ) into the induced phrase table.</p><p>After the computation of the above four scores for each phrase pair in the induced phrase table, the phrase table is plugged in an SMT system to perform what we denote in the remainder of this paper as iteration 0 of USMT.</p><p>Computing lexicalized reordering models for the phrase pairs in the induced phrase table from monolingual data is feasible and helpful as shown by <ref type="bibr" target="#b14">Klementiev et al. (2012)</ref>. However, for the sake of simplicity, we do not compute these lexical reordering models for iteration 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion about unsupervised tuning</head><p>State-of-the-art supervised SMT performs the weighted log-linear combination of different models <ref type="bibr" target="#b20">(Och and Ney, 2002)</ref>. The model weights are tuned given a small development set of bilingual sentence pairs. For completely unsupervised SMT, we cannot assume the availability of this development set. In other words, model weights must be tuned without the supervision of manually produced bilingual data. <ref type="bibr" target="#b17">Lample et al. (2018b)</ref> used some pre-existing default weights that work reasonably well. On the other hand, <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> obtained better results by using 10k monolingual sentences paired with their back-translations as a development set. Nonetheless, to create this development set, they also relied on the same pre-exisintg default weights used by <ref type="bibr" target="#b17">Lample et al. (2018b)</ref>. To be precise, both used the default weights of the Moses framework <ref type="bibr" target="#b15">(Koehn et al., 2007)</ref>. In this preliminary work, we present results with supervised tuning and with the Moses's default weights.</p><p>However, regarding the use of default weights as "unsupervised tuning" is arguable, since these default weights have been determined manually to work well for European languages. For translation between much more distant languages, 9 these default weights would likely result in a very poor translation quality. We argue that unsupervised tuning remains one of the main issues in current approaches for USMT.</p><p>Note that while creating large training bilingual data manually for a particular language pairs is very costly, which is one of the fundamental motivations of unsupervised MT, we can assume that a small set of sentence pairs required for tuning can be created at a reasonable cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Refinement without back-translation</head><p>Artetxe et al. (2018b) and <ref type="bibr" target="#b17">Lample et al. (2018b)</ref> presented the same idea of performing so-called refinement steps. Those steps use USMT to generate synthetic parallel data to train a new phrase table, with refined translation probabilities. This can be repeated for several iterations to improve USMT. The initial system at iteration 0 uses the induced phrase table (see Section 3.1), while the following iterations use only a phrase table and a lexicalized reordering model trained on the synthetic parallel data generated by USMT. They both fixed the number of iterations. <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> and <ref type="bibr" target="#b17">Lample et al. (2018b)</ref> generated the synthetic parallel data through backtranslation: a target-to-source USMT system was used to back-translate sentences in the target language, then the pairs of each sentence in the target language and its USMT output in the source language were used as synthetic parallel data to train a new source-to-target USMT system. This way of using back-translation has originally been proposed to improve NMT systems <ref type="bibr" target="#b22">(Sennrich et al., 2016a)</ref> with a specific motivation to enhance the decoder by exploiting fluent sentences in the target language. In contrast, however, using backtranslation for USMT lacks motivation. Since the source side of the synthetic parallel data, i.e., decoded results of USMT, is not fluent, USMT will learn a phrase table with many ungrammatical source phrases, or foreign words, that will never be seen in the source language, meaning that many phrase pairs in the phrase table will never be used. Moreover, possible and frequent source phrases, or even source words, may not be generated via back-translation and will be consequently absent from the trained phrase table.</p><p>We rather consider that the language model already trained on a large monolingual corpus in the target language can play a much more important role in generating more fluent translations. This motivates us to perform the refinement steps on synthetic parallel data made of source sentences translated into the target language by the sourceto-target system, i.e., "forward translation," as opposed to back-translation. In fact, the idea of retraining an SMT system on synthetic parallel data generated by a source-to-target system has already been proven beneficial <ref type="bibr" target="#b27">(Ueffing et al., 2007)</ref>.</p><p>At each iteration, we randomly sample new N source sentences from the monolingual corpus and translate them with the latest USMT system to generate synthetic parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Phrase table pruning</head><p>Generating synthetic parallel data through decoding millions of sentences is one of the most computationally expensive parts of the refinement steps, requiring also a large memory to store the whole phrase table. 10 In SMT, decoding speed can be improved by reducing the size of the phrase table. The phrase tables trained during the re-10 To decode a particular test set, usually consisting of thousands of sentences, the phrase table can be drastically filtered by keeping only the phrase pairs applicable to the source sentences to translate. For the refinement steps of USMT, this filtering is impractical since we need to translate a very large number of sentences. In other words, it would still remain a large number of phrase pairs. Another alternative is to binarize the phrase table so that the system can load only applicable phrase pairs on-demand at decoding time. However, we did not consider it in our framework since the binarization is itself very costly to perform, and more importantly, the phrase table of each refinement step is used only once. finement steps are expected to be very noisy and very large since they are trained on noisy parallel data. Therefore, we assume that a large number of phrase pairs can be removed without sacrificing translation quality. On this assumption, we use the well-known algorithm for pruning phrase table <ref type="bibr" target="#b11">(Johnson et al., 2007)</ref>, which has shown good performance in removing less reliable phrase pairs without any significant drop of the translation quality. This pruning can be done for each refinement step to reduce the phrase table size, and consequently to speed up the decoding. Note that we cannot prune the induced phrase table used at iteration 0, since it was not learned from parallel data: we do not have co-occurrence statistics for the phrase pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNMT as NMT trained exclusively on synthetic parallel data</head><p>To make NMT able to learn how to translate from monolingual data only, previous work on UNMT <ref type="bibr" target="#b2">(Artetxe et al., 2018c;</ref><ref type="bibr">Lample et al., 2018a,b;</ref><ref type="bibr" target="#b29">Yang et al., 2018)</ref> proposed dedicated architectures, such as denoising autoencoders, shared latent representations, weight sharing, pre-trained sub-word embeddings, and adversarial training.</p><p>In this paper, we propose to train UNMT systems exclusively on synthetic parallel data, using existing frameworks for supervised NMT. Specifically, we train the first UNMT system on synthetic parallel data generated by USMT through back-translating monolingual sentences in the target language, expecting that they are of a better quality than those generated by existing UNMT frameworks.</p><p>Our approach is significantly different from Lample et al. (2018b)'s "PBSMT+NMT" configuration in the following two aspects. First, while it uses synthetic parallel data generated by USMT only to further tune their UNMT system, ours uses it for initialization. Second, they assumed certain level of relatedness between source and target languages, which is a prerequisite to jointly pre-train bilingual sub-word embeddings. Our approach does not make this assumption.</p><p>However, training an NMT system only on synthetic parallel data generated by USMT, as we proposed, will hardly make an UNMT system significantly better than USMT systems. To obtain better UNMT systems, we propose the following (see also <ref type="figure" target="#fig_2">Figure 3</ref>).  • Section 4.1: we propose an incremental training strategy for UNMT that gradually increases the quality and the quantity of synthetic parallel data.</p><p>• Section 4.2: we propose to filter the synthetic parallel data to remove before training the sentence pairs with the noisiest synthetic sentences, aiming at speeding up training and improving translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Incremental training</head><p>To train UNMT, we first use the synthetic parallel data generated by the last refinement step of our USMT system. Since it has been shown that backtranslated monolingual data significantly improves translation quality in NMT, as opposed to the refinement of our USMT (see Section 3.3), we train source-to-target and target-to-source UNMT systems on synthetic parallel data respectively generated by a target-to-source and source-to-target USMT systems.</p><p>In contrast to supervised NMT where synthetic parallel data are used in combination with humanmade parallel data, we can presumably use as much synthetic parallel data as possible, since seeing more and more fluent target sentences will be helpful to train a better decoder while we can assume that the quality of synthetic source side remains constant. In practice, generating a large quantity of synthetic parallel data is costly. Therefore, to train the first UNMT system, we use the same number, N , of synthetic sentence pairs generated by the final USMT system.</p><p>Since the source side of the synthetic parallel data is generated by USMT, it is expected to be of worse quality than those that state-of-the-art supervised NMT can generate. Therefore, we propose to refine UNMT through gradually increasing the quality and quantity of synthetic parallel data. First, we back-translate a new set of N monolingual sentences using our UNMT systems at iteration 1 in order to generate new synthetic parallel data. Then, new UNMT systems at iteration 2 are trained from scratch on the 2N synthetic sentence pairs consisting of the new N synthetic data and N synthetic data generated by USMT. Note that we do not re-back-translate the monolingual data used at iteration 1 but keep them as they are for iteration 2 to reduce the computational cost. Similarly to the refinement steps of USMT, we can again perform this back-translation/re-training step for a pre-defined number of iterations to keep improving the quality of the source side of the synthetic data while increasing the number of new target sentences. At each iteration i, (N ×i) synthetic sentence pairs are used for training.</p><p>This can be seen as an extension of Hoang et al. (2018)'s work, which performs a so-called iterative back-translation to improve NMT. The difference is that we introduce better synthetic parallel data, with new target sentences, at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Filtering of synthetic parallel data</head><p>Our UNMT system is trained on purely synthetic parallel data in which a large proportion of source sentences may be very noisy. We assume that removing the sentence pairs with the noisiest source sentences will improve translation quality. Inevitably it also reduces the training time.</p><p>Each sentence pair in the synthetic parallel data is evaluated by the following normalized source language model score:</p><formula xml:id="formula_3">ppl(S) = lm(S) len(S) + 1 (4)</formula><p>where S is a (synthetic) source sentence, lm(·) the language model score, and len(·) a function returning the number of tokens in the sentence. We add 1 to the number of tokens to account for the special token used by NMT that marks the end of a sentence. This scoring function has a negligible computational cost, but has shown satisfying performances in our preliminary experiments. While we do not limit the language model to be specific type, in our experiment, we use a recurrent neural network (RNN) language model trained on the entire source monolingual data. There are many ways to make use of the above score during NMT training. For instance, weighting the sentence pairs with this score during training is a possible alternative, and this idea is close to one used by <ref type="bibr" target="#b5">Cheng et al. (2017)</ref> in their joint training framework for NMT. However, given that many of the source sentences would be noisy, we rather choose to discard potentially noisy pairs for training. It would also remove potentially useful target sentences, but we assume that the impact of this removal could be compensated at the succeeding iterations of UNMT, where we incrementally introduce new target sentences.</p><p>At each iteration i of incremental training, we keep only the cleanest (α × N × i) synthetic sentence pairs 11 selected according to the score computed by Equation <ref type="formula">(4)</ref>, where α (0 &lt; α ≤ 1) is the filtering ratio. 12 This aggressive filtering will speed up training while relying only on the most fluent sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present experiments for evaluating our USMT and UNMT systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>For these preliminary experiments, we chose the language pair English-German (en-de) and the evaluation task WMT16 (newstest2016) for both translation directions, following previous work <ref type="bibr" target="#b1">(Artetxe et al., 2018b;</ref><ref type="bibr" target="#b17">Lample et al., 2018b)</ref>. To train our USMT and UNMT, we used only monolingual data: English and German News Crawl corpora respectively containing around 238M and 237M sentences. 13 All our data were tokenized and truecased with Moses's tokenizer 14 and truecaser, respectively. The statistics for truecasing were learned from 10M sentences randomly sampled from the monolingual data.</p><p>For the phrase table induction, the source and target word embeddings were learned from the entire monolingual data with the default parameters of fasttext <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref>, 15 except that we set to 200 the number of dimensions. <ref type="bibr">16</ref> For a reasonably fast computation, we retained only the embeddings for the 300k most frequent words. Word embeddings for two languages were then aligned in the same space using the -unsupervised option of vecmap. 17 From the entire monolingual data, we also collected phrases of up to 6 tokens in each language using word2phrase. 18 To maintain the experiments feasible and to make sure that we have a word embedding for all of the constituent words, we retained only 300k most frequent phrases made of words among the 300k most frequent words. We conserved the 300-best target phrases for each source phrase, according to Equation <ref type="formula">(2)</ref>, consequently resulting in the initial phrase table for USMT containing 90M (300k×300) phrase pairs.</p><p>We used Moses and its default parameters to conduct experiments for USMT. The language models used by our USMT systems were 4-gram models trained with LMPLZ <ref type="bibr" target="#b9">(Heafield et al., 2013)</ref> on the entire monolingual data. In each refinement step, we trained a phrase table and a lexicalized reordering model on synthetic parallel data using mgiza. <ref type="bibr">19</ref> We compared USMT systems with and without supervised tuning. For supervised tuning, we used kb-mira <ref type="bibr" target="#b6">(Cherry and Foster, 2012)</ref> and the WMT15 newstest (newstest2015). For the configurations without tuning, we used Moses's default weights as in previous work.</p><p>For UNMT, we used the Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017</ref><ref type="bibr">) model implemented in Marian (Junczys-Dowmunt et al., 2018</ref> with the hyperparameters proposed by <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref>. 21 word2vec/ 19 fast_align <ref type="bibr" target="#b8">(Dyer et al., 2013)</ref> is a significantly faster alternative for a similar performance on en-de <ref type="bibr" target="#b7">(Durrani et al., 2014)</ref>. We used mgiza since it is integrated in Moses. 20 https://marian-nmt.github.io/, version 1.6. <ref type="bibr">21</ref> Considering the computational cost of our approach for UNMT, we did not experiment with the "big" version of the Transformer model while it would probably have resulted in a better translation quality.</p><p>We reduced the vocabulary size by using bytepair-encoding (BPE) with 8k symbols jointly learned for English and German from 10M sentences sampled from the monolingual data. BPE was then applied to the entire source and target monolingual data. <ref type="bibr">22</ref> We used the same BPE vocabulary throughout our UNMT experiments. <ref type="bibr">23</ref> We validated our model during UNMT training as proposed by <ref type="bibr" target="#b17">Lample et al. (2018b)</ref>: we did a supervised validation using 100 human-made sentence pairs randomly extracted from new-stest2015. We consistently used the same validation set throughout our UNMT experiments. To filter the synthetic parallel sentences (see Section 4.2), we used an RNN language model trained on the entire monolingual data, without BPE, with a vocabulary size of 100k. <ref type="bibr">24</ref> For each of USMT and UNMT, we performed 4 refinement iterations. USMT has one more system in the beginning, which exploits an induced phrase table. At each iteration, we sampled new 3M monolingual sentences: i.e., N = 3000000. <ref type="bibr">25</ref> For reference, we also trained supervised NMT with Marian on 5.6M, 2.8M, and 1.4M humanmade parallel sentences provided by the WMT18 conference for the German-English news translation task. <ref type="bibr">26</ref> We evaluated our systems with detokenized and detruecased BLEU-cased <ref type="bibr" target="#b21">(Papineni et al., 2002)</ref>. Note that our results should not be directly compared with the tokenized BLEU scores reported in <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> and <ref type="bibr" target="#b17">Lample et al. (2018b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Our results for USMT and UNMT are presented in <ref type="table">Table 1</ref>.</p><p>We can first observe that supervised tuning for USMT improves translation quality, with 2.0 BLEU points of improvements, for instance between systems #5 and #6. Another interesting observation is that this improvement is carried on until the final iteration of UNMT (#11 and #12). These results show the importance of development data for tuning that could be created at a reason-  <ref type="table">Table 1</ref>: Results of our USMT and UNMT systems (denoted "this work") evaluated with BLEU for the WMT16 German-English news translation task. We present results for USMT with back-translation (#3 and #4) and forward translation (#5 and #6) during the refinement steps. Results for UNMT are presented without (#9 and #10) and with (#11 and #12) filtering of synthetic parallel data. "*" indicates the scores shown in the original paper for indicative purpose only, since they are tokenized BLEU scores and thus not directly comparable with our results.</p><p>able cost (see Section 3.2).</p><p>Our USMT systems benefited more from forward translation (#5 and #6) than back-translation (#3 and #4) during the refinement steps, with an improvement of 1.6 and 0.4 BLEU points for de→en and en→de (with supervised tuning), respectively. Pruning the phrase table (see Section 3.4) did not hurt translation quality but removed around 93% of the phrase pairs in the phrase tables for each refinement step. Nonetheless, our USMT systems seem to significantly underperform the state-of-the-art USMT proposed by <ref type="bibr" target="#b17">Lample et al. (2018b)</ref>  <ref type="figure" target="#fig_0">(#1)</ref> and <ref type="bibr">Artetxe et al. (2018b) (#2)</ref>. This is potentially the consequence of the following: we used much lower dimensions for our word embeddings and much less phrases (300k source and target phrases), than in <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> (1M source and target phrases). In our future work, we will investigate whether their parameters improve the performance of our USMT systems.</p><p>While our USMT systems do not seem to outperform previous work, we can observe that the synthetic parallel data that they generated are of sufficient quality to initialize our UNMT. Incremental training improved significantly translation quality. To the best of our knowledge, we report the best results of unsupervised MT for this task which is, for de→en, only 3.7 BLEU points lower (#11) than a supervised NMT system trained on 1.4M parallel sentences (#13). <ref type="bibr">27</ref> Our best UNMT systems (#11 and #12) significantly outperformed our USMT systems (#5 and #6) by more than 6.0 BLEU points, for de→en. Filtering the synthetic parallel sentences at each iteration significantly improved the training speed 28 for a comparable or better translation quality for both translation directions. The results confirm the importance of filtering the very noisy synthetic source sentences generated by back-translation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Learning curves</head><p>In this section, we present the evolution of the translation quality during training of USMT and UNMT.</p><p>The learning curves of our systems, for the same experiments presented in Section 5.1, are given in <ref type="figure" target="#fig_3">Figures 4a and 4b</ref> for de→en and en→de, respectively. Iteration 0 of our USMT, using an induced phrase table, performed very poorly; for instance systems without supervised tuning (leftmost points of blue lines) achieved only 11.2 and 7.3 absolute BLEU points for de→en and en→de, respectively. Iterations 1 and 2 of USMT were very effective and covered most of the improvements between iteration 0 and iteration 4. After 4 iterations, we observed improvements of 9.0 and 8.1 BLEU points for de→en and en→de, respectively.</p><p>The learning curves of UNMT were very different for the two translation directions. The first iteration of UNMT, trained on the synthetic parallel data generated by USMT, performed slightly lower than USMT for de→en while for en→de we observed around 2.0 BLEU points of improvements. This confirms the ability of NMT in generating significantly better sentences than SMT for morphologically-rich target languages <ref type="bibr" target="#b3">(Bentivogli et al., 2016)</ref>. Then, the second iteration of UNMT improved the translation quality significantly for de→en, but much more moderately for en→de. For instance, in the configuration without supervised tuning and with language model filtering (blue solid lines), we observed 5.4 and 0.9 BLEU points of improvements for de→en and en→de, respectively. Succeeding iterations continued to improve translation quality but more moderately.</p><p>For both translation directions, the learning curves highlighted that improving the synthetic parallel data generated by USMT, and used to initialize UNMT, is critical to improve UNMT: synthetic parallel data generated with tuned USMT were consistently more useful for UNMT than the synthetic parallel data of lower quality generated by USMT without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion an future work</head><p>We proposed a new approach for UNMT that can be straightforwardly exploited with wellestablished architectures and frameworks used for supervised NMT without any modifications. It only assumes for initialization the availability of synthetic parallel data that can be, for instance, easily generated by USMT. We showed that improving the quality of the synthetic parallel data used for initialization is crucial to improve UNMT. We obtained with our approach a new state-ofthe-art performance for unsupervised MT on the WMT16 German-English news translation task.</p><p>For future work, we will extend our experiments to cover many more language pairs, including distant language pairs for which we expect that our approach will perform better than previous work that assumes the relatedness between source and target languages. We will also analyze the impact of using synthetic parallel data of a much better quality to initialize UNMT. Moreover, we would like to investigate the use of much noisier and not comparable source and target monolingual corpora to train USMT and UNMT, since we consider it as a more realistic scenario when dealing with truly low-resource languages. We will also study our approach in the semi-supervised scenario where we assume the availability of some human-made bilingual sentence pairs for training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our USMT framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) p(t j |s i ): forward phrase translation probability (b) p(s i |t j ): backward phrase translation probability (c) lex(t j |s i ): forward lexical translation probability (d) lex(s i |t j ): backward lexical translation probability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Our UNMT framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves of our USMT (#5 and #6) and UNMT (#9, #10, #11, and #12) systems presented in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>table, we used the Equation proposed by Lample et al. (2018b): 7</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Mainly due to the difficulty of training accurate unsupervised bilingual word/sub-word embeddings for distant languages<ref type="bibr" target="#b25">(Søgaard et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This transformation is performed by simply replacing the space between the two tokens/phrases with an underscore.5  We chose a maximum phrase length of 6, since this value</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For instance, Lample et al. (2018b) presented for Urdu-English only the results with supervised tuning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://fasttext.cc/ 16 While Artetxe et al. (2018b) and Lample et al. (2018b) used 300 and 512 dimensions, respectively, we chose a smaller number of dimensions for faster computation, even though this might lead to lower quality. 17 https://github.com/artetxem/vecmap 18 https://code.google.com/archive/p/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">We did not use BPE for USMT.23  Re-training BPE at each iteration of UNMT on synthetic data did not improve the translation quality in our preliminary experiments.24  We used also Marian to train the RNN language models.25  <ref type="bibr" target="#b1">Artetxe et al. (2018b)</ref> and<ref type="bibr" target="#b17">Lample et al. (2018b)</ref> respectively sampled 2M and 5M monolingual sentences.26  We did not use the ParaCrawl corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27">A fair supervised NMT baseline should also use, in addition to human-made parallel sentences, back-translated data for training.28  For instance, for the last iteration of UNMT for de→en, the training using 4 GPUs consumed 30 hours with filtering while it took 52 hours without filtering.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno>abs/1809.01272</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural versus phrase-based machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint training for pivotbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conferences on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3974" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s phrasebased machine translation systems for WMT-14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative back-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving translation quality by discarding most of the phrasetable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="967" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<editor>Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch</editor>
		<meeting>ACL 2018, System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a monolingual parallel corpus for text simplification using sentence similarity based on alignment between word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1147" to="1158" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1804.07755</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phrase table induction using monolingual data for low-resource statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
		<idno type="DOI">10.1145/3168054</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the limitations of unsupervised bilingual dictionary induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised sparse vector densification for short text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1275" to="1280" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transductive learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation with weight sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A systematic comparison of phrase table pruning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="972" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning translation models from monolingual continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
