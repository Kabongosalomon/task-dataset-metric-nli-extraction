<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Aggregation Networks for Efficient Interactive Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
							<email>jiaxu.miao@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Aggregation Networks for Efficient Interactive Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interactive video object segmentation (iVOS) aims at efficiently harvesting high-quality segmentation masks of the target object in a video with user interactions. Most previous state-of-the-arts tackle the iVOS with two independent networks for conducting user interaction and temporal propagation, respectively, leading to inefficiencies during the inference stage. In this work, we propose a unified framework, named Memory Aggregation Networks (MA-Net), to address the challenging iVOS in a more efficient way. Our MA-Net integrates the interaction and the propagation operations into a single network, which significantly promotes the efficiency of iVOS in the scheme of multi-round interactions. More importantly, we propose a simple yet effective memory aggregation mechanism to record the informative knowledge from the previous interaction rounds, improving the robustness in discovering challenging objects of interest greatly. We conduct extensive experiments on the validation set of DAVIS Challenge 2018 benchmark. In particular, our MA-Net achieves the J@60 score of 76.1% without any bells and whistles, outperforming the state-of-thearts with more than 2.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) aims at separating a foreground object from a video sequence and can benefit many important applications, including video editing, scene understanding, and self-driving cars. Most existing VOS approaches can be roughly divided into two settings: unsupervised (no manual annotation) and semi-supervised (give the annotation at the first frame). However, these two settings have their own limitations and are not realistic in practice: 1) unsupervised methods have no guiding signal for the user to select the object of interest, which † Corresponding author.</p><p>‡ Part of this work was done when Jiaxu Miao was an intern at Baidu Research.  <ref type="figure">Figure 1</ref>. Round-based iVOS. The mask of the target object is generated by user annotations at one frame (e.g. green scribbles at frame 58), and the computed mask is propagated to generate the masks for the entire video. The user can refine the segmentation masks by repeatedly providing annotations on the false negative and false positive areas (e.g. green and red scribbles at frame 28). is problematic especially for the multiple-object case; 2) semi-supervised methods need a fully annotated mask of the first frame, which is tedious to acquire (around 79 seconds per instance) <ref type="bibr" target="#b5">[6]</ref>. Furthermore, for both two schemes, users have no chance to correct those low-quality segments to meet their requirements.</p><p>Interactive video object segmentation (iVOS) overcomes the above-mentioned limitations by providing a userfriendly annotation form, e.g., scribbles. In this scheme, users can gradually refine the outputs by drawing scribbles on the falsely predicted regions. Previous iVOS methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1]</ref> utilize a rotoscoping procedure <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, where a user sequentially processes a video frame-byframe. These methods are inefficient due to requiring a lot of user interactions at each frame.</p><p>Recently, Caelles et al. <ref type="bibr" target="#b5">[6]</ref> propose a round-based interaction scheme, as shown in <ref type="figure">Fig. 1</ref>. In this setting, users firstly draw scribbles on the target objects at one selected frame, and an algorithm is then employed to compute the segmentation masks for all video frames with temporal propagation. The procedures of user annotation and mask segmentation are repeated until acceptable results are obtained. Such a round-based interaction scheme is more efficient since it requires fewer user annotations (only a few scribbles at one frame per round). Besides, it is flexible for users to control the quality of segmentation masks, since more rounds of user interactions will guarantee more accurate segmentation results.</p><p>In this paper, we explore how to build an efficient interactive system to tackle the iVOS problem under the roundbased interaction setting. While some recent deep learning based methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref> have been proposed to deal with the round-based iVOS, there are several limitations: 1) the user interaction and the temporal propagation are usually processed by two independent networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>; 2) the whole neural network has to start a new feed-forward computation in each interaction round <ref type="bibr" target="#b21">[21]</ref>, or needs postprocessing <ref type="bibr" target="#b20">[20]</ref> to make a further refinement, which is timeconsuming; 3) only the outputs of latest round are utilized to refine the segmentation results, while the informative multiround interactions are usually ignored <ref type="bibr" target="#b11">[12]</ref>.</p><p>Considering these limitations, we propose a unified, efficient, and accurate framework named Memory Aggregation Networks (MA-Net) to deal with the iVOS in a more elegant and effective manner. Concretely, our MA-Net integrates the interaction network and propagation network into a unified pixel embedding learning framework by sharing the same backbone. In this way, after extracting the pixel embedding with the shared backbone, the MA-Net adopts two "shallow" convolutional segmentation heads to predict the object segments of the scribble-labeled frame and all the other frames, respectively. Under the round-based iVOS scheme, we only need to extract the pixel embedding of all the frames in the first round. In all the following rounds, these extracted embedding can be simply applied to make a further refinement with two "shallow" segmentation heads, resulting in our MA-Net much faster than previous methods. More importantly, we propose a simple yet effective memory aggregation mechanism, which is used to record informative knowledge of the user's interactions and the predicted masks during the previous interaction rounds. Such aggregated information makes the MA-Net robust to the target instances with a wide variety of appearances, improving the accuracy of our model greatly.</p><p>Our MA-Net is quantitatively evaluated on the interactive benchmark at the DAVIS Challenge 2018 <ref type="bibr" target="#b5">[6]</ref>. On the DAVIS validation set, our MA-Net achieves the J@60 score of 76.1% without any bells and whistles, such as introducing additional optical flow information <ref type="bibr" target="#b11">[12]</ref> or applying time-consuming CRF for post-processing <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b13">14]</ref>. In addition, our MA-Net can accomplish 7-round interactions within 60 seconds, which is more efficient than the state of the art one <ref type="bibr" target="#b21">[21]</ref> of 5-round interactions within 60 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Video Object Segmentation. Unsupervised VOS does not need any user annotations. Most unsupervised segmentation models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30]</ref> learn to automatically segment visually salient objects based on the motion information or the appearance information. The limitation of unsupervised VOS is that users cannot select the object of interest. Semi-supervised Video Object Segmentation. Semisupervised VOS employs the full annotation of the first frame to select the objects of interest. Many semisupervised VOS methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref> have been proposed and achieve good performance.</p><p>Some semi-supervised VOS approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18</ref>] rely on fine-tuning using the first frame annotation at test time. For instance, OSVOS <ref type="bibr" target="#b4">[5]</ref> employs a convolutional neural network pre-trained for foreground-background segmentation and fine-tunes the model using first-frame ground truth when testing. OnAVOS <ref type="bibr" target="#b28">[28]</ref> and OSVOS-S <ref type="bibr" target="#b19">[19]</ref> further improve OSVOS by updating the network online using instance-level semantic information. PReMVOS <ref type="bibr" target="#b17">[18]</ref> integrates different networks with fine-tuning and merging, which achieves superior performance. Online fine-tuning methods achieve good performance, but poor efficiency due to the fine-tuning process at test time.</p><p>Recently, some VOS approaches without first-frame fine-tuning have been proposed and achieve very high speed and effectively. One type of these methods is propagationbased <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b1">2]</ref>, which usually takes as input the combination of the image and predicted segmentation mask of the previous frame. For instance, RGMP <ref type="bibr" target="#b32">[32]</ref> employs a siamese architecture network. One stream encodes the feature of the target frame and the mask of the previous frame while another stream encodes the first frame together with its given ground truth. Another type of fine-tuning free methods is matching-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31]</ref>, which utilizes the pixel embedding learning. For instance, PML <ref type="bibr" target="#b7">[8]</ref> learns a pixel embedding space by a triplet loss together with a nearest neighbor classifier. VideoMatch <ref type="bibr" target="#b12">[13]</ref> proposes a soft matching mechanism by calculating similarity score maps of matching features to generate smooth predictions. FEELVOS <ref type="bibr" target="#b27">[27]</ref> employs pixel-wise embedding together with a global and a local matching mechanism. By considering foreground-background integration, CFBI <ref type="bibr" target="#b36">[36]</ref> achieves the new state of the art. Our method is inspired by FEELVOS <ref type="bibr" target="#b27">[27]</ref>, and utilizes the global and local matching maps to transfer information of the scribble-annotated and previous frame to the target frame. Interactive Video Object Segmentation. In the interactive VOS setting, users can provide various types of inputs (e.g. points, scribbles) to select the objects of interest and refine the segmentation results by providing more interactions. Previous interactive methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1]</ref>, either use  <ref type="figure">Figure 2</ref>. The pipeline of our MA-Net, including the pixel embedding backbone, the interaction branch, and the propagation branch. During inference, the pixel embedding of all frames is extracted only once in the first round. The interaction branch employs "shallow" convolutional layers to predict the mask of the interactive frame. The propagation branch uses a memory aggregation mechanism to record informative knowledge and "shallow" convolutional layers to generate masks of other frames. In the matching processes shown in the figure, the deeper the green, the higher the probability of being predicted as the target object. Best viewed in color. hand-crafted features or need a lot of interactions, can not achieve a good performance or efficiency.</p><p>Recently, some round-based deep learning methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b2">3]</ref> for iVOS have been proposed. Benard et al. <ref type="bibr" target="#b2">[3]</ref> and Heo <ref type="bibr" target="#b11">[12]</ref> treat the interactive VOS as two sub-tasks: using the scribbles to generate segmentation masks, and using the generated mask to infer masks of other frames as semi-supervised VOS. Oh <ref type="bibr" target="#b21">[21]</ref> uses two networks, interaction and propagation, to tackle these two sub-tasks. These two networks are connected both internally and externally. These methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">21]</ref> have several limitations: (1) they use two independent networks without shared weights, and need new feed-forward computation in each interaction round <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">12]</ref>, making it inefficient when rounds grow up; (2) they do not utilize the multi-round information adequately. Recently, Oh <ref type="bibr" target="#b22">[22]</ref> proposes a space-time memory mechanism to store informative knowledge and achieves state-of-the-art performance. Different from our memory mechanism, they need complicated key-value computation. Besides, they also need new feed-forward computation in each interaction round, which is time-consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Round-based iVOS aims at cutting out the target objects in all frames of a video given user annotations (e.g. scribbles) on one frame. Users can provide additional feedback annotations on a frame after reviewing the segmentation results to refine the segmentation mask of the next round.</p><p>Previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b2">3]</ref> chose to adopt two independent neural networks (interaction and propagation) without shared weights or connect two networks by medial layers, which usually affects the inference efficiency. In this paper, we deal with the two sub-tasks (interaction and propagation) under a unified pixel embedding learning framework.</p><p>To this end, we propose MA-Net, which contains three modules: a pixel embedding encoder, an interaction branch, and a propagation branch, as shown in <ref type="figure">Fig. 2</ref>. The pixel embedding encoder takes the RGB frames of the given video as inputs and encodes each pixel into an embedding vector. The interaction branch leverages the user's annotations (scribbles) and the pixel embedding of the user-annotated frame to generate the instance segmentation mask. The propagation branch propagates the informative knowledge of the user-annotated frame and the previous frame to the current frame using the pixel embedding. Both the two branches share weights of the pixel embedding encoder, and then employ two "shallow" networks with several convolutional layers as the segmentation heads, respectively. The pixel embeddings of all frames are extracted only in the first interaction round. During the refinement process in the following rounds, only the two "shallow" segmentation heads are used, making our MA-Net more efficient than previous methods. In this paper, we denote the current processing frame as the t th frame, the previous frame as the (t − 1) th frame, and the user-annotated frame as thet th frame. Pixels of the current processing frame are denoted as p, and pixels   annotated or predicted to belong to the target object o as q.</p><p>In the following, we will describe each of the modules in more detail.</p><p>Pixel Embedding Encoder. The purpose of pixel embedding learning is to learn an embedding space where pixels belonging to the same object are close while pixels belonging to different objects are far away. We employ the DeepLabv3+ architecture <ref type="bibr" target="#b6">[7]</ref> based on ResNet101 <ref type="bibr" target="#b10">[11]</ref> as our backbone, and add an embedding layer consisting of one depth-wise separable convolution with a kernel size of 3×3. The stride of the pixel embedding feature is 4, and the dimension is 100. For each pixel p in the input RGB frame, we learn a semantic embedding vector e p in the learned embedding space. In this paper, we encode the pixel embedding into a Euclidean space, where the Euclidean norm between two pixels in the same object is expected to be small. Similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">27]</ref>, we define the distance between pixels p and q in terms of their corresponding embedding vectors e p and e q as  .</p><formula xml:id="formula_0">d(p, q) = 1 − 2 1 + exp( e p − e q</formula><p>This operation aims at normalizing the pixel distance between 0 and 1. We follow the strategy of FEELVOS <ref type="bibr" target="#b27">[27]</ref> to employ the pixel distances as a soft cue, which is further refined by two "shallow" segmentation heads. Propagation Branch. The propagation branch aims at propagating information from the user-annotated frame and the previous frame to predict the segmentation mask of the target object at the current frame. Following FEELVOS <ref type="bibr" target="#b27">[27]</ref>, we employ the global and local matching map as the soft cues of the user-annotated frame and the previous frame, respectively. The matching processes of the global and local maps are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Different from FEELVOS <ref type="bibr" target="#b27">[27]</ref>, our MA-Net proposes to employ a memory aggregation mechanism to record and aggregate the informative knowledge during the previous multiple interaction rounds, which is specially designed for iVOS.</p><p>Global Map Memory. Let P t denotes the set of all pixels of the current t th frame and Pt ,o,r denotes the set of userannotated pixels of the interactivet th frame in the r th round of interaction. As show in the left of <ref type="figure" target="#fig_1">Fig. 3</ref>, for each pixel p ∈ P t , we can calculate the distance of its nearest neighbour in Pt ,o,r to construct a global matching distance map, which is defined by</p><formula xml:id="formula_2">G t,r (p) = min q∈Pt ,o,r d(p, q).<label>(2)</label></formula><p>Different from the semi-supervised VOS who obtains a fully annotated frame, the interactive setting only provides a small number of scribble annotations to the objects of interest in each round. Therefore, the produced global matching map in one round is usually insufficient to discover the entire target object. To tackle this problem, we build a global memory unit to record and aggregate the historical global matching maps to enrich the information of the target object. Let M g ∈ R n,o,h,w denotes the global map memory, where n, o, h, w denotes the total number of video frames, the target object, the height and width of the embedding feature maps, respectively. Consider that the range of the values in the matching map is from 0 to 1, where the value of pixels closer to 0 is more likely to belong to the selected object and vice versa. We initialize M g with 1 and update M g by preserving the minimum value of each pixel in different interaction rounds. We demonstrate the updating process of the global map memory in <ref type="figure">Figure. 5 (a)</ref>. Formally, for the round of r and the frame at time t, M g is written by</p><formula xml:id="formula_3">M g t,r = min(M g t,r−1 , G t,r ).<label>(3)</label></formula><p>When we read the accumulated global map of round r, we directly use the updated global map memory M g t,r . Local Map Memory and Forgetting. Since the motion between two adjacent frames is usually small, to take advantage of the information of predicted mask from the previous frame, we further introduce the local matching map <ref type="bibr" target="#b27">[27]</ref>. To avoid false-positive matches as well as save computation time, we only calculate the matching distance map with a small local region. Let P t−1,o denote the pixels of frame at time t−1 which are predicted to be the object o. N(p) denotes the neighborhood set of pixel p, which contains pixels at most k pixels far away from p. As shown in the right of <ref type="figure" target="#fig_1">Fig. 3</ref>  <ref type="figure">Figure 5</ref>. (a) The global map memory mechanism. The global map in the propagation branch and the augmented map in the interaction branch are recorded and aggregated in the memory. (b) The local map memory and forgetting mechanism. The local map in each interaction round is recorded in the memory, and the nearest map of time in past R rounds is read to compute the masks. Local maps of early interaction rounds are forgotten with rounds growing up. The blue arrows denote the temporal propagation.</p><p>can then compute the local matching distance map L t,r by</p><formula xml:id="formula_4">L t,r (p) = min q∈P N t−1,o d(p, q) if P N t−1,o = ∅ 1 otherwise,<label>(4)</label></formula><p>where P N t−1,o := P t−1,o ∩ N(p) is the intersection set of the previous frame pixel set P t−1,o and the neighbour set N(p).</p><p>Different from the scribble annotations provided by users, the mask information of the previous frame is unreliable since the segmentation mask of the previous frame is predicted by the algorithm. In practice, we found that the error will accumulate due to drifts and occlusions during the propagation. The segmentation result will get worse if the current frame far away from the user-annotated frame. Therefore, to prevent the error accumulation, we additionally build a local memory unit M l ∈ R n,r,o,h,w to record the historical local matching maps in the previous interaction rounds. Formally, the local map L t,r for the t th frame in round r is written into the local memory by</p><formula xml:id="formula_5">M l t,r = L t,r ,<label>(5)</label></formula><p>which means that the writing process of the local memory is simply recording. When reading from the local memory, for the current t th frame, we calculate the distance of time to the userannotatedt th frame of each round r, dist r = |t −t r |, and select the nearest one to the user-annotated frame as the final local map. However, with the interactive round grows up, the accuracy of segmentation becomes better and better. For instance, a processing frame using the local map of round 8, although far away from the user-annotated frame in this round, may be better than using the local map of round 1 adjacent to the user-annotated frame. Hence we employ a forgetting mechanism by using the nearest local map to the user-annotated frame in only past R rounds. Local maps of early interaction rounds will be forgotten. R = 1 means we only use local maps of the current round and do not employ the memory mechanism. The local map memory and forgetting mechanism is shown in <ref type="figure">Fig. 5 (b)</ref>. Formally, denote the final local map for the current t th frame in round r * as L t,r * , then L t,r * is read from M l by L t,r * = M l t,r , r = arg min r |t−t r | and |r −r * | ≤ R <ref type="formula">(6)</ref> We utilize the propagation head with four convolutional layers to predict a one-dimensional map of logits for each selected object. The propagation head takes as input the concatenation of the pixel embedding, the global and local matching map read from memories, and the predicted mask of the previous frame. We stack the logits, apply softmax over the object dimension to obtain the probability map for each pixel.</p><p>Interaction Branch. The interaction branch aims at generating a segmentation mask of the user-annotated frame (interactive frame) given user annotations. As shown in <ref type="figure">Fig. 2</ref>, for generating the segmentation mask of the interactive frame in the current round, we concatenate the pixel embedding, the scribbles and the predicted mask from last round along the channel dimension, and use an interaction segmentation head with four convolutional layers to generate the segmentation logits of the target object o. For the multi-object cases, the interaction segmentation head extracts one-dimensional feature maps of logits for all objects, which are then stacked together to obtain the probability map for each pixel by applying the softmax operation over the object dimension.</p><p>In iVOS, the interaction branch need not only generate the segmentation mask of the interactive frame in the current round but also record and accumulate informative knowledge of the scribbles for improving the segment results of this frame in the next rounds. We propose a matching map to augment the incomplete scribbles by mining the property of the pixel embedding space, and record the augment map into the global memory M g . In the pixel embedding space, the pixels close to the annotated pixels have a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotated Frame</head><p>Original Scribbles in Round 1 Rough ROI with Annotated Background <ref type="figure">Figure 6</ref>. In the first round, there are no annotations of the background. We use a rough ROI and annotate pixels out of ROI as the background (black area). Green and blue scribbles annotate the first and second objects, respectively. higher probability of belonging to the same object. Similar to the local map proposed in the propagation branch, we employ a matching distance map to augment the scribbles. Suppose Pt denote the set of all pixels (with a stride of 4 in the embedding space) of the user-annotated frame at timet and Pt ,o denote the set of scribble-annotated pixels belonging to the target object o. For each pixel p ∈ Pt, we compute the distance of its nearest neighbor in the annotated pixels Pt ,o to construct the matching distance map. To avoid introducing the unexpected noisy pixels that are similar to the annotated ones but with large spatial distances, for each pixel p ∈ Pt, we only consider those pixels within its local neighborhood as the searching candidates. We denote N(p) as the neighbourhood set of p, where N(p) contains pixels at most k pixels far away from p. Therefore, the augmented map At(p) for pixel p is defined by</p><formula xml:id="formula_6">At(p) = min q∈P N t,o d(p, q) if P N t,o = ∅ 1</formula><p>otherwise, <ref type="bibr" target="#b6">(7)</ref> where P N t,o := Pt ,o ∩ N(p) is the intersection set of the scribble-annotated set Pt ,o and the neighbourhood set N(p). <ref type="figure" target="#fig_2">Fig. 4</ref> shows the comparison of the scribbles and the augmented maps, and we can find that the augmented map contains more information about the selected objects. The augmented map At will be recorded and aggregated in the global map M g . For the interactive frame at the timet in the round of r, M g is updated by</p><formula xml:id="formula_7">M ĝ t,r = min(M ĝ t,r−1 , At ,r ).<label>(8)</label></formula><p>This operation can benefit the segmentation result of the interactive frame in next rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training and Inference</head><p>Training Procedure. We employ a two-stage training procedure to train our MA-Net. In Stage 1, we train the propagation branch with the pixel embedding encoder. To simulate the video propagation process, we randomly select three frames from one training video as a training batch. One of the frames serves as the reference frame, i.e., it plays the role of the frame annotated by scribbles. Two adjacent frames serve as the previous frame and the current processing frame. Some methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">12]</ref> leverage the synthesized scribbles for the reference frame to train the propagation network. However, the synthesized scribbles are all densely generated from the ground-truth masks. After performing the training for a large number of iterations, ground-truth masks are actually used. Since the propagation branch is trained independently and densely generating synthesized scribbles from groundruth in an online manner is often timeconsuming, we directly use the ground-truth instance mask of the reference frame. In practice, we found that the reference frame using ground truth achieves similar performance to using the synthesized scribbles during training.</p><p>In Stage 2, after training the pixel embedding encoder and the propagation branch, we fixed the pixel embedding encoder and trained the interaction branch. It is not feasible to collect a large number of scribbles annotated by users. Therefore, we train our model with synthesized scribbles. In the first round, we use the scribbles of the training set provided by the DAVIS Challenge 2018 <ref type="bibr" target="#b5">[6]</ref>. In the following rounds, scribbles are synthesized within false negative and false positive areas. There is a gap between the first round and the following rounds since the first round only provides positive scribbles while following rounds provide both positive and negative scribbles. Hence we use the background label as the mask of the previous round for the first round.</p><p>Inference. We follow the round-based interactive setting of the DAVIS Challenge 2018. In the first round, users provide positive scribbles and no negative scribbles. To eliminate the gap between training and testing, we use a rough Region of Interest (ROI) that contains all positive scribbles and enlarge ROI by enough space to make sure it contains all parts of the target object. Then we annotate all the pixels out of the enlarged ROI as the background <ref type="figure">(Fig. 6)</ref>. We extract the pixel embedding of each frame and utilize the interaction branch and propagation branch to generate segmentation masks of the target video. In the following round, users annotate the frame of the video with the worst performance using scribbles. Our model extracts the pixel embeddings of all frames for only once in the first round. The extracted pixel embeddings are further employed to compute the refined segmentation masks with the interaction and propagation heads in the following rounds, leading to our MA-Net more efficient than previous methods. Implementation Details. We use the DeepLabv3+ architecture <ref type="bibr" target="#b6">[7]</ref> based on ResNet101 <ref type="bibr" target="#b10">[11]</ref> as our backbone, which produces an output feature maps with a stride of 4. On the top of the backbone, we add an embedding layer consisting of one depth-wise separable convolution with a kernel size of 3 × 3. The dimension of the pixel embedding is 100 advised by <ref type="bibr" target="#b27">[27]</ref>.</p><p>For the interaction and propagation segmentation heads, we employ four depth-wise separable convolutional layers with a dimension of 256, a kernel size of 7×7 for the depthwise convolutions, a batch normalization operation and a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Object</head><p>Multiple Objects <ref type="figure">Figure 7</ref>. The qualitative results on the DAVIS-2017 validation set. All the user interactions are automatically simulated by the robot agent provided by <ref type="bibr" target="#b5">[6]</ref>. All result masks are sampled after 8 rounds.</p><p>ReLU activation function. Finally, a 1 × 1 convolution is employed to extract the predicted logits.</p><p>When computing the local matching map, we downsample the pixel embedding by a factor of 2 for computational efficiency. In practice, we set the local window size as k = 12 in this paper, considering the trade-off between accuracy and efficiency. We utilize SGD optimization with a learning rate of 0.0007 and a batch size of 2. We employ the adaptive bootstrapped cross-entropy loss <ref type="bibr" target="#b23">[23]</ref>, which takes into account 100% to 15% hardest pixels from step 0 to step 50000 for computing the loss. All input images are augmented by random flipping, scaling, and cropping. The input size is 416×416 pixels. When processing the training of the first stage, we initialize the weights of the backbone with the weights pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> and COCO <ref type="bibr" target="#b16">[17]</ref>, and we train the pixel embedding encoder and the propagation head on the training set of DAVIS <ref type="bibr" target="#b24">[24]</ref> for 100000 steps. When training our model in the second stage, we use a round-based training with three rounds per circle. The first round uses only the positive scribbles while the following two rounds use both the positive and negative scribbles and the previous round masks. We train the second stage on the training set of DAVIS <ref type="bibr" target="#b24">[24]</ref> for 80000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Evaluating iVOS quantitatively is difficult since the user input is directly related to the segmentation results, and different users may provide different scribbles. To tackle this problem, Caelles et al. <ref type="bibr" target="#b5">[6]</ref> proposes a robot agent service to simulate human interaction for a fair comparison.</p><p>Quantitative Results. To fairly compare our MA-Net with the state-of-the-art methods, we evaluated our model on the DAVIS validation set following the interactive track benchmark in the DAVIS Challenge 2018 <ref type="bibr" target="#b5">[6]</ref>. In this benchmark, a robot agent interacts with each model for 8 rounds, <ref type="bibr">Method</ref> +OF +CRF +YV AUC J@60 Najafi et al. <ref type="bibr" target="#b20">[20]</ref> 0.702 0.548 Heo et al. <ref type="bibr" target="#b11">[12]</ref> 0.698 0.691 Heo et al. <ref type="bibr" target="#b11">[12]</ref> 0.704 0.725 Oh et al. <ref type="bibr" target="#b21">[21]</ref> 0.691 0.734 MA-Net(Ours) 0.749 0.761 <ref type="table">Table 1</ref>. Comparison of our MA-Net with the previous methods on the validation set in DAVIS2017. The entries are ordered according to the J@60 score. +OF denotes using optical flow, +CRF denotes using the CRF <ref type="bibr" target="#b13">[14]</ref> as post-processing and +YV denotes using additional YoutubeVOS training set <ref type="bibr" target="#b33">[33]</ref> when training. and the model is expected to compute masks within 30 seconds per interaction for each object. There are two evaluation metrics: area under the curve (AUC) and Jaccard at 60 seconds (J@60s). AUC is designed to measure the overall accuracy of the evaluation. J@60 measures the accuracy with a limited time budget (60 seconds). <ref type="table">Table.</ref> 1 shows the comparison of our method and previous state-ofthe-art iVOS methods. Comparing with the best competing method Heo <ref type="bibr" target="#b11">[12]</ref>, according to accuracy, our method surpasses it by +4.7% AUC. Comparing with the best competing method Oh et al. <ref type="bibr" target="#b21">[21]</ref>, according to efficiency, our method surpasses it by +2.7% J@60s. Besides, our model does not use any bells and whistles such as optical flow, post-processing (CRF), or additional video training set, i.e., YoutubeVOS <ref type="bibr" target="#b33">[33]</ref>. In addition, our MA-Net can accomplish 7-round interactions within 60seconds, which is more efficient than the state of the art one <ref type="bibr" target="#b21">[21]</ref> of 5-round interactions within 60 seconds 1 . In summary, our MA-Net outperforms previous methods in both accuracy and efficiency.</p><p>Qualitative Results. <ref type="figure">Fig. 7</ref> shows qualitative results on the DAVIS 2017 validation set. It can be seen that our MA-Net produces accurate segmentation masks in multiple  cases of large variance, including the single object condition and multiple objects condition. Qualitative results also show that our method can handle the occlusion issue (the 3rd row). In some difficult cases, e.g., a video contains multiple objects of the same class and the objects are occluded by each other (pigs in the 4th row), our method may make mistakes in some similar parts of different objects. This is most likely because the pixel embedding vectors of similar parts are close to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The Effectiveness of the Memory Mechanism. We conduct ablation studies using the DAVIS 2017 validation dataset to validate the effectiveness of our proposed memory mechanism. <ref type="figure" target="#fig_4">Fig. 8</ref> and <ref type="figure" target="#fig_5">Fig. 9</ref> show the Jaccard score of ablation models with growing number of interactions. In <ref type="figure" target="#fig_4">Fig. 8</ref>, we compare our method with and without global and local memories. No Global indicates we use the model without the global memory, which means we only use the global map calculated in the first round and do not aggregate it in the following rounds. No Local indicates that we only use the local map calculated in the current round and do not access local maps from previous rounds. No Global and Local is a model without using both the global map memory and local map memory. We can find that both the global map memory and the local map memory take effects in the iVOS and greatly improves the performance since utilizing all scribble information of previous rounds.</p><p>As described in Section 3, for the memory of the local map, there is a trade-off between choosing the nearest frame and the closest round. In practice, the segmentation mask far away from the annotated frame achieves worse results due to the error accumulation during propagation, so we choose the local map in which round it is nearest to the annotated frame. However, with the interactive round grows up, the accuracy of segmentation becomes better and better. Therefore, we use the nearest map to the annotated frame in the past R rounds. R = 1 means we only use local maps of the current round while R = 8 means we use the nearest map in all previous rounds. <ref type="figure" target="#fig_5">Fig. 9</ref> shows that when R &gt; 1, the segmentation accuracy will improve, indicating the effectiveness of the local map memory. When R = 2, our method achieves the best performance, and we choose R = 2 for our final model. The Effectiveness of the Augmented Map. The augmented map of the interactive frame is stored in the global memory in the current interaction round, which will help this frame be correct segmented in the following interaction rounds. Therefore, without the augmented map, the valuable interactive information of this frame will be lost during the propagation in the following interaction rounds. Besides, since our MA-Net also takes local matching into account, the improvements of all the interactive frames will further implicitly bring additional benefits to their subsequent non-interactive frames during the propagation. To be specific, the AUC score will drop from 0.749 to 0.744 if the augment map is removed from the global memory.</p><p>The Impact of the Local Window Size. In addition, we also study the impact of the local window size k, as shown in <ref type="table">Table.</ref> 2. When k is smaller, the local map computation is more efficient. However, a small k will affect the accuracy of our model. In practice, we choose k = 12 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Video object segmentation (VOS) is a fundamental task in computer vision. In this paper, we propose a user-friendly framework to generate accurate segmentation masks of a video with a few user annotations. Our MA-Net integrates the interaction and propagation operations into a unified pixel embedding learning framework, which promotes the efficiency of the round-based interactive VOS. More importantly, we propose a novel memory aggregation mechanism to record and aggregate the information of the user interactions and predictions of previous interaction rounds, which improves the segmentation accuracy greatly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>User</head><label></label><figDesc>Interaction in Each Round … Temporal Propagation through Frames</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Global matching and local matching process. For each pixel in the current processing frame at time t, distances are calculated with pixels of the target object annotated by scribbles (global map) or predicted mask (local map) and the smallest value of distances (nearest neighbor) are used to construct the matching map.Object ImageScribbles Augmented Map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of the augmented map computed by the pixel embedding and scribbles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Ablation study on DAVIS 2017 validation set to show the effectiveness of our proposed global and local memories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>The impact of R in the local map memory. R denotes that local maps in past R rounds in the memory are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>conv conv conv Share weights Pixel Embedding Backbone Interaction Branch Propagation Branch concat Local Memory</head><label></label><figDesc></figDesc><table><row><cell cols="2">Interaction r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">User Annotated Frame ! "</cell><cell>Pixel embedding</cell><cell cols="3">Pixel embedding Scribble at Round r</cell><cell></cell><cell>concat</cell></row><row><cell></cell><cell cols="2">Embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>conv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Interaction</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Scribble Augment Mask at Round r-1</cell><cell></cell><cell>Segm. Head</cell></row><row><cell>Current Frame t</cell><cell cols="2">Pixel embedding Embedding Encoder</cell><cell>Write Write</cell><cell>Global Memory</cell><cell>Read</cell><cell>Global Map</cell><cell>Pixel embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Global Matching</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Pixel embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Embedding Encoder</cell><cell>Write</cell><cell></cell><cell>Read</cell><cell></cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell></row><row><cell>Previous Frame t-1</cell><cell></cell><cell></cell><cell>Local Matching</cell><cell></cell><cell></cell><cell>Local Map</cell><cell>Mask of Frame t-1</cell><cell>Segm. Head Propagation</cell></row><row><cell cols="3">Mask of Frame t-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, for each pixel p belonging to the frame at time t, we</figDesc><table><row><cell>Frame 49 Interaction 1</cell><cell cols="2">Frame 16 Interaction 2</cell><cell>Frame 15 Interaction 3</cell><cell></cell><cell></cell><cell>Round 1</cell><cell>Round 2</cell><cell>Round 3</cell><cell>Round 4 Round</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell>Frame 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Current Frame</cell><cell>Past R Rounds</cell></row><row><cell>Frame 14</cell><cell></cell><cell cols="2">Calculate Global Map</cell><cell></cell><cell>Frame 12</cell><cell></cell></row><row><cell>Current Processing Frame</cell><cell></cell><cell cols="2">Memory Writing</cell><cell>…</cell><cell>Frame 14</cell><cell>Local Map Memory</cell></row><row><cell>Global Map Memory</cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell>Frame 20</cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell cols="2">Memory Reading</cell><cell></cell><cell>Frame 60</cell><cell>Time</cell><cell>(b)</cell><cell>Current Round</cell></row></table><note>Interaction dist Read</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>The impact of the local window size k.</figDesc><table><row><cell cols="4">Local window size k</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell></cell><cell>15</cell></row><row><cell></cell><cell cols="2">AUC</cell><cell></cell><cell cols="5">0.724 0.737 0.749 0.748</cell></row><row><cell></cell><cell cols="2">J@60</cell><cell></cell><cell cols="5">0.730 0.753 0.761 0.761</cell></row><row><cell></cell><cell>0.78</cell><cell>Complete</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell>No Local No Global</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jaccard(mIOU)</cell><cell>0.66 0.68 0.70 0.72 0.74</cell><cell cols="2">No Global and Local</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Number of Interactions (Round)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To fairly compare the efficiency, we test our model on a 1080Ti GPU following Oh<ref type="bibr" target="#b21">[21]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is in part supported by ARC DP200100938 and ARC DECRA DE190101315.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higherorder spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00269</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin Bratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rotoscoping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Routledge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using sparse-to-dense networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Yeong Jun Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Roto++: Accelerating professional rotoscoping using shape manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Similarity learning for dense label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR Workshops</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5247" to="5256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Livecut: Learning-based interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravin</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3064" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mhp-vos: Multiple hypotheses propagation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08333</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
