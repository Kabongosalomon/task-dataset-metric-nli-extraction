<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Lu</surname></persName>
							<email>xiaopen2@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
							<email>tianchez@soco.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">SOCO Inc Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
							<email>kyusongl@soco.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">SOCO Inc Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image retrieval is an essential task in multi-modal information retrieval, i.e. retrieving relevant images from a large and unlabelled image dataset given textual queries. In this paper, we propose VisualSparta, a novel text-to-image retrieval model that shows substantial improvement over existing models on both accuracy and efficiency. We show that VisualSparta is capable of outperforming all previous scalable methods in MSCOCO  and Flickr30K. It also shows substantial retrieving speed advantages, i.e. for an index with 1 million images, VisualSparta gets over 391x speed up compared to standard vector search. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for very large dataset, with significant accuracy improvement compared to previous state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-to-image retrieval is the task of retrieving the most relevant images given a text query. Given a text description, the model needs to output a list of relevant images from the indexed corpus. In this paper, we argue that current text-to-image retrieval models face two main challenges: accuracy challenge and latency challenge. We further propose VisualSparta, an accurate and efficient retrieval model that significantly outperforms other existing models on both aspects.</p><p>Achieving high accuracy is challenging in textto-image retrieval task. In order to find the most relevant images given text query, the model needs to not only have good representations for both text and image modalities, but also understand the fine-grained relationships between two representations. For example, given a query "A girl in black jacket drinking milk and eating pizza.", the model needs to first have good representations for the important concepts such as "girl", "black jacket", "milk". For each image, the model has to capture the large amounts of fine-grained information lying in it. Then, the model needs to learn to pick the most relevant images given the text query. Much work has been done on text-to-image retrieval task. Some methods rely on a dual-encoder architecture, which uses two encoder to separately encode text query and image answer, then optimize the encoder weights for both sides <ref type="bibr" target="#b3">(Faghri et al., 2017;</ref><ref type="bibr">Wang et al., 2019a)</ref>. Other recent methods leverage an transformer-based architecture <ref type="bibr" target="#b2">(Devlin et al., 2018;</ref>. In this case, each pair of text and image is encoded by concatenating and passing into one single network, instead of encoded by two separate encoders <ref type="bibr" target="#b12">Li et al., 2020)</ref>. This transformerbased method borrows the knowledge from large pretrained models and shows better results compared to dual-encoder method.</p><p>Another challenge that previous works did not touch upon is the retrieval latency challenge. Retrieval latency is a long-existing challenge in information retrieval (IR) area, as the design of an IR system needs to fit user's real-time information seeking needs <ref type="bibr" target="#b16">(Manning et al., 2008)</ref>. Although text information retrieval communities start focusing on this problem for a long time and view latency as an important metric, latency problem has not been well studied in text-to-image retrieval problems yet. In this paper, we evaluate and analyze the speed performance of our method, and do detailed speed comparisons with existing methods.</p><p>In this work, we propose VisualSparta (Sparse Transformer Fragment-level Matching), a simple yet effective text-to-image retrieval model that per-forms better than all existing retrieval models on both accuracy and retrieval latency. By modeling fragment-level information for the query and region information for the answer, our model is capable of borrowing information from large pretrained models, while benefiting from the efficiency of sparse retrieval method. To the best of our knowledge, this is the first model that integrates the power of transformer-based models with the real-time searching capabilities, showing that large pretrained models can be used in a way that uses significantly less amount of memory and computing time.</p><p>Contributions of this paper can be concluded as the following:</p><p>• A novel image-to-text retrieval model that based on fragment-level interaction and shows state-of-the-art results over two benchmark datasets.</p><p>• Inverted index are shown to be effective in text-to-image search and produces promising results in terms of accuracy and efficiency.</p><p>• Detailed analysis on accuracy-latency comparisons over current text-to-image retrieval models to show the strong performance of our proposed model in large-scale settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large amounts of work have been done on learning a joint representation between texts and images <ref type="bibr" target="#b8">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b7">Huang et al., 2018;</ref><ref type="bibr" target="#b24">Wehrmann et al., 2019;</ref><ref type="bibr" target="#b12">Li et al., 2020;</ref>. In this section, we revisit dual-encoder based retrieval model and transformer-based retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual-encoder Matching Network</head><p>Most of the work in text-to-image retrieval task choose to use dual-encoder network to encode information from text and image modalities. In <ref type="bibr" target="#b8">Karpathy and Fei-Fei (2015)</ref>, the author used a Bi-directional Recurrent Neural Network (BRNN) to encode the textual information and used an Region Convolutional Neural Netork (RCNN) to encode the image information, and the final similarity score is computed via the interaction of features from two encoders.  proposed stacked cross-attention network, where the text features are passed through two attention layers to learn interactions with the image region. <ref type="bibr">Wang et al. (2019a)</ref> encoded the location information as yet another feature, and used both deep Faster-RCNN features <ref type="bibr" target="#b20">(Ren et al., 2016)</ref> and the fine-grained location features for the Region of Interest (ROI) as image representation. In , the author utilized the information from external corpus (Wikipedia) to construct a Graph Neural Network (GNN) to help model the relationships across objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Language Models (PLM)</head><p>Large pretrained language models (PLM) show great success over multiple tasks in NLP areas in recent years <ref type="bibr" target="#b2">(Devlin et al., 2018;</ref>. After that, research has also been done on cross-modal transformer-based models and proves that the self-attention mechanism also helps jointly capture text-image relationships <ref type="bibr" target="#b19">Qi et al., 2020;</ref><ref type="bibr" target="#b12">Li et al., 2020)</ref>. By first pretraining model under large-scale text-image dataset, these transformer-based models capture rich semantic information from both texts and images. Models are then fine-tuned for the textto-image retrieval task and show improvements by a large margin. However, the problem of using transformer-based models is that it is prohibitively slow in the retrieval context: the model needs to compute pair-wise similarity scores between all queries and answers, making it almost impossible to use the model in any real-world scenarios. Our proposed method borrows the power of large pretrained models, while reducing the inference time by orders of magnitude. PLM has shown promising results in Information Retrieval (IR), despite of its prohibitively slow speed due to the complex model structure. IR communities has recently started working on empowering the classical full-text retrieval methods with contextualized information from PLMs <ref type="bibr" target="#b0">(Dai and Callan, 2019;</ref><ref type="bibr" target="#b15">MacAvaney et al., 2020;</ref><ref type="bibr" target="#b27">Zhao et al., 2020)</ref>. <ref type="bibr" target="#b0">Dai and Callan (2019)</ref> proposed DeepCT, a model which learns to generate the query importance score from the contextualized representation of large transformer-based models. <ref type="bibr" target="#b27">Zhao et al. (2020)</ref> proposed sparse transformer matching model, where the model learns term-level interaction between query and text answers and generates weighted term representations for answers during index time. Our work is motivated by works in this direction, and extends the scope to the cross-modal understanding and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VisualSparta Retriever</head><p>In this section, we present VisualSparta retriever, a fragment-level transformer-based model for efficient text-image matching. The focus of our proposed model is two-fold:</p><p>• Accuracy: fine-grained relationship between query tokens and image regions are learned to enrich the cross-modal understanding.</p><p>• Efficiency: learning query and answer embedding independently allows the model to index all the images offline, and the whole VisualSparta model can be adopted to a classical inverted-index search engine for efficient search.</p><p>3.1 Model Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Query representation</head><p>As the query processing is an online operation during retrieval, efficiency of encoding query needs to be well considered. Previous methods pass the query sentence into an bi-RNN to give token representation given surrounding tokens <ref type="bibr">Wang et al., 2019a</ref>. Instead of encoding the query in a sequential manner, we drop the order information of the query and only use the pretrained word embedding to represent each token. In other words, we do not encode the local contextual information for the query, and purely rely on independent word embedding E word of each token. Let a query be q = w 1 , ..., w m :</p><formula xml:id="formula_0">w i = E word (w i )<label>(1)</label></formula><p>where w i is the i-th word of the query. Therefore, a query is represented asŵ = {ŵ 1 , ...,ŵ m },ŵ i ∈ R d H . In this way, each token can be represented independently and agnostic to the local context. This is essential for the efficient indexing and inference, as described next in 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Visual Representation</head><p>Compared with query which needs to be processed at real time, answer processing can be rich and complex, as answer corpus can be indexed offline before the query comes. Therefore, we follow the previous work <ref type="bibr" target="#b12">(Li et al., 2020)</ref> and use the contextualized representation for the answer corpus.</p><p>Specifically, for an image, we represent it using information from three sources: regional deep features, regional location features, and object label features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regional deep features and location features</head><p>Given an image v, we pass it through Faster- <ref type="bibr">RCNN (Ren et al., 2016)</ref> to get n regional deep features v i and their corresponding location fea- <ref type="formula">(2)</ref> and the location features are the normalized top left and bottom right right positions of the region proposed from Faster-RCNN, plus the region width and height:</p><formula xml:id="formula_1">tures l i : v 1 , ..., v n = Faster-RCNN(v), v i ∈ R drcnn</formula><formula xml:id="formula_2">l i = [l xmin , l xmax , l ymin , l ymax , l width , l height ]</formula><p>(3) Therefore, we represent one region by the concatenation of two features:</p><formula xml:id="formula_3">E i = [v i ; l i ] (4) E image = [E 1 , ..., E n ], E i ∈ R drcnn+d loc (5)</formula><p>where E image is the representation for a single image.</p><p>Object label features Additional to the deep representations from the proposed image region, previous work <ref type="bibr" target="#b12">(Li et al., 2020)</ref> shows that the object label information is also useful as an additional representation for the image. We also encode the predicted objects obtained from Faster-RCNN model with pretrained word embeddings:</p><formula xml:id="formula_4">o i = E word (o i ) + E pos (o i ) + E seg (o i )<label>(6)</label></formula><formula xml:id="formula_5">E label = [ô 1 , ...,ô k ],ô i ∈ R d H<label>(7)</label></formula><p>where o i represents one object label, and E word , E pos , E seg represent word embedding, position embedding, and segment embeddings respectively, same as the embedding structure in <ref type="bibr" target="#b2">Devlin et al. (2018)</ref>. Therefore, one image can be represented as linear transformed image features concatenated with label features:</p><formula xml:id="formula_6">a = [(E image W + b); E label ]<label>(8)</label></formula><p>where W ∈ R (drcnn+d loc )×d H and b ∈ R d H are the trainable linear combination weights and bias. The concatenated a are then passed into a Transformer encoder T image , and the final image feature is the hidden output of it:</p><formula xml:id="formula_7">H image = T image (a)<label>(9)</label></formula><p>where H image ∈ R (n+k)×d H is the final contextualized representation for one answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Scoring Function</head><p>Given the visual and query representations, now we are ready to compute the matching score between a query and an image. Different from other dualencoder based interaction model, we adopt the finegrained interaction model proposed by <ref type="bibr" target="#b27">(Zhao et al., 2020)</ref> to to compute the relevance score by:</p><formula xml:id="formula_8">y i = max j∈[1,n+k] (ŵ T i h j ) (10) φ(y i ) = ReLU(y i + b) (11) f (q, v) = m i=0 log(φ(y i ) + 1)<label>(12)</label></formula><p>where Eq.10 captures the fragment-level interaction between every image element and every query word token; Eq.11 produces sparse embedding outputs via a combination of ReLu and trainable bias and Eq.12 sums up the score and prevents overly large score using log operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retriever training</head><p>Following training method (Zhao et al., 2020), we use cross entropy loss to train VisualSparta. Concretely, we maximize the objective in Eq. 13, which tries to decide between the ground truth image v + and irrelevant/random images V − for each text query q. The parameters to learn include both the query encoder E word and the image transformer encoder T image . Parameters are optimized using Adam <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref>.</p><formula xml:id="formula_9">J = f (q, v + ) − log k∈K − e f (q,v k ))<label>(13)</label></formula><p>In order to achieve efficient training, we use other image samples from the same batch as negative examples for each training data, an effective technique that is widely used in response selection <ref type="bibr" target="#b26">(Zhang et al., 2018;</ref><ref type="bibr" target="#b5">Henderson et al., 2019)</ref>. Preliminary experiments found that this simple approach performs equally well compared to other more sophisticated methods, e.g. sample similar images that have nearby labels etc, as long as the batch size is large enough, e.g. 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Indexing and Inference</head><p>One major advantage of VisualSparta is how one can use it for real-time inference. That is for a testing query q = [w 0 , ...w m ], the ranking score between q and an image is:</p><formula xml:id="formula_10">CACHE(w, v) = log(Eq. 11) w ∈ W (14) f (q, v) = m i=1 CACHE(w i , v)<label>(15)</label></formula><p>Since the query term embedding is non-contextual, we can compute the rank feature φ(w, v) for every possible term w in the vocabulary W with every image v. The resulting score is cached during indexing as shown in Eq. 14. At inference time, the final ranking score can be computed via O(1) look up plus a simple summation as shown in Eq. 15. More importantly, the above computation can be efficiently implemented via a Inverted Index <ref type="bibr" target="#b16">(Manning et al., 2008)</ref>, which is the underlying data structure for modern search engines as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This enables us to index a very large image dataset using modern search engine, e.g. Elasticsearch <ref type="bibr" target="#b4">(Gheorghe et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In this paper, we use MSCOCO <ref type="bibr" target="#b13">(Lin et al., 2014)</ref> and <ref type="bibr">Flickr30K (Plummer et al., 2015)</ref> datasets for the training and evaluation of text-to-image retrieval task. MSCOCO is a large-scale multi-task datasets including object detection, semantic segmentation, and image captioning data. In this experiment, we use the image cpationing dataset split as the source of data for text-to-image model training and evaluation. Following the experimental settings from <ref type="bibr" target="#b8">Karpathy and Fei-Fei (2015)</ref>, we split the data into 113,287 images for training, 5,000 images for development, and 5,000 images for testing. Each image is paired with 5 captions from different annotators. The performance of both 1,000 and 5,000 test splits are reported and compared with previous results.</p><p>Flickr30K <ref type="bibr" target="#b18">(Plummer et al., 2015)</ref> is another large scale image captioning datasets, which contains 31, 783 images in total. Following the split from <ref type="bibr" target="#b8">Karpathy and Fei-Fei (2015)</ref>, 29, 783 images are used for training and 1,000 images are used for testing. The final model is tested on 1,000 test images.</p><p>For large-scale efficiency experiment, since there is no existing large-scale image captioning datasets available, we manually design 113K and 1M datasets for testing the inference speed of different models in the large-scale setting. Note that for these two datasets, since we are only interested in speed comparison, the accuracy/quality of the data itself can be ignored. The 113K dataset refers to the MSCOCO training set, which contains 113, 287 images, ∼23 times bigger than the MSCOCO 5K test set. The 1M dataset we design consists of 1 million images randomly sampled from the MSCOCO training set. All the efficiency test experiment all done using original MSCOCO 1K and 5K test splits, plus 113K and 1M splits as test beds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Following previous works, we use recall rate as our accuracy evaluation metrics. In both MSCOCO and Flikr30k datasets, we report Recall@k, k= <ref type="bibr">[1,</ref><ref type="bibr">5,</ref><ref type="bibr">10]</ref> and compare with previous works.</p><p>For large-scale efficiency evaluation, we choose query per second and latency(ms) as the evaluation metric to test how each model performs in terms of speed under different sizes of image index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Recall Performance</head><p>We compare the model performance with current state-of-the-art retrieval model in text-to-image search. As shown in table 1, the results reveals that our model is competitive compared with previous methods, and achieves state-of-the-art results in most of the evaluation metrics.</p><p>Specifically, in MSCOCO 1K test set, our model improves the R@1 performance by 1.9% from previous best methods , and gets the same results as previous best method in R@5 and R@10. In 5K split, for R@1, 5, 10, our model outperforms the previous best method by 5.8%, 3.5%, and 2.0% respectively. Speaking of Flickr30K dataset, the VisualSparta performance is not higher compared with CVSE  in terms of for R@5 and R@10, whereas in terms of R@1, VisualSparta is still 1.3% higher than CVSE. In short, VisualSparta achieves best results over previous methods on 7 out of 9 evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Speed Performance</head><p>As discussed in section 4.2, to show the efficiency of VisualSparta model, in addition to original 1K and 5K test split, we also create 113K dataset and 1M dataset, two large-scale benchmark datasets for retrieval speed comparison.</p><p>To make fair comparison, we use the optimized hardware for both previous method (GPU accelerated) and our VisualSparta method (CPUmultithread accelerated). For VisualSparta, we use the top-1000 term scores settings for the experiment. For all three models, we use the MSCOCO test-1K split as the source of query to test the speed performance. Since all three models need the same MSCOCO-1k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO-5k</head><p>Flickr 30K Model R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 SM-LSTM <ref type="bibr" target="#b6">(Huang et al., 2017)</ref>   <ref type="bibr" target="#b3">(Faghri et al., 2017)</ref> 52.0 -92.0 30.3 -72.4 39.6 -79.5 CAMP <ref type="bibr">(Wang et al., 2019b)</ref> 58.5 87.9 95.0 39.0 68.9 80.2 51.5 77.1 85.3 SCAN  58  form of Faster-RCNN image region features as input, we does not take the processing time of this part into consideration. Instead, image region features are directly used as input.</p><p>As we can see from <ref type="table" target="#tab_2">Table 2</ref>, in all four data split, VisualSparta model significantly outperforms the best dual-encoders retrieval model (CVSE ) and the best transformer-based retrieval model (Oscar <ref type="bibr" target="#b12">(Li et al., 2020)</ref>). Specifically, in 113K dataset, the speed of VisualSparta is 51 times faster than CVSE model, and 91,833 times faster than Oscar model. In 1M dataset, the speed of VisualSparta is 391 times faster than CVSE model. and 391, 000 times faster than Oscar model. <ref type="table" target="#tab_2">Table 2</ref> also reveals that as the number of images increases, the performance drop is much slower when comparing VisualSparta with other two methods. When the dataset size increases from 113K to 1M, the speed of VisualSparta only decreases by 2.35 times. Compared with it, the speed of CVSE decreases by 13.5 times, whereas the speed of Oscar decreases by 7 times, both of which substantially higher than that of VisualSparta. This experiment shows the absolute speed advantage of VisualSparta compared with previous methods in the large-scale settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>All experiments are done using PyTorch library. During training, one NVIDIA Titan X GPU is used. During speed performance evaluation, for CVSE and Oscar model, one NVIDIA Titan X GPU is used for acceleration. For VisualSparta, a 10-core Intel 9820X CPU is used. For the image encoder, we initialize the model weights from Oscar-base model <ref type="bibr" target="#b12">(Li et al., 2020)</ref> with 12 layers and 768 hidden dimensions. For the query embedding, we also initialize it from the Oscar-base word embedding. The learning rate is set to 5e-5 with batch size 160. The number of training epochs is set to 20.   <ref type="table">Table 3</ref>: Effect of top-K term scores in terms of speed and accuracy tested in MSCOCO dataset; ↑ means higher the better, and ↓ means lower the better K term scores based on their memory constraint or speed requirement. <ref type="table">Table 3</ref> compares different choices of K value and their corresponding recall and speed performance, in both MSCOCO 1K and 5K split. <ref type="figure">Figure 2</ref> visualizes the trade-off between model accuracy and inference speeed. The x axis represents the latency(ms) of a single query, and the y axis denotes the Recall@1 score under MSCOCO 1K test set. Each dot represents the model performance under certain top-K term score settings. The curve reveals that with the increase of the K, the recall becomes higher, whereas each query takes longer time to get the retrieval result. From the comparison between VisualSparta and CVSE model, we observe that VisualSparta can already beat the accuracy performance of CVSE when setting top-K term score to 1000. In this small-scale case with only 1K image index, we already get an absolute ∼2.3X speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Retrieved Images in a Large-scale Setting</head><p>Since no public large-scale retrieval dataset existed, we query the VisualSparta model on the 113K split and manually check the results. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, most of the retrieved results make sense and are highly relevant to the query provided. This shows that VisualSparta is capable of retrieving relevant results in the large-scale settings. Moreover, by inspecting top terms corresponding to each retrieved image, we observe that these terms are also very relevant to their corresponding images, which implies that weighted terms is a valid and rich representations for these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In conclusion, this paper presents VisualSparta, an accurate and efficient text-to-image retrieval model which shows the state-of-the-art scalable performance in both MSCOCO and Flickr30K. Its main novelty lies in the combination of powerful pretrained image encoder with fragment-level scoring.</p><p>Detailed analysis also demonstrates that our approach has substantial scalability advantages compared to previous best methods when indexing large image datasets for real-time searching, making it suitable for real-world deployment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visual-SPARTA Model. It first computes contextual image region representation and query token representation. Then it computes matching score between every query token and image region that can be stored in an inverted index for efficient searching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>As described in 3.3, each image can be well represented by a list of weighted tokens independently. This feature makes VisualSparta flexible during indexing time: users can choose to index using top-Examples of retrieved images and corresponding sparse embedding (terms) under MSCOCO 113K split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model Speed vs.</figDesc><table><row><cell>Index Size: VisualSparta</cell></row><row><cell>experiments are done under setting top-K term scores</cell></row><row><cell>to 1000. All experiments are performed using default</cell></row><row><cell>acceleration hardware as described in 4.3.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Figure 2 :</head><label>2</label><figDesc>Inference Speed vs. Model Accuracy. Each dot represents model performance using top-K term scores. Larger K gives higher accuracy and longer latency time. By setting K to 1000, our model already outperforms CVSE model, with ∼2.3X speedup. Experiments are done on MSCOCO 1K test split.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Context-aware sentence/passage term importance estimation for first stage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Elasticsearch in action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gheorghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Lee</forename><surname>Hinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Russo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Manning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03688</idno>
		<title level="m">Convert: Efficient and accurate conversational representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instanceaware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stacked cross attention for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Expansion via prediction of importance with contextualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14245</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pretraining with large-scale weak-supervised imagetext data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Camp: Cross-modal adaptive message passing for textimage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Consensus-aware visual-semantic embedding for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language-agnostic visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonatas</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5804" to="5813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>do you have pets too? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sparta: Efficient open-domain question answering via sparse transformer matching retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13013</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
