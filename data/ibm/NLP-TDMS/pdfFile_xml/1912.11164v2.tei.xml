<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Scene Adaptation with Memory Regularization in vivo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<email>zhedong.zheng@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Scene Adaptation with Memory Regularization in vivo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing methods focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo to exploit the intradomain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 → Cityscapes and SYNTHIA → Cityscapes, yielding +11.1% and +11.3% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0% mIoU improvement is observed on the cross-city benchmark: Cityscapes → Oxford RobotCar.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the unaffordable cost of the segmentation annotation, unsupervised scene adaptation is to adapt the learned model to a new domain without extra annotation. In contrast to the conventional segmentation tasks, unsupervised scene adaptation reaches one step closer to the real-world practice. In the real-world scenario, the annotation of the target scene is usually hard to acquire. In contrast, abundant source data is easy to access. To improve the model scalability on the unlabeled target domain, most researchers resort to transfer the common knowledge learned from the source domain to the target domain.</p><p>The existing scene adaptation methods typically focus on reducing the discrepancy between the source domain and the target domain. The alignment between the source and target <ref type="figure">Figure 1</ref>: We leverage the auxiliary classifier of the widely-used baseline model <ref type="bibr">[Tsai et al., 2018]</ref> to pinpoint the intra-domain uncertainty. While the predictions of the source domain input are relatively consistent, the unlabeled input from the target domain suffers from the uncertain prediction. The model provides different class predictions for the same pixel. It implies that the intra-domain consistency is under-explored, especially in the unlabeled target domain. In contrast to the existing works, which focus on the interdomain alignment, we focus on one orthogonal direction of mining intra-domain knowledge. domains could be conducted on different levels, such as pixel level <ref type="bibr" target="#b4">[Hoffman et al., 2018;</ref><ref type="bibr" target="#b13">Wu et al., 2018]</ref>, feature level <ref type="bibr" target="#b4">[Hoffman et al., 2018;</ref><ref type="bibr" target="#b14">Yue et al., 2019;</ref><ref type="bibr" target="#b8">Luo et al., 2019b;</ref><ref type="bibr" target="#b15">Zhang et al., 2019a]</ref> and semantic level <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b12">Tsai et al., 2019;</ref>. Despite the great success, the brute-force alignment drives the model to learn the domain-agnostic shared features of both domains. We consider that this line of methods is sub-optimal in that it ignores the domain-specific feature learning on the target domain, and compromise the final adaptation performance.</p><p>Since the domain-specific knowledge is ignored for the target unlabeled data, the regularization by the data itself does not aid in the domain adaptation. To qualitatively verify this, we leverage the auxiliary classifier of the baseline model <ref type="bibr">[Tsai et al., 2018]</ref> as a probe to pinpoint the inconsistency. As shown in <ref type="figure">Fig. 1</ref>, we observe that the model predicts one consistent supervised result of the source labeled data, while the unlabeled target data suffers from the inconsistency. The predicted result of the primary classifier is different from the auxiliary classifier prediction, especially in the target domain. It implies that the intra-domain consistency has not  <ref type="bibr" target="#b1">[Chen et al., 2018]</ref> applies an extra memory module to save the class prediction while training. (b) Mean teacher <ref type="bibr">[Tarvainen and Valpola, 2017]</ref> and mutual learning <ref type="bibr" target="#b15">[Zhang et al., 2018b</ref>] apply one external model to memorize predictions and regularize the training. (c) Different from existing methods, the proposed method does not need extra modules or external models. We leverage the running network itself, as the memory model. Given one input sample as the key, we could obtain the two predictions (values) from the primary classifier and the auxiliary classifier. been learned automatically, when we minor the inter-domain discrepancy.</p><p>To effectively exploit the intra-domain knowledge and reduce the target prediction inconsistency, we propose a memory mechanism into the deep neural network training, called memory regularization in vivo. Different from the previous works focusing on the inter-domain alignment, the proposed method intends to align the different predictions within the same domain to regularize the training. As shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, we consider the inputs as key and the output prediction as the corresponding value. In other words, the proposed method deploys the model itself as the memory module, which memorizes the historical prediction and learns the key-value projection. Since we have the auxiliary classifier and the primary classifier, we could obtain two values for one key. We note that the proposed method is also different from other semisupervised works deploying the extra memory terms. Since the proposed method does not require additional parameters or modules, we use the term "in vivo" to differentiate our method from <ref type="bibr" target="#b1">[Chen et al., 2018;</ref><ref type="bibr">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b15">Zhang et al., 2018b]</ref>; these methods deploy external memory modules.</p><p>Our contribution is two-fold: (1) We propose to leverage the memory of model learning to pinpoint the prediction uncertainty and exploit the intra-domain knowledge. This is in contrast to most existing adaption methods focusing on the inter-domain alignment.</p><p>(2) We formulate the memory regularization in vivo as the internal prediction discrepancy between the two classifiers. Different from the existing memory-based models, the proposed method does not need extra parameters, and is compatible with most scene segmentation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Adaptation for Segmentation</head><p>Most existing works typically focus on minoring the domain discrepancy between the source domain and the target domain to learn the shared knowledge. Some pioneering works <ref type="bibr" target="#b4">[Hoffman et al., 2018;</ref><ref type="bibr" target="#b13">Wu et al., 2018]</ref> apply the image generator to transfer the source data to the style of the target data, and intend to reduce the low-level visual appearance difference. Similarly, Yu et al. <ref type="bibr" target="#b14">[Yue et al., 2019]</ref> and Wu et al. <ref type="bibr" target="#b13">[Wu et al., 2019]</ref> generate the training images of different styles to learn the domain-agnostic feature. Adversarial loss is also widely studied. Tsai et al. <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b12">Tsai et al., 2019]</ref> apply the adversarial losses to different network layers to enforce the domain alignment. Luo et al. <ref type="bibr" target="#b8">[Luo et al., 2019b]</ref> leverage the attention mechanism and the classaware adversarial loss to further improve the performance. Besides, some works also focus on mining the target domain knowledge, which is close to our work. Zou et al. <ref type="bibr" target="#b16">[Zou et al., 2018;</ref><ref type="bibr" target="#b16">Zou et al., 2019]</ref> leverage the confident pseudo labels to further fine-tune the model on the target domain, yielding a competitive benchmark. <ref type="bibr">Recently, Shen et al.[Shen et al., 2019]</ref> propose to utilize the discriminator to find the confident pseudo label. Different from the pseudo label based methods, the proposed method focuses on target domain knowledge by mining the intrinsic uncertainty of the model learning on the unlabeled target-domain data. We note that the proposed method is orthogonal to the existing methods, including the inter-domain alignment <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b12">Tsai et al., 2019;</ref><ref type="bibr" target="#b8">Luo et al., 2019b]</ref> and self-training with pseudo labels <ref type="bibr" target="#b16">[Zou et al., 2018;</ref><ref type="bibr" target="#b16">Zou et al., 2019]</ref>. In Section 4.2, we show the proposed method can be integrated with other domain adaption methods to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory-based Learning</head><p>As one of the early works, Weston et al. <ref type="bibr" target="#b13">[Weston et al., 2014]</ref> propose to use external memory module to store the longterm memory. In this way, the model could reason with the related experience more effectively. Chen et al. <ref type="bibr" target="#b1">[Chen et al., 2018]</ref> further apply the memory to the semi-supervised learning to learn from the unlabeled data. In this work, we argue that the teacher model, which is applied in many frameworks, also could be viewed as one kind of external memory terms. Because the teacher model distills the knowledge of the original setting, and memorizes the key concepts to the final prediction <ref type="bibr" target="#b3">[Hinton et al., 2015]</ref>. For instance, one of the early work, called temporal ensemble <ref type="bibr" target="#b7">[Laine and Aila, 2016]</ref>, uses the historical models to regularize the running model, yielding the competitive performance. The training sample could be viewed as the key, and the historical models are the memory model to find the corresponding value for the key. Since the historical models memorize the experience from the previous training samples, the temporal ensemble could provide stable and relatively accurate predictions of the unlabeled data. Except for <ref type="bibr" target="#b7">[Laine and Aila, 2016]</ref>, there are different kinds of external memory models. Mean Teacher <ref type="bibr">[Tarvainen and Valpola, 2017]</ref> leverages the weight moving average model as the memory model to regularize the training. Further, French et al. <ref type="bibr" target="#b1">[French et al., 2017]</ref> extend Mean Teacher for visual domain adaptation. Zhang et al. <ref type="bibr" target="#b15">[Zhang et al., 2018b]</ref> propose mutual learning, which learns the knowledge from multiple student models.</p><p>Different from existing memory-based methods <ref type="bibr" target="#b1">[Chen et al., 2018;</ref><ref type="bibr">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b15">Zhang et al., 2018b]</ref>, the proposed method leverages the memory of the model itself to regularize the running model. The proposed memory regularization does not introduce extra parameters and external modules. (see <ref type="figure" target="#fig_0">Fig. 2)</ref> 3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm Overview</head><p>Formulation. We denote the images from the source domain and the target domain as</p><formula xml:id="formula_0">X s = {x i s } M i=1 and X t = {x j t } N j=1 ,</formula><p>where M, N are the number of the source images and target images. Every source domain data in X s is annotated with corresponding ground-truth segmentation maps</p><formula xml:id="formula_1">Y s = {y i s } M i=1 .</formula><p>Given one unlabeled target domain image x j t , we intend to learn a function to project the image to the segmen-tation map y j t . Following the practice in <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b8">Luo et al., 2019b]</ref>, we adopt the modified DeepLabv2 as our baseline model, which contains one backbone model and two classifiers, i.e., the primary classifier C p and the auxiliary classifier C a . To simplify, we denote the two functions F p and F a as the segmentation functions, where F p projects the image to the prediction of the primary classifier, and F a maps the input to the prediction of the auxiliary classifier. Overview. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the proposed method has two training stages, i.e., Stage-I and Stage-II, to progressively transfer the learned knowledge from the labeled source data to the unlabeled target data. In the Stage-I, we follow the conventional domain adaptation methods to minor the interdomain discrepancy between the source domain and the target domain. When training, we regularize the model by adding the memory regularization. The memory regularization helps to minor the intra-domain inconsistency, yielding the performance improvement. In the Stage-II, we leverage the trained model to predict the label for the unlabeled target data. Then the model is further fine-tuned on the target domain. With the help of pseudo labels, the model could focus on learning domain-specific knowledge on the target domain. The pseudo labels inevitably contain noise, and the memory regularization in Stage-II could prevent the model from overfitting to the noise in pseudo labels. Next we introduce different objectives for the model adaptation in detail. We divide the losses into two classes: (1) Domain-agnostic learning to learn the shared inter-domain features from the source domain; (2) Domain-specific learning to learn the intra-domain knowledge, especially the features for the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain-agnostic learning</head><p>Segmentation loss. First, we leverage the annotated sourcedomain data to learn the source-domain knowledge. The segmentation loss is widely applied, and could be formulated as the pixel-wise cross-entropy loss:</p><formula xml:id="formula_2">L p seg = − H h=1 W w=1 C c=1 y i s log(F p (x i s )),<label>(1)</label></formula><formula xml:id="formula_3">L a seg = − H h=1 W w=1 C c=1 y i s log(F a (x i s )),<label>(2)</label></formula><p>where the first loss is for the primary prediction, and the second objective is for the auxiliary prediction. H and W denote the height and the width of the input image, and C is the number of segmentation classes. Adversarial loss. Segmentation loss only focuses on the source domain. We demand one objective to minor the discrepancy of the target domain and the source domain, and hope that the model could transfer the source-domain knowledge to the target domain. We, therefore, introduce the adversarial loss <ref type="bibr">[Tsai et al., 2018]</ref> to minor the discrepancy of the source domain and the target domain. The adversarial loss is applied to both predictions of the primary classifier and the auxiliary classifier: <ref type="formula">(4)</ref> where D denotes the discriminator. In this work, we deploy two different discriminators, i.e., D p and D a , for the primary prediction and the auxiliary prediction, respectively. The discriminator is to find out whether the target prediction F (x t ) is close to the source prediction F (x s ) in the semantic space. By optimizing the adversarial loss, we force the model to bridge the inter-domain gap on the semantic level.</p><formula xml:id="formula_4">L p adv = E[log(D p (F p (x i s ))) + log(1 − D p (F p (x j t )))], (3) L a adv = E[log(D a (F a (x i s ))) + log(1 − D a (F a (x j t )))],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain-specific learning</head><p>However, the segmentation loss and the adversarial loss do not solve the intra-domain inconsistency, especially in the target domain. In the Stage-I, we consider leveraging the uncertainty in the target domain and propose the memory regularization in vivo to enforce the consistency. In the Stage-II, we further utilize the memory to regularize the training and prevent the model overfitting to the noisy pseudo labels. Memory regularization. In this paper, we argue that the model itself could be viewed as one kind of memory module, in that the model memorizes the historical experience. Without introducing extra parameters or external modules, we enforce the model to learn from itself. In particular, we view the input image as the key, and the model as the memory module. Given the input image (key), the model could generate the value by simply feeding forward the key. We could obtain two values by the primary classifier and the auxiliary classifier, respectively. To minor the uncertainty of the model learning on the target domain, we hope that the two values of the same key could be as close to each other as possible, so we deploy the KL-divergence loss:</p><formula xml:id="formula_5">L mr = − H h=1 W w=1 C c=1 F a (x i t ) log(F p (x i t )) − H h=1 W w=1 C c=1 F p (x i t ) log(F a (x i t )).<label>(5)</label></formula><p>We only apply the memory regularization loss on the target domain X t and ask the mapping functions F a and F p to generate a consistent prediction on the unlabeled target data. Discussion. 1. What is the advantage of the memory regularization? By using the memory regularization, we enable the model to learn the intra-domain knowledge on the unlabeled target data with an explicit and complementary objective. As discussed in the <ref type="bibr">[Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2018]</ref>, we could not ensure that the memory always provides a right class prediction for the unlabeled data. The memory mechanism is more likely to act as a teacher model, providing the class distribution based on the historical experience. 2.Will the auxiliary classifier hurt the primary classifier? As shown in many semi-supervised methods <ref type="bibr" target="#b15">[Zhang et al., 2018b;</ref><ref type="bibr">Tarvainen and Valpola, 2017]</ref>, the bad-student model also could provide essential information for the top-student models. Our experiment also verifies that the sub-optimal auxiliary classifier could help the primary classifier learning, and vice versa (see Section 4.2). Self-training with pseudo labels. In the Stage-II, we do not use the source data anymore. The model is fine-tuned on the unlabeled target data and mine the target domain knowledge. Following the self-training policy in <ref type="bibr" target="#b16">[Zou et al., 2018;</ref><ref type="bibr" target="#b16">Zou et al., 2019]</ref>, we retrain the model with the pseudo labelŷ j t . The pseudo label combines the output of F p (x j t ) and F a (x j t ) from the trained model in the Stage-I. In particular, we set theŷ j t = arg max(F p (x j t ) + 0.5F a (x j t )). The pseudo segmentation loss could be formulated as:</p><formula xml:id="formula_6">L p pseg = − H h=1 W w=1 C c=1ŷ j t log(F p (x j t )),<label>(6)</label></formula><formula xml:id="formula_7">L a pseg = − H h=1 W w=1 C c=1ŷ j t log(F a (x j t )).<label>(7)</label></formula><p>We apply the pixel-wise cross-entropy loss as the supervised segmentation loss. Since most pseudo labels are correct, the model still could learn from the noisy labels. In Section 4.2, we show the self-training with pseudo labels further boosts the performance on the target domain despite the noise in pseudo labels. Discussion. What is the advantage of the memory regularization in the Stage-II? In fact, we treat the pseudo labels as the supervised annotations in the Stage-II. However, the pseudo labels contain the noise and may mislead the model to overfit the noise. The proposed memory regularization in the Stage-II works as a smoothing term, which enforces the consistency in the model prediction, rather than focusing on fitting the pseudo label extremely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>We integrate the above-mentioned losses. The total loss of the Stage-I and Stage-II training could be formulated as:</p><formula xml:id="formula_8">L S1 (F a , F p , D a , D p ) = L seg + L adv + λ mr L mr , (8) L S2 (F a , F p ) = L pseg + λ mr L mr ,<label>(9)</label></formula><p>where λ mr is the weight for the memory regularization. We follow the setting in PSPNet [Zhao et al., 2017] to set 0.5 for segmentation losses on the auxiliary classifier. L seg = L p seg + 0.5L a seg , L pseg = L p pseg + 0.5L a pseg . For adversarial losses, we follow the setting in <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b8">Luo et al., 2019b]</ref>, and select small weights for adversarial loss terms L adv = 0.001L p adv + 0.0002L a adv . Besides, we fix the weight of memory regularization as λ mr = 0.1 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>Network Architectures.</p><p>We deploy the widely-used Deeplab-v2 <ref type="bibr" target="#b1">[Chen et al., 2017]</ref> as the baseline model, which adopts the ResNet-101 <ref type="bibr" target="#b2">[He et al., 2016]</ref> as the backbone model. Since the auxiliary classifier has been widely adopted in the scene segmentation frameworks, such as PSPNet <ref type="bibr" target="#b15">[Zhao et al., 2017]</ref> and modified DeepLab <ref type="bibr">[Tsai et al., 2018;</ref><ref type="bibr" target="#b8">Luo et al., 2019b]</ref>, for fair comparison, we also applied the auxiliary classifier in our baseline model as well as the final full model. We also insert the dropout layer before the classifier layer, and the dropout rate is 0.1. Besides, we follow the PatchGAN <ref type="bibr" target="#b6">[Isola et al., 2017]</ref> and deploy the multi-scale discriminator model. Implementation Details. The input image is resized to 1280 × 640, and we randomly crop 1024 × 512 for training. We deploy the SGD optimizer with the batch size 2 for the segmentation model, and the initial learning rate is set to 0.0002. The optimizer of the discriminator is Adam and the learning rate is set to 0.0001. Following <ref type="bibr" target="#b15">[Zhao et al., 2017;</ref><ref type="bibr">Zhang et al., 2019b]</ref>, both segmentation model and discriminator deploy the ploy learning rate decay by multiplying the factor (1 − iter total−iter ) 0.9 . We set the total iteration as 100k iteration and adopt the early-stop policy. The model is first trained without the memory regularization for 10k to avoid the initial prediction noise, and then we add the memory regularization to the model training. For Stage-I, we train the model with 25k iterations. We further finetune the model in the Stage-II for 25k iterations. We also adopt the class balance policy in the <ref type="bibr" target="#b16">[Zou et al., 2018]</ref> to increase the weight of the rare class, and the small-scale objects. When inference, we combine the outputs of both classifiersŷ j t = arg max(F p (x j t ) + 0.5F a (x j t )). Our implementation is based on Pytorch. We will release our code for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metric</head><p>We mainly evaluate the proposed method on the two unsupervised scene adaption settings, i.e., GTA5 <ref type="bibr">[Richter et</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Effect of the memory regularization. To investigate how the memory helps both classifiers, we report the results of the single classifier in <ref type="table" target="#tab_0">Table 1</ref>. The observation suggests two points: First, memory regularization helps both classifier learning and improves the performance of both classifiers, especially the auxiliary classifier. Second, the accuracy of the primary classifier prediction does not decrease due to the relatively poor results of the auxiliary classifier. The primary classifier also increases by 2.18% mIoU. It verifies that the proposed memory regularization helps to reduce the inconsistency and mine intra-domain knowledge. Furthermore, we report the results of the full model after Stage-I training, which combines the predictions of both classifiers. The full model arrives 45.46% mIoU accuracy, which is slightly higher than the prediction accuracy of the primary classifier. It also indicates that the predictions of the auxiliary classifier and primary classifier are complementary. Effect of different losses in Stage-I. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the full model could improve the performance from 37.23% to 45.46% mIoU. When only using the adversarial loss L adv , the model equals to the widely-used domain adaptation method <ref type="bibr">[Tsai et al., 2018]</ref>. We note that the model only using the memory regularization L mr also achieves significant improvement comparing to the baseline model without adaption. We speculate that the memory regularization helps to mine     <ref type="bibr" target="#b16">[Zou et al., 2018;</ref><ref type="bibr" target="#b16">Zou et al., 2019]</ref>. However, this line of previous methods usually demands a well-designed threshold for the label confidence. In contrast, we do not introduce any threshold, but apply the memory regularization to prevent overfitting toward noisy labels. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the full model with memory regularization arrives 48.31% mIoU accuracy, which is higher than the result of the model only trained on the pseudo labels. It verifies that the proposed memory regularization also helps the model learning from noisy labels. Hyperparameter Analysis. In this work, we introduce λ mr as the weight of the memory regularization. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we evaluate different weight values {0, 0.01, 0.05, 0.1, 0.2, 0.5}. We observe that the model is robust to the value of λ mr . However, when the value λ mr is too large or small, the model may mislead to overfitting or underfitting the consistency. Therefore, without loss of generality, we use λ mr = 0.1 for all experiments.</p><formula xml:id="formula_9">- - - - - - - - - - - - - - - - - - 29.2 FCAN [Zhang et al., 2018a] - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with state-of-the-art methods</head><p>Synthetic-to-real. We compare the proposed method with different domain adaptation methods on GTA5 → Cityscapes (See <ref type="table" target="#tab_6">Table 4</ref>). For a fair comparison, we mainly show the results based on the same backbone, i.e., DeepLabv2.</p><p>The proposed method has achieved 48.3% mIoU, which is higher than the competitive methods, e.g., pixel-level alignment <ref type="bibr" target="#b4">[Hoffman et al., 2018]</ref>, semantic level alignment <ref type="bibr">[Tsai et al., 2018]</ref>, as well as the self-training methods, i.e., <ref type="bibr" target="#b16">[Zou et al., 2018;</ref><ref type="bibr" target="#b16">Zou et al., 2019]</ref>. Compared with the strong source-only model, the proposed method yields +11.1% improvement. Besides, we observe a similar result on SYN-THIA → Cityscapes (see <ref type="table" target="#tab_7">Table 5</ref>). The proposed method arrives 53.8% mIoU* and 46.5% mIoU, which is also competitive to other methods. We obtain +11.3% improvement in mIoU accuracy over the baseline.</p><p>Cross-city. We also evaluate the proposed method on adapting the model between different cities. The two real datasets, i.e., Cityscapes and Oxford RobotCar, are different from collection locations as well as weather conditions. Cityscapes is collected in the sunny days when Oxford RobotCar contains rainy scenarios. As shown in <ref type="table" target="#tab_9">Table 6</ref>, the proposed method also achieves competitive results, i.e., 73.9% mIoU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a memory regularization method for unsupervised scene adaption. Our model leverages the intra-domain knowledge and reduces the uncertainty of model learning. Without introducing any extra parameters or external modules, we deploy the model itself as the memory module to regularize the training. Albeit simple, the proposed method is complementary to previous works and achieves competitive results on two synthetic-to-real benchmarks, i.e., GTA5 → Cityscapes and SYNTHIA → Cityscapes, and one cross-city benchmark, i.e., Cityscapes → Oxford RobotCar.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different Memory-based Methods: (a) MA-DNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed framework. In the Stage-I, we train the model with the source domain input x i s and the target domain input x i t to learn the inter-domain and intra-domain knowledge. In the Stage-II, the model focus on the target-domain data and is further fine-tuned with pseudo labels. The proposed memory regularization Lmr is applied to regularize the model training in both stages, yielding the performance improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sensitivity of mIoU to the hyper-parameter λmr on Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of the memory regularization on both classifiers, i.e., the auxiliary classifier and the primary classifier, in the Stage-I training. The result suggests that the memory regularization helps both classifiers, especially the auxiliary classifier. The final results of the full model combine the results of both classifiers, and therefore improve the performance further.</figDesc><table><row><cell>Method</cell><cell>without Lmr</cell><cell>with Lmr</cell></row><row><cell>Auxiliary Classifier</cell><cell>40.04</cell><cell>44.45</cell></row><row><cell>Primary Classifier</cell><cell>43.11</cell><cell>45.29</cell></row><row><cell>Ours (Stage-I)</cell><cell>42.73</cell><cell>45.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of different losses in the Stage-I training. We gradually add the adversarial loss L adv and the memory regularization Lmr into consideration.</figDesc><table><row><cell>Method</cell><cell>Lpseg</cell><cell>Lmr</cell><cell>mIoU</cell></row><row><cell>Ours (Stage-I)</cell><cell></cell><cell></cell><cell>45.46</cell></row><row><cell>Pseudo Label</cell><cell></cell><cell></cell><cell>47.90</cell></row><row><cell>Ours (Stage-II)</cell><cell></cell><cell></cell><cell>48.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of different losses in the Stage-II training. The result suggests that the memory regularization could prevent the model from overfitting to the noise in the pseudo labels.</figDesc><table><row><cell>al., 2016] → Cityscapes [Cordts et al., 2016] and SYN-</cell></row><row><cell>THIA [Ros et al., 2016] → Cityscapes [Cordts et al., 2016].</cell></row><row><cell>Both source datasets, i.e., GTA5 and SYNTHIA, are the</cell></row><row><cell>synthetic datasets. GTA5 contains 24, 966 training images,</cell></row><row><cell>while SYNTHIA has 9, 400 images for training. The target</cell></row><row><cell>dataset, Cityscapes, is collected in the realistic scenario, in-</cell></row><row><cell>cluding 2, 975 unlabeled training images. Besides, we also</cell></row><row><cell>evaluate the proposed method on the cross-city benchmark:</cell></row><row><cell>Cityscapes [Cordts et al., 2016] → Oxford RobotCar [Mad-</cell></row><row><cell>dern et al., 2017]. We follow the setting in [Tsai et al.,</cell></row><row><cell>2019] and evaluate the model on the Cityscapes validation</cell></row><row><cell>set/ RobotCar validation set. For the evaluation metric, we re-</cell></row><row><cell>port the mean Intersection over Union (mIoU), averaged over</cell></row><row><cell>all classes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on GTA5 → Cityscapes. We present pre-class IoU and mIoU. The best accuracy in every column is in bold.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="5">Road SW Build Wall* Fence* Pole* TL</cell><cell>TS Veg. Sky</cell><cell cols="5">PR Rider Car Bus Motor Bike mIoU* mIoU</cell></row><row><cell>Source MCD [Saito et al., 2018]</cell><cell>DRN-105</cell><cell>14.9 11.4 58.7 84.8 43.6 79.0</cell><cell>1.9 3.9</cell><cell>0.0 0.2</cell><cell>24.1 29.1</cell><cell>1.2 7.2</cell><cell cols="2">6.0 68.8 76.0 54.3 5.5 83.8 83.1 51.0 11.7 79.9 27.2 7.1 34.2 15.0</cell><cell>0.8 6.2</cell><cell>0.0 0.0</cell><cell>26.8 43.5</cell><cell>23.4 37.3</cell></row><row><cell>Source</cell><cell></cell><cell>55.6 23.8 74.6</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3</cell><cell>13.7</cell><cell>25.0</cell><cell>38.6</cell><cell>−</cell></row><row><cell>SIBAN [Luo et al., 2019a]</cell><cell></cell><cell>82.5 24.0 79.4</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">16.5 12.7 79.2 82.8 58.3 18.0 79.3 25.3</cell><cell>17.6</cell><cell>25.9</cell><cell>46.3</cell><cell>−</cell></row><row><cell>PatchAlign [Tsai et al., 2019] AdaptSegNet [Tsai et al., 2018]</cell><cell>DeepLabv2</cell><cell>82.4 38.0 78.6 84.3 42.7 77.5</cell><cell>8.7 −</cell><cell>0.6 −</cell><cell>26.0 −</cell><cell cols="3">3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6 4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2</cell><cell>19.3 18.9</cell><cell>31.7 32.3</cell><cell>46.5 46.7</cell><cell>40.0 −</cell></row><row><cell>CLAN [Luo et al., 2019b]</cell><cell></cell><cell>81.3 37.0 80.1</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9</cell><cell>22.6</cell><cell>30.7</cell><cell>47.8</cell><cell>−</cell></row><row><cell>APODA [Yang et al., 2020]</cell><cell></cell><cell>86.4 41.3 79.3</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">22.6 17.3 80.3 81.6 56.9 21.0 84.1 49.1</cell><cell>24.6</cell><cell>45.7</cell><cell>53.1</cell><cell>−</cell></row><row><cell>AdvEnt [Vu et al., 2019]</cell><cell>DeepLabv2</cell><cell>85.6 42.2 79.7</cell><cell>8.7</cell><cell>0.4</cell><cell>25.9</cell><cell>5.4</cell><cell cols="2">8.1 80.4 84.1 57.9 23.8 73.3 36.4</cell><cell>14.2</cell><cell>33.0</cell><cell>48.0</cell><cell>41.2</cell></row><row><cell>Source</cell><cell></cell><cell>64.3 21.3 73.1</cell><cell>2.4</cell><cell>1.1</cell><cell>31.4</cell><cell cols="3">7.0 27.7 63.1 67.6 42.2 19.9 73.1 15.3</cell><cell>10.5</cell><cell>38.9</cell><cell>40.3</cell><cell>34.9</cell></row><row><cell>CBST [Zou et al., 2018]</cell><cell>DeepLabv2</cell><cell>68.0 29.9 76.3</cell><cell>10.8</cell><cell>1.4</cell><cell cols="4">33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5</cell><cell>18.8</cell><cell>39.8</cell><cell>48.9</cell><cell>42.6</cell></row><row><cell>MRKLD [Zou et al., 2019]</cell><cell></cell><cell>67.7 32.2 73.9</cell><cell>10.7</cell><cell>1.6</cell><cell cols="4">37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0</cell><cell>19.4</cell><cell>45.3</cell><cell>50.1</cell><cell>43.8</cell></row><row><cell>Source</cell><cell></cell><cell>44.0 19.3 70.9</cell><cell>8.7</cell><cell>0.8</cell><cell cols="4">28.2 16.1 16.7 79.8 81.4 57.8 19.2 46.9 17.2</cell><cell>12.0</cell><cell>43.8</cell><cell>40.4</cell><cell>35.2</cell></row><row><cell>Ours (Stage-I)</cell><cell>DeepLabv2</cell><cell>82.0 36.5 80.4</cell><cell>4.2</cell><cell>0.4</cell><cell cols="4">33.7 18.0 13.4 81.1 80.8 61.3 21.7 84.4 32.4</cell><cell>14.8</cell><cell>45.7</cell><cell>50.2</cell><cell>43.2</cell></row><row><cell>Ours (Stage-II)</cell><cell></cell><cell>83.1 38.2 81.7</cell><cell>9.3</cell><cell>1.0</cell><cell cols="4">35.1 30.3 19.9 82.0 80.1 62.8 21.1 84.4 37.8</cell><cell>24.5</cell><cell>53.3</cell><cell>53.8</cell><cell>46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on SYNTHIA → Cityscapes. We present pre-class IoU, mIoU and mIoU*. mIoU and mIoU* are averaged over 16 and 13 categories, respectively. The best accuracy in every column is in bold.the target domain knowledge, yielding the better performance on the target domain. After combining all three loss terms, the full model arrives 45.46% mIoU on Cityscapes. Effect of different losses in Stage-II. If we only deploy the pseudo segmentation loss L pseg , the model equals to several previous self-training methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>49.3 73.1 55.6 37.3 36.1 54.0 81.3 49.7 61.9 AdaptSegNet [Tsai et al., 2018] 95.1 64.0 75.7 61.3 35.5 63.9 58.1 84.6 57.0 69.5 PatchAlign [Tsai et al., 2019] 94.4 63.5 82.0 61.3 36.0 76.4 61.0 86.5 58.6 72.0 Ours (Stage-I) 95.9 73.5 86.2 69.3 31.9 87.3 57.9 88.8 61.5 72.5 Ours (Stage-II) 95.1 72.5 87.0 72.2 37.4 87.9 63.4 90.5 58.9 73.9</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>light</cell><cell>sign</cell><cell>sky</cell><cell>person</cell><cell>automobile</cell><cell>two-wheel</cell><cell>mIoU</cell></row><row><cell>Without Adaptation</cell><cell>79.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results on the cross-city benchmark: Cityscapes → Oxford RobotCar.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Self-ensembling for visual domain adaptation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hoffman</surname></persName>
		</author>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain transfer through deep activation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila ; Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
	<note>Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<idno type="arXiv">arXiv:1907.12282</idno>
	</analytic>
	<monogr>
		<title level="m">NeurlPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<idno type="arXiv">arXiv:1901.05427</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Domain adaptation for structured output via discriminative representations</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classspecific reconstruction transfer learning for visual recognition across domains</title>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<idno>arXiv:1904.06268</idno>
	</analytic>
	<monogr>
		<title level="m">Sumit Chopra, and Antoine Bordes. Memory networks</title>
		<meeting><address><addrLine>Jason Weston</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ace: Adapting to changing environments for semantic segmentation. Yang et al., 2020. An adversarial perturbation oriented domain adaptation approach for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulationto-real generalization without accessing target domain data</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Manifold criterion guided transfer learning via intermediate domain generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Zhang et al., 2019b] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, and Philip HS Torr</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
