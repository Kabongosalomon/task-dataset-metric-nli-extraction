<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
							<email>chaitanya.joshi@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
							<email>tlaurent@lmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Loyola Marymount University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@mila.quebec</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<addrLine>4 CIFAR</addrLine>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xbresson@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework 2 , with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets. * Equal contribution 2 https://github.com/graphdeeplearning/benchmarking-gnns arXiv:2003.00982v3 [cs.LG] 3 Jul 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last few years, graph neural networks (GNNs) have seen a great surge of interest with promising methods being developed for myriad of domains including chemistry <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, physics <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b72">73]</ref>, social sciences <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">65]</ref>, knowledge graphs <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b15">16]</ref>, recommendation <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b90">91]</ref>, and neuroscience <ref type="bibr" target="#b31">[32]</ref>. Historically, three classes of GNNs have been developed. The first models <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b32">33]</ref> aimed at extending the original convolutional neural networks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> to graphs. The second class enhanced the original models with anisotropic operations on graphs <ref type="bibr" target="#b68">[69]</ref>, such as attention and gating mechanisms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b10">11]</ref>. The recent third class has introduced GNNs that improve upon theoretical limitations of previous models <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b76">77]</ref>. Specifically, the first two classes can only differentiate simple non-isomorphic graphs and cannot separate automorphic nodes. Developing powerful and theoretically expressive GNN architectures is a key concern towards practical applications and real-world adoption of graph machine learning. However, tracking recent progress has been challenging as most models are evaluated on small datasets such as Cora, Citeseer and TU, which are inappropriate to differentiate complex, simple and graph-agnostic architectures <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17]</ref>, and do not have consensus on a unifying experimental setting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b86">87]</ref>.</p><p>Consequently, our motivation is to benchmark GNNs to identify and quantify what types of architectures, first principles or mechanisms are universal, generalizable, and scalable when we move to larger and more challenging datasets. Benchmarking provides a strong paradigm to answer these fundamental questions. It has proved to be beneficial for driving progress, identifying essential ideas, and solving domain-specific problems in several areas of science <ref type="bibr" target="#b85">[86]</ref>. Recently, the famous 2012 ImageNet challenge <ref type="bibr" target="#b23">[24]</ref> has provided a benchmark dataset that has triggered the deep learning revolution <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55]</ref>. Nevertheless, designing successful benchmarks is highly challenging as it requires both a coding framework with a rigorous experimental setting for fair comparisons, all while being reproducible, as well as using appropriate datasets that can statistically separate model performance. The lack of benchmarks has been a major issue in GNN literature as the aforementioned requirements have not been rigorously enforced.</p><p>A major contribution of this work is to design a benchmark infrastructure that can fairly evaluate GNN architectures on medium-scale datasets. Specifically, the coding infrastructure can be used to implement new GNNs from the most popular and theoretically designed classes of GNNs, and compare their performance in a rigorous manner. For nomenclature, we refer to the popular messagepassing GNNs as graph convolutional networks (GCNs) and the theoretically expressive GNNs as Weisfeiler-Lehman GNNs (WL-GNNs), see Section 2.2. The main findings of our extensive numerical experiments are presented in Section 4, and summarized below:</p><p>• Message-passing GCNs <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b10">11]</ref> are able to better leverage the basic building blocks of deep learning such as batching, residual connections and normalization, outperforming theoretically designed WL-GNNs <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b17">18]</ref> on the 7 datasets considered in this paper.</p><p>• Theoretically designed WL-GNNs such as <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b17">18]</ref> are prohibitive in terms of time/space complexity and not amenable to batched training, suggesting the need for additional developments for these models to be competitive with GCNs on practical tasks. In contrast, GCNs rely on sparse matrix computations, which are computationally and memory efficient.</p><p>• Overall, anisotropic GCNs which leverage attention <ref type="bibr" target="#b79">[80]</ref> and gating <ref type="bibr" target="#b10">[11]</ref> mechanisms perform consistently across graph, node and edge-level tasks, improving over isotropic GCNs on 5 out of 7 datasets. Additionally, for link prediction tasks, learning features for edges as joint representations of incident nodes during message passing significantly boosts performance. Their consistent results suggest further analysis on the expressivity of anisotropic models.</p><p>• Graph positional encodings with Laplacian eigenvectors <ref type="bibr" target="#b8">[9]</ref> are an elegant approach to overcome the theoretical limitation of low structural expressivity of GCNs <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b76">77]</ref>, and boost performance on 3 out of 4 datasets without positional information.</p><p>Our benchmarking framework and medium-scale datasets are open-sourced via GitHub to enable researchers to seamlessly explore new ideas in graph representation learning and track the progress of GNN architectures.</p><p>2 Proposed GNN Benchmarking Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coding Infrastructure</head><p>Designing successful benchmarks requires a modular coding framework which is experimentally rigorous and reproducible for fair comparisons. However, recent literature on GNNs does not have a consensus on training, validation and test splits as well as evaluation protocols, making it unfair to compare the performance of new ideas and architectures <ref type="bibr" target="#b25">[26]</ref>. Additionally, different hyperparameters, loss functions and learning rate schedules make it difficult to identify new advances in architectures. It is also unclear how to perform good data splits on graphs beyond randomizes splits, which are known to provide over-optimistic predictions <ref type="bibr" target="#b52">[53]</ref>. A unifying experimental setting is much needed given the heterogeneity in GNN evaluation procedures.</p><p>Our benchmarking infrastructure builds upon PyTorch <ref type="bibr" target="#b67">[68]</ref> and DGL <ref type="bibr" target="#b84">[85]</ref>, and has been developed with the following fundamental objectives: (a) Ease-of-use and modularity, enabling new users to experiment and study the building blocks of GNNs; (b) Experimental rigour and fairness for all models being benchmarked; and (c) Being future-proof and comprehensive for tracking the progress of graph machine learning tasks and new GNNs. At a high level, our benchmark unifies independent components for: (i) Data pipelines; (ii) GNN layers and models; (iii) Training and evaluation functions; (iv) Network and hyperparameter configurations; and (v) Scripts for reproducibility. We believe that a standardized framework can be of immense help to the community, enabling researchers to explore new ideas at any stage of the pipeline without setting up everything else. We direct readers to the README user manual included in our GitHub repository for detailed instructions on using the coding infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>We benchmark two broad classes of GNNs. Firstly, we consider the widely used message passingbased graph convolutional networks (GCNs), which update node representations from one layer to the other according to the formula: h +1 i = f (h i , {h j } j∈Ni ). Note that the update equation is local, only depending on the neighborhood N i of node i, and independent of graph size, making the space/time complexity O(E) reducing to O(n) for sparse graphs. Thus, GCNs are highly parallelizable on GPUs and are implemented via sparse matrix multiplications in modern graph machine learning frameworks <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b26">27]</ref>. GCNs draw parallels to ConvNets for computer vision <ref type="bibr" target="#b49">[50]</ref> by considering a convolution operation with shared weights across the graph domain. We instantiate a class of isotropic GCNs when the node update equation treats every "edge direction" equally, i.e. each neighbor contributes equally to the update of the central node by receiving the same weight value:</p><formula xml:id="formula_0">h +1 i = σ W 1 h i + j∈Ni W 2 h j , h , h +1 ∈ R n×d , W 1,2 ∈ R d×d ,<label>(1)</label></formula><p>where σ is a non-linear point-wise activation like ReLU. Popular isotropic GCNs include vanilla GCNs-Graph Convolutional Networks <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b43">44]</ref> and GraphSage <ref type="bibr" target="#b32">[33]</ref>. On the other hand, when the update equation treats every edge direction differently, we instantiate anisotropic GCNs:</p><formula xml:id="formula_1">h +1 i = σ W 1 h i + j∈Ni η ij W 2 h j , h , h +1 ∈ R n×d , W 1,2 ∈ R d×d ,<label>(2)</label></formula><p>where η ij = f (h i , h j ) and f is a parameterized function whose weights are learned during training. The η ij can be scalars or vectors. In the latter case the multiplication between the term η ij and the term W 2 h j should be understood as element-wise multiplication. MoNet-Gaussian Mixture Model Networks <ref type="bibr" target="#b62">[63]</ref>, GatedGCN-Graph Convolutional Networks <ref type="bibr" target="#b10">[11]</ref>, and GAT-Graph Attention Networks <ref type="bibr" target="#b79">[80]</ref> propose edge weights based on GMMs, gating mechanism and sparse attention for computing η ij , respectively.</p><p>The second class we investigate is the recent Weisfeiler-Lehman GNNs based on the WL test <ref type="bibr" target="#b87">[88]</ref>. Authors in <ref type="bibr" target="#b88">[89]</ref> introduced GIN-Graph Isomorphism Network, a provable 1-WL GNN, which can distinguish two non-isomorphic graphs w.r.t. the 1-WL test. Higher k-WL isomorphic tests lead to more discriminative k-WL GNNs in <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b57">58]</ref>. However, k-WL GNNs require the use of tensors of rank k, which is intractable in practice for k &gt; 2. As a result, <ref type="bibr" target="#b57">[58]</ref> proposed a model, namely 3-WL GNNs, that uses rank-2 tensors while being 3-WL provable. This 3-WL model improves the space/time complexities of <ref type="bibr" target="#b65">[66]</ref> from O(n 3 )/O(n 4 ) to O(n 2 )/O(n 3 ) respectively. The layer update equation of 3-WL GNNs is defined as:</p><formula xml:id="formula_2">h +1 = Concat M W 1 (h ).M W 2 (h ), M W 3 (h ) , h , h +1 ∈ R n×n×d , W 1,2,3 ∈ R d×d×2 ,<label>(3)</label></formula><p>where M W are 2-layer MLPs applied to the feature dimension. Authors in <ref type="bibr" target="#b17">[18]</ref> proposed RingGNNs, which also use rank-2 tensors and achieve higher learning capacity than 2-WL GNNs. The layer update equation of RingGNN is:</p><formula xml:id="formula_3">h +1 = σ w 1 L W 1 (h ) + w 2 L W 2 (h ).L W 3 (h ) , h , h +1 ∈ R n×n×d , W 1,2,3 ∈ R d×d×17 ,<label>(4)</label></formula><p>and w 1,2 ∈ R. This model uses the equivariant linear layer L W defined in <ref type="bibr" target="#b58">[59]</ref> as</p><formula xml:id="formula_4">L W (h) ·,·,k = 17 i=1 d j=1 W i,j,k L i (h) ·,·,j , where {L i } 15 i=1</formula><p>is the set of all basis functions for all linear equivariant functions from R n×n → R n×n and {L i } 17 i=16 are the basis for the bias terms. RingGNNs have the same space/time complexities as 3-WL GNNs. We refer the readers to the supplementary material for detailed formulations of these models.</p><p>All GCNs can be upgraded with basic building blocks of deep networks, i.e. residual connections <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref> and batch normalization <ref type="bibr" target="#b35">[36]</ref>. We discuss batch normalization and residual connections for WL-GNNs in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Datasets</head><p>Issues with Prevalent Datasets. New ideas in the field of GNNs have mostly been evaluated on the realistic but small scale Cora <ref type="bibr" target="#b60">[61]</ref>, Citeseer <ref type="bibr" target="#b29">[30]</ref> and TU datasets <ref type="bibr" target="#b40">[41]</ref>. For example, Cora is a single graph of 2.7K nodes, TU-IMDB has 1.5K graphs with 13 nodes on average and TU-MUTAG has 188 molecules with 18 nodes. Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures. As mentioned previously, another major issue with prevalent datasets is the lack of reproducibility of experimental results. Most published papers do not use the same train-validation-test split <ref type="bibr" target="#b25">[26]</ref>. Besides, even for the same split, the performance of GNNs present a large standard deviation on a regular 10-fold cross-validation due to the small size, see supplementary material.</p><p>At the same time, collecting representative, realistic and large scale graph datasets presents several challenges. It is unclear what theoretical tools can define the quality of a dataset or validate its statistical representativeness for a given task. Additionally, there are several arbitrary choices when preparing graphs, such as node and edge features. For example, e-commerce product features can be given by a specialized bag-of-words, or computed from word embeddings from the title as well as description. Finally, it is unclear how to classify dataset size/scale as the appropriate size may depend on the complexity of the task as well as the dimensionality and statistics of underlying data. Very large graph datasets also present a computational challenge and require extensive GPU resources to be studied <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b70">71]</ref>. The recent Open Graph Benchmark (OGB) project <ref type="bibr" target="#b86">[87]</ref> is a much needed initiative in the community to tackle these challenges. OGB offers a collection of medium-scale real-world graph machine learning datasets and evaluation protocols, with an emphasis on out-of-distribution generalization performance through meaningful data splits.</p><p>Proposed Datasets. In our benchmark, we define appropriate datasets as those that are able to statistically separate the performance of GNNs. It is important to note that small datasets like Cora, Citeseer and TU datasets do not fulfil this requirement, as all GNNs perform almost statistically the same. <ref type="table" target="#tab_0">Table 1</ref> presents a summary of 7 medium-scale datasets and one small-scale dataset included in our benchmarking framework. We cover the four most fundamental supervised graph machine learning tasks <ref type="bibr" target="#b14">[15]</ref>: graph regression (ZINC), graph classification (MNIST, CIFAR10, CSL), node classification (PATTERN, CLUSTER), and link prediction (TSP, COLLAB), coming from the domains of chemistry, mathematical modelling, computer vision, combinatorial optimization, and social networks. Four datasets (PATTERN, CLUSTER, TSP, CSL) are artificially generated, two datasets (MNIST, CIFAR10) are semi-artificial, and two (ZINC, COLLAB) are real-world datasets. The dataset sizes in terms of total number of nodes vary between 0.27M to 7M.</p><p>Relevance. ZINC <ref type="bibr" target="#b36">[37]</ref> is one of the most popular real-world molecular dataset of 250K graphs, out of which we randomly select 12K for efficiency. We consider the task of graph property regression for contrained solubility, an important chemical property for designing generative GNNs for molecules <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b91">92]</ref>. PATTERN and CLUSTER are node classification tasks generated with Stochastic Block Models <ref type="bibr" target="#b0">[1]</ref>, which are widely used to model communities in social networks by modulating the intra-and extra-communities connections, thereby controlling the difficulty of the task. PATTERN tests the fundamental graph task of recognizing specific predetermined subgraphs (as proposed in <ref type="bibr" target="#b74">[75]</ref>) and CLUSTER aims at identifying community clusters in a semi-supervised setting <ref type="bibr" target="#b43">[44]</ref>. All SBM graphs are augmented with node features to simulate user attributes such that the tasks are more natural and not purely structural clustering tasks. MNIST <ref type="bibr" target="#b49">[50]</ref> and CIFAR10 <ref type="bibr" target="#b46">[47]</ref> are classical image classification datasets converted into graphs using so called super-pixels <ref type="bibr" target="#b1">[2]</ref> and assigning each node's features as the super-pixel coordinates and intensity. These datasets are sanity-checks, as we expect most GNNs to perform close to 100% for MNIST and well enough for CIFAR10. TSP, based on the classical Travelling Salesman Problem, tests link prediction on 2D Euclidean graphs to identify edges belonging to the optimal TSP solution given by the Concorde solver <ref type="bibr" target="#b3">[4]</ref>. TSP is the most intensely studied NP-Hard combinatorial problem with a growing body of literature on leveraging GNNs to learn better solvers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10]</ref>. COLLAB is a link prediction dataset proposed by OGB <ref type="bibr" target="#b86">[87]</ref> corresponding to a collaboration network between scientists indexed by Microsoft Academic Graph <ref type="bibr" target="#b83">[84]</ref>. The task is to predict future author collaboration relationships given past collaboration links. Lastly, CSL is a synthetic dataset introduced in <ref type="bibr" target="#b66">[67]</ref> to test the expressivity of GNNs. In particular, graphs are isomorphic if they have the same degree and the task is to classify non-isomorphic graphs.</p><p>Finally, it is worth noting that our benchmarking infrastructure is complementary to the OGB initiative, and is well-suited to integrate current and future OGB dataset and evaluation protocols, as demonstrated by the inclusion of the COLLAB dataset. Finally, CSL has 150 graphs and we follow 5-fold cross validation with stratified sampling to ensure class distribution remains the same across the splits. See supplementary for more details on generation and preparation of the datasets.</p><p>Training. We use the Adam optimizer <ref type="bibr" target="#b42">[43]</ref> with the same learning rate decay strategy for all models. An initial learning rate is selected in {10 −3 , 10 −4 } which is reduced by half if the validation loss does not improve after a fixed number of epochs, either 5 or 10. We do not set a maximum number of epochs -the training is stopped either when the learning rate has reached the small value of 10 −6 , or the computational time reaches 12 hours. We run each experiment with 4 different seeds and report the statistics of the 4 results. More details are provided in the supplementary.</p><p>Task-based network layer. The node representations generated by the final layer of GCNs, or the dense tensor obtained at the final layer of the higher order WL-GNNs, are passed to a network suffix which is usually a downstream MLP of 3 layers. For GIN, RingGNN, and 3WL-GNN, we follow the original instructions of network suffixes to consider feature outputs from each layer of the network, similar to that of Jumping Knowledge Networks <ref type="bibr" target="#b89">[90]</ref>. See supplementary material for more details.</p><p>Parameter budgets. Our goal is not to find the optimal set of hyperparameters for a specific GNN model (which is computationally expensive), but to compare and benchmark the model and/or their building blocks within a budget of parameters and a maximal computational time. Therefore, we decide on using two parameter budgets: (1) 100k parameters for each GNNs for all the tasks, and (2) 500k parameters for GNNs for which we investigate scaling a model to larger parameters and deeper layers. The number of hidden layers and hidden dimensions are selected accordingly to match these budgets, the details of which are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarking GNNs</head><p>This section highlights the main take-home messages from the experiments in <ref type="table" target="#tab_2">Tables 2, 3</ref> and 4, which evaluate the GNNs from Section 2.2 with the experimental setup described in Section 3.</p><p>Graph-agnostic NNs perform poorly. As a sanity check, we compare all GNNs to a simple graphagnostic MLP baseline which updates each node independent of one-other, h +1 i = σ W h i , and passes these features to the task-based layer. MLP presents consistently low scores across all datasets <ref type="table" target="#tab_2">(Tables 2 and 3)</ref>, which shows the necessity to use graph structure for these tasks. All proposed • MNIST, CIFAR10 use multi-label classification accuracy.</p><p>• TSP uses binary F1 score for the positive edges.</p><p>• COLLAB uses Hits@50 via the evaluator provided by OGB <ref type="bibr" target="#b86">[87]</ref>.</p><p>• ZINC uses mean absolute regression error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation:</head><p>• Models with the suffix -E use input edge features to initialize edge representations (ZINC: bond type, TSP: Euclidean distance, COLLAB: collaboration frequency and year).  datasets used in our study are appropriate to statistically separate GNN performance, which has remained an issue with the widely used but small graph datasets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>GCNs outperform WL-GNNs on the proposed datasets. Although provably powerful in terms of graph isomorphism tests and invariant function approximation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b65">66]</ref>, the recent 3WLGNNs and RingGNNs were not able to outperform GCNs for our medium-scale datasets, as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>These new models are limited in terms of space/time complexities, with O(n 2 )/O(n 3 ) respectively, not allowing them to scale to larger datasets. On the contrary, GCNs with linear complexity w.r.t. the number of nodes for sparse graphs, can scale conveniently to 16 layers and show the best performance on all datasets. 3WL-GNNs and RingGNNs face loss divergence and/or out-of-memory errors when trying to build deeper networks, see <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Anisotropic mechanisms improve GCNs. Among the models in the GCN class, the best results point towards the anisotropic models, particularly GAT and GatedGCN, which are based on sparse and dense attention mechanisms, respectively. For instance, results for ZINC, PATTERN, CLUSTER, MNIST and CIFAR in <ref type="table" target="#tab_2">Table 2</ref> show that the performance of the 100K-parameter anisotropic GNNs (GAT, MoNet, GatedGCN) are consistently better than the isotropic models (GCN, GraphSage), except for GraphSage-MNIST and MoNet-CIFAR10. <ref type="table" target="#tab_5">Table 4</ref>, discussed later, dissects and demonstrates the importance of anisotropy for the link prediction tasks, TSP and COLLAB. Overall, our results suggest that understanding the expressive power of attention-based neighborhood aggregation functions is a meaningful avenue of research.</p><p>Underlying challenges for training WL-GNNs. We consistently observe a relatively high standard deviation in the performance of WL-GNNs (recall that we average across 4 runs using 4 different seeds). We attribute this fluctuation to the absence of universal training procedures like batching and batch normalization, as these GNNs operate on dense rank-2 tensors of variable sizes. On the other hand, GCNs running on sparse tensors better leverage batched training and normalization for stable and fast training. Leading graph machine learning libraries represent batches of graphs as sparse block diagonal matrices, enabling batched training of GCNs through parallelized computation <ref type="bibr" target="#b37">[38]</ref>. Dense tensors are incompatible with the prevalent approach, disabling the use of batch normalization for WL-GNNs. We experimented with layer normalization <ref type="bibr" target="#b4">[5]</ref> but without success. We were also unable to train WL-GNNs on CPU memory for the single COLLAB graph. Practical applications of the new WL-GNNs may require redesigning the best practices and common building blocks of deep learning, i.e. batching of variable-sized data, normalization schemes, and residual connections.</p><p>3WL-GNNs perform the best among their class. Among the models in the WL-GNN class, 3WL-GNN provide better results than its similar counter-part RingGNN. The GIN model, while being less expressive, is able to scale better and provides overall good performance.</p><p>Laplacian eigenvectors as positional embeddings. Background. In <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b76">77]</ref>, it was pointed out that standard GCNs might perform poorly when dealing with graphs that exhibit some symmetries in their structures, such as node or edge isomorphism. To address this issue, authors in <ref type="bibr" target="#b66">[67]</ref> introduced a framework, called Graph Relational Pooling (GRP), that assigns to each node an identifier that depends on the index ordering. This approach can be computationally expensive as it requires to account for all n! node permutations, thus requiring some sampling in practice.</p><p>Proposition. As in <ref type="bibr" target="#b66">[67]</ref>, we keep the overall GCN architecture and simply add positional features to each node before processing the graph through the GCN. The positional features should be chosen such that nodes which are far apart in the graph have different positional features whereas nodes which are nearby have similar positional features. In <ref type="bibr" target="#b66">[67]</ref>, authors used one-hot encoding of node indices. As an alternative, we propose to use the graph Laplacian eigenvectors <ref type="bibr" target="#b8">[9]</ref>, which have less ambiguities and which better describe the distance between nodes on the graph. Formally, Laplacian eigenvectors are spectral techniques that embed the graphs into the Euclidean space. These vectors form a meaningful local coordinate system, while preserving the global graph structure. Mathematically, they are defined via the factorization of the graph Laplacian matrix;</p><formula xml:id="formula_5">∆ = I − D −1/2 AD −1/2 = U T ΛU,<label>(5)</label></formula><p>where A is the n × n adjacency matrix, D is the degree matrix, and Λ, U correspond respectively to the eigenvalues and eigenvectors. Laplacian eigenvectors also represent a natural generalization of the Transformer <ref type="bibr" target="#b78">[79]</ref> positional encodings (PE) for graphs as the eigenvectors of a discrete line (NLP graph) are the cosine and sinusoidal functions. The computational complexity O(E 3/2 ), with E being the number of edges, can be improved with, e.g. the Nystrom method <ref type="bibr" target="#b27">[28]</ref>. The eigenvectors are defined up to the factor ±1 (after being normalized to unit length), so the sign of eigenvectors will be randomly flipped during training. For the experiments, we use the k smallest non-trivial eigenvectors, where the k value is given in <ref type="table" target="#tab_2">Table 2</ref>. The smallest eigenvectors provide smooth encoding coordinates of neighboring nodes. See Section D in the supplementary for a discussion about positional encodings.  Analysis. First, we study the usefulness of these PE with CSL, a mathematical dataset introduced in <ref type="bibr" target="#b66">[67]</ref> to demonstrate the failure of GCNs to provide meaningful node representations for highly automorphic graphs. <ref type="table" target="#tab_4">Table 3</ref> compares the GCNs using the Laplacian eigenvectors as PE and the WL-GNNs. The GCN models were the most accurate with 99% of mean accuracy, while 3WL-GNN obtained 97% and RingGNN 25% with our experimental setting. Then, we study ZINC, PATTERN, CLUSTER and COLLAB with PE (note that MNIST, CIFAR10 and TSP do not need PE as the nodes in these graphs already have features describing their positions in R 2 ). We observe a boost of performance for ZINC and CLUSTER (it was expected as eigenvectors are good indicators of clusters <ref type="bibr" target="#b81">[82]</ref>), an improvement for PATTERN, and statistically the same result for COLLAB, see <ref type="table" target="#tab_2">Table 2</ref>. As a future work, we plan to compare with the recent technique <ref type="bibr" target="#b92">[93]</ref> which uses GNNs to learn simultaneously node structural and positional encodings.</p><p>Edge representations improve link prediction. Context. The TSP and COLLAB edge classification tasks present an interesting empirical result for GCNs: Isotropic models (GCN, GraphSage) are consistently outperformed by their Anisotropic counterparts which use joint representations of adjacent nodes as edge features during aggregation (GAT, GatedGCN). In <ref type="table" target="#tab_5">Table 4</ref>, we systematically study the impact of anisotropy by instantiating three variants of GAT and GatedGCN:</p><p>(1) Isotropic aggregation (such as vanilla GCNs <ref type="bibr" target="#b43">[44]</ref>) with node updates of the form: <ref type="table" target="#tab_5">Table 4</ref>;</p><formula xml:id="formula_6">h +1 i = σ j∈Ni W h j , identified by (E.Feat,E.Repr=x,x) in</formula><p>(2) Anisotropy using edge features (such as GAT by default <ref type="bibr" target="#b79">[80]</ref>) with node updates as:</p><formula xml:id="formula_8">h +1 i = σ j∈Ni f V (h i , h j ) · W h j , with (E.Feat,E.Repr= ,x);<label>(7)</label></formula><p>and <ref type="formula" target="#formula_2">(3)</ref> Anisotropy with edge features and explicit edge representations updated at each layer with node/edge updates as (such as in GatedGCN by default <ref type="bibr" target="#b10">[11]</ref>):</p><formula xml:id="formula_9">h +1 i = σ j∈Ni e ij · W h j , e +1 ij = f V h i , h j , e ij ), with (E.Feat,E.Repr= , ).<label>(8)</label></formula><p>GatedGCN-E and GAT-E in <ref type="table" target="#tab_5">Table 4</ref> are models using input edge features from the datasets to initialize the edge representations e ij . Detailed equations are available in supplementary material. As maintaining edge representations comes with a time and memory cost for the large COLLAB graph, all models use a reduced budget of 27K parameters to fit the GPU memory, and are allowed to train for a maximum of 24 hours for convergence. Analysis. On both TSP and COLLAB, upgrading isotropic models with edge features significantly boosts performance given the same model parameters (e.g. 0.75 vs. 0.64 F1 score on TSP, 50.6% vs. <ref type="bibr" target="#b34">35</ref>.9% Hits@50 on COLLAB for GatedGCN with edge features vs. the isotropic variant).</p><p>Maintaining explicit edge representations across layers further improves F1 score for TSP, especially when initializing the edge representations with euclidean distances between nodes (e.g. 0.78 vs. 0.67 F1 score for GAT-E vs. standard GAT). On COLLAB, adding explicit edge representations and inputs degrades performance, suggesting that the features (collaboration frequency and year) are not useful for the link prediction task (e.g. 47.2 vs. 51.5 Hits@50 for GatedGCN-E vs. GatedGCN). As suggested by <ref type="bibr" target="#b86">[87]</ref>, it would be interesting to treat COLLAB as a multi-graph with temporal edges, motivating the development of task-specific anisotropic edge representations beyond generic attention and gating mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a new benchmarking framework to rigorously evaluate the performance of graph neural networks on medium-scale datasets, and demonstrate its usefulness for analyzing message-passing based and theoretically expressive GNNs. As we make our code open-source, easy to use and reproducible, we hope the community will find this project useful to prototype the latest GNN architectures and track progress in graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This paper proposes a reproducible benchmarking infrastructure that can fairly and rigorously evaluate graph neural network (GNN) architectures, and track progress in graph representation learning. Our framework is likely to drive the development of general-purpose and theoretically driven GNN models which may be deployed in a variety of downstream applications. We briefly discuss positive use cases and possible negative outcomes in this section.</p><p>Better GNN architectures. Graphs are met in a wide range of data-driven problems, and GNNs can be used to tackle them: Social media and e-commerce platforms are using GNNs to improve content recommendation and advertising <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b93">94]</ref>. GNNs are also driving improvements in content quality and inclusivity of these platforms, e.g. monitoring hate speech or fake news spread <ref type="bibr" target="#b64">[65]</ref>. Similarly, real-time optimization problems which are modelled by interaction graphs make use of GNNs as their backbone, e.g. scheduling of processor chips and power units in hardware systems <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21]</ref>. GNNs are also driving advancements for complex and high-impact problems in drug discovery <ref type="bibr" target="#b69">[70]</ref>, circuit design <ref type="bibr" target="#b61">[62]</ref>, neuroscience <ref type="bibr" target="#b31">[32]</ref>, and genomics <ref type="bibr" target="#b28">[29]</ref>.</p><p>Conversely, the same architectures which lead to positive outcomes may also be used for malicious purposes, especially in social networks and e-commerce: Models monitoring the spread of fake news could end up helping bad actors in designing adversarial strategies for spreading counterfeit content, or for manipulating behaviour based on network effects. Increased personalization of social media and e-commerce platforms has raised important policy questions regarding the collection, ownership and storage of highly sensitive and personal user information.</p><p>New benchmarking frameworks. The act of developing new benchmarks often sets precedence and drives the directions of research in particular communities <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b6">7]</ref>. Thus, community-driven benchmarks must progress and evolve to reflect the best practices in the community. Benchmark creators must be wary about not letting their frameworks be anchors which weigh a field down, and be open to suggestions and contributions by the broader community.</p><p>Collecting and preparing graph datasets also comes with many challenges and arbitrary choice <ref type="bibr" target="#b86">[87]</ref>. For example, the datasets used for benchmarking may push for research in favor of specific domains and applications, or contain biases which disadvantage particular communities. We would encourage users of our benchmark to understand the limitations of current graph machine learning datasets and consider the negative outcomes arising from data-driven systems in real-world scenarios. Ultimately, we believe that questions surrounding personal data and digital privacy are important considerations from both technical as well as public policy standpoints <ref type="bibr" target="#b2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and experimental details</head><p>We now provide additional information related to the preparation of the datasets described in Section 2.3 of the proposed benchmarking framework, as well as the corresponding experimental setting for training and performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Node Classification with SBM Datasets</head><p>The SBM datasets consider node-level tasks of graph pattern recognition <ref type="bibr" target="#b74">[75]</ref> -PATTERN and semi-supervised graph clustering -CLUSTER. The graphs are generated with the Stochastic Block Model (SBM) <ref type="bibr" target="#b0">[1]</ref>, which is widely used to model communities in social networks by modulating the intra-and extra-communities connections, thereby controlling the difficulty of the task. A SBM is a random graph which assigns communities to each node as follows: any two vertices are connected with the probability p if they belong to the same community, or they are connected with the probability q if they belong to different communities (the value of q acts as the noise level).</p><p>PATTERN: The graph pattern recognition task, presented in <ref type="bibr" target="#b74">[75]</ref>, aims at finding a fixed graph pattern P embedded in larger graphs G of variable sizes. For all data, we generate graphs G with 5 communities with sizes randomly selected between <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>. The SBM of each community is p = 0.5, q = 0.35, and the node features on G are generated with a uniform random distribution with a vocabulary of size 3, i.e. {0, 1, 2}. We randomly generate 100 patterns P composed of 20 nodes with intra-probability p P = 0.5 and extra-probability q P = 0.5 (i.e., 50% of nodes in P are connected to G). The node features for P are also generated as a random signal with values {0, 1, 2}.</p><p>The graphs are of sizes 44-188 nodes. The output node labels have value 1 if the node belongs to P and value 0 if it is in G.</p><p>CLUSTER: For the semi-supervised clustering task, we generate 6 SBM clusters with sizes randomly selected between <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref> and probabilities p = 0.55, q = 0.25. The graphs are of sizes 40-190 nodes. Each node can take an input feature value in {0, 1, 2, .., 6}. If the value is 1, the node belongs to class 0, value 2 corresponds to class 1, . . . , value 6 corresponds to class 5. Otherwise, if the value is 0, the class of the node is unknown and will be inferred by the GNN. There is only one labelled node that is randomly assigned to each community and most node features are set to 0. The output node labels are defined as the community/cluster class labels.</p><p>Splitting. The PATTERN dataset has 10000 train/2000 validation/2000 test graphs and CLUSTER dataset has 10000 train/1000 validation/1000 test graphs. We save the generated splits and use the same sets in all models for fair comparison.</p><p>Training. As presented in the standard experimental protocol in Section 3, we use Adam optimizer with a learning rate decay strategy. For all GNNs, an initial learning rate is set to 1 × 10 −3 , the reduce factor is 0.5, the patience value is 5, and the stopping learning rate is 1 × 10 −5 . Performance Measure. The performance measure is the average node-level accuracy weighted with respect to the class sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Graph Classification with Super-pixel Datasets</head><p>The super-pixels datasets test graph classification using the popular MNIST and CIFAR10 image classification datasets. Our main motivation to use these datasets is as sanity-checks: we expect most GNNs to perform close to 100% for MNIST and well enough for CIFAR10.</p><p>The original MNIST and CIFAR10 images are converted to graphs using super-pixels. Super-pixels represent small regions of homogeneous intensity in images, and can be extracted with the SLIC technique <ref type="bibr" target="#b2">[3]</ref>. We use SLIC super-pixels from <ref type="bibr" target="#b44">[45]</ref>  <ref type="bibr" target="#b3">4</ref> . For each sample, we build a k-nearest neighbor adjacency matrix with</p><formula xml:id="formula_10">W k−NN ij = exp − x i − x j 2 σ 2 x ,<label>(9)</label></formula><p>where x i , x j are the 2-D coordinates of super-pixels i, j, and σ x is the scale parameter defined as the averaged distance x k of the k nearest neighbors for each node. We use k = 8 for both MNIST and CIFAR10, whereas the maximum number of super-pixels (nodes) are 75 and 150 for MNIST and CIFAR10, respectively. The resultant graphs are of sizes 40-75 nodes for MNIST and 85-150 nodes for CIFAR10. <ref type="figure" target="#fig_1">Figure 1</ref> presents visualizations of the super-pixel graphs.</p><p>Splitting. We use the standard splits of MNIST and CIFAR10. MNIST has 55000 train/5000 validation/10000 test graphs and CIFAR10 has 45000 train/5000 validation/10000 test graphs. The 5000 graphs for validation set are randomly sampled from the training set and the same splits are used for every GNN.</p><p>Training. The learning decay rate strategy is adopted with an initial learning rate of 1 × 10 −3 , reduce factor 0.5, patience value 10, and the stopping learning rate 1 × 10 −5 for all GNNs, except for 3WLGNN and RingGNN where we experienced a difficulty in training, leading us to slightly adjust their learning rate schedule hyperparameters. For both 3WLGNN and RingGNN, the patience value is changed to 5. For RingGNN, the initial learning rate is changed to 1 × 10 −4 and the stopping learning rate is changed to 1 × 10 −6 . Performance Measure. The classification accuracy between the predicted and groundtruth label for each graph is the performance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Edge Classification/Link Prediction with TSP Dataset</head><p>Leveraging machine learning for solving NP-hard combinatorial optimization problems (COPs) has been the focus of intense research in recent years <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b9">10]</ref>. Recently proposed learning-driven solvers for COPs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b45">46]</ref> combine GNNs with classical search to predict approximate solutions directly from problem instances (represented as graphs). Consider the intensively studied Travelling Salesman Problem (TSP), which asks the following question: "Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?" Formally, given a 2D Euclidean graph, one needs to find an optimal sequence of nodes, called a tour, with minimal total edge weights (tour length). TSP's multi-scale nature makes it a challenging graph task which requires reasoning about both local node neighborhoods as well as global graph structure.</p><p>For our experiments with TSP, we follow the learning-based approach to COPs described in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b39">40]</ref>, where a GNN is the backbone architecture for assigning probabilities to each edge as belonging/not belonging to the predicted solution set. The probabilities are then converted into discrete decisions through graph search techniques. Each instance is a graph of n node locations sampled uniformly in the unit square S = {x i } n i=1 and x i ∈ [0, 1] 2 . We generate problems of varying size and complexity by uniformly sampling the number of nodes n ∈ [50, 500] for each instance.</p><p>In order to isolate the impact of the backbone GNN architectures from the search component, we pose TSP as a binary edge classification task, with the groundtruth value for each edge belonging to the TSP tour given by Concorde <ref type="bibr" target="#b3">[4]</ref>. For scaling to large instances, we use sparse k = 25 nearest neighbor graphs instead of full graphs, following <ref type="bibr" target="#b41">[42]</ref>. See <ref type="figure" target="#fig_2">Figure 2</ref> for sample TSP instances of various sizes.</p><p>Splitting. TSP has 10000 train, 1000 validation and 1000 test graphs. Training. All GNNs use a consistent learning rate strategy: an initial learning rate is set to 1 × 10 −3 , the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is 1 × 10 −5 . Performance Measure. Given the high class imbalance, i.e., only the edges in the TSP tour have positive label, we use the F1 score for the positive class as our performance measure. Non-learnt Baseline. In addition to reporting performance of GNNs, we compare with a simple k-nearest neighbor heuristic baseline, defined as follows: Predict true for the edges corresponding to the k nearest neighbors of each node, and false for all other edges. We set k = 2 for optimal performance. Comparing GNNs to the non-learnt baseline tells us whether models learn something more sophisticated than identifying a node's nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Link Prediction with COLLAB dataset</head><p>COLLAB is a link prediction dataset proposed by OGB <ref type="bibr" target="#b86">[87]</ref> corresponding to a collaboration network between approximately 235K scientists, indexed by Microsoft Academic Graph <ref type="bibr" target="#b83">[84]</ref>. Nodes represent scientists and edges denote collaborations between them. For node features, OGB provides 128dimensional vectors, obtained by averaging the word embeddings of a scientist's papers. The year and number of co-authored papers in a given year are concatenated to form edge features. The graph can also be viewed as a dynamic multi-graph, since two nodes may have multiple temporal edges between if they collaborate over multiple years.</p><p>Through the introduction of the COLLAB dataset, we additionally want to demonstrate that our benchmarking infrastructure is complementary to the OGB initiative, and is well-suited to integrate current and future OGB dataset and evaluation protocols.</p><p>Splitting. We use the realistic training, validation and test edge splits provided by OGB. Specifically, they use collaborations until 2017 as training edges, those in 2018 as validation edges, and those in 2019 as test edges.</p><p>Training. All GNNs use a consistent learning rate strategy: an initial learning rate is set to 1 × 10 −3 , the reduce factor is 0.5, the patience value is 10, and the stopping learning rate is 1 × 10 −5 . Performance Measure. We use the evaluator provided by OGB, which aims to measure a model's ability to predict future collaboration relationships given past collaborations. Specifically, they rank each true collaboration among a set of 100,000 randomly-sampled negative collaborations, and count the ratio of positive edges that are ranked at K-place or above (Hits@K). They suggested using K = 10 through their preliminary experiments, but we found K = 50 to better for statistically separating the performance of GNNs. Matrix Factorization Baseline. In addition to GNNs, we report performance for a simple matrix factorization baseline <ref type="bibr" target="#b86">[87]</ref>, which trains 256-dimensional embeddings for each of the 235K nodes. Comparing GNNs to matrix factorization tells us whether models leverage node features in addition to graph structure, as matrix factorization can be thought of as feature-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Graph Regression with ZINC dataset</head><p>We use a subset (12K) of ZINC molecular graphs (250K) dataset <ref type="bibr" target="#b36">[37]</ref> to regress a molecular property known as the constrained solubility. For each molecular graph, the node features are the types of heavy atoms and the edge features are the types of bonds between them.</p><p>Splitting. ZINC has 10000 train, 1000 validation and 1000 test graphs.</p><p>Training. For the learning rate strategy across all GNNs, an initial learning rate is set to 1 × 10 −3 , the reduce factor is 0.5, and the stopping learning rate is 1 × 10 −5 . The patience value is 5 for 3WLGNN and RingGNN, and 10 for all other GNNs. Performance Measure. The performance measure is the mean absolute error (MAE) between the predicted and the groundtruth constrained solubility for each molecular graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Graph Classification and Isomorphism Testing with CSL dataset</head><p>The Circular Skip Link dataset is a symmetric graph dataset introduced in <ref type="bibr" target="#b66">[67]</ref> to test the expressivity of GNNs. Each CSL graph is a 4-regular graph with edges connected to form a cycle and containing skip-links between nodes. Formally, it is denoted by G N,C where N is the number of nodes and C is the isomorphism class which is the skip-link of the graph. We use the same dataset G 41,C with C ∈ {2, 3, 4, 5, 6, 9, 11, 12, 13, 16}. The dataset is class-balanced with 15 graphs for every C resulting in a total of 150 graphs.</p><p>Splitting. We perform a 5-fold cross validation split, following <ref type="bibr" target="#b66">[67]</ref>, which gives 5 sets of train, validation and test data indices in the ratio 3 : 1 : 1. We use stratified sampling to ensure that the class distribution remains the same across splits. The indices are saved and used across all experiments for fair comparisons.</p><p>Training. For the learning rate strategy across all GNNs, an initial learning rate is set to 5 × 10 −4 , the reduce factor is 0.5, the patience value is 5, and the stopping learning rate is 1 × 10 −6 . We train on the 5-fold cross validation with 20 different seeds of initialization, following <ref type="bibr" target="#b17">[18]</ref>. Performance Measure. We use graph classification accuracy between the predicted labels and groundtruth labels as our performance measure. The model performance is evaluated on the test split of the 5 folds at every run, and following <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b17">18]</ref>, we report the maximum, minimum, average and the standard deviation of the 100 scores, i.e., 20 runs of 5-folds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Graph Neural Networks</head><p>This section formally describes our experimental pipeline, illustrated in <ref type="figure">Figure 3</ref> for GCNs and <ref type="figure">Figure 4</ref> for WL-GNNs. In Section B.1, we describe the components of the setup of the GCN class with vanilla GCN <ref type="bibr" target="#b43">[44]</ref>, GraphSage <ref type="bibr" target="#b32">[33]</ref>, MoNet <ref type="bibr" target="#b62">[63]</ref>, GAT <ref type="bibr" target="#b79">[80]</ref>, and GatedGCN <ref type="bibr" target="#b10">[11]</ref>, including the input layers, the GNN layers and the task based MLP classifier layers. We also include the description of GIN <ref type="bibr" target="#b88">[89]</ref> in this section as this model can be interpreted as a GCN, although it was designed to differentiate non-isomorphic graphs. In Section B.2, we present the GNN layers and the task based MLP classifier layers for the class of WL-GNN models with Ring-GNNs <ref type="bibr" target="#b17">[18]</ref> and 3WL-GNNs <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Message-Passing GCNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Input Layer</head><p>Given a graph, we are given node features α i ∈ R a×1 for each node i and (optionally) edge features β ij ∈ R b×1 for each edge connecting node i and node j. The input features α i and β ij are embedded   <ref type="figure">Figure 3</ref>: A standard experimental pipeline for GCNs, which embeds the graph node and edge features, performs several GNN layers to compute convolutional features, and finally makes a prediction through a task-specific MLP layer.</p><p>Node feat.</p><p>Edge feat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Tensor WL-GNN Layer Prediction Layer</head><p>Input 3D tensor MLP* MLP*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Predictions</head><p>Graph Prediction</p><p>Edge Predictions MLP* *Details in Section B.2.3 <ref type="figure">Figure 4</ref>: A standard experimental pipeline for WL-GNNs, which inputs to a GNN a graph with all node and edge information (if available) represented by a dense tensor, performs several GNN layer computations over the dense tensor, and finally makes a prediction through a task-specific MLP layer.</p><p>to d-dimensional hidden features h =0 i and e =0 ij via a simple linear projection before passing them to a graph neural network:</p><formula xml:id="formula_11">h 0 i = U 0 α i + u 0 ; e 0 ij = V 0 β ij + v 0 ,<label>(10)</label></formula><p>where U 0 ∈ R d×a , V 0 ∈ R d×b and u 0 , v 0 ∈ R d . If the input node/edge features are one-hot vectors of discrete variables, then biases u 0 , v 0 are not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 GCN layers</head><p>Each GCN layer computes d-dimensional representations for the nodes/edges of the graph through recursive neighborhood diffusion (or message passing), where each graph node gathers features from its neighbors to represent local graph structure. Stacking L GCN layers allows the network to build node representations from the L-hop neighborhood of each node.</p><p>Let h i denote the feature vector at layer associated with node i. The updated features h +1 i at the next layer + 1 are obtained by applying non-linear transformations to the central feature vector h i and the feature vectors h j for all nodes j in the neighborhood of node i (defined by the graph structure). This guarantees the transformation to build local reception fields, such as in standard ConvNets for computer vision, and be invariant to both graph size and vertex re-indexing.</p><p>Thus, the most generic version of a feature vector h +1 i at vertex i at the next layer in the GNN is:</p><formula xml:id="formula_12">h +1 i = f h i , {h j : j → i} ,<label>(11)</label></formula><p>where {j → i} denotes the set of neighboring nodes j pointed to node i, which can be replaced by {j ∈ N i }, the set of neighbors of node i, if the graph is undirected. In other words, a GNN is defined by a mapping f taking as input a vector h i (the feature vector of the center vertex) as well as an un-ordered set of vectors {h j } (the feature vectors of all neighboring vertices), see <ref type="figure">Figure 5</ref>. The arbitrary choice of the mapping f defines an instantiation of a class of GNNs. Vanilla Graph ConvNets (GCN) <ref type="bibr" target="#b43">[44]</ref> In the simplest formulation of GNNs, Graph ConvNets iteratively update node features via an isotropic averaging operation over the neighborhood node features, i.e.,</p><formula xml:id="formula_13">h +1 i = ReLU U Mean j∈Ni h j ,<label>(12)</label></formula><formula xml:id="formula_14">= ReLU U 1 deg i j∈Ni h j ,<label>(13)</label></formula><p>where U ∈ R d×d (a bias is also used, but omitted for clarity purpose), deg i is the in-degree of node i, see <ref type="figure">Figure 6</ref>. Eq. (12) is called a convolution as it is a linear approximation of a localized spectral convolution. Note that it is possible to add the central node features h i in the update (12) by using self-loops or residual connections.</p><p>GraphSage <ref type="bibr" target="#b32">[33]</ref> GraphSage improves upon the simple GCN model by explicitly incorporating each node's own features from the previous layer in its update equation:</p><formula xml:id="formula_15">h +1 i = ReLU U Concat h i , Mean j∈Ni h j ,<label>(14)</label></formula><p>where U ∈ R d×2d , see <ref type="figure">Figure 7</ref>. Observe that the transformation applied to the central node features h i is different to the transformation carried out to the neighborhood features h j . The node features are then projected onto the 2 -unit ball before being passed to the next layer: The authors also define more sophisticated neighborhood aggregation functions, such as Max-pooling or LSTM aggregators:</p><formula xml:id="formula_16">h +1 i =ĥ +1 i ĥ +1 i 2 .<label>(15)</label></formula><formula xml:id="formula_17">h +1 i = ReLU U Concat h i , Max j∈Ni ReLU V h j ,<label>(16)</label></formula><formula xml:id="formula_18">h +1 i = ReLU U Concat h i , LSTM j∈Ni h j ,<label>(17)</label></formula><p>where V ∈ R d×d and the LSTM cell also uses learnable weights. In our experiments, we use the Max-pooling version of GraphSage, Eq. <ref type="bibr" target="#b15">(16)</ref>.</p><p>Graph Attention Network (GAT) <ref type="bibr" target="#b79">[80]</ref> GAT uses the attention mechanism of <ref type="bibr" target="#b5">[6]</ref> to introduce anisotropy in the neighborhood aggregation function. The network employs a multi-headed architecture to increase the learning capacity, similar to the Transformer <ref type="bibr" target="#b78">[79]</ref>. The node update equation is given by:</p><formula xml:id="formula_19">h +1 i = Concat K k=1 ELU j∈Ni e k, ij U k, h j ,<label>(18)</label></formula><p>where U k, ∈ R d K ×d are the K linear projection heads, and e k, ij are the attention coefficients for each head defined as:</p><formula xml:id="formula_20">e k, ij = exp(ê k, ij ) j ∈Ni exp(ê k, ij ) ,<label>(19)</label></formula><formula xml:id="formula_21">e k, ij = LeakyReLU V k, Concat U k, h i , U k, h j ,<label>(20)</label></formula><p>where V k, ∈ R 2d K , see <ref type="figure">Figure 8</ref>. GAT learns a mean over each node's neighborhood features sparsely weighted by the importance of each neighbor.</p><p>MoNet <ref type="bibr" target="#b62">[63]</ref> The MoNet model introduces a general architecture to learn on graphs and manifolds using the Bayesian Gaussian Mixture Model (GMM) <ref type="bibr" target="#b22">[23]</ref>. In the case of graphs, the node update equation is defined as: Gated Graph ConvNet (GatedGCN) <ref type="bibr" target="#b10">[11]</ref> GatedGCN considers residual connections, batch normalization and edge gates to design another anisotropic variant of GCN. The authors propose to explicitly update edge features along with node features:</p><formula xml:id="formula_22">h +1 i = ReLU K k=1 j∈Ni e k, ij U k, h j ,<label>(21)</label></formula><formula xml:id="formula_23">e k, ij = exp − 1 2 (u ij − µ k ) T (Σ k ) −1 (u ij − µ k ) ,<label>(22)</label></formula><formula xml:id="formula_24">u ij = Tanh A (deg −1/2 i , deg −1/2 j ) T + a ,<label>(23)</label></formula><formula xml:id="formula_25">h +1 i = h i + ReLU BN U h i + j∈Ni e ij V h j ,<label>(24)</label></formula><p>where U , V ∈ R d×d , is the Hadamard product, and the edge gates e ij are defined as:</p><formula xml:id="formula_26">e ij = σ(ê ij ) j ∈Ni σ(ê ij ) + ε ,<label>(25)</label></formula><formula xml:id="formula_27">e ij =ê −1 ij + ReLU BN A h −1 i + B h −1 j + C ê −1 ij ,<label>(26)</label></formula><p>where σ is the sigmoid function, ε is a small fixed constant for numerical stability, A , B , C ∈ R d×d , see <ref type="figure" target="#fig_1">Figure 10</ref>. Note that the edge gates <ref type="bibr" target="#b24">(25)</ref> can be regarded as a soft attention process, related to the standard sparse attention mechanism <ref type="bibr" target="#b5">[6]</ref>. Different from other anisotropic GNNs, the GatedGCN architecture explicitly maintains edge featuresê ij at each layer, following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b88">[89]</ref> The GIN architecture is based the Weisfeiler-Lehman Isomorphism Test <ref type="bibr" target="#b87">[88]</ref> to study the expressive power of GNNs. The node update equation is defined as:</p><formula xml:id="formula_28">h +1 i = ReLU U ReLU BN V ĥ +1 i ,<label>(27)</label></formula><formula xml:id="formula_29">h +1 i = (1 + ) h i + j∈Ni h j ,<label>(28)</label></formula><p>where is a learnable constant, U , V ∈ R d×d , BN denotes Batch Normalization. See <ref type="figure" target="#fig_1">Figure 11</ref> for illustration of the update equation.</p><p>Normalization and Residual Connections As a final note, we augment each message-passing GCN layer with batch normalization (BN) <ref type="bibr" target="#b35">[36]</ref> and residual connections <ref type="bibr" target="#b33">[34]</ref>. As such, we consider a more specific class of GCNs than <ref type="formula" target="#formula_0">(11)</ref>:</p><formula xml:id="formula_30">h +1 i = h i + σ BN ĥ +1 i ,<label>(29)</label></formula><formula xml:id="formula_31">h +1 i = g GCN h i , {h j : j → i} ,<label>(30)</label></formula><p>where σ is a non-linear activation function and g GCN is a specific message-passing GCN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Task-based Layer</head><p>The final component of each network is a prediction layer to compute task-dependent outputs, which are given to a loss function to train the network parameters in an end-to-end manner. The input of the prediction layer is the result of the final message-passing GCN layer for each node of the graph (except GIN, which uses features from all intermediate layers).</p><p>Graph classifier layer To perform graph classification, we first build a d-dimensional graph-level vector representation y G by averaging over all node features in the final GCN layer:</p><formula xml:id="formula_32">y G = 1 V V i=0 h L i ,<label>(31)</label></formula><p>The graph features are then passed to a MLP, which outputs un-normalized logits/scores y pred ∈ R C for each class:</p><formula xml:id="formula_33">y pred = P ReLU (Q y G ) ,<label>(32)</label></formula><p>where P ∈ R d×C , Q ∈ R d×d , C is the number of classes. Finally, we minimize the cross-entropy loss between the logits and groundtruth labels.</p><p>Graph regression layer For graph regression, we compute y G using Eq.(31) and pass it to a MLP which gives the prediction score y pred ∈ R:</p><formula xml:id="formula_34">y pred = P ReLU (Q y G ) ,<label>(33)</label></formula><p>where P ∈ R d×1 , Q ∈ R d×d . The L1-loss between the predicted score and the groundtruth score is minimized during the training.</p><p>Node classifier layer For node classification, we independently pass each node's feature vector to a MLP for computing the un-normalized logits y i,pred ∈ R C for each class:</p><formula xml:id="formula_35">y i,pred = P ReLU Q h L i ,<label>(34)</label></formula><p>where P ∈ R d×C , Q ∈ R d×d . The cross-entropy loss weighted inversely by the class size is used during training.</p><p>Edge classifier layer To make a prediction for each graph edge e ij , we first concatenate node features h i and h j from the final GNN layer. The concatenated edge features are then passed to a MLP for computing the un-normalized logits y ij,pred ∈ R C for each class:</p><formula xml:id="formula_36">y ij,pred = P ReLU Q Concat h L i , h L j ,<label>(35)</label></formula><p>where P ∈ R d×C , Q ∈ R d×2d . The standard cross-entropy loss between the logits and groundtruth labels is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Weisfeiler-Lehman GNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Input Tensor</head><p>For a given graph with adjacency matrix A ∈ R n×n , node features h node ∈ R n×d and edge features h edge ∈ R n×n×de , the input tensor to the RingGNN and 3WL-GNN networks is defined as  </p><formula xml:id="formula_37">h =0 ∈ R n×n×(1+d+de) ,<label>(36)</label></formula><formula xml:id="formula_38">h =0 i,j,1 = A ij ∈ R, ∀i, j<label>(37)</label></formula><formula xml:id="formula_39">h =0 i,j,2:d+1 = h node i ∈ R d , ∀i = j 0 , ∀i = j (38) h =0 i,j,d+2:d+de+1 = h edge ij ∈ R de<label>(39)</label></formula><formula xml:id="formula_40">h +1 = Concat M W 1 (h ) . M W 2 (h ), M W 3 (h ) ,<label>(40)</label></formula><p>where h , h +1 ∈ R n×n×d , and M W are 2-layer MLPs applied along the feature dimension:</p><formula xml:id="formula_41">M W ={Wa,W b } (h) = σ h W a W b ,<label>(41)</label></formula><p>where W a , W b ∈ R d×d . As h ∈ R n×n×d , the MLP (41) is implemented with a standard 2Dconvolutional layer with 1 × 1 kernel size. Eventually, the matrix multiplication in <ref type="formula" target="#formula_3">(40)</ref> is carried out along the first and second dimensions such that:</p><formula xml:id="formula_42">M W1 (h) . M W2 (h) i,j,k = n p=1 M W1 (h) i,p,k . M W2 (h) p,j,k ,<label>(42)</label></formula><p>with complexity O(n 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ring-GNNs [18]</head><p>These models proposed to improve the order-2 equivariant GNNs of <ref type="bibr" target="#b58">[59]</ref> with the multiplication of two equivariant linear layers. The layer update equation of Ring-GNNs is designed as:</p><formula xml:id="formula_43">h +1 = σ w 1 L W 1 (h ) + w 2 L W 2 (h ).L W 3 (h ) ,<label>(43)</label></formula><p>where h , h +1 ∈ R n×n×d , w 1,2 ∈ R, and L W are the equivariant linear layers defined as</p><formula xml:id="formula_44">L W (h) i,j,k = 17 p=1 d q=1 W p,q,k L i (h) i,j,q ,<label>(44)</label></formula><p>where W ∈ R d×d×17 and {L i } 15 i=1 is the set of all basis functions for all linear equivariant functions from R n×n → R n×n (see Appendix A in <ref type="bibr" target="#b58">[59]</ref> for the complete list of these 15 operations) and</p><formula xml:id="formula_45">{L i } 17</formula><p>i=16 are the basis for the bias terms. Matrix multiplication in (43) also implies a time complexity O(n 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Task-based network layers</head><p>We describe the final network layers depending on the task at hand. The loss functions corresponding to the task are the same as the GCNs, and presented in Section B.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph classifier layer</head><p>We have followed the original author implementations in <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b17">18]</ref> to design the classifier layer for 3WL-GNNs and Ring-GNNs. Similar to <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b88">89]</ref>, the classifier layer for Ring-GNNs uses features from all intermediate layers and then passes the features to a MLP:</p><formula xml:id="formula_46">y G = Concat L =1 n i,j=1 h ij ∈ R Ld ,<label>(45)</label></formula><formula xml:id="formula_47">y pred = P ReLU (Q y G ) ∈ R C ,<label>(46)</label></formula><p>where P ∈ R d×C , Q ∈ R Ld×d , C is the number of classes.</p><p>For 3WL-GNNs, Eqn. <ref type="formula" target="#formula_3">(45)</ref> is replaced by a diagonal and off-diagonal max pooling readout <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> at every layer:</p><formula xml:id="formula_48">y G = Concat max i h ii , max i =j h ij ∈ R 2d ,<label>(47)</label></formula><p>and the final prediction score is defined as:</p><formula xml:id="formula_49">y pred = L =1 P y G ∈ R C ,<label>(48)</label></formula><p>where P ∈ R 2d×C , C is the number of classes.</p><p>Graph regression layer Similar to the graph classifier layer with P ∈ R d×1 for Ring-GNNs, and P ∈ R 2d×1 for 3WL-GNNs.</p><p>Node classifier layer For node classification, the prediction in Ring-GNNs is done as follows:</p><formula xml:id="formula_50">y node i = Concat L =1 n j=1 h ij ∈ R Ld ,<label>(49)</label></formula><formula xml:id="formula_51">y i,pred = P ReLU Q y node i ∈ R C ,<label>(50)</label></formula><p>where P ∈ R d×C , Q ∈ R Ld×d , C is the number of classes.</p><p>In 3WL-GNNs, the final prediction score is defined as:</p><formula xml:id="formula_52">y node, i = n j=1 h ij ∈ R d ,<label>(51)</label></formula><formula xml:id="formula_53">y i,pred = L =1 P y node, i ∈ R C ,<label>(52)</label></formula><p>where P ∈ R d×C , C is the number of classes.</p><p>Edge classifier layer For link prediction, for both Ring-GNNs and 3WL-GNNs, the edge features are obtained by concatenating the node features such as:</p><formula xml:id="formula_54">y node i = Concat L =1 n j=1 h ij ∈ R Ld ,<label>(53)</label></formula><formula xml:id="formula_55">y ij,pred = P ReLU Q Concat y node i , y node j ∈ R C ,<label>(54)</label></formula><p>where P ∈ R d×C , Q ∈ R 2Ld×d , C is the number of classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments on TU datasets</head><p>Apart from the proposed datasets in our benchmark (Section 3), we perform experiments on 3 TU datasets for graph classification -ENZYMES, DD and PROTEINS. Our goal is to empirically highlight some of the challenges of using these conventional datasets for benchmarking GNNs.</p><p>Splitting. Since the 3 TU datasets that we use do not have standard splits, we perform a 10-fold cross validation split which gives 10 sets of train, validation and test data indices in the ratio 8 : 1 : 1.</p><p>We use stratified sampling to ensure that the class distribution remains the same across splits. Training. We use Adam optimizer with a similar learning rate strategy as used in our benchmark's experimental protocol. An initial learning rate is tuned from a range of 1 × 10 −3 to 7 × 10 −5 using grid search for every GNN models. The learning rate reduce factor is 0.5, the patience value is 25 and the stopping learning rate is 1 × 10 −6 . Performance Measure. We use classification accuracy between the predicted labels and groundtruth labels as our performance measure. The model performance is evaluated on the test split of the 10 folds for all TU datasets, and reported as the average and the standard deviation of the 10 scores.</p><p>Our numerical results on the TU datasets -ENZYMES, DD and PROTEINS are presented in <ref type="table" target="#tab_9">Table  6</ref>. We observe all NNs have similar statistical test performance as the standard deviation is quite large. We also report a second run of these experiments with the same experimental protocol, i.e. the same 10-fold splitting and hyperparameters but different initialization (seed). We observe a change of model ranking, which we attribute to the small size of the datasets and the non-determinism of gradient descent optimizers. We also observe that, for DD and PROTEINS, the graph-agnostic MLP baselines perform as good as GNNs. Our observations reiterate how experiments on the small TU datasets are difficult to determine which GNNs are powerful and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Laplacian Positional Encodings</head><p>Standard GCNs are not able to differentiate isomorphic nodes <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b92">93]</ref>. To overcome this issue, positional encoding (PE) of nodes was proposed. Ideally, PEs should be unique for each node, and nodes which are far apart in the graph should have different positional features whereas nodes which are nearby have similar positional features. Note that in a graph that has some symmetries, positional features cannot be assigned in a canonical way. For example, if node i and node j are structurally symmetric, and we have positional features p i = a, p j = b that differentiate them, then it is also possible to arbitrary choose p i = b, p j = a since i and j are completely symmetric by definition. In other words, the PE is always arbitrary up to the number of symmetries in the graph. As a consequence, the network will have to learn to deal with these ambiguities during training. The simplest possible positional encodings is to give an (arbitrary) ordering to the nodes, among n! possible orderings. During training, the orderings are uniformly sampled from the n! possible choices in order for the network to learn to be independent to these arbitrary choices <ref type="bibr" target="#b66">[67]</ref>.</p><p>We propose an alternative to reduce the sampling space, and therefore the amount of ambiguities to be resolved by the network. Laplacian eigenvectors are hybrid positional and structural encodings, as they are invariant by node re-parametrization. However, they are also limited by natural symmetries such as the arbitrary sign of eigenvectors (after being normalized to have unit length). The number of possible sign flips is 2 k , where k is the number of eigenvectors. In practice we choose k n, and therefore 2 k is much smaller n! (the number of possible ordering of the nodes). During the training, the eigenvectors will be uniformly sampled at random between the 2 k possibilities. If we do not seek to learn the invariance w.r.t. all possible sign flips of eigenvectors, then we can remove the sign ambiguity of eigenvectors by taking the absolute value. This choice seriously degrades the expressivity power of the positional features.</p><p>Numerical results for different positional encodings are reported in <ref type="table" target="#tab_11">Table 7</ref>. For all results, we use the GatedGCN model <ref type="bibr" target="#b10">[11]</ref>. We study 5 types of positional encodings; EigVecs-k corresponds to the smallest non-trivial k eigenvectors, Rand sign(EigVecs) randomly flips the sign of the k smallest non-trivial eigenvectors in each batch, Abs(EigVecs) takes the absolute value of the k eigenvectors, Fixed node ordering uses the original node ordering of graphs, and Rand node ordering randomly permutes ordering of nodes in each batch. We observed that the best results are consistently produced with the Laplacian PEs with random sign flipping at training. For index PEs, randomly permuting the ordering of nodes also improves significantly the performances over keeping fixed the original node ordering. However, Laplacian PEs clearly outperform index PEs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Dissecting GNNs for Edge Representation Analysis</head><p>In Section 4, <ref type="table" target="#tab_5">Table 4</ref>, we systematically study the impact of anisotropy by instantiating three variants of GAT and GatedGCN: (1) Isotropic aggregation, such as vanilla GCNs, Eq.(13); (2) Anisotropy using edge features, such as GAT by default, Eq.(18); and (3) Anisotropy with edge features and explicit edge representations updated at each layer, such as in GatedGCN by default, Eq.(24). This section provides formal equations for each model variant. (Note that there may be a multitude of approaches to instantiating anisotropic GNNs and using edge features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b12">13]</ref> besides the ones we consider.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 GatedGCN</head><p>Isotropic, similar to vanilla GCNs with sum aggregation:</p><formula xml:id="formula_56">h +1 i = h i + ReLU BN U h i + j∈Ni V h j , where U , V ∈ R d×d .<label>(55)</label></formula><p>Anisotropic with intermediate edge features computed as joint representations of adjacent node features at each layer:</p><formula xml:id="formula_57">h +1 i = h i + ReLU BN U h i + j∈Ni e ij V h j ,<label>(56)</label></formula><formula xml:id="formula_58">e ij = σ(ê ij ) j ∈Ni σ(ê ij ) + ε ,ê ij = A h −1 i + B h −1 j ,<label>(57)</label></formula><p>where U , V ∈ R d×d , is the Hadamard product, and e ij are the edge gates.</p><p>Anisotropic with edge features as well as explicit edge representations updated across layers in addition to node features, as in GatedGCN by default, Eq. <ref type="formula" target="#formula_1">(24)</ref>:</p><formula xml:id="formula_59">h +1 i = h i + ReLU BN U h i + j∈Ni e ij V h j ,<label>(58)</label></formula><formula xml:id="formula_60">e ij = σ(ê ij ) j ∈Ni σ(ê ij ) + ε ,<label>(59)</label></formula><formula xml:id="formula_61">e ij =ê −1 ij + ReLU BN A h −1 i + B h −1 j + C ê −1 ij ,<label>(60)</label></formula><p>where U , V ∈ R d×d , is the Hadamard product, and e ij are the edge gates. The input edge features from the datasets (e.g. distances for TSP, collaboration year and frequency for COLLAB) can optionally be used to initialize the edge representationsê =0 ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 GAT</head><p>Isotropic, similar to multi-headed vanilla GCNs with sum aggregation: ,ê k,</p><formula xml:id="formula_62">h +1 i = Concat K k=1 ELU BN j∈Ni U k, h j , where U k, ∈ R d K ×d .<label>(61)</label></formula><formula xml:id="formula_63">ij = LeakyReLU V k, Concat U k, h i , U k, h j ,<label>(63)</label></formula><p>where U k, ∈ R d K ×d , V k, ∈ R 2d K are the K linear projection heads and e k, ij are the attention coefficients for each head.</p><p>Anisotropic with edge features as well as explicit edge representations updated across layers in addition to node features:</p><formula xml:id="formula_64">h +1 i = h i + ReLU BN U Concat h i , Max j∈Ni ReLU σ ê ij V h j ,<label>(71)</label></formula><formula xml:id="formula_65">e ij = A h −1 i + h −1 j + B e −1 ij , e +1 ij = e ij + ReLU BN ê ij ,<label>(72)</label></formula><p>where U ∈ R d×2d , V , A , B ∈ R d×d , is the Hadamard product, andê ij are the edge gates. The input edge features from the datasets can optionally be used to initialize the edge representations e =0 ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F A Note on Graph Size Normalization</head><p>Intuitively, batching graphs of variable sizes may lead to node representation at different scales, making it difficult to learn the optimal statistics µ and σ for BatchNorm across irregular batch sizes and variable graphs. A preliminary version of this work introduced a graph size normalization technique called GraphNorm, which normalizes the node features h i w.r.t. the graph size, i.e.,</p><formula xml:id="formula_66">h i = h i × 1 √ V ,<label>(73)</label></formula><p>where V is the number of graph nodes. The GraphNorm layer is placed before the BatchNorm layer.</p><p>We would like to note that GraphNorm does not have any concrete theoretical basis as of now, and was proposed based on initially promising empirical results on datasets such as ZINC and CLUSTER. Future work shall investigate more principled approaches towards designing normalization layers for graph structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Hardware Details</head><p>Timing research code can be tricky due to differences of implementations and hardware acceleration. Nonetheless, we take a practical view and report the average wall clock time per epoch and the total training time for each model. All experiments were implemented in DGL/PyTorch. We run experiments for MNIST, CIFAR10, ZINC, TSP, COLLAB and TUs on an Intel Xeon CPU E5-2690 v4 server with 4 Nvidia 1080Ti GPUs, and for PATTERN and CLUSTER on an Intel Xeon Gold 6132 CPU with 4 Nvidia 2080Ti GPUs. Each experiment was run on a single GPU and 4 experiments were run on the server at any given time (on different GPUs). We run each experiment for a maximum of 12 hours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Models with the suffix -PE use Laplacian Eigenvectors as node positional encodings, with dimension 8 for ZINC, 2 for PATTERN and 20 for others. • Results denoted by Diverged indicate unstable and divergent runs across all 4 seeds and initial learning rate values {10 −3 , 10 −4 , 10 −5 }. • Results denoted by OOM indicate runs which throw out of memory errors on our hardware configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Sample images and their superpixel graphs. The graphs of SLIC superpixels (at most 75 nodes for MNIST and 150 nodes for CIFAR10) are 8-nearest neighbor graphs in the Euclidean space and node colors denote the mean pixel intensities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample graphs from the TSP dataset. Nodes are colored blue and edges on the groundtruth TSP tours are colored red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 Figure 7 :</head><label>567</label><figDesc>A generic graph neural network layer.Figure adapted from<ref type="bibr" target="#b10">[11]</ref>.ReLU GraphSage Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>MoNet Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>GIN Layer where U k, ∈ R d×d , µ k , (Σ k ) −1 , a ∈ R 2 and A ∈ R 2×2are the (learnable) parameters of the GMM, seeFigure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>ConcatFigure 12 :Figure 13 :</head><label>1213</label><figDesc>3WL-GNN Layer ReLU + RingGNN Layer where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics of datasets included in the proposed benchmark.</figDesc><table><row><cell>Domain &amp; Construction</cell><cell>Dataset</cell><cell cols="3">#Graphs #Nodes Total #Nodes</cell><cell>Task</cell></row><row><cell>Chemistry: Real-world molecular graphs</cell><cell>ZINC</cell><cell>12K</cell><cell>9-37</cell><cell>277,864</cell><cell>Graph Regression</cell></row><row><cell>Mathematical Modelling: Artificial graphs generated from Stochastic Block Models</cell><cell>PATTERN CLUSTER</cell><cell>14K 12K</cell><cell>44-188 41-190</cell><cell>1,664,491 1,406,436</cell><cell>Node Classification</cell></row><row><cell>Computer Vision: Graphs constructed with SLIC super-pixels of images</cell><cell>MNIST CIFAR10</cell><cell>70K 60K</cell><cell>40-75 85-150</cell><cell>4,939,668 7,058,005</cell><cell>Graph Classification</cell></row><row><cell>Combinatorial Optimization: Uniformly generated artificial Euclidean graphs</cell><cell>TSP</cell><cell>12K</cell><cell>50-500</cell><cell>3,309,140</cell><cell>Edge Classification</cell></row><row><cell>Social Networks: Real-world citation graph</cell><cell>COLLAB</cell><cell>1</cell><cell>235,868</cell><cell>235,868</cell><cell>Edge Classification</cell></row><row><cell cols="2">Circular Skip Links: Isomorphic graphs with same degree CSL</cell><cell>150</cell><cell>41</cell><cell>6,150</cell><cell>Graph Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Benchmarking results for MP-GCNs and WL-GNNs across 7 medium-scale graph classification/regression and node/link prediction datasets. Results are averaged over 4 runs with 4 different seeds.Red: the best model, Violet: good models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NODE CLASSIFICATION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PATTERN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLUSTER</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">L #Param Test Acc.±s.d.</cell><cell>Train Acc.±s.d.</cell><cell>#Epoch</cell><cell>Epoch/Total</cell><cell>#Param</cell><cell cols="3">Test Acc.±s.d. Train Acc.±s.d. #Epoch</cell><cell>Epoch/Total</cell></row><row><cell>MLP</cell><cell cols="2">4 105263</cell><cell>50.519±0.000</cell><cell>50.487±0.014</cell><cell>42.25</cell><cell>8.95s/0.11hr</cell><cell>106015</cell><cell>20.973±0.004</cell><cell>20.938±0.002</cell><cell>42.25</cell><cell>5.83s/0.07hr</cell></row><row><cell>GCN</cell><cell cols="2">4 100923</cell><cell>63.880±0.074</cell><cell>65.126±0.135</cell><cell>105.00</cell><cell>118.85s/3.51hr</cell><cell>101655</cell><cell>53.445±2.029</cell><cell>54.041±2.197</cell><cell>70.00</cell><cell>65.72s/1.30hr</cell></row><row><cell></cell><cell cols="2">16 500823</cell><cell>71.892±0.334</cell><cell>78.409±1.592</cell><cell>81.50</cell><cell>492.19s/11.31hr</cell><cell>501687</cell><cell>68.498±0.976</cell><cell>71.729±2.212</cell><cell>79.75</cell><cell>270.28s/6.08hr</cell></row><row><cell>GraphSage</cell><cell cols="2">4 101739</cell><cell>50.516±0.001</cell><cell>50.473±0.014</cell><cell>43.75</cell><cell>93.41s/1.17hr</cell><cell>102187</cell><cell>50.454±0.145</cell><cell>54.374±0.203</cell><cell>64.00</cell><cell>53.56s/0.97hr</cell></row><row><cell></cell><cell cols="2">16 502842</cell><cell>50.492±0.001</cell><cell>50.487±0.005</cell><cell>46.50</cell><cell>391.19s/5.19hr</cell><cell>503350</cell><cell>63.844±0.110</cell><cell>86.710±0.167</cell><cell>57.75</cell><cell>225.61s/3.70hr</cell></row><row><cell>MoNet</cell><cell cols="2">4 103775</cell><cell>85.482±0.037</cell><cell>85.569±0.044</cell><cell>89.75</cell><cell>35.71s/0.90hr</cell><cell>104227</cell><cell>58.064±0.131</cell><cell>58.454±0.183</cell><cell>76.25</cell><cell>24.29s/0.52hr</cell></row><row><cell></cell><cell cols="2">16 511487</cell><cell>85.582±0.038</cell><cell>85.720±0.068</cell><cell>81.75</cell><cell>68.49s/1.58hr</cell><cell>511999</cell><cell>66.407±0.540</cell><cell>67.727±0.649</cell><cell>77.75</cell><cell>47.82s/1.05hr</cell></row><row><cell>GAT</cell><cell cols="2">4 109936</cell><cell>75.824±1.823</cell><cell>77.883±1.632</cell><cell>96.00</cell><cell>20.92s/0.57hr</cell><cell>110700</cell><cell>57.732±0.323</cell><cell>58.331±0.342</cell><cell>67.25</cell><cell>14.17s/0.27hr</cell></row><row><cell></cell><cell cols="2">16 526990</cell><cell>78.271±0.186</cell><cell>90.212±0.476</cell><cell>53.50</cell><cell>50.33s/0.77hr</cell><cell>527874</cell><cell>70.587±0.447</cell><cell>76.074±1.362</cell><cell>73.50</cell><cell>35.94s/0.75hr</cell></row><row><cell>GatedGCN</cell><cell cols="2">4 104003</cell><cell>84.480±0.122</cell><cell>84.474±0.155</cell><cell>78.75</cell><cell>139.01s/3.09hr</cell><cell>104355</cell><cell>60.404±0.419</cell><cell>61.618±0.536</cell><cell>94.50</cell><cell>79.97s/2.13hr</cell></row><row><cell></cell><cell cols="2">16 502223</cell><cell>85.568±0.088</cell><cell>86.007±0.123</cell><cell>65.25</cell><cell>644.71s/11.91hr</cell><cell>502615</cell><cell>73.840±0.326</cell><cell>87.880±0.908</cell><cell>60.00</cell><cell>400.07s/6.81hr</cell></row><row><cell cols="3">GatedGCN-PE 16 502457</cell><cell>86.508±0.085</cell><cell>86.801±0.133</cell><cell>65.75</cell><cell>647.94s/12.08hr</cell><cell>504253</cell><cell>76.082±0.196</cell><cell>88.919±0.720</cell><cell>57.75</cell><cell>399.66s/6.58hr</cell></row><row><cell>GIN</cell><cell cols="2">4 100884</cell><cell>85.590±0.011</cell><cell>85.852±0.030</cell><cell>93.00</cell><cell>15.24s/0.40hr</cell><cell>103544</cell><cell>58.384±0.236</cell><cell>59.480±0.337</cell><cell>74.75</cell><cell>10.71s/0.23hr</cell></row><row><cell></cell><cell cols="2">16 508574</cell><cell>85.387±0.136</cell><cell>85.664±0.116</cell><cell>86.75</cell><cell>25.14s/0.62hr</cell><cell>517570</cell><cell>64.716±1.553</cell><cell>65.973±1.816</cell><cell>80.75</cell><cell>20.67s/0.47hr</cell></row><row><cell>RingGNN</cell><cell cols="2">2 105206</cell><cell>86.245±0.013</cell><cell>86.118±0.034</cell><cell>75.00</cell><cell>573.37s/12.17hr</cell><cell>104746</cell><cell cols="2">42.418±20.063 42.520±20.212</cell><cell>74.50</cell><cell>428.24s/8.79hr</cell></row><row><cell></cell><cell cols="2">2 504766</cell><cell>86.244±0.025</cell><cell>86.105±0.021</cell><cell>72.00</cell><cell>595.97s/12.15hr</cell><cell>524202</cell><cell>22.340±0.000</cell><cell>22.304±0.000</cell><cell>43.25</cell><cell>501.84s/6.22hr</cell></row><row><cell></cell><cell cols="2">8 505749</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>514380</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell></row><row><cell>3WLGNN</cell><cell cols="2">3 103572</cell><cell>85.661±0.353</cell><cell>85.608±0.337</cell><cell>95.00</cell><cell>304.79s/7.88hr</cell><cell>105552</cell><cell>57.130±6.539</cell><cell>57.404±6.597</cell><cell>116.00</cell><cell>219.51s/6.52hr</cell></row><row><cell></cell><cell cols="2">3 502872</cell><cell>85.341±0.207</cell><cell>85.270±0.198</cell><cell>81.75</cell><cell>424.23s/9.56hr</cell><cell>507252</cell><cell>55.489±7.863</cell><cell>55.736±8.024</cell><cell>66.00</cell><cell>319.98s/5.79hr</cell></row><row><cell></cell><cell cols="2">8 581716</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>586788</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GRAPH CLASSIFICATION</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#Epoch</cell><cell>Epoch/Total</cell></row><row><cell>MLP</cell><cell cols="2">4 104044</cell><cell>95.340±0.138</cell><cell>97.432±0.470</cell><cell>232.25</cell><cell>22.74s/1.48hr</cell><cell>104380</cell><cell>56.340±0.181</cell><cell>65.113±1.685</cell><cell>185.25</cell><cell>29.48s/1.53hr</cell></row><row><cell>GCN</cell><cell cols="2">4 101365</cell><cell>90.705±0.218</cell><cell>97.196±0.223</cell><cell>127.50</cell><cell>83.41s/2.99hr</cell><cell>101657</cell><cell>55.710±0.381</cell><cell>69.523±1.948</cell><cell>142.50</cell><cell>109.70s/4.39hr</cell></row><row><cell>GraphSage</cell><cell cols="2">4 104337</cell><cell>97.312±0.097</cell><cell>100.000±0.000</cell><cell>98.25</cell><cell>113.12s/3.13hr</cell><cell>104517</cell><cell>65.767±0.308</cell><cell>99.719±0.062</cell><cell>93.50</cell><cell>124.61s/3.29hr</cell></row><row><cell>MoNet</cell><cell cols="2">4 104049</cell><cell>90.805±0.032</cell><cell>96.609±0.440</cell><cell>146.25</cell><cell>93.19s/3.82hr</cell><cell>104229</cell><cell>54.655±0.518</cell><cell>65.911±2.515</cell><cell>141.50</cell><cell>97.13s/3.85hr</cell></row><row><cell>GAT</cell><cell cols="2">4 110400</cell><cell>95.535±0.205</cell><cell>99.994±0.008</cell><cell>104.75</cell><cell>42.26s/1.25hr</cell><cell>110704</cell><cell>64.223±0.455</cell><cell>89.114±0.499</cell><cell>103.75</cell><cell>55.27s/1.62hr</cell></row><row><cell>GatedGCN</cell><cell cols="2">4 104217</cell><cell>97.340±0.143</cell><cell>100.000±0.000</cell><cell>96.25</cell><cell>128.79s/3.50hr</cell><cell>104357</cell><cell>67.312±0.311</cell><cell>94.553±1.018</cell><cell>97.00</cell><cell>154.15s/4.22hr</cell></row><row><cell>GIN</cell><cell cols="2">4 105434</cell><cell>96.485±0.252</cell><cell>100.000±0.000</cell><cell>128.00</cell><cell>39.22s/1.41hr</cell><cell>105654</cell><cell>55.255±1.527</cell><cell>79.412±9.700</cell><cell>141.50</cell><cell>52.12s/2.07hr</cell></row><row><cell>RingGNN</cell><cell cols="2">2 105398</cell><cell>11.350±0.000</cell><cell>11.235±0.000</cell><cell>14.00</cell><cell>2945.69s/12.77hr</cell><cell>105165</cell><cell cols="2">19.300±16.108 19.556±16.397</cell><cell>13.50</cell><cell>3112.96s/13.00hr</cell></row><row><cell></cell><cell cols="2">2 505182</cell><cell>91.860±0.449</cell><cell>92.169±0.505</cell><cell>16.25</cell><cell>2575.99s/12.63hr</cell><cell>504949</cell><cell cols="2">39.165±17.114 40.209±17.790</cell><cell>13.75</cell><cell>2998.24s/12.60hr</cell></row><row><cell></cell><cell cols="2">8 506357</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>510439</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell></row><row><cell>3WLGNN</cell><cell cols="2">3 108024</cell><cell>95.075±0.961</cell><cell>95.830±1.338</cell><cell>27.75</cell><cell>1523.20s/12.40hr</cell><cell>108516</cell><cell>59.175±1.593</cell><cell>63.751±2.697</cell><cell>28.50</cell><cell>1506.29s/12.60hr</cell></row><row><cell></cell><cell cols="2">3 501690</cell><cell>95.002±0.419</cell><cell>95.692±0.677</cell><cell>26.25</cell><cell>1608.73s/12.42hr</cell><cell>502770</cell><cell>58.043±2.512</cell><cell>61.574±3.575</cell><cell>20.00</cell><cell>2091.22s/12.55hr</cell></row><row><cell></cell><cell cols="2">8 500816</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>501584</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LINK PREDICTION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TSP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>COLLAB</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">L #Param</cell><cell>Test F1±s.d.</cell><cell>Train F1±s.d.</cell><cell>#Epoch</cell><cell>Epoch/Total</cell><cell>#Param (L = 3)</cell><cell cols="3">Test Hits±s.d. Train Hits±s.d. #Epoch</cell><cell>Epoch/Total</cell></row><row><cell>MLP</cell><cell>4</cell><cell>96956</cell><cell>0.544±0.001</cell><cell>0.544±0.001</cell><cell>164.25</cell><cell>50.15s/2.31hr</cell><cell>39441</cell><cell>20.350±2.168</cell><cell>29.807±3.360</cell><cell>147.50</cell><cell>2.09s/0.09hr</cell></row><row><cell>GCN</cell><cell>4</cell><cell>95702</cell><cell>0.630±0.001</cell><cell>0.631±0.001</cell><cell>261.00</cell><cell>152.89s/11.15hr</cell><cell>40479</cell><cell>50.422±1.131</cell><cell>92.112±0.991</cell><cell>122.50</cell><cell>351.05s/12.04hr</cell></row><row><cell>GraphSage</cell><cell>4</cell><cell>99263</cell><cell>0.665±0.003</cell><cell>0.669±0.003</cell><cell>266.00</cell><cell>157.26s/11.68hr</cell><cell>39856</cell><cell>51.618±0.690</cell><cell>99.949±0.052</cell><cell>152.75</cell><cell>277.93s/11.87hr</cell></row><row><cell>MoNet</cell><cell>4</cell><cell>99007</cell><cell>0.641±0.002</cell><cell>0.643±0.002</cell><cell>282.00</cell><cell>84.46s/6.65hr</cell><cell>39751</cell><cell>36.144±2.191</cell><cell>61.156±3.973</cell><cell>167.50</cell><cell>26.69s/1.26hr</cell></row><row><cell>GAT</cell><cell>4</cell><cell>96182</cell><cell>0.671±0.002</cell><cell>0.673±0.002</cell><cell>328.25</cell><cell>68.23s/6.25hr</cell><cell>42637</cell><cell>51.501±0.962</cell><cell>97.851±1.114</cell><cell>157.00</cell><cell>18.12s/0.80hr</cell></row><row><cell>GatedGCN</cell><cell>4</cell><cell>97858</cell><cell>0.791±0.003</cell><cell>0.793±0.003</cell><cell>159.00</cell><cell>218.20s/9.72hr</cell><cell>40965</cell><cell>52.635±1.168</cell><cell>96.103±1.876</cell><cell>95.00</cell><cell>453.47s/12.09hr</cell></row><row><cell>GatedGCN-PE</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>41889</cell><cell>52.849±1.345</cell><cell>96.165±0.453</cell><cell>94.75</cell><cell>452.75s/12.08hr</cell></row><row><cell>GatedGCN-E</cell><cell>4</cell><cell>97858</cell><cell>0.808±0.003</cell><cell>0.811±0.003</cell><cell>197.00</cell><cell>218.51s/12.04hr</cell><cell>40965</cell><cell>49.212±1.560</cell><cell>88.747±1.058</cell><cell>95.00</cell><cell>451.21s/12.03hr</cell></row><row><cell cols="3">GatedGCN-E 16 500770</cell><cell>0.838±0.002</cell><cell>0.850±0.001</cell><cell>53.00</cell><cell>807.23s/12.17hr</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>GIN</cell><cell>4</cell><cell>99002</cell><cell>0.656±0.003</cell><cell>0.660±0.003</cell><cell>273.50</cell><cell>72.73s/5.56hr</cell><cell>39544</cell><cell>41.730±2.284</cell><cell>70.555±4.444</cell><cell>140.25</cell><cell>8.66s/0.34hr</cell></row><row><cell>RingGNN</cell><cell cols="2">2 106862</cell><cell>0.643±0.024</cell><cell>0.644±0.024</cell><cell>2.00</cell><cell>17850.52s/17.19hr</cell><cell>-</cell><cell>OOM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3WLGNN</cell><cell cols="2">2 507938 8 506564 3 106366 3 506681</cell><cell>0.704±0.003 Diverged 0.694±0.073 0.288±0.311</cell><cell>0.705±0.003 Diverged 0.695±0.073 0.290±0.312</cell><cell>3.00 Diverged 2.00 2.00</cell><cell>12835.53s/16.08hr Diverged 17468.81s/16.59hr 17190.17s/16.51hr</cell><cell>----</cell><cell>OOM OOM OOM OOM</cell><cell cols="3">RingGNN and 3WLGNN rely on dense tensors which leads to OOM on both GPU and CPU memory.</cell></row><row><cell></cell><cell cols="2">8 508832</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>-</cell><cell>OOM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>k-NN Heuristic</cell><cell></cell><cell>k =2</cell><cell>Test F1: 0.693</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matrix Fact.</cell><cell>0</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">60546561 44.206±0.452</cell><cell>100.000±0.000</cell><cell>254.33</cell><cell>2.66s/0.21hr</cell></row></table><note>L #Param Test Acc.±s.d. Train Acc.±s.d. #Epoch Epoch/Total #Param Test Acc.±s.d. Train Acc.±s.d.GRAPH REGRESSION -ZINC Evaluation Metrics: (higher is better, except for ZINC) • CLUSTER, PATTERN use weighted accuracy w.r.t. the class sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Extended training:• For TSP, RingGNN/3WLGNN with 100K parameters achieved 0.733±0.020 and 0.649±0.051 respectively after 48hr of training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#Epoch</cell><cell>Epoch/Total</cell></row><row><cell>MLP</cell><cell cols="2">4 108975</cell><cell>0.706±0.006</cell><cell>0.644±0.005</cell><cell>116.75</cell><cell>1.01s/0.03hr</cell></row><row><cell>GCN</cell><cell cols="2">4 103077</cell><cell>0.459±0.006</cell><cell>0.343±0.011</cell><cell>196.25</cell><cell>2.89s/0.16hr</cell></row><row><cell></cell><cell cols="2">16 505079</cell><cell>0.367±0.011</cell><cell>0.128±0.019</cell><cell>197.00</cell><cell>12.78s/0.71hr</cell></row><row><cell>GraphSage</cell><cell>4</cell><cell>94977</cell><cell>0.468±0.003</cell><cell>0.251±0.004</cell><cell>147.25</cell><cell>3.74s/0.15hr</cell></row><row><cell></cell><cell cols="2">16 505341</cell><cell>0.398±0.002</cell><cell>0.081±0.009</cell><cell>145.50</cell><cell>16.61s/0.68hr</cell></row><row><cell>MoNet</cell><cell cols="2">4 106002</cell><cell>0.397±0.010</cell><cell>0.318±0.016</cell><cell>188.25</cell><cell>1.97s/0.10hr</cell></row><row><cell></cell><cell cols="2">16 504013</cell><cell>0.292±0.006</cell><cell>0.093±0.014</cell><cell>171.75</cell><cell>10.82s/0.52hr</cell></row><row><cell>GAT</cell><cell cols="2">4 102385</cell><cell>0.475±0.007</cell><cell>0.317±0.006</cell><cell>137.50</cell><cell>2.93s/0.11hr</cell></row><row><cell></cell><cell cols="2">16 531345</cell><cell>0.384±0.007</cell><cell>0.067±0.004</cell><cell>144.00</cell><cell>12.98s/0.53hr</cell></row><row><cell>GatedGCN</cell><cell cols="2">4 105735</cell><cell>0.435±0.011</cell><cell>0.287±0.014</cell><cell>173.50</cell><cell>5.76s/0.28hr</cell></row><row><cell>GatedGCN-E</cell><cell cols="2">4 105875</cell><cell>0.375±0.003</cell><cell>0.236±0.007</cell><cell>194.75</cell><cell>5.37s/0.29hr</cell></row><row><cell></cell><cell cols="2">16 504309</cell><cell>0.282±0.015</cell><cell>0.074±0.016</cell><cell>166.75</cell><cell>20.50s/0.96hr</cell></row><row><cell cols="3">GatedGCN-E-PE 16 505011</cell><cell>0.214±0.013</cell><cell>0.067±0.019</cell><cell>185.00</cell><cell>10.70s/0.56hr</cell></row><row><cell>GIN</cell><cell cols="2">4 103079</cell><cell>0.387±0.015</cell><cell>0.319±0.015</cell><cell>153.25</cell><cell>2.29s/0.10hr</cell></row><row><cell></cell><cell cols="2">16 509549</cell><cell>0.526±0.051</cell><cell>0.444±0.039</cell><cell>147.00</cell><cell>10.22s/0.42hr</cell></row><row><cell>RingGNN</cell><cell>2</cell><cell>97978</cell><cell>0.512±0.023</cell><cell>0.383±0.020</cell><cell>90.25</cell><cell>327.65s/8.32hr</cell></row><row><cell>RingGNN-E</cell><cell cols="2">2 104403</cell><cell>0.363±0.026</cell><cell>0.243±0.025</cell><cell>95.00</cell><cell>366.29s/9.76hr</cell></row><row><cell></cell><cell cols="2">2 527283</cell><cell>0.353±0.019</cell><cell>0.236±0.019</cell><cell>79.75</cell><cell>293.94s/6.63hr</cell></row><row><cell></cell><cell cols="2">8 510305</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell><cell>Diverged</cell></row><row><cell>3WLGNN</cell><cell cols="2">3 102150</cell><cell>0.407±0.028</cell><cell>0.272±0.037</cell><cell>111.25</cell><cell>286.23s/8.88hr</cell></row><row><cell>3WLGNN-E</cell><cell cols="2">3 103098</cell><cell>0.256±0.054</cell><cell>0.140±0.044</cell><cell>117.25</cell><cell>334.69s/10.90hr</cell></row><row><cell></cell><cell cols="2">3 507603</cell><cell>0.303±0.068</cell><cell>0.173±0.041</cell><cell>120.25</cell><cell>329.49s/11.08hr</cell></row><row><cell></cell><cell cols="2">8 582824</cell><cell>0.303±0.057</cell><cell>0.246±0.043</cell><cell>52.50</cell><cell>811.27s/12.15hr</cell></row></table><note>Model L #Param Test MAE±s.d. Train MAE±s.d.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for the CSL dataset, with and without Laplacian Positional Encodings. Results are from 5-fold cross validation, run 20 times with different seeds. Red: the best model, Violet: good models. 100.000±0.000 100.000 100.000 100.000±0.000 100.000 100.000 125.64 0.40s/0.07hr GraphSage 4 105867 99.933±0.467 100.000 96.667 100.000±0.000 100.000 100.000 155.00 0.50s/0.11hr MoNet 4 105579 99.967±0.332 100.000 96.667 100.000±0.000 100.000 100.000 130.39 0.49s/0.09hr GAT 4 101710 99.933±0.467 100.000 96.667 100.000±0.000 100.000 100.000 133.18 0.61s/0.12hr GatedGCN 4 105407 99.600±1.083 100.000 96.667 100.000±0.000 100.000 100.000 147.06 0.66s/0.14hr 700±14.850 100.000 30.000 95.700±14.850 100.000 30.000 475.81 2.29s/1.51hr 3 506106 97.800±10.916 100.000 30.000 97.800±10.916 100.000 30.000 283.80 2.28s/0.90hr</figDesc><table><row><cell cols="2">Model L #Param</cell><cell cols="2">Test Accuracy Mean±s.d. Max</cell><cell>Min</cell><cell cols="2">Train Accuracy Mean±s.d. Max</cell><cell>Min</cell><cell>#Epoch</cell><cell>Epoch/ Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Node Positional Encoding with Laplacian Eigenmaps</cell><cell></cell><cell></cell></row><row><cell cols="2">MLP 4 101235</cell><cell>22.567±6.089</cell><cell>46.667</cell><cell>10.000</cell><cell>30.389±5.712</cell><cell>43.333</cell><cell>10.000</cell><cell cols="2">109.39 0.16s/0.03hr</cell></row><row><cell cols="2">GCN 4 103847 GIN 4 107304</cell><cell cols="6">99.333±1.333 100.000 96.667 100.000±0.000 100.000 100.000</cell><cell>62.98</cell><cell>0.44s/0.04hr</cell></row><row><cell cols="2">RingGNN 2 102726</cell><cell>17.233±6.326</cell><cell>40.000</cell><cell cols="3">10.000 26.122±14.382 58.889</cell><cell>10.000</cell><cell cols="2">122.75 2.93s/0.50hr</cell></row><row><cell cols="2">2 505086</cell><cell>25.167±7.399</cell><cell>46.667</cell><cell cols="3">10.000 54.533±18.415 82.222</cell><cell>10.000</cell><cell cols="2">120.58 3.11s/0.51hr</cell></row><row><cell cols="2">3WLGNN 3 102054</cell><cell>30.533±9.863</cell><cell>56.667</cell><cell>10.000</cell><cell cols="3">99.644±1.684 100.000 88.889</cell><cell>74.66</cell><cell>2.33s/0.25hr</cell></row><row><cell cols="2">3 505347</cell><cell>30.500±8.197</cell><cell>56.667</cell><cell cols="4">13.333 100.000±0.000 100.000 100.000</cell><cell>66.64</cell><cell>2.38s/0.23hr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">No Node Positional Encoding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MP-GNNs 4</cell><cell>100K</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RingGNN 2 101138</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell cols="2">103.23 3.09s/0.45hr</cell></row><row><cell cols="2">2 505325</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell>10.000±0.000</cell><cell>10.000</cell><cell>10.000</cell><cell>90.04</cell><cell>3.28s/0.42hr</cell></row><row><cell cols="3">3WLGNN 3 102510 95.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>4</cell><cell>99026</cell><cell>0.646±0.002</cell><cell>0.648±0.002</cell><cell>197.50</cell><cell>150.83s/8.34hr</cell></row><row><cell></cell><cell>GatedGCN</cell><cell></cell><cell>x</cell><cell>4</cell><cell>98174</cell><cell>0.757±0.009</cell><cell>0.760±0.009</cell><cell>218.25</cell><cell>197.80s/12.06hr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>97858</cell><cell>0.791±0.003</cell><cell>0.793±0.003</cell><cell>159.00</cell><cell>218.20s/9.72hr</cell></row><row><cell>TSP</cell><cell>GatedGCN-E</cell><cell>x</cell><cell>x</cell><cell>4 4</cell><cell>97858 95462</cell><cell>0.808±0.003 0.643±0.001</cell><cell>0.811±0.003 0.644±0.001</cell><cell>197.00 132.75</cell><cell>218.51s/12.04hr 325.22s/12.10hr</cell></row><row><cell></cell><cell>GAT</cell><cell></cell><cell>x</cell><cell>4</cell><cell>96182</cell><cell>0.671±0.002</cell><cell>0.673±0.002</cell><cell>328.25</cell><cell>68.23s/6.25hr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>96762</cell><cell>0.748±0.022</cell><cell>0.749±0.022</cell><cell>93.00</cell><cell>462.22s/12.10hr</cell></row><row><cell></cell><cell>GAT-E</cell><cell></cell><cell></cell><cell>4</cell><cell>96762</cell><cell>0.782±0.006</cell><cell>0.783±0.006</cell><cell>98.00</cell><cell>438.37s/12.11hr</cell></row><row><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>3</cell><cell>26593</cell><cell>35.989±1.549</cell><cell>60.586±4.251</cell><cell>148.00</cell><cell>263.62s/10.90h</cell></row><row><cell></cell><cell>GatedGCN</cell><cell></cell><cell>x</cell><cell>3</cell><cell>26715</cell><cell>50.668±0.291</cell><cell>96.128±0.576</cell><cell>172.00</cell><cell>384.39s/18.44hr</cell></row><row><cell>COLLAB</cell><cell>GatedGCN-E GAT</cell><cell>x</cell><cell>x x</cell><cell>3 3 3 3</cell><cell>27055 27055 28201 28561</cell><cell>51.537±1.038 47.212±2.016 41.141±0.701 50.662±0.687</cell><cell>96.524±1.704 85.801±0.984 70.344±1.837 96.085±0.499</cell><cell>188.67 156.67 153.50 174.50</cell><cell>376.67s/19.85hr 377.04s/16.49hr 371.50s/15.97hr 403.52s/19.69hr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>26676</cell><cell>49.674±0.105</cell><cell>92.665±0.719</cell><cell>201.00</cell><cell>349.19s/19.59hr</cell></row><row><cell></cell><cell>GAT-E</cell><cell></cell><cell></cell><cell>3</cell><cell>26676</cell><cell>44.989±1.395</cell><cell>82.230±4.941</cell><cell>120.67</cell><cell>328.29s/11.10hr</cell></row></table><note>Study of anisotropy and edge representations for link prediction on TSP and COLLAB. Red: the best model, Violet: good models.Model E.Feat. E.Repr. L #Param Test Acc.±s.d. Train Acc.±s.d. #Epochs Epoch/Total</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Summary statistics of all datasets. Numbers in parentheses of Node features and Edge features are the dimensions.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Graphs #Classes Avg. Nodes Avg. Edges</cell><cell>Node feat. (dim)</cell><cell>Edge feat. (dim)</cell></row><row><cell>ZINC</cell><cell>12000</cell><cell>-</cell><cell>23.16</cell><cell>49.83</cell><cell>Atom Type (28)</cell><cell>Bond Type (4)</cell></row><row><cell>PATTERN</cell><cell>14000</cell><cell>2</cell><cell>117.47</cell><cell>4749.15</cell><cell>Node Attr (3)</cell><cell>N.A.</cell></row><row><cell>CLUSTER</cell><cell>12000</cell><cell>6</cell><cell>117.20</cell><cell>4301.72</cell><cell>Node Attr (7)</cell><cell>N.A.</cell></row><row><cell>MNIST</cell><cell>70000</cell><cell>10</cell><cell>70.57</cell><cell>564.53</cell><cell>Pixel+Coord (3)</cell><cell>Node Dist (1)</cell></row><row><cell>CIFAR10</cell><cell>60000</cell><cell>10</cell><cell>117.63</cell><cell cols="2">941.07 Pixel[RGB]+Coord (5)</cell><cell>Node Dist (1)</cell></row><row><cell>TSP</cell><cell>12000</cell><cell>2</cell><cell>275.76</cell><cell>6894.04</cell><cell>Coord (2)</cell><cell>Node Dist (1)</cell></row><row><cell>COLLAB</cell><cell>1</cell><cell>-</cell><cell cols="2">235868.00 2358104.00</cell><cell>Word Embs (128)</cell><cell>Year &amp; Weight (2)</cell></row><row><cell>CSL</cell><cell>150</cell><cell>10</cell><cell>41.00</cell><cell>164.00</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell>ENZYMES</cell><cell>600</cell><cell>6</cell><cell>32.63</cell><cell>62.14</cell><cell>Node Attr (18)</cell><cell>N.A.</cell></row><row><cell>DD</cell><cell>1178</cell><cell>2</cell><cell>284.32</cell><cell>715.66</cell><cell>Node Label (89)</cell><cell>N.A.</cell></row><row><cell>PROTEINS</cell><cell>1113</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell><cell>Node Attr (29)</cell><cell>N.A.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance on the TU datasets with 10-fold cross validation (higher is better). Two runs of all the experiments using the same hyperparameters but different random seeds are shown separately to note the differences in ranking and variation for reproducibility.The top 3 performance scores are highlighted as First, Second, Third. Test Acc.±s.d. Train Acc.±s.d. #Epoch Epoch/Total Test Acc.±s.d. Train Acc.±s.d. #Epoch Epoch/Total</figDesc><table><row><cell>Dataset</cell><cell>Model L #Param</cell><cell>seed 1</cell><cell>seed 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The indices are saved and used across all experiments for fair comparisons. There are 480 train/60 validation/60 test graphs for ENZYMES, 941 train/118 validation/119 test graphs for DD, and 889 train/112 validation/112 test graphs for PROTEINS datasets in each of the folds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Study of positional encodings (PEs) with the GatedGCN model<ref type="bibr" target="#b10">[11]</ref>. Performance reported on the test sets of CSL, ZINC, PATTERN, CLUSTER and COLLAB (higher is better, except for ZINC). Red: the best model. type L #Param Test Acc.±s.d.Train Acc.±s.d. #Epochs Epoch/Total</figDesc><table><row><cell>PE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Anisotropic with intermediate edge features computed as joint representations of adjacent node features at each layer, as in GAT by default, Eq.(18):</figDesc><table><row><cell>h +1 i</cell><cell cols="2">= h i + ELU BN Concat K k=1</cell><cell>e k, ij U k, h j</cell><cell>,</cell><cell>(62)</cell></row><row><cell></cell><cell></cell><cell>j∈Ni</cell><cell></cell><cell></cell></row><row><cell>e k, ij</cell><cell>=</cell><cell>exp(ê k, ij )</cell><cell></cell><cell></cell></row></table><note>j ∈Ni exp(ê k, ij )</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.montrealdeclaration-responsibleai.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/bknyaz/graph_attention_pool</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>XB is supported by NRF Fellowship NRFF2017-10.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anisotropic with edge features as well as explicit edge representations updated across layers in addition to node features:</p><p>where</p><p>K are the K linear projection heads and a k, ij are the attention coefficients for each head. The input edge features from the datasets can optionally be used to initialize the edge representations e =0 ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 GraphSage</head><p>Interestingly, in <ref type="table">Table 2</ref> for COLLAB, we found that the isotropic GraphSage with max aggregation performs close to GAT and GatedGCN models, both of which perform anisotropic mean aggregation. On the other hand, models which use sum aggregation (GIN, MoNet) are unable to beat the simple matrix factorization baseline. This result indicates that aggregation functions which are invariant to node degree (max and mean) provide a powerful inductive bias for COLLAB.</p><p>We instantiate two anisotropic variants of GraphSage, as described in the following paragraphs, and compare them to GAT and GatedGCN on COLLAB in <ref type="table">Table 8</ref>. We find that upgrading max aggregators with edge features does not significantly boost performance. On the other hand, maintaining explicit edge representations across layers hurts the models, presumably due to using very small hidden dimensions. (As previously mentioned, maintaining representations for both 235K nodes and 2.3M edges leads to significant GPU memory usage and requires using smaller hidden dimensions.)</p><p>Isotropic, as in GraphSage by default, Eq. <ref type="formula">(16)</ref>:</p><p>where U ∈ R d×2d , V ∈ R d×d .</p><p>Anisotropic with intermediate edge features computed as joint representations of adjacent node features at each layer:</p><p>where U ∈ R d×2d , V , A ∈ R d×d , is the Hadamard product, and e ij are the edge gates.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ribert</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasek</forename><surname>Chvatal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cook</surname></persName>
		</author>
		<title level="m">Concorde tsp solver</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. NeurIPS workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9448" to="9458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Machine learning for combinatorial optimization: a methodological tour d&apos;horizon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Prouvost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06128</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A two-step graph convolutional decoder for molecule generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Machine Learning and the Physical Sciences</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gnn-film: Graph neural networks with feature-wise linear modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Low-dimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Are powerful graph neural nets necessary? a dissection on graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning symbolic physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Miles D Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The deep learning revolution and its implications for computer architecture and chip design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Solid-State Circuits Conference-(ISSCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral grouping using the nystrom method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mm Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced methods for knowledge discovery from complex data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transient networks of spatiotemporal connectivity map communication pathways in brain functional systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Griffa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ricaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirell</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Daducci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Hagmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="490" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Redundancy-free computation graphs for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03707</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6348" to="6358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention, learn to solve routing problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<title level="m">Deepgcns: Making gcns go as deep as cnns. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sampling: design and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lohr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Nelson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxhell</forename><surname>Luzhnica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04682</idno>
		<title level="m">On graph classification networks, datasets and baselines</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Technical perspective: What led computer vision to deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning scheduling algorithms for data processing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Shaileshh Bojja Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication</title>
		<meeting>the ACM Special Interest Group on Data Communication</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="270" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04826</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-Hamu</surname></persName>
		</author>
		<title level="m">Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<title level="m">On the universality of invariant networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Yazgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10746</idno>
		<title level="m">Chip placement with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<title level="m">Fake news detection on social media using geometric deep learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>high-performance deep learning library</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A survey of deep learning for scientific discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11755</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09405</idno>
		<title level="m">Learning to simulate complex physics with graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<title level="m">On the equivalence between node embeddings and structural graph representations. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Essential guidelines for computational method benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robrecht</forename><surname>Saelens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Cannoodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Soneson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hapfelmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvan</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark D</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<editor>Marinka Zitnik Yuxiao Dong Hongyu Ren Bowen Liu Michele Catasta Jure Leskovec Weihua Hu, Matthias Fey</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Aligraph: a comprehensive graph neural network platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2094" to="2105" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<title level="m">Study of anisotropic edge features and representations for link prediction on COLLAB, including GraphSage models. Red: the best model, Violet: good models</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
