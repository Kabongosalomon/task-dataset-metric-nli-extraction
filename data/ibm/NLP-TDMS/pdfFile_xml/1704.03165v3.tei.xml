<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">struc2vec: Learning Node Representations from Structural Identity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems Eng. and Comp. Science Dep</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">H P</forename><surname>Saverese</surname></persName>
							<email>savarese@land.ufrj.br</email>
							<affiliation key="aff1">
								<orgName type="department">Federal University of Rio de Janeiro Systems Eng. and Comp. Science Dep</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
							<email>daniel@land.ufrj.br</email>
							<affiliation key="aff2">
								<orgName type="department">Federal University of Rio de Janeiro Systems Eng. and Comp. Science Dep</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">struc2vec: Learning Node Representations from Structural Identity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3097983.3098061</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS •Computing methodologies → Unsupervised learning</term>
					<term>Learn- ing latent representations</term>
					<term>•Arti cial Intelligence → Learning</term>
					<term>KEYWORDS feature learning</term>
					<term>node embeddings</term>
					<term>structural identity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural identity is a concept of symmetry in which network nodes are identi ed according to the network structure and their relationship to other nodes. Structural identity has been studied in theory and practice over the past decades, but only recently has it been addressed with representational learning techniques.</p><p>is work presents struc2vec, a novel and exible framework for learning latent representations for the structural identity of nodes. struc2vec uses a hierarchy to measure node similarity at di erent scales, and constructs a multilayer graph to encode structural similarities and generate structural context for nodes. Numerical experiments indicate that state-of-the-art techniques for learning node representations fail in capturing stronger notions of structural identity, while struc2vec exhibits much superior performance in this task, as it overcomes limitations of prior approaches. As a consequence, numerical experiments indicate that struc2vec improves performance on classi cation tasks that depend more on structural identity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An example of two nodes (u and ) that are structurally similar (degrees 5 and 4, connected to 3 and 2 triangles, connected to the rest of the network by two nodes), but very far apart in the network.</p><p>emerges when node function is de ned solely by the network structure. In this context, not even the labels of the nodes ma er but just their relationship to other nodes (edges). Indeed, mathematical sociologists have worked on this problem since the 1970s, de ning and computing structural identity of individuals in social networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. Beyond sociology, the role of webpages in the webgraph is another example of identity (in this case, hubs and authorities) emerging from the network structure, as de ned by the celebrated work of Kleinberg <ref type="bibr" target="#b7">[8]</ref>.</p><p>e most common practical approaches to determine the structural identity of nodes are based on distances or recursions. In the former, a distance function that leverages the neighborhood of the nodes is used to measure the distance between all node pairs, with clustering or matching then performed to place nodes into equivalent classes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. In the later, a recursion with respect to neighboring nodes is constructed and then iteratively unfolded until convergence, with nal values used to determine the equivalent classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>. While such approaches have advantages and disadvantages, we provide an alternative methodology, one based on unsupervised learning of representations for the structural identity of nodes (to be presented). Recent e orts in learning latent representations for nodes in networks have been quite successful in performing classi cation and prediction tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, these e orts encode nodes using as context a generalized notion of their neighborhood (e.g., w steps of a random walk, or nodes with neighbors in common). In a nutshell, nodes that have neighborhoods with similar sets of nodes should have similar latent representations. But neighborhood is a local concept de ned by some notion of proximity in the network. us, two nodes with neighborhoods that are structurally similar but that are far apart will not have similar latent representations. <ref type="figure">Figure 1</ref> illustrates the problem, where nodes u and play similar roles (i.e., have similar local structures) but are very far apart in the network. Since their neighborhoods have no common nodes, recent approaches cannot capture their structural similarity (as we soon show).</p><p>It is worth noting why recent approaches for learning node representations such as DeepWalk <ref type="bibr" target="#b15">[16]</ref> and node2vec <ref type="bibr" target="#b5">[6]</ref> succeed in classi cation tasks but tend to fail in structural equivalence tasks.</p><p>e key point is that many node features in most real networks exhibit a strong homophily (e.g., two blogs with the same political inclination are much more likely to be connected than at random). Neighbors of nodes with a given feature are more likely to have the same feature. us, nodes that are "close" in the network and in the latent representation will tend to share features. Likewise, two nodes that are "far" in the network will tend to be separated in the latent representation, independent of their local structure. us, structural equivalence will not properly be captured in the latent representation. However, if classi cation is performed on features that depend more on structural identity and less on homophily, then such recent approaches are likely to be outperformed by latent representations that be er capture structural equivalence (as we soon show).</p><p>Our main contribution is a exible framework for learning latent representations for the structural identity of nodes, called struc2vec. It is an alternative and powerful tool to the study of structural identity through latent representations. e key ideas of struc2vec are:</p><p>• Assess structural similarity between nodes independently of node and edge a ributes as well as their position in the network. us, two nodes that have a similar local structure will be considered so, independent of network position and node labels in their neighborhoods. Our approach also does not require the network to be connected, and identi es structurally similar nodes in di erent connected components.</p><p>• Establish a hierarchy to measure structural similarity, allowing progressively more stringent notions of what it means to be structurally similar. In particular, at the bottom of the hierarchy, structural similarity between nodes depend only on their degrees, while at the top of the hierarchy similarity depends on the entire network (from the viewpoint of the node). • Generates random contexts for nodes, which are sequences of structurally similar nodes as observed by a weighted random walk traversing a multilayer graph (and not the original network). us, two nodes that frequently appear with similar contexts will likely have similar structure. Such context can be leveraged by language models to learn latent representation for the nodes.</p><p>We implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks, comparing its performance with DeepWalk <ref type="bibr" target="#b15">[16]</ref> and node2vec [6]two state-of-the-art techniques for learning latent representations for nodes, and with RolX [7] -a recent approach to identify roles of nodes. Our results indicate that while DeepWalk and node2vec fail to capture the notion of structural identity, struc2vec excels on this task -even when the original network is subject to strong random noise (random edge removal). We also show that struc2vec is superior in a classi cation task where node labels depends more on structural identity (i.e., air-tra c networks with labels representing airport activity). e remainder of this paper is organized as follows. Section 2 brie y overviews the recent related work on learning latent representations of nodes in networks. Section 3 presents the struc2vec framework in detail. Experimental evaluation and comparison to other methods are shown in Section 4. Finally, Section 5 concludes the paper with a brief discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Embedding network nodes in (Euclidean) space has received much a ention over the past decades from di erent communities. e technique is instrumental for Machine Learning applications that leverage network data, as node embeddings can be directly used in tasks such as classi cation and clustering.</p><p>In Natural Language Processing <ref type="bibr" target="#b1">[2]</ref>, generating dense embeddings for sparse data has a long history. Recently, Skip-Gram <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> was proposed as an e cient technique to learn embeddings for text data (e.g., sentences). Among other properties, the learned language model places semantically similar words near each other in space.</p><p>Learning a language model from a network was rst proposed by DeepWalk <ref type="bibr" target="#b15">[16]</ref>. It uses random walks to generate sequences of nodes from the network, which are then treated as sentences by Skip-Gram. Intuitively, nodes close in the network will tend to have similar contexts (sequences) and thus have embeddings that are near one another. is idea was later extended by node2vec <ref type="bibr" target="#b5">[6]</ref>. By proposing a biased second order random walk model, node2vec provides more exibility when generating the context of a vertex. In particular, the edge weights driving the biased random walks can be designed in an a empt to capture both vertex homophily and structural equivalence. However, a fundamental limitation is that structurally similar nodes will never share the same context if their distance (hop count) is larger than the Skip-Gram window.</p><p>subgraph2vec <ref type="bibr" target="#b13">[14]</ref> is another recent approach for learning embeddings for rooted subgraphs, and unlike the previous techniques it does not use random walks to generate context. Alternatively, the context of a node is simply de ned by its neighbors. Additionally, subgraph2vec captures structural equivalence by embedding nodes with the same local structure to the same point in space. Nonetheless, the notion of structural equivalence is very rigid since it is de ned as a binary property dictated by the Weisfeiler-Lehman isomorphism test <ref type="bibr" target="#b20">[21]</ref>. us, two nodes that are structurally very similar (but fail the test) and have non-overlapping neighbors may not be close in space.</p><p>Similarly to subgraph2vec, considerable e ort has recently been made on learning richer representations for network nodes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. However, building representations that explicitly capture structural identity is a relative orthogonal problem that has not received much a ention. is is the focus of struc2vec.</p><p>A recent approach to explicitly identify the role of nodes using just the network structure is RolX <ref type="bibr" target="#b6">[7]</ref>. is unsupervised approach is based on enumerating various structural features for nodes, nding the more suited basis vector for this joint feature space, and then assigning for every node a distribution over the identi ed roles (basis), allowing for mixed membership across the roles. Without explicitly considering node similarity or node context (in terms of structure), RolX is likely to miss node pairs that are structurally equivalent (to be shown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STRUC2VEC</head><p>Consider the problem of learning representations that capture the structural identity of nodes in the network. A successful approach should exhibit two desired properties:</p><p>• e distance between the latent representation of nodes should be strongly correlated to their structural similarity.</p><p>us, two nodes that have identical local network structures should have the same latent representation, while nodes with di erent structural identities should be far apart.</p><p>• e latent representation should not depend on any node or edge a ribute, including the node labels. us, structurally similar nodes should have close latent representation, independent of node and edge a ributes in their neighborhood. e structural identity of nodes must be independent of its "position" in the network. Given these two properties, we propose struct2vec, a general framework for learning latent representations for nodes composed of four main steps, as follows:</p><p>(1) Determine the structural similarity between each vertex pair in the graph for di erent neighborhood sizes. is induces a hierarchy in the measure for structural similarity between nodes, providing more information to assess structural similarity at each level of the hierarchy.</p><p>(2) Construct a weighted multilayer graph where all nodes in the network are present in every layer, and each layer corresponds to a level of the hierarchy in measuring structural similarity. Moreover, edge weights among every node pair within each layer are inversely proportional to their structural similarity. (3) Use the multilayer graph to generate context for each node.</p><p>In particular, a biased random walk on the multilayer graph is used to generate node sequences. ese sequences are likely to include nodes that are more structurally similar. (4) Apply a technique to learn latent representation from a context given by the sequence of nodes, for example, Skip-Gram. Note that struct2vec is quite exible as it does not mandates any particular structural similarity measure or representational learning framework. In what follows, we explain in detail each step of struct2vec and provide a rigorous approach to a hierarchical measure of structural similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measuring structural similarity</head><p>e rst step of struct2vec is to determine a structural similarity between two nodes without using any node or edge a ributes. Moreover, this similarity metric should be hierarchical and cope with increasing neighborhood sizes, capturing more re ned notions of structural similarity. Intuitively, two nodes that have the same degree are structurally similar, but if their neighbors also have the same degree, then they are even more structurally similar.</p><p>Let G = (V , E) denote the undirected, unweighted network under consideration with vertex set V and edge set E, where n = |V | denotes the number of nodes in the network and k * its diameter. Let R k (u) denote the set of nodes at distance (hop count) exactly k ≥ 0 from u in G. Note that R 1 (u) denotes the set of neighbors of u and in general, R k (u) denotes the ring of nodes at distance k. Let s(S) denote the ordered degree sequence of a set S ⊂ V of nodes.</p><p>By comparing the ordered degree sequences of the rings at distance k from both u and we can impose a hierarchy to measure structural similarity. In particular, let f k (u, ) denote the structural distance between u and when considering their k-hop neighborhoods (all nodes at distance less than or equal to k and all edges among them). In particular, we de ne:</p><formula xml:id="formula_0">f k (u, ) = f k −1 (u, ) + (s(R k (u)), s(R k ( ))), k ≥ 0 and |R k (u)|, |R k ( )| &gt; 0<label>(1)</label></formula><p>where (D 1 , D 2 ) ≥ 0 measures the distance between the ordered degree sequences D 1 and D 2 and f −1 = 0. Note that by de nition f k (u, ) is non-decreasing in k and is de ned only when both u or have nodes at distance k. Moreover, using the ring at distance k in the de nition of f k (u, ) forces the comparison between the degree sequences of nodes that are at the same distance from u and . Finally, note that if the k-hop neighborhoods of u and are isomorphic, and maps u onto , then f k −1 (u, ) = 0.</p><p>A nal step is determining the function that compares two degree sequences. Note that s(R k (u)) and s(R k ( )) can be of di erent sizes and its elements are arbitrary integers in the range [0, n − 1] with possible repetitions. We adopt Dynamic Time Warping (DTW) to measure the distance between two ordered degree sequences, a technique that can cope be er with sequences of di erent sizes and loosely compares sequence pa erns <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Informally, DTW nds the optimal alignment between two sequences A and B. Given a distance function d(a, b) for the elements of the sequence, DTW matches each element a ∈ A to b ∈ B, such that the sum of the distances between matched elements is minimized. Since elements of sequence A and B are degrees of nodes, we adopt the following distance function:</p><formula xml:id="formula_1">d(a, b) = max(a, b) min(a, b) − 1<label>(2)</label></formula><p>Note that when a = b then d(a, b) = 0. us, two identical ordered degree sequences will have zero distance. Also note that by taking the ratio between the maximum and the minimum, the degrees 1 and 2 are much more di erent than degrees 101 and 102, a desired property when measuring the distance between node degrees. Finally, while we use DTW to assess the similarity between two ordered degree sequences, any other cost function could be adopted by our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing the context graph</head><p>We construct a multilayer weighted graph that encodes the structural similarity between nodes. Recall that G = (V , E) denotes the original network (possibly not connected) and k * its diameter. Let M denote the multilayer graph where layer k is de ned using the k-hop neighborhoods of the nodes. Each layer k = 0, . . . , k * is formed by a weighted undirected complete graph with node set V , and thus, n 2 edges. e edge weight between two nodes in a layer is given by:</p><formula xml:id="formula_2">w k (u, ) = e −f k (u, ) , k = 0, . . . , k *<label>(3)</label></formula><p>Note that edges are de ned only if f k (u, ) is de ned and that weights are inversely proportional to structural distance, and assume values smaller than or equal to 1, being equal to 1 only if f k (u, ) = 0. Note that nodes that are structurally similar to u will have larger weights across various layers of M. We connect the layers using directed edges as follows. Each vertex is connected to its corresponding vertex in the layer above and below (layer permi ing). us, every vertex u ∈ V in layer k is connected to the corresponding vertex u in layer k + 1 and k − 1.</p><p>e edge weight between layers are as follows:</p><formula xml:id="formula_3">w(u k , u k +1 ) = log(Γ k (u) + e), k = 0, . . . , k * − 1 w(u k , u k −1 ) = 1, k = 1, . . . , k *<label>(4)</label></formula><p>where Γ k (u) is number of edges incident to u that have weight larger than the average edge weight of the complete graph in layer k. In particular:</p><formula xml:id="formula_4">Γ k (u) = ∈V 1(w k (u, ) &gt; w k ) (5) where w k = (u, )∈( V 2 ) w k (u, )/ n 2 .</formula><p>us, Γ k (u) measures the similarity of node u to other nodes in layer k. Note that if u has many similar nodes in the current layer, then it should change layers to obtain a more re ned context. Note that by moving up one layer the number of similar nodes can only decrease. Last, the log function simply reduces the magnitude of the potentially large number of nodes that are similar to u in a given layer. Finally, note that M has nk * vertices and at most k * n 2 + 2n(k * − 1) weighted edges. In Section 3.5 we discuss how to reduce the complexity of generating and storing M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating context for nodes</head><p>e multilayer graph M is used to generate structural context for each node u ∈ V . Note that M captures the structure of structural similarities between nodes in G using absolutely no label information. As in previous works, struct2vec uses random walks to generate sequence of nodes to determine the context of a given node. In particular, we consider a biased random walk that moves around M making random choices according to the weights of M. Before each step, the random walk rst decides if it will change layers or walk on the current layer (with probability q &gt; 0 the random walk stays in the current layer).</p><p>Given that it will stay in the current layer, the probability of stepping from node u to node in layer k is given by:</p><formula xml:id="formula_5">p k (u, ) = e −f k (u, ) Z k (u)<label>(6)</label></formula><p>where Z k (u) is the normalization factor for vertex u in layer k, simply given by:</p><formula xml:id="formula_6">Z k (u) = ∈V u e −f k (u, )<label>(7)</label></formula><p>Note that the random walk will prefer to step onto nodes that are structurally more similar to the current vertex, avoiding nodes that have very li le structural similarity with it. us, the context of a node u ∈ V is likely to have structurally similar nodes, independent of their labels and position in the original network G.</p><p>With probability 1 −q, the random walk decides to change layers, and moves to the corresponding node either in layer k + 1 or layer k − 1 (layer permi ing) with probability proportional to the edge weights. In particular:</p><formula xml:id="formula_7">p k (u k , u k +1 ) = w(u k , u k +1 ) w(u k , u k +1 ) + w(u k , u k −1 ) p k (u k , u k −1 ) = 1 − p k (u k , u k +1 )<label>(8)</label></formula><p>Note that every time the walker steps within a layer it includes the current vertex as part of its context, independent of the layer. us, a vertex u may have a given context in layer k (determined by the structural similarity of this layer), but have a subset of this context at layer k + 1, as the structural similarity cannot increase as we move to higher layers. is notion of a hierarchical context across the layers is a fundamental aspect of the proposed methodology.</p><p>Finally, for each node u ∈ V , we start a random walk in its corresponding vertex in layer 0. Random walks have a xed and relatively short length (number of steps), and the process is repeated a certain number of times, giving rise to multiple independent walks (i.e., the multiple contexts of node u).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning a language model</head><p>Recent language modeling techniques have been extensively used to learn word embeddings, and only require sets of sentences in order to generate meaningful representations. Informally, the task can be de ned as learning word probabilities given a context. In particular, Skip-Gram <ref type="bibr" target="#b11">[12]</ref> has proven to be e ective at learning meaningful representations for a variety of data. In order to apply it to networks, it su ces to use arti cially generated node sequences instead of word sentences. In our framework, these sequences are generated by biased random walks on the multilayer graph M. Given a node, Skip-Gram aims to maximize the likelihood of its context in a sequence, where a node's context is given by a window of size w centered on it.</p><p>For this work we use Hierarchical So max, where conditional symbol probabilities are calculated using a tree of binary classiers. For each node j ∈ V , Hierarchical So max assigns a speci c path in the classi cation tree, de ned by a set of tree nodes n( j , 1), n( j , 2), . . . , n( j , h), where n( j , h) = j . In this se ing, we have:</p><formula xml:id="formula_8">P( j | i ) = h k =1 C(n( j , k), i )<label>(9)</label></formula><p>where C is a binary classi er present in every node in the tree. Note that since Hierarchical So max operates on a binary tree, we have that h = O(log |V |). We train Skip-Gram according to its optimization problem given by equation <ref type="bibr" target="#b8">(9)</ref>. Note that while we use Skip-Gram to learn node embeddings, any other technique to learn latent representations for text data could be used in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity and optimizations</head><p>In order to construct M, the structural distance between every node pair for every layer must be computed, namely, f k (u, ) for u, ∈ V , and 0 ≤ k ≤ k * . However, f k (u, ) uses the result of a DTW calculation between two degree sequences. While classic implementation of DTW has complexity O( 2 ), fast techniques have complexity O( ), where is the size of the largest sequence <ref type="bibr" target="#b19">[20]</ref>. Let d max denote the largest degree in the network. en, the size of the degree sequence |s(R k (u))| ≤ min(d k max , n), for any node u and layer k. Since in each layer there are n 2 pairs, the complexity of computing all distances for layer k is O(n 2 min(d k max , n)). e nal complexity is then O(k * n 3 ). In what follows we describe a series of optimizations that will signi cantly reduce the computation and memory requirements of the framework.</p><p>Reducing the length of degree sequences (OPT1). Although degree sequences at layer k have lengths bounded by min(d k max , n), for some networks this can be quite large even for small k (e.g., for k = 3 the sequences are already O(n)). To reduce the cost of comparing large sequences, we propose compressing the ordered degree sequence as follows. For each degree in the sequence, we count the number of occurrences of that degree. e compressed ordered degree sequence is a tuple with the degree and the number of occurrences. Since many nodes in a network tend to have the same degree, in practice the compressed ordered degree sequence can be an order of magnitude smaller than the original.</p><p>Let A and B denote the compressed degree sequences of A and B, respectively. Since the elements of A and B are tuples, we adapt the DTW pairwise distance function as follows:</p><formula xml:id="formula_9">dist(a, b) = max(a 0 , b 0 ) min(a 0 , b 0 ) − 1 max(a 1 , b 1 )<label>(10)</label></formula><p>where a = (a 0 , a 1 ) and b = (b 0 , b 1 ) are tuples in A and B , respectively; a 0 and b 0 are the degrees; a 1 and b 1 are the number of occurrences. Note that using the compressed degree sequence leads to comparisons between pieces of the original sequences that have the same degree (as opposed to comparing every degree). us, equation (10) leads to an approximation of the DTW on the original degree sequences, as given by equation <ref type="bibr" target="#b1">(2)</ref>. However, DTW now operates on A and B , which are much shorter than A and B, respectively.</p><p>Reducing the number of pairwise similarity calculations (OPT2). While the original framework assesses the similarity between every node pair at every layer k, clearly this seems unnecessary. Consider two nodes with very di erent degrees (eg., 2 and 20). eir structural distance even for k = 0 will be large, and consequently the edge between them in M will have a very small weight. us, when generating context for these nodes, the random walk is unlikely to traverse this edge. Consequently, not having this edge in M will not signi cantly change the model.</p><p>We limit the number of pairwise similarity calculations to Θ(log n) per node, for every level k. Let u denote the set of nodes that will be neighbors of u in M, which will be the same for every level. u should have the nodes most structurally similar to u. In order to determine u , we take the nodes that have degrees most similar to u. is can be computed e ciently by performing a binary search on the ordered degree sequence of all nodes in the network (for the degree of node u), and taking log n consecutive nodes on each direction a er the search completes. us, computing u has complexity Θ(log n). Computing u for all nodes has complexity Θ(n log n) which is also needed for sorting the degrees of the network. As for memory requirements, each layer of M will now have Θ(n log n) edges as opposed to Θ(n 2 ).</p><p>Reducing the number of layers (OPT3). e number of layers in M is given by the diameter of the network, k * . However, for many networks the diameter can be much larger than the average distance. Moreover, the importance of assessing the structural similarity between two nodes diminishes with arbitrarily large values for k. In particular, when k is near k * the length of the degree sequences of the rings become relatively short, and thus f k (u, ) is not much di erent from f k −1 (u, ). erefore, we cap the number the layers in M to a xed constant k &lt; k * , capturing the most important layers for assessing structural similarity. is signi cantly reduces the computational and memory requirements for constructing M.</p><p>Although the combination of the above optimizations a ects the capacity of the framework in generating good representations for nodes that are structurally similar, we will show that their impact is marginal and sometimes even bene cial. us, the bene ts in reducing computational and memory requirements of the framework greatly outweighs its drawbacks. Last, we make struc2vec available at: h ps://github.com/leoribeiro/struc2vec</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>In what follows we evaluate struct2vec in di erent scenarios in order to illustrate its potential in capturing the structural identity of nodes, also in light of state-of-the-art techniques for learning node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Barbell graph</head><p>We denote B(h, k) as the (h, k)-barbell graph which consists of two complete graphs K 1 and K 2 (each having h nodes) connected by a path graph P of length k. Two nodes b 1 ∈ V (K 1 ) and b 2 ∈ V (K 2 ) act as the bridges. Using {p 1 , . . . , p k } to denote V (P), we connect b 1 to p 1 and b 2 to p k , thus connecting the three graphs.</p><p>e barbell graph has a signi cant number of nodes with the same structural identity. Let C 1 = V (K 1 ) \ {b 1 } and C 2 = V (K 2 ) \ {b 2 }. Note that all nodes ∈ {C 1 ∪ C 2 } are structurally equivalent, in the strong sense that there exists an automorphism between any pair of such nodes. Additionally, we also have that all node pairs {p i , p k −i }, for 1 ≤ i ≤ k − 1, along with the pair {b 1 , b 2 }, are structurally equivalent in the same strong sense. <ref type="figure" target="#fig_0">Figure 2a</ref> illustrates a B(10, 10) graph, where structurally equivalent nodes have the same color.</p><p>us, we expect struct2vec to learn vertex representations that capture the structural equivalence mentioned above. Every node pair that is structurally equivalent should have similar latent representation. Moreover, the learned representations should also capture structural hierarchies: while the node p 1 is not equivalent to neither nodes p 2 or b 1 , we can clearly see that from a structural point of view it is more similar to p 2 (it su ces to compare their degrees). <ref type="figure" target="#fig_0">Figure 2</ref> shows the latent representations learned by DeepWalk, node2vec and struct2vec for B <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b9">10)</ref>. DeepWalk fails to capture structural equivalences, which is expected since it was not designed to consider structural identities. As illustrated, node2vec does not capture structural identities even with di erent variations of its parameters p and q. In fact, it learns mostly graph distances, placing closer in the latent space nodes that are closer (in hops) in the graph. Another limitation of node2vec is that Skip-Gram's window size makes it impossible for nodes in K 1 and K 2 to appear in the same context. struct2vec, on the other hand, learns representations that properly separate the equivalent classes, placing structurally equivalent nodes near one another in the latent space. Note that nodes of the same color are tightly grouped together. Moreover, p 1 and p 10 are placed close to representations for nodes in K 1 and K 2 , as they are the bridges. Finally, note that none of the three optimizations have any signi cant e ect on the quality of the representations. In fact, structurally equivalent nodes are even closer to one another in the latent representations under OPT1.</p><p>Last, we apply RolX to the barbell graph (results in <ref type="figure" target="#fig_0">Figure 2(b)</ref>). A total of six roles were identi ed and some roles indeed precisely captured structural equivalence (roles 1 and 3). However, structurally equivalent nodes (in K 1 and K 2 ) were placed in three different roles (role 0, 2, and 5) while role 4 contains all remaining nodes in the path. us, although RolX does capture some notion of structural equivalence when assigning roles to nodes, struct2vec be er identi es and separates structural equivalence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Karate network</head><p>e Zachary's Karate Club <ref type="bibr" target="#b24">[25]</ref> is a network composed of 34 nodes and 78 edges, where each node represents a club member and edges denote if two members have interacted outside the club. In this network, edges are commonly interpreted as indications of friendship between members.</p><p>We construct a network composed of two copies G 1 and G 2 of the Karate Club network, where each node ∈ V (G 1 ) has a mirror node u ∈ V (G 2 ). We also connect the two networks by adding an edge between mirrored node pairs 1 and 37. Although this is not necessary for our framework, DeepWalk and node2vec cannot place in the same context nodes in di erent connected components of the graph. us, we add the edge for a more fair comparison   <ref type="figure" target="#fig_2">Figure 4a</ref> shows mirrored network with corresponding pairs having the same color. <ref type="figure" target="#fig_1">Figure 3</ref> shows the representations learned by DeepWalk, node2vec and struct2vec. Clearly, Deepwalk and node2vec fail to group in the latent space structurally equivalent nodes, including mirrored nodes.</p><p>Once again, struct2vec manages to learn features that properly capture the structural identity of nodes. Mirrored pairs -that is, nodes with the same color -stay close together in the latent space, and there is a complex structural hierarchy in the way the representations are grouped together.</p><p>As an example, note that nodes 1, 34 and their correspondent mirrors (37 and 42) are in a separate cluster in the latent space. Interestingly, these are exactly the nodes that represent the club instructor Mr. Hi and his administrator John A. e network was gathered a er a con ict between the two split the members of the club into two groups -centered on either Mr. Hi or John A. erefore, nodes 1 and 34 have the speci c and similar role of leaders in the network. Note that struct2vec captures their function even though there is no edge between them.</p><p>Another visible cluster in the latent space is composed of nodes 2, 3, 4 and 33, also along with their mirrors. ese nodes also have a speci c structural identity in the network: all of them have high degrees and are also connected to at least one of the leaders. Lastly, nodes 26 and 25 (far right in the latent space) have extremely close representations, which agrees with their structural role: both have low degree and are 2 hops away from leader 34.</p><p>struct2vec also captures non-trivial structural equivalences. Note that nodes 7 and 50 (pink and yellow) are mapped to close points in the latent space. Surprisingly, these two nodes are structurally equivalent -there exists an automorphism in the graph that maps one into the other. is can be more easily seen once we note that nodes 6 and 7 are also structurally equivalent, and 50 is the mirrored version of node 6 (therefore also structurally equivalent).</p><p>Last, <ref type="figure" target="#fig_2">Figure 4b</ref> shows the roles identi ed by RolX in the mirrored Karate network (28 roles were identi ed). Note that leaders 1 and 34 were placed in di erent roles. e mirror for 1 (node 37) was also placed in a di erent role, while the mirror for 34 (node 42) was placed in the same role as 34. A total of 7 corresponding pairs (out of 34) were placed in the same role. However, some other structural similarities were also identi ed -e.g., nodes 6 and 7 are structurally equivalent and were assigned the same role. Again, RolX seems to capture some notion of structural similarities among network nodes but struct2vec can be er identify and separate structural equivalences using latent representations.</p><p>Consider the distance between the latent representation for nodes. We measure the distance distribution between pairs corresponding to mirrored nodes and among all node pairs (using the representation shown in <ref type="figure" target="#fig_1">Figure 3</ref>). <ref type="figure" target="#fig_3">Figure 5</ref> shows the two distance distributions for the representations learned by node2vec and struc2vec. For node2vec the two distributions are practically identical, indicating that distances between mirrored pairs blend well with all pairs. In contrast, struc2vec exhibits two very di erent distributions: 94% of mirrored node pairs have distance smaller than 0.25 while 68% of all node pairs have distance larger than 0.25. Moreover, the average distance between all node pairs is 5.6 times larger than that of mirrored pairs, while this ratio is about slightly smaller than 1 for node2vec.</p><p>To be er characterize the relationship between structural distance and distances in the latent representation learned by struc2vec, we compute the correlation between the two distances for all node pairs. In particular, for each layer k we compute the Spearman and Pearson correlation coe cients between f k (u, ), as given by equation <ref type="bibr" target="#b0">(1)</ref>, and the euclidean distance between u and in the learned representation. Results shown in <ref type="table" target="#tab_0">Table 1</ref> for the mirrored Karate network indeed corroborate that there is a very strong correlation between the two distances, for every layer, captured by both coe cients. is suggests that struc2vec indeed captures in the latent space the measure for structural similarity adopted by the methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness to edge removal</head><p>We illustrate the potential of the framework in e ectively representing structural identity in the presence of noise. In particular, we randomly remove edges from the network, directly changing its structure. We adopt the parsimonious edge sampling model to instantiate two structurally correlated networks <ref type="bibr" target="#b14">[15]</ref>. e model works by taking a xed graph G = (V , E) and generating a graph G 1 by sampling each edge e ∈ E with probability s,  <ref type="figure" target="#fig_1">Figure 3</ref>). Curves marked with × correspond to distances between mirrored pairs while + corresponds to all pairs; corresponding averages indicated by vertical lines.  independently. us, each edge of G is present in G 1 with probability s. Repeat the process again using G to generate another graph G 2 . us, G 1 and G 2 are structurally correlated through G, and s controls the amount of structural correlation. Note that when s = 1, G 1 and G 2 are isomorphic, while when s = 0 all structural identity is lost. We apply the edge sampling model to an egonet extracted from Facebook (224 nodes, 3192 edges, max degree 99, min degree 1) <ref type="bibr" target="#b9">[10]</ref> to generate G 1 and G 2 with di erent values for s. We relabel the nodes in G 2 (to avoid identical labels) and consider the union of the two graphs as the input network to our framework. Note that this graph has at least two connected components (corresponding to G 1 and G 2 ) and every node (in G 1 ) has a corresponding pair (in G 2 ). <ref type="figure" target="#fig_5">Figure 6</ref> shows the distance (latent space with 2 dimensions) distribution between corresponding node pairs and all node pairs for various values for s (corresponding averages are shown in <ref type="table">Table 2</ref>). For s = 1, the two distance distributions are strikingly di erent, with the average distance for all pairs being 21 times larger than that for corresponding pairs. More interestingly, when s = 0.9 the two distributions are still very di erent. Note that while further decreasing s does not signi cantly a ect the distance distribution of all pairs, it slowly increases the distribution of corresponding pairs. However, even when s = 0.3 (which means that the probability that an original edge appears both in G 1 and G 2 is 0.09, s 2 ), the framework still places corresponding nodes closer in the latent space.</p><p>is experiment indicates the robustness of the framework in uncovering the structural identity of nodes even in the presence of structural noise, modeled here through edge removals. <ref type="table">Table 2</ref>: Average and standard deviation for distances between node pairs in the latent space representation (see corresponding distributions in <ref type="figure" target="#fig_5">Figure 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Classi cation</head><p>A common application of latent representations for network nodes is classi cation. struc2vec can be leveraged for this task when labels for nodes are more related to their structural identity than to the labels of their neighbors. To illustrate this potential, we consider air-tra c networks: unweighted, undirected networks where nodes correspond to airports and edges indicate the existence of commercial ights. Airports will be assigned a label corresponding to their level of activity, measured in ights or people (discussed below). We consider the following datasets (collected for this study): • Brazilian air-tra c network: Data collected from the National Civil Aviation Agency (ANAC) <ref type="bibr" target="#b0">1</ref>   <ref type="bibr" target="#b2">3</ref> from January to November 2016. e network has 399 nodes, 5,995 edges (diameter is 5). Airport activity is measured by the total number of landings plus takeo s in the corresponding period. For each airport, we assign one of four possible labels corresponding to their activity. In particular, for each dataset, we use the quartiles obtained from the empirical activity distribution to split the dataset in four groups, assigning a di erent label for each group. us, label 1 is given to the 25% less active airports, and so on. Note that all classes (labels) have the same size (number of airports). Moreover, classes are related more to the role played by the airport.</p><p>We learn latent representations for nodes of each air-tra c network using struc2vec and node2vec using a grid search to select the best hyperparameters for each case. Note that this step does not use any node label information. e latent representation for each node becomes the feature that is then used to train a supervised classi er (one-vs-rest logistic regression with L2 regularization). We also consider just the node degree as a feature since it captures a very basic notion of structural identity. Last, since classes have identical sizes, we use just the accuracy to assess performance. Experiments are repeated 10 times using random samples to train the classi er (80% of the nodes used for training) and we report on the average performance. <ref type="figure" target="#fig_6">Figure 7</ref> shows the classi cation performance of the di erent features for all air-tra c networks. Clearly, struc2vec outperforms the other approaches, and its optimizations have li le in uence. For the Brazilian network, struc2vec improves classi cation accuracy by 50% in comparison to node2vec. Interestingly, for this network node2vec has average performance (slightly) inferior to node degree, indicating the importance played by the structural identity of the nodes in classi cation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scalability</head><p>In order to illustrate its scalability, we apply struc2vec with the rst two optimizations to instances of the Erdös-Rényi random graph model (using 128 dimensions, 10 walks per node, walk length 80, Skip-Gram window 10). We compute the average execution time for 10 independent runs on graphs with sizes from 100 to 1,000,000 nodes and average degree of 10. In order to speed up training the language model, we use Skip-Gram with Negative Sampling <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_7">Figure 8</ref> shows the execution time (in log-log scale) indicating that struc2vec scales super-linearly but closer to linear than to n 1.5 (dashed lines). us, despite its unfavorable worst case time and space complexity, in practice struc2vec can be applied to very large networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Structural identity is a concept of symmetry in networks in which nodes are identi ed based on the network structure. e concept is strongly related to functions or roles played by nodes in the network, an important problem in social and hard sciences.</p><p>We propose struc2vec, a novel and exible framework to learn representations that capture the structural identity of nodes in a network. struc2vec assesses the structural similarity of node pairs by considering a hierarchical metric de ned by the ordered degree sequence of nodes and uses a weighted multilayer graph to generate context.</p><p>We have shown that struc2vec excels in capturing the structural identity of nodes, in comparison to state-of-the-art techniques such as DeepWalk, node2vec and RolX. It overcomes their limitation by focusing explicitly on structural identity. Not surprising, we also show that struc2vec is superior in classi cation tasks where node labels are more dependent on their role or structural identity. Last, di erent models to generate representations tend to capture different properties, and we argue that structural identity is clearly important when considering possible node representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Barbell graph B(10, 10). (b) Roles identi ed by RolX. Latent representations in R 2 learned by (c) DeepWalk, (d) node2vec and (e,f,g,h) struc2vec. Parameters used for all methods: number of walks per node: 20, walk length: 80, skipgram window size: 5. For node2vec: p = 1 and q = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Node representations for the mirrored Karate network created by (a) DeepWalk, (b) node2vec and (c) struc2vec. Parameters used for all methods: number of walks per node: 5, walk length: 15, Skip-Gram window size: 3. For node2vec: p = 1 and q = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) Mirrored Karate network. Identical colors correspond to mirrored nodes. (b) Roles identi ed by RolX with the two baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Distance distributions between node pairs (mirrored pairs and all pairs) in the latent space, for the mirrored Karate network learned by node2vec and struc2vec (as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Distance distribution between node pairs in latent space representation (2 dimensions) under the edge sampling model (di erent values for s). Bottom curves (marked with ×) are for corresponding node pairs; top curves (marked with +) are for all node pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Average accuracy for multi-class node classi cation in air-tra c networks of Brazil, USA and Europe for di erent node features used in supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Average execution time of struc2vec on Erdös-Rényi graphs with average degree of 10. Training time refers to the additional time required by Skip-Gram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Pearson and Spearman correlation coe cients between structural distance and euclidean distance in latent space for all node pairs in the mirrored Karate network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>from January to December 2016. e network has 131 nodes, 1,038 edges (diameter is 5). Airport activity is measured by the total number of landings plus takeo s in the corresponding year. • American air-tra c network: Data collected from the Bureau of Transportation Statistics 2 from January to October, 2016. e network has 1,190 nodes, 13,599 edges (diameter is 8). Airport activity is measured by the total number of people that passed (arrived plus departed) the airport in the corresponding period. • European air-tra c network: Data collected from the Statistical O ce of the European Union (Eurostat)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">h p://www.anac.gov.br/ 2 h ps://transtats.bts.gov/ 3 h p://ec.europa.eu/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparative analysis of protein networks: hard problems, practical solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Atias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roded</forename><surname>Sharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A measure of similarity between graph vertices: Applications to synonym extraction and web searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gajardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heymans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Dooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random-Walk Computation of Similarities Between Nodes of a Graph with Application to Collaborative Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piro E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vertex similarity in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elizabeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pe</forename><surname>Leicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Er Holme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structural equivalence of individuals in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Lorrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical sociology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">E cient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Rey Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">sub-graph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saminathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the privacy of anonymized networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Pedarsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grossglauser</forename><surname>Ma Hias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structural Identity and Equivalence of Individuals in Social Networks Beyond Duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narciso</forename><surname>Pizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Sociology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>T Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Westover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM TKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural equivalence: Meaning and de nition, computation and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">Douglas</forename><surname>Sailer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FastDTW: Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Min. Temp. and Seq. Data, ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Weisfeiler-Lehman Graph Kernels. JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Global alignment of multiple protein interaction networks with application to functional orthology detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An information ow model for con ict and ssion in small groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graph similarity scoring and matching. Applied mathematics le ers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George C</forename><surname>Zager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verghese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
