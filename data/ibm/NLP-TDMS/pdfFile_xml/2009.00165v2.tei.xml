<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Architecture Search For Keyword Spotting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-02">2 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Niu</surname></persName>
							<email>dniu@ualberta.camohammad.salameh</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangling</forename><surname>Jui</surname></persName>
							<email>jui.shangling@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Architecture Search For Keyword Spotting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-02">2 Sep 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms: Keyword Spotting, Neural Architecture Search</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have recently become a popular solution to keyword spotting systems, which enable the control of smart devices via voice. In this paper, we apply neural architecture search to search for convolutional neural network models that can help boost the performance of keyword spotting based on features extracted from acoustic signals while maintaining an acceptable memory footprint. Specifically, we use differentiable architecture search techniques to search for operators and their connections in a predefined cell search space. The found cells are then scaled up in both depth and width to achieve competitive performance. We evaluated the proposed method on Google's Speech Commands Dataset and achieved a state-ofthe-art accuracy of over 97% on the setting of 12-class utterance classification commonly reported in the literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Keyword spotting (KWS) aims to identify a set of keywords in utterances. KWS was traditionally performed in the cloud based on audio recordings uploaded by users <ref type="bibr" target="#b0">[1]</ref>. Nowadays, on-device KWS applications are becoming increasingly popular, e.g., Apple's "Siri", Microsoft's "Cortana" and Amazon's "Alexa", which help preserve user privacy and avoid data leakage during transmission. The deployment of KWS models on resource-constrained smart devices requires a small footprint while retaining accuracy. Thus, small-footprint KWS focuses on the recognition of simple commands, such as "yes", "no", "on" and "off", which are sufficient to support frequent userdevice interactions.</p><p>In recent years, various convolutional neural networks (CNNs) have been applied to KWS and achieved remarkable results. Sainath et al. <ref type="bibr" target="#b1">[2]</ref> introduce CNNs into KWS and show that CNNs perform well when limiting the number of parameters. Tang et al. <ref type="bibr" target="#b0">[1]</ref> use variants of the deep residual network (ResNet) to build a neural KWS model, and achieve an accuracy of 95.8% with 239K parameters on the Google Speech Commands Dataset (v1) <ref type="bibr" target="#b2">[3]</ref> using Res15. Choi et al. <ref type="bibr" target="#b3">[4]</ref> combine temporal convolutions with ResNet to construct TC-ResNet models and improve the accuracy to 96.6% with 305K parameters. Mittermaier et al. <ref type="bibr" target="#b4">[5]</ref> use parameterized Sinc-convolutions from SincNet to classify keywords based on raw audio, and reduce the number of parameters to 122K while maintaining the accuracy of TC-ResNet. Kao et al. <ref type="bibr" target="#b5">[6]</ref> propose a sub-band CNN architecture to apply different convolutional kernels on each feature sub-band, and achieve an accuracy of around 90.0% on the second version of Google Speech Commands Dataset while reducing the computation by 39.7% * Equal contributions, listed in alphabetical order. compared to a full-band CNN model. Zeng et al. <ref type="bibr" target="#b6">[7]</ref> use DenseNet with BiLSTM and achieve an accuracy of 96.2% following Google's setup <ref type="bibr" target="#b1">[2]</ref>. Pons et al. <ref type="bibr" target="#b7">[8]</ref> propose a model that uses randomly weighted CNNs as feature extractors to conduct audio classification. Chen et al. <ref type="bibr" target="#b9">[9]</ref> propose a compact and efficient convolutional network (CENet) for small-footprint KWS, and insert the graph convolutional network (GCN) for contextual feature augmentation to CENet as CENet-GCN, which can achieve an accuracy of 96.8% with 72.3K parameters when only using Mel-frequency Cepstrum Coefficient (MFCC) features as the input. Majumdar et al. <ref type="bibr" target="#b10">[10]</ref> propose MatchboxNet that contains residual blocks of 1D time-channel separable convolutions, batch-normalization (BN), ReLU, and dropout layers, achieving an accuracy of around 97.48% with 93K parameters, though on a different setting of 30-class utterance classification with the help of data augmentation (while the majority of the literature evaluates a 12-class benchmark).</p><p>In this paper, we propose to use Neural Architecture Search (NAS) to automate the neural network architecture design for KWS. NAS is widely used and evaluated for image classification and language modeling tasks. Zoph et al. <ref type="bibr" target="#b11">[11]</ref> first use a reinforcement learning approach to train a neural network architecture with the maximum validation accuracy on CIFAR-10. However, this method is computationally expensive, requiring hundreds of GPUs, and the model could not be transferred to a large dataset. The same authors then design a NASNet search space to search for the best convolutional layer (or "cell") and stack copies of this cell to form a NASNet architecture <ref type="bibr" target="#b12">[12]</ref>. Though NASNet is trained faster and able to generalize to larger datasets, the whole search process still takes over four days with 500 GPUs. Other NAS methods, e.g., AmoebaNet <ref type="bibr" target="#b13">[13]</ref>, Progressive NAS <ref type="bibr" target="#b14">[14]</ref>, have been proposed to further optimize the search process. However, all of them search over a discrete domain where more architecture evaluations are required. To make the search space continuous, Liu et al. <ref type="bibr" target="#b15">[15]</ref> propose a differentiable architecture search (DARTS) and enable the efficient search of neural architectures through gradient descent.</p><p>To date, there have been some efforts on NAS for KWS, although not achieving state-of-the-art results. Veniat et al. <ref type="bibr" target="#b16">[16]</ref> propose a stochastic adaptive neural architecture search approach for KWS that automatically adapts the architecture by a recurrent neural network (RNN) according to the difficulty of the prediction problem at each time step, and achieve an 86.5% prediction accuracy on the Google Speech Commands Dataset <ref type="bibr" target="#b2">[3]</ref>. Anderson et al. <ref type="bibr" target="#b17">[17]</ref> propose a performance-oriented neural architecture search approach based on information about the hardware and achieve a 95.11% prediction accuracy on the same dataset.</p><p>In this paper, we leverage DARTS <ref type="bibr" target="#b15">[15]</ref>, a gradient-based differentiable NAS technique to search for the best convolutional network architecture for KWS. A typical NAS process involves searching for the best architecture for a given task, followed by training the found best architecture from scratch. The search process involves three dimensions <ref type="bibr" target="#b18">[18]</ref>. The search space defines which architectures are considered and the operations that compose them. Search strategies define the strategy used to explore the search space, e.g., reinforcement learning (RL) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>, evolutionary algorithm <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b13">13]</ref> and gradient-based approaches <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. It is computationally intensive to evaluate the proposed architecture by the search strategy from scratch. Performance estimation estimates the performance of an architecture without the need to train it fully. Research in NAS aims to improve in these dimensions in order to discover highly performing architectures while minimizing the search cost (in terms of GPU days). We choose a differentiable NAS approach, DARTS, because it has remarkable efficiency, as compared to earlier NAS techniques operated in a discrete domain based on RNN controllers <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. DARTS can finish searching in a single GPU day. Besides, DARTS does not rely on performance predictors <ref type="bibr" target="#b14">[14]</ref> and can find architectures with complex structures in a rich search space.</p><p>We evaluate the proposed NAS method on the public Google Speech Commands Dataset <ref type="bibr" target="#b2">[3]</ref>. Our experimental results have shown that the proposed method can find architectures that achieve a state-of-the-art accuracy of over 97% on the common benchmark setting of 12-class utterance classification, which is the same evaluation setting adopted by most KWS literature <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">16</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We search for a convolutional neural network (CNN) to optimize the classification performance based on a matrix of MFCC features extracted from each audio sample. As is shown in <ref type="figure">Figure</ref> 1, the CNN we will search for is composed of a head layer that performs a preliminary 3 × 3 convolution, followed by a sequence of L stacked layers, each called a cell, and finally, a stem that performs the classification. The preprocessing procedure to process audio in MFCC features will be described in Section 3. To reduce the complexity of the search, we search for cell architectures rather than searching for the entire network architecture. As is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, two types of cells are searched for: normal cells and reduction cells. A normal cell ensures that the size of its output is the same as that of its input by using a stride of one. A reduction cell, on the other hand, doubles the number of channels and divides the height and width of its input by one half. All the normal cells share the same neural architecture. So do all the reduction cells. Once the best normal cell and reduction cell architectures are found from the search phase, we will scale up the depth and width of the network by stacking the found cells and tuning the number of channels at the initial layer. When stacking cells sequentially to form a deeper architecture, the same stacking rule applies-every two normal cells are followed by a reduction cell.</p><p>We leverage a cost-efficient differentiable architecture search algorithm, DARTS <ref type="bibr" target="#b15">[15]</ref>, to find the best normal and reduction cell architectures for KWS. Specifically, a cell can be represented by a directed acyclic graph (DAG) consisting of ordered nodes and directed edges, as is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. There are two inputs to a cell (green), which correspond to the outputs of the previous two cells, while the output of the cell (yellow) is a concatenation of all intermediate nodes.</p><p>Each node is a latent representation, while each edge comprises mixed operations from a predefined operation set O, e.g., For simplicity, let α denote the architecture weights, i.e., the vector concatenating all α (i,j) 's on all the edges, and let w denote the model weights of the corresponding architecture. Denote the training loss by Ltrain and the validation loss by L val . The DARTS algorithm searches for the best architecture (encoded by α) by solving a bi-level optimization problem:</p><formula xml:id="formula_0">min α L val (w * (α), α) s.t. w * (α) = arg min w Ltrain(w, α),<label>(1)</label></formula><p>where α and w are the upper level and lower level parameters, respectively. The goal is to find α * that minimizes the validation loss L val (w * (α), α) such that w * under the given architecture weights α is obtained by minimizing the training loss Ltrain(w, α). Architecture weights α and model weights w are learned jointly using gradient descent <ref type="bibr" target="#b15">[15]</ref> until convergence: 1) updating the architecture weights α by descending based on ∇αL val (w, α); 2) updating the neural network weights w by descending based on ∇wLtrain(w, α). At the end of the search, the operation o with the highest weight α (i,j),o on edge (i, j) will be finally selected, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> by the solid circles. Only the selected operations and the edges connected to them are kept to produce the resulting cell architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Performance Evaluation</head><p>We evaluate the proposed method for keyword spotting on Google Speech Commands Dataset (v1) <ref type="bibr" target="#b2">[3]</ref>. This dataset contains 65,000 one-second-long audio utterances pertaining to 30 words. There are approximately 2,200 samples for each word. Following the same setting as <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b4">5]</ref>, we cast the problem as a classification task that distinguishes among 12 classes, i.e., "yes", "no", "up", "down", "left", "right", "on", "off", "go", "stop", an unknown class, and a silence class. The unknown class contains utterances sampled from the remaining 20 words other than the above ten words, while the silence class has utterances with only background noise. We split the entire dataset into 40% training, 40% validation, and 20% testing sets. The training set and validation set are used during architecture search, and are further combined to form a new training set for evaluating the best architecture on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental setup</head><p>We follow the preprocessing procedure of Honk <ref type="bibr" target="#b27">[26]</ref> to process the acoustic signals, which are adopted by multiple smallfootprint KWS studies <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">9]</ref>. To generate training data, we first add background noise to each sample with 80% probability at each epoch, followed by a random t-second time shift where t is sampled from a UNIFORM[−100, 100] distribution on each sample to enhance robustness. Then, we apply a 20Hz/4kHz filter. Finally, each raw audio file is split into 101 frames using a window size of 30 milliseconds and a frameshift of 10 milliseconds. We extract 40 Mel-Frequency cepstral coefficients (MFCC) features for each frame and stack them across the time axis.</p><p>During neural architecture search, we set the number of cells to 6 and train the network for 50 epochs. The batch size and the initial number of channels are both set to 16 to ensure that the network fits into one GPU. We use stochastic gradient descent (SGD) to update the weights ω with a momentum of 0.9 and a weight decay of 3×10 −4 . The learning rate for ω is set to 0.025, following a cosine annealing scheduler. We optimize the architecture parameters α with Adam (β1 = 0.5, β2 = 0.999), and set weight decay and the initial learning rate to 10 −3 and 3 × 10 −4 , respectively.</p><p>During the evaluation, we instantiate the network to be tested based on the best cell architecture with the highest validation score found by the search phase, and experiment with a depth of 6 and 12. A network of depth 6 is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. A network of depth 12 is obtained by stacking the 6-cell network twice. We randomly re-initialize the weights in the net- work and re-train it from scratch for 200 epochs to report the evaluation results.</p><p>In our cell searches, each normal/reduction cell consists of 7 nodes. <ref type="table" target="#tab_0">Table 1</ref> summarizes the candidate operations used. In total, 7 candidate operations have been considered : skip connection (or identity), zero, average pooling, max pooling, dilated convolution, separable convolution (depthwise separable convolution), and regular convolution. Zero means no connection between two nodes, identity represents identity mapping. The dilated convolution introduces a dilation rate (set to two in our experiments) to the regular convolution. Each convolution operation follows the sequence of execution: ReLU, Convolution, Batch Normalization (BN). Each separable convolution executes two ReLU-Conv-BN sequences.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we conduct searches based on two sets of operators. NAS1 uses separable convolutions, dilated convolutions and pooling, while NAS2 uses regular convolutions instead of separable convolutions. The separable convolution consists of a depth-wise convolution conducted independently over each channel of the input, followed by a point-wise convolution, i.e., a 1 × 1 convolution, to combine information across channels <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. Dilated convolutions are known to be able to expand the receptive field exponentially without loss of coverage <ref type="bibr" target="#b30">[29]</ref>, while separable convolutions can reduce the number of parameters and computational cost <ref type="bibr" target="#b31">[30]</ref>. Separable convolutions are also frequently used in neural KWS literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10]</ref> to improve performance and reduce model size.</p><p>On the other hand, NAS2 uses the regular convolution, which is the convolutional operation traditionally used in ResNet and has been applied to KWS by <ref type="bibr" target="#b0">[1]</ref>. NAS2 considers the same operations used in <ref type="bibr" target="#b0">[1]</ref> to test whether our search strategy is effective at producing architectures that can beat traditional ResNet models <ref type="bibr" target="#b0">[1]</ref> when using similar operations.</p><p>We evaluate the models discovered under both NAS1 and NAS2 search spaces in terms of the accuracy and model size, under different scaling-up settings, by varying the depth (number of cells) and the initial number of channels. We compare to the following baseline models that utilize CNN blocks and are evaluated on the same dataset and 12 classes as our method * :</p><p>• Res15: a ResNet variant based on regular convolutions achieving the highest accuracy in <ref type="bibr" target="#b0">[1]</ref>. It consists of 6 residual blocks and 45 feature maps.</p><p>• TC-ResNet14-1.5: a ResNet variant achieving the highest accuracy in <ref type="bibr" target="#b3">[4]</ref>, which uses a 3 × 1 temporal convolution instead of regular convolutions to reduce the footprint. 6 residual blocks are used. A width multiplier of 1.5 is applied to expand the number of channels at each layer.</p><p>• SincConv+DSConv: the best model reported in <ref type="bibr" target="#b4">[5]</ref>, which first uses the Sinc-convolution to extract features from raw audio and then applies separable convolutions with a kernel length of 9 to reduce the model size.</p><p>• CENet-GCN-40: the best model in <ref type="bibr" target="#b9">[9]</ref>, which mainly consists of bottleneck blocks and a GCN module. Each bottleneck block is a stack of 1×1, 3×3 and 1×1 convolutions to reduce model complexity. The GCN module is introduced to learn non-local relations of convolutional features.   , which uses a similar operation space. It is worth noting that NAS2 only uses the operations that appear in Res15, and does not use any temporal convolutions or separable convolutions, thus could lead to a fairly large model size. However, they all achieve a better accuracy of over 96.7%, outperforming Res15, while the accuracy of NAS2 with 6 cells and 16 initial channels is 0.94 percentage points higher than Res15 with a model size 24% smaller than that of Res15. The results of this set of experiments demonstrate the benefits and necessity of architecture search even under the same operation space. Although using the same set of operations, architectures with better performance can be found with NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>Moreover, we investigate the impact of the depth by changing the number of cells, and the impact of the width by changing the number of initial channels. From NAS1 and NAS2, we observe that the model performance can be improved by increasing the depth or width, although at a cost of an increased model size. In addition, NAS1 models tend to have fewer parameters than NAS2 models due to the use of separable convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Existing methods for neural keyword spotting rely on manually designed convolutional neural networks and other neural networks. In this paper, we perform differentiable neural architecture search to search for CNN architectures that can lead to a high accuracy and a relatively small footprint. Our approach is robust and finds architectures with accuracy over 96% under different sets of operations. The best models found by neural architecture search achieves a state-of-the-art accuracy of over 97% accuracy on the Google Speech Commands Dataset, outperforming a range of existing baseline models under the same experimental setup, while maintaining competitive footprints. These observations demonstrate the enormous potential of conducting neural architecture search for keyword spotting, especially toward other types of neural networks and the adoption of KWS-friendly operations, which open up avenues for future investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The composition of the convolutional neural network to be searched for. A stack of six cells is used during search, where each blue rectangle represents a normal cell, while each yellow one represents a reduction cell. Once the best cells are found, the network can be scaled up in both depth and width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the inner search space of a cell. Each circle is an operation in O, while the solid ones are those finally selected by the search algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3 × 3 convolution, 5 × 5 convolution, max-pooling, etc. A directed edge connecting node i and node j represents the direction of information flow and performs a weighted sum fi,j (xi) of all possible operations o(.) ∈ O applied onto the latent representation xi of node i, i.e., fi,j (xi) = o∈O α (i,j),o o(xi).Let α (i,j) denote the vector of α (i,j),o 's for all o(.) ∈ O on edge (i, j). The weights α (i,j) 's are learnable parameters, which encode the cell structure. The latent representation xj for each intermediate node j is then computed as the sum of outputs from all its preceding nodes, i.e., xj = i&lt;j fi,j (xi).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The normal cell found on the NAS1 search space. The reduction cell found on the NAS1 search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The normal cell found on the NAS2 search space. The reduction cell found on the NAS2 search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figures 3 -</head><label>3</label><figDesc>6 illustrate the cells found on each search space. The search costs for NAS1 and NAS2 remain at a low level of 0.58 GPU day and 0.29 GPU day, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The candidate operations used. × 3 max pool, 3 × 3 avg pool, identity, 3 × 3 and 5 × 5 dil conv, 5 × 5, 7 × 7, and 9 × 9 sep conv NAS2 zero, 3 × 3 max pool, 3 × 3 avg pool, identity, 3 × 3 and 5 × 5 dil conv, 3 × 3 regular conv</figDesc><table><row><cell>Model</cell><cell>Search space</cell></row><row><cell>zero, 3</cell><cell></cell></row><row><cell>NAS1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows a performance comparison between our models and baseline models. From this table, we can observe that the model found by NAS1 with 6 cells and 16 initial channels outperform Res15, TC-ResNet and SincConv in terms of both accuracy and the number of parameters, while the rest of the NAS1 models can achieve × 2 × 64<ref type="bibr" target="#b10">[10]</ref> proposes a deep residual network and achieves state-of-the-art results on Google Speech Dataset v1 on 30 keyword classes. Thus, their setup is not comparable to ours or the listed baselines. It also uses data augmentation, e.g., time shift perturbations and SpecAugment to boost the performance, which is not used in our method or the listed baselines. Similarly, we do not compare to DenseNet-BiLSTM<ref type="bibr" target="#b6">[7]</ref> which relies on attention BiLSTM.</figDesc><table /><note>* MatchboxNet-3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of the models found by the proposed method and baseline models. The numbers marked with † are taken from the corresponding papers. '-' means not available. The best results among different methods are marked in bold.</figDesc><table><row><cell>Model</cell><cell>Cell (#)</cell><cell>Channels (#)</cell><cell>Acc. (%)</cell><cell>Par. (K)</cell></row><row><cell>Res15 [1]</cell><cell>-</cell><cell>-</cell><cell cols="2">95.8 † 239</cell></row><row><cell>TC-ResNet14-1.5 [4]</cell><cell>-</cell><cell>-</cell><cell cols="2">96.6 † 305</cell></row><row><cell>SincConv+DSConv [5]</cell><cell>-</cell><cell>-</cell><cell cols="2">96.6 † 122</cell></row><row><cell>CENet-GCN-40 [9]</cell><cell>-</cell><cell>-</cell><cell cols="2">96.8 † 72.3</cell></row><row><cell></cell><cell>6</cell><cell>16</cell><cell>96.74</cell><cell>107</cell></row><row><cell>NAS1</cell><cell>6 6</cell><cell>24 36</cell><cell>96.90 96.96</cell><cell>223 474</cell></row><row><cell></cell><cell>12</cell><cell>16</cell><cell>97.06</cell><cell>281</cell></row><row><cell></cell><cell>6</cell><cell>16</cell><cell>96.74</cell><cell>182</cell></row><row><cell>NAS2</cell><cell>6 6</cell><cell>24 36</cell><cell>96.86 97.22</cell><cell>400 886</cell></row><row><cell></cell><cell>12</cell><cell>16</cell><cell>96.81</cell><cell>281</cell></row><row><cell cols="5">an accuracy higher than CENet-GCN-40. Notably, NAS1 with</cell></row><row><cell cols="5">12 cells and 16 channels could achieve a new state-of-the-art</cell></row><row><cell cols="5">accuracy of 97.06% with an acceptable model size of 281K pa-</cell></row><row><cell cols="5">rameters, under the setting of 12 classes on the same dataset.</cell></row><row><cell cols="3">For NAS2 models, comparing with Res15 [1]</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Small-footprint keyword spotting on raw audio data with sincconvolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kürzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Waschneck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02086</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Subband convolutional neural networks for small-footprint spoken term classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01448</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective combination of densenet and bilstm for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10" to="767" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randomly weighted cnns for (music) audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="336" to="340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Smallfootprint keyword spotting with graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05124</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Matchboxnet-1d time-channel separable convolutional neural network architecture for speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic adaptive neural architecture search for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Véniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schwander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2842" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Performanceoriented neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02976</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient multiobjective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1812.09926</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<ptr target="http://arxiv.org/abs/1812.09926" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Honk: A pytorch reimplementation of convolutional neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Depthwise separable convolutions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
