<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crowd Counting via Segmentation Guided Attention Networks and Curriculum Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20201">AUGUST 2020 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Qian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
						</author>
						<title level="a" type="main">Crowd Counting via Segmentation Guided Attention Networks and Curriculum Loss</title>
					</analytic>
					<monogr>
						<title level="j" type="main">JOURNAL OF XXX</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20201">AUGUST 2020 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Crowd counting</term>
					<term>Curriculum loss</term>
					<term>Inception-v3</term>
					<term>Segmentation guided attention networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic crowd behaviour analysis is an important task for intelligent transportation systems to enable effective flow control and dynamic route planning for varying road participants. Crowd counting is one of the keys to automatic crowd behaviour analysis. Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in recent years. Researchers have devoted much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model. Due to the insufficient expressive capacity, the backbone network of VGG16 is usually followed by another cumbersome network specially designed for good counting performance. Although VGG models have been outperformed by Inception models in image classification tasks, the existing crowd counting networks built with Inception modules still only have a small number of layers with basic types of Inception modules. To fill in this gap, in this paper, we firstly benchmark the baseline Inception-v3 model on commonly used crowd counting datasets and achieve surprisingly good performance comparable with or better than most existing crowd counting models. Subsequently, we push the boundary of this disruptive work further by proposing a Segmentation Guided Attention Network (SGANet) with Inception-v3 as the backbone and a novel curriculum loss for crowd counting. We conduct thorough experiments to compare the performance of our SGANet with prior arts and the proposed model can achieve state-of-the-art performance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and UCF_QNRF, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Automatic crowd behaviour analysis is an important task for intelligent transportation systems to enable effective flow control and dynamic route planning for varying road participants. Crowd counting is one of the keys to automatic crowd behaviour analysis. Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in recent years. Researchers have devoted much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model. Due to the insufficient expressive capacity, the backbone network of VGG16 is usually followed by another cumbersome network specially designed for good counting performance. Although VGG models have been outperformed by Inception models in image classification tasks, the existing crowd counting networks built with Inception modules still only have a small number of layers with basic types of Inception modules. To fill in this gap, in this paper, we firstly benchmark the baseline Inception-v3 model on commonly used crowd counting datasets and achieve surprisingly good performance comparable with or better than most existing crowd counting models. Subsequently, we push the boundary of this disruptive work further by proposing a Segmentation Guided Attention Network (SGANet) with Inception-v3 as the backbone and a novel curriculum loss for crowd counting. We conduct thorough experiments to compare the performance of our SGANet with prior arts and the proposed model can achieve state-of-the-art performance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and UCF_QNRF, respectively.</p><p>Index Terms-Crowd counting, Curriculum loss, Inception-v3, Segmentation guided attention networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTOMATIC crowd counting has attracted increasing attention in the research community since its valuable impacts in public surveillance and intelligent transportation systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Crowd behaviour can have a big effect in the efficiency of public transportation. Intelligent transportation systems deployed in a smart city should be able to capture real-time crowd behaviour information from public surveillance and dynamically adjust the planning for effective transportation. Accurate people and vehicle counting in varying conditions provide basic information for automatic crowd behaviour analysis. People and vehicle counting can be formulated in a unified object counting framework which aims to estimate the number of target objects in still images or video frames and has been applied in many real-world applications. For instance, there have been works focusing on automatic Q. Wang is with the Department of Computer Science, Durham University, United Kingdom, e-mail: qian.wang173@hotmail.com TP. Breckon is with the Department of Computer Science and Department of Engineering, Durham University, United Kingdom, e-mail: toby.breckon@durham.ac.uk counting different objects including cells <ref type="bibr" target="#b5">[6]</ref>, vehicles <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, leaves <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and people <ref type="bibr" target="#b3">[4]</ref>.</p><p>In earlier years, crowd counting in images was implemented by detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> or direct count regression <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Counting by detection methods assume people signatures (i.e. the whole body or the head) in images are detectable and the count can be easily achieved from the detection results. This assumption, however, does not always hold in real scenarios, especially when the crowd is extremely dense. Counting by direct count regression aims to learn a regression model (e.g., support vector machine <ref type="bibr" target="#b14">[15]</ref> or neural networks <ref type="bibr" target="#b13">[14]</ref>) mapping the hand-crafted image features directly to the count of people in the image. Methods falling into this category only give the final counts hence lack of explainability and reliability.</p><p>Recently, crowd counting has been overwhelmingly dominated by density estimation based methods since the idea of density map was first proposed in <ref type="bibr" target="#b15">[16]</ref>. The use of deep Convolutional Neural Networks <ref type="bibr" target="#b16">[17]</ref> to estimate the density map along with the availability of large-scale datasets <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> further improved the accuracy of crowd counting in more challenging real-world scenarios. Recent works in crowd counting have been focusing on the design of novel architectures of deep neural networks (e.g., multi-column CNN <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> and attention mechanism <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>) for accurate density map estimation. The motivations of these designs are usually to improve the generalization to scale-variant crowd images. Among them, the Inception module <ref type="bibr" target="#b22">[23]</ref> has been employed and showed effectiveness in crowd counting <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, although only the basic Inception modules are used and the networks are relative shallow compared with the state-of-the-art deep CNN models for image classification such as Inception-v3 <ref type="bibr" target="#b22">[23]</ref> which uses heterogeneous Inception modules to improve the expressive power of the network. Although VGG16, VGG19 and ResNet101 have been used as the backbone networks for crowd counting in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, to our best knowledge, the Inception models have not been investigated.</p><p>In this paper, we make the first attempt to investigate the effectiveness of Inception-v3 model for crowd counting. We modify the original Inception-v3 to make it suitable for crowd density estimation. Without bells and whistles, the Inception-v3 model can achieve surprisingly good performance comparable with or even better than most existing crowd counting models on commonly used crowd counting datasets. Subsequently, we add a segmentation map guided attention layer to the Inception-v3 model to enhance the salient feature extraction for accurate density map estimation and propose a arXiv:1911.07990v2 [cs.CV] 3 Aug 2020 novel curriculum loss strategy to address the issues caused by extremely dense regions in crowd counting. As a result, the proposed SGANet with curriculum loss is able to achieve state-of-the-art performance for crowd counting with the embarrassingly simple design. The contributions of this paper are summarized as follows:</p><p>-We make the first attempt to investigate the effectiveness of Inception-v3 in crowd counting and achieve disruptive results which are important for the research community. -We present a Segmentation Guided Attention Network (SGANet) with a novel curriculum loss function based on the Inception-v3 model for crowd counting. -Extensive evaluations are conducted on benchmark datasets and the results demonstrate the superior performance of SGANet and the effectiveness of curriculum loss in crowd counting. The remainder of this paper is organized as follows. Section II reviews related work of crowd counting and curriculum learning. In Section III we introduce our proposed segmentation guided attention networks for crowd counting with curriculum loss. Section IV presents the experiments and results on several benchmark datasets and we conclude our work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we first review related works on CNN based crowd counting and focus mainly on the diverse network architectures against which our proposed crowd counting model is compared. Subsequently, we introduce works related to curriculum learning and how they can potentially be used in the task of crowd counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Crowd Counting Networks</head><p>Successful efforts have been devoted to the design of novel network architectures to improve the performance of crowd counting. Commonly used principles of network design for crowd counting include multi-column networks, rich feature fusion and attention mechanism.</p><p>Multi-column neural networks were employed to address the scale-variant issue in crowd counting <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. As one of the earliest CNN based models for crowd counting, MCNN <ref type="bibr" target="#b17">[18]</ref> consists of three branches aiming to handle crowds of different densities. Following this idea, Sam et al. <ref type="bibr" target="#b28">[29]</ref> proposed SwithCNN which employs a classifier to explicitly select one of the three branches for a given input patch based on its level of crowd density. While these methods aim to use different kernel sizes in different branches to capture scale-variant information, Liu et al. <ref type="bibr" target="#b30">[31]</ref> proposed a model consisting of multiple branches of VGG16 networks with shared weights to process scaled input images respectively. Similarly, Ranjan et al. <ref type="bibr" target="#b31">[32]</ref> devised a two-column network which learns the lowand high-resolution density maps iteratively via two branches of CNN. The success of these specially designed network architectures has validated that multi-column CNN models are capable of capturing scale-variant features for crowd counting.</p><p>The second direction of network design is to pursue effective fusion of rich features from different layers <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>These attempts are based on the fact different layers have variant receptive fields hence capturing features of variantscale information. Different feature fusion strategies including direct fusion <ref type="bibr" target="#b32">[33]</ref>, top-down fusion <ref type="bibr" target="#b33">[34]</ref> and bidirectional fusion <ref type="bibr" target="#b34">[35]</ref> have been employed in crowd counting.</p><p>To take advantage of the two aforementioned ideas for crowd counting, one straightforward solution is to utilise the Inception module <ref type="bibr" target="#b22">[23]</ref> which was firstly proposed in <ref type="bibr" target="#b35">[36]</ref> and has evolved into a variety of more efficient forms to date. The Inception modules have been employed in crowd counting models before in SANet <ref type="bibr" target="#b23">[24]</ref> and the TEDNet <ref type="bibr" target="#b24">[25]</ref>. Both of them use only the basic types of Inception modules similar to those used in the first version of Inception net (i.e. GoogLeNet <ref type="bibr" target="#b35">[36]</ref>). In our work, we aim to explore the more advanced Inception modules in the framework of Inception-v3.</p><p>The attention mechanism is another useful technique considered when designing network architectures for crowd counting <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Attention layers are usually combined with multi-column structures so that regions of different semantic information (e.g., background, sparse, dense, etc.) can be attended and processed by different branches respectively. Attention maps learned by these models have proved to be aware of semantic regions <ref type="bibr" target="#b25">[26]</ref>, however, they cannot provide fine-grained scale awareness within the images. To address this issue explicitly, perspective maps have been employed to guide the accurate estimation of density maps <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In many scenarios where the perspective maps are not available, it is possible to estimate these perspective maps from the crowd images via a specially designed and trained network <ref type="bibr" target="#b40">[41]</ref>.</p><p>Alternatively, binary segmentation maps generated from point annotation <ref type="bibr" target="#b41">[42]</ref> are introduced as additional supervision for the training of crowd counting networks via multi-task learning <ref type="bibr" target="#b41">[42]</ref>. In our work, binary segmentation maps are treated as explicit attention maps guiding the learning of salient visual features for density map estimation. In this sense, our work is more related to <ref type="bibr" target="#b42">[43]</ref> and <ref type="bibr" target="#b43">[44]</ref> in which the segmentation maps are also used as attention maps but in essentially different ways as explained in Section III-B and validated in Section IV-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Curriculum Learning</head><p>Curriculum learning is a strategy of model training (e.g., neural networks) in machine learning and was proposed by Bengio et al. <ref type="bibr" target="#b44">[45]</ref>. The idea of curriculum learning can date back to no later than 1993 when Elman <ref type="bibr" target="#b45">[46]</ref> proved the benefit of training neural networks to learn a simple grammar by "starting small". The strategy of curriculum learning is inspired by the way how humans learn knowledge from easy concepts to hard abstractions gradually. In the specific case of training a machine learning model, curriculum learning selects easy examples at the beginning of training and allows more difficult ones added to the training set gradually. A curriculum is usually defined as a ranking of training examples by some prior knowledge to determine the level of difficulty of a given example. Jiang et al. <ref type="bibr" target="#b46">[47]</ref> extended curriculum learning to a socalled self-paced curriculum learning by integrated the ideas of original curriculum learning and self-paced learning <ref type="bibr" target="#b47">[48]</ref> in a unified framework.</p><p>In this work, we apply the strategy of curriculum learning in crowd counting to address the issue of large variance of the crowd density in the images. Curriculum learning has been employed for crowd counting in <ref type="bibr" target="#b48">[49]</ref> where the curriculum is designed on the image level, i.e., a difficulty score is calculated for each training image. The training images are divided into multiple subsets based on their difficulty scores and the easiest subset is added into the training set first. By contrast, our curriculum learning strategy is characterized by a novel curriculum loss defined on the pixel level as described in Section III-D. We define that density map pixels of higher values than a threshold have higher difficulty scores because these pixels are within regions of denser crowds. We use all training images throughout the training process but set the threshold to a low value at the beginning and increase it gradually so that the difficult pixels become easy ones and contribute more to the training. As a result, our curriculum learning strategy is simple to implement with zero extra cost and has been proved effective especially when there exist extremely dense crowd regions in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SEGMENTATION GUIDED ATTENTION NETWORK</head><p>Crowding counting is formulated as a density map regression problem in this study. Given a crowd image I, we aim to learn a Fully Convolutional Network (FCN) denoted as F so that the corresponding density map M den can be estimated by:M den = F (I; Θ).</p><p>(</p><p>where Θ is a collection of parameters of the FCN. As shown in <ref type="figure">Figure 1</ref>, our proposed network is adapted from the famous Inception-v3 originally designed for image classification by Google Research <ref type="bibr" target="#b22">[23]</ref>. We first modify Inception-v3 to an FCN so that it can process images of arbitrary sizes and generates the estimated density maps M den as the outputs. An attention layer is added to the network to filter out features within the background region and concentrate on the foreground features for accurate density map estimation. Since the attention maps generated by this attention layer aim to discriminate the regions of background and foreground of the feature maps, we use a ground truth segmentation map, which can be easily derived from point annotations, as extra guidance for the training of the attention layer. As a result, the learned attention maps are forced to be similar to the segmentation maps during training.</p><p>We also investigate the use of curriculum loss in the training of crowd counting networks. Specifically, we define a curriculum based on the pixel-wise difficulty level so that the network starts training by focusing more on the "easy" regions (sparse) within the density maps and down-weighting the "hard" pixels (dense). During training, the "hard" pixels are gradually exposed to the model and finally, the learned model can perform well for all situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Density and Segmentation Maps</head><p>In this study, we use simple ways to generate density and segmentation maps from the point annotations although more complicated ones <ref type="bibr" target="#b43">[44]</ref> might benefit the performance. For density maps M den ∈ R + H×W , where H and W are the height and width of the image, we follow <ref type="bibr" target="#b17">[18]</ref> using a Gaussian kernel G σ ∈ R +15×15 with fixed σ = 4:</p><formula xml:id="formula_1">M den (x) = N i=1 δ(x − x i ) * G σ (x).<label>(2)</label></formula><p>For segmentation maps M seg ∈ {0, 1} H×W , we use a similar method:</p><formula xml:id="formula_2">M seg (x) = N i=1 δ(x − x i ) * J n (x),<label>(3)</label></formula><p>where J n (x) is an all-one matrix of size n × n centred at the position x. As a result, ones and zeros in the matrix M seg denote the pixels belong to the foreground and background regions, respectively. We empirically set n = 25 across all our experiments to ensure that a specific head within an image is characterized by more pixels in the segmentation map than in the density map to avoid losing useful contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Configuration</head><p>Instead of designing a novel network from scratch, we exploit the state-of-the-art CNN model for image classification Inception-v3 in our study. To apply the original Inception-v3 network in crowd counting, some favourable modifications have been made. Firstly, we remove the final fully-connected layers and reserve all the convolutional layers. The input size of the original Inception-v3 network is 299×299 and the output size of the final convolutional layer is 8 × 8. That is to say, feature maps generated by the last convolutional layer have approximately 1 2 5 spatial resolutions of the input image. This is achieved by the first convolutional layers (stride of 2), two max-pooling layers (stride of 2) and two Inception modules in which max-pooling (stride of 2) is employed. To ensure the spatial resolution of the output density map which is important in crowd counting, we remove the first two max-pooling layers from the original network and add one upsample layer before the final Inception module. As a result, the output of the modified network has exactly 1 4 spatial resolution of the input image when the input size is 2 n (e.g., 128 × 128 in our case). Such modification does not change the number of parameters of the network hence the pre-trained weights can be directly loaded and used. However, since the spatial resolutions of intermediate feature maps have been increased, the number of operations is also increased. This modified model will also denoted as Inception-v3 without introducing ambiguity and used as a baseline method in our experiments.</p><p>Distinct from existing works using the segmentation map in the framework of multi-task learning <ref type="bibr" target="#b41">[42]</ref> to extract more salient features for density map estimation, we claim that the segmentation map can be used as an ideal attention map to emphasize the contributions of features within the foreground regions to the density map estimation whilst compressing the effects of features within the background regions.  <ref type="figure">Fig. 1</ref>. The framework of our proposed Segmentation Guided Attention Network (SGANet) which is adapted from Inception-v3 by: 1) removing the fullyconnected layers; 2) removing two maxpooling layers to reserve high spatial resolution feature maps; 3) adding an upsampling layer before the last Inception module; 4) adding an attention layer whose output is applied to the feature maps generated by the last Inception module; 5) adding a convolutional layer for density map estimation.</p><p>end, we add an attention layer to estimate the attention map. The attention layer is a convolutional layer followed by a sigmoid layer which restricts the output values in the range of 0-1. The attention layer takes the feature maps generated by the second last Inception module as input and outputs a one-channel attention map of the same spatial resolution as the input. Subsequently, the attention map estimated by the attention layer is applied to the feature map generated by the last Inception module by an element-wise product with each channel of the feature map.</p><formula xml:id="formula_3">F l = F l−1 M att<label>(4)</label></formula><p>The attention layer designed in our framework is similar to that in <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. However, a so-called inverse attention map is estimated in <ref type="bibr" target="#b42">[43]</ref> while our attention layer generates an attention map directly applied to the feature map. Also, the foreground regions in the ground truth segmentation map in <ref type="bibr" target="#b42">[43]</ref> are derived by thresholding the density map hence both maps have the same positive fields for each head while ours are different (c.f. Eq.(2-3)). In <ref type="bibr" target="#b43">[44]</ref>, the attention layer takes the feature map as input to estimate an attention map which again is applied to the same feature map. This may limit the capacity of the model since it is forced to learn two different maps from the same feature map via two convolutional layers which have limited parameters. In contrast, as mentioned above, the input of our attention layer is the feature map from the previous layer which has higher spatial resolutions and is different from the one the generated attention map will be applied to. These favourable distinctions collectively benefit the estimation of the density map and will be empirically evaluated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function</head><p>We first describe the loss function used to train the SGANet without curriculum loss in this section and describe the curriculum loss in the following section. The loss function consists of two components. The first one is an L2 loss applied to the estimation of the density map and is denoted as L den . The density map loss can be calculated as follows:</p><formula xml:id="formula_4">L den (Θ) = 1 2N N i=1 ||M den i − M den i || 2 F .<label>(5)</label></formula><p>The second component of the loss function is the segmentation map loss L seg which is defined as the cross-entropy loss:</p><formula xml:id="formula_5">L seg (Θ) = − 1 N N i=1 ||M seg i log(M seg i ) + (1 − M seg i ) log(1 −M seg i )|| 1 .<label>(6)</label></formula><p>where || · || 1 denotes the elementwise matrix norm, i.e., the sum of all elements in a matrix, and denotes elementwise multiplication of two matrices with the same size. These two components are combined during network training and the compositional loss function is:</p><formula xml:id="formula_6">L(Θ) = L den (Θ) + λL seg (Θ)<label>(7)</label></formula><p>where λ is a hyper-parameter which ensures the two components to have comparable values and is set 20 across our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Curriculum Loss</head><p>To benefit from the strategy of curriculum learning, we present a novel curriculum loss function in this section to replace the traditional density map loss function defined in Eq. <ref type="bibr" target="#b4">(5)</ref>. The curriculum loss function is designed to be aware of the pixel-wise difficulty level when computing the density map loss. Based on the fact that dense crowds are generally more difficult to count than sparse ones, we design a curriculum where pixels of higher values than a dynamic threshold in the density map are defined as difficult pixels. We set the dynamic threshold and assign variant weights to different pixels of the density map when calculating the density map loss. Specifically, we define a weight matrix W as follows:</p><formula xml:id="formula_7">W = T(e) max{M den − T(e), 0} + T(e) .<label>(8)</label></formula><p>The weight matrix W has the same size as the density map matrix M den used in Eq.(5) and the pixel-wise weights are determined by the dynamic threshold T(e) and the pixel values in the density map. If the pixel value of the density map is higher than the threshold, this pixel is treated as a difficult one and the corresponding weight is set less than one, otherwise the weight is equal to one. The higher the pixel values are, the smaller the weights will be. As a result, the training will focus more on the pixels of lower density value than T(e).</p><p>The dynamic threshold T(e) is defined as a function of the training epoch index in the form of:</p><formula xml:id="formula_8">T(e) = ke + b<label>(9)</label></formula><p>where k and b can be determined based on the prior knowledge of the pixel values in the ground truth density map. The value of b is the initial threshold which should be equivalent to the maximum density value in the region characterizing a single head. The value of k controls the speed of increasing the difficulty which can also be easily derived from the learning curve when the curriculum learning strategy is not used. Finally, the curriculum loss function for density map can be derived by modifying Eq.(5) as:</p><formula xml:id="formula_9">L den (Θ) = 1 2N N i=1 ||W (e) (M den i − M den i )|| 2 F .<label>(10)</label></formula><p>where W (e) is also a function with respect to the training epoch index e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Extensive experiments have been conducted on benchmark datasets to evaluate the performance of SGANet and the effectiveness of curriculum loss in crowd counting. We will briefly describe the datasets and evaluation metrics used in our experiments, details of experimental protocols and network training. Experimental results are compared with state-of-theart methods and analysed. We also present an ablation study to investigate the contributions of different components to the performance of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>ShanghaiTech dataset was collected and published by Zhang et al. <ref type="bibr" target="#b17">[18]</ref> consisting of two parts. Part A consists of 300 and 182 images of different resolutions for training and testing respectively. The minimum and maximum counts are 33 and 3139 respectively, and the average count is 501.4. Part B consists of 400 and 316 images of a unique resolution (768×1024) for training and testing respectively. Compared with part A, the numbers of people in these images are much smaller with the minimum and maximum counts of 9 and 578 respectively, and the average count is 123.6.</p><p>UCF_QNRF dataset <ref type="bibr" target="#b18">[19]</ref> contains 1,535 high-quality images, among which 1201 images are used for training and 334 images for testing. The minimum and maximum counts are 49 and 12,865 respectively, and the average count is 815.</p><p>UCF_CC_50 dataset <ref type="bibr" target="#b49">[50]</ref> contains 50 images with the minimum and maximum counts of 94 and 4,534 respectively.</p><p>It is a challenging dataset due to the limited number of images. Following the suggestion in <ref type="bibr" target="#b49">[50]</ref> and many other works, we use 5-fold cross-validation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We follow the previous works using two metrics, i.e., the mean absolute error (MAE) and the root mean squared error (RMSE), to evaluate the performance of different models in our experiments. The two metrics can be calculated as follows:</p><formula xml:id="formula_10">M AE = 1 N test N t e s t i=1 |y i −ŷ i | (11) RMSE = 1 N test N t e s t i=1 (y i −ŷ i ) 2<label>(12)</label></formula><p>where y i andŷ i are the ground truth and predicted count for i-th test image respectively, N test is the number of test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Training</head><p>SGANet is implemented in PyTorch <ref type="bibr" target="#b50">[51]</ref> and the source code is publicly available 1 . The "Adam" optimizer <ref type="bibr" target="#b51">[52]</ref> is employed for training. The initial learning rate is set to 1e-4 and reduced by a factor of 0.5 after every 50 epochs. The total number of training epochs is set 500 since the model can always converge much earlier than that. The network is trained with image patches with a size of 128×128 randomly cropped from the training images. Instead of preparing the patches in advance, we do the random patch cropping on-the-fly during training. Specifically, we randomly select 8 images from the training set and 4 patches are randomly cropped from each selected image. This leads to a batch of 32 training patches in each iteration of training. The training patches generated in this way can be more diverse and help to alleviate the potential over-fitting problem. Since the output of SGANet has the size of 32 × 32 (i.e. 1/4 of the input size), we use sum-pooling to adapt the ground truth density and segmentation map so that they have the same size of 32 × 32 as the output. The training patches, as well as their corresponding density and segmentation maps, are horizontally flipped with a probability of 0.5 for data augmentation which has been shown beneficial in many works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b52">[53]</ref>. For testing, we feed the whole image into the network and obtain the density map from which the predicted count can be computed. For the UCF_QNRF dataset, to save the memory usage during testing, we also resize the images from both training and test sets so that all images are limited to have resolutions no higher than 2048 whilst the original aspect ratios are kept, if not specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparative Study</head><p>We select both classic and state-of-the-art models for the comparison, including MCNN <ref type="bibr" target="#b17">[18]</ref> which is a three-column CNN, CSRNet <ref type="bibr" target="#b53">[54]</ref> which uses VGG16 as the front-end and dilated convolutional layers as the back-end, SANet <ref type="bibr" target="#b23">[24]</ref> which employs the basic Inception modules but has a relatively 1 https://github.com/hellowangqian/sganet-crowd-counting The experimental results are listed in <ref type="table" target="#tab_1">Table I</ref> where the best result in each column is highlighted in bold and the second best in underscored italic. From <ref type="table" target="#tab_1">Table I</ref>, we can see our modified Inception-v3 can achieve very competitive performance on all four datasets. Especially on ShanghaiTech part B, it achieves the second best MAE of 6.4 and the best RMSE of 9.8. On the UCF_QNRF dataset, Inception-v3 also achieves significantly better results than most existing models including TEDNet (MAE: 95.6 vs 113 and MSE: 165.4 vs 188) which also employs the Inception modules. These results demonstrate the superiority of heterogeneous Inception modules in classification problems can be transferred to the task of crowd counting hence different Inception modules deserve more attention when designing a novel CNN architecture for crowd counting as well as other tasks suffering from the issue of scale variance. On the other hand, the disruptive performance of Inception-v3 in crowd counting provides more insight for the research community regarding the selection of backbone models when designing novel network architectures for crowd counting.</p><p>By adding the segmentation guided attention layer, our SGANet can achieve better performance on all datasets in terms of MAE (i.e. 58.0 vs 60.1 for ShanghaiTech part A, 89.1 vs 95.6 for UCF-QNRF and 224.6 vs 236.0 for UCF-CC-50), although the improvement on ShanghaiTech part B dataset is very marginal (i.e. 6.3 vs 6.4). Regarding RMSE, SGANet achieves better performance on ShanghaiTech part A (i.e. 100.4 vs 105.0) and UCF_QNRF (i.e. 150.6 vs 165.4) but worse results on the other two datasets (i.e. 10.6 vs 9.8 for ShanghaiTech part B and 314.6 vs 304.9 for UCF_CC_50). Overall, our proposed SGANet with the combination of Inception-v3 and a segmentation guided attention layer can achieve state-of-the-art performance on several benchmark datasets.</p><p>The use of curriculum loss (SGANet+CL) further improves the performance of SGANet on three out of four datasets and these three datasets (i.e. ShanghaiTech part A, UCF_QNRF and UCF_CC_50) consist of crowds with significant density variations. On the ShanghaiTech part B dataset, the use of curriculum loss does not improve the performance because the images from this dataset contain crowds with a relatively small variance of head scales. These results provide evidence that the issue of large scale variance can be further alleviated by the use of our proposed curriculum loss. We will provide more evidence for the effectiveness of curriculum loss in the following ablation study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on Curriculum Loss</head><p>The use of curriculum loss has shown a positive effect when applied to SGANet for crowd counting <ref type="table" target="#tab_1">(Table I)</ref>. In this section, we attempt to explore the effectiveness of curriculum learning in the training of other crowd counting networks. To this end, we consider "MCNN", "CSRNet", "SANet", "CAN-Net", "DADNet" and our modified "Inception-v3" and use the curriculum loss when training these networks on ShanghaiTech part A. Firstly, we try to reproduce the results of these crowd counting models using conventional density map loss under our training protocols to remove the effects of various factors such as the ways of density map generation, patch cropping, data augmentation and so on for a fair comparison and focus on the effect of curriculum loss. It is noteworthy that the generated density maps have different sizes for these models (e.g., the size ratio between input and output is 1 for "SANet", 2 for "DADNet", 4 four "MCNN" and "Inception-v3", 8 for "CSRNet" and "CANNet"). The ground truth density maps need to be resized by sum pooling to have the same size as the corresponding outputs. As a result, the pixel values of the ground truth density maps for different models will have different distributions. This leads to model-specific curriculum designs (i.e. the parameter values in Eq. <ref type="formula" target="#formula_8">(9)</ref>). Specifically, we set b as the maximum value in the Gaussian kernel matrix G σ used for density map generation (c.f. Eq. <ref type="formula" target="#formula_1">(2)</ref>) so that the sparse crowd regions without annotation overlapping will not be affected throughout the training process. The value of k in Eq. (9) is determined by the number of epochs so that all the crowd regions will contribute to the loss equally before training is finished. In our experiments, we set k = 1e − 3 and b = 0.1 for SGANet. Experimental results are shown in <ref type="table" target="#tab_1">Table  II</ref>. The use of curriculum loss improves the performance of most models. Specifically, the MAE decrease for all models except "DADNet" and the RMSE decrease for all models except "CSRNet". These experimental results demonstrate the curriculum loss is useful not only for our SGANet but also many other crowd counting models.</p><p>To evaluate the effect of crowd density in the performance of SGANet and the curriculum loss, an additional experiment is conducted on the UCF_QNRF dataset. As mentioned above, we have changed the image resolutions in this dataset to be no higher than 2048 for computation efficiency. In this experiment, we create two more datasets by setting the image resolution thresholds as 1024 and 512 respectively. As a result, the images in the UCF_QNRF_512 dataset will have higher crowd density than those in the UCF_QNRF_1024 dataset which again consists of denser crowds than the UCF_QNRF_2048 dataset. We use SGANet on these three datasets and the experimental results are shown in <ref type="table" target="#tab_1">Table III</ref>. It is obvious the image resolutions make a significant different in the performance and the models perform the best on the UCF_QNRF_2048 dataset whose image resolutions are higher hence have less crowded images. By comparing the performance of SGANet without and with curriculum loss, the use of curriculum loss leads to better results on all three datasets in terms of both MAE and RMSE except that in the last column of <ref type="table" target="#tab_1">Table III</ref>. The performance gains achieved by the use of curriculum loss are also related to the image resolutions or the crowd densities in the datasets. Specifically, the MAE and RMSE are reduced by 7.6 and 18.9 respectively on UCF_QNRF_512, 5.0 and 9.2 on UCF_QNRF_1024, 1.5 and -1.9 on UCF_QNRF_2048. These results provide more evidence that the use of curriculum loss is more effective when the crowds are denser in the images.</p><p>In summary, the experimental results in Tables I-III provide sufficient evidence that the use of curriculum learning can benefit the training of crowd counting models in most cases especially when the head scales vary a lot in the crowd images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on Segmentation Guided Attention</head><p>From <ref type="table" target="#tab_1">Table I</ref> we can see the performance enhancement contributed by the segmentation guided attention layer by comparing the performance between Inception-v3 and SGANet. To validate the superiority of our segmentation guided attention layer to other similar designs <ref type="bibr" target="#b43">[44]</ref>, we conduct an experiment on ShanghaiTech part A and UCF_QNRF. In this experiment, we follow <ref type="bibr" target="#b43">[44]</ref> and modify the SGANet by feeding the feature maps of the last Inception module into the attention layer and keeping the rest unchanged. The experimental results are shown in <ref type="table" target="#tab_1">Table IV</ref> from which we conclude the way segmentation maps are used in our SGANet outperforms that in <ref type="bibr" target="#b43">[44]</ref>. To give an intuitive evidence on how the attention layer helps for density map estimation, we visualize the estimated attention maps and density maps for five exemplar test images from ShanghaiTech part A. In <ref type="figure">Figure 2</ref>, we show the original images, ground truth density maps, predicted density maps and predicted segmentation maps in four columns respectively. The real and predicted counts are also shown on the density maps for a direct comparison. We can see that the prediction errors for the top three examples are relatively low given the accurately predicted segmentation maps. However, the bottom two images suffer from higher errors since the model can not predict accurate foreground regions. For example, the image in the fourth row contains people raising their hands in the air and the hands are easy to be counted since they have similar colours with human faces. In the bottom image, the trees in the background are mistakenly recognised as foreground and result in the over-estimated count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND CONCLUSION</head><p>In this paper, we address an important problem crowd counting which can be of great values to intelligent transportation systems. We investigated the effectiveness of Inception-v3 in crowd counting and proposed a segmentation guided attention network using Inception-v3 as the backbone. We also proposed a novel curriculum loss function for crowd counting by defining pixel-wise difficulty levels to resolve the issue of scale variance in crowd images. Experimental results on four commonly used datasets demonstrate the proposed SGANet can achieve superior performance due to the combination of Inception-v3 and the segmentation guided attention layer. The proposed strategy of curriculum learning is also proved to be helpful for a variety of existing crowd counting models in general. This is the first attempt to use the whole Inception-v3 model for crowd counting and achieves state-of-the-art performance on commonly used datasets. Although the employed Inception-v3 model (with our own modifications) is not designed from scratch, it is quantitatively shown to be able to achieve superior performance to many specially designed models in the recent couple of years. To these ends, our work is both disruptive and important to the crowd counting research community. Researchers in this community have devoted too much effort to the design of variant CNN architectures and most of them are based on the pre-trained VGG16 model which just has insufficient expressive capacity for crowd counting tasks. In this sense, we believe it is important and necessary to make the community aware of the fact Inception-v3 is a more suitable architecture for effective crowd counting and divert the attention of the community to more diverse research directions.</p><p>Most existing crowd counting methods including ours in this paper rely on sufficient training data which require extensive data collection and annotation. In real-world applications, it is challenging to get access to sufficient training data for various scenarios (e.g., different camera resolutions, illumination conditions, weather conditions and perspectives). To solve this realistic problem, our future work will focus on weakly supervised learning such as domain adaptation <ref type="bibr" target="#b58">[59]</ref> and transfer learning <ref type="bibr" target="#b27">[28]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>RESULTS WITH STATE-OF-THE-ART MODELS FOR CROWD COUNTING ( -DENOTES THE RESULTS ARE NOT AVAILABLE; CL DENOTES CURRICULUM LOSS).</figDesc><table><row><cell>Model</cell><cell cols="8">ShTechA MAE RMSE MAE RMSE MAE RMSE MAE RMSE ShTechB UCF-QNRF UCF-CC-50</cell></row><row><cell>MCNN [18]</cell><cell cols="2">110.2 173.2</cell><cell>26.4</cell><cell>41.3</cell><cell>-</cell><cell>-</cell><cell cols="2">377.6 509.1</cell></row><row><cell>CSRNet [54]</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell><cell>-</cell><cell>-</cell><cell cols="2">266.1 397.5</cell></row><row><cell>SANet [24]</cell><cell>67.0</cell><cell>104.5</cell><cell>8.4</cell><cell>13.6</cell><cell>-</cell><cell>-</cell><cell cols="2">258.4 334.9</cell></row><row><cell>DADNet [26]</cell><cell>64.2</cell><cell>99.9</cell><cell>8.8</cell><cell>13.5</cell><cell cols="4">113.2 189.4 285.5 389.7</cell></row><row><cell>CANNet [55]</cell><cell>62.3</cell><cell>100.0</cell><cell>7.8</cell><cell>12.2</cell><cell>107</cell><cell>183</cell><cell cols="2">212.2 243.7</cell></row><row><cell>TEDNet [25]</cell><cell>64.2</cell><cell>109.1</cell><cell>8.2</cell><cell>12.8</cell><cell>113</cell><cell>188</cell><cell cols="2">249.4 354.5</cell></row><row><cell>Wan et al. [56]</cell><cell>64.7</cell><cell>97.1</cell><cell>8.1</cell><cell>13.6</cell><cell>101</cell><cell>176</cell><cell>-</cell><cell>-</cell></row><row><cell>RANet[53]</cell><cell>59.4</cell><cell>102.0</cell><cell>7.9</cell><cell>12.9</cell><cell>111</cell><cell>190</cell><cell cols="2">239.8 319.4</cell></row><row><cell>ANF [57]</cell><cell>63.9</cell><cell>99.4</cell><cell>8.3</cell><cell>13.2</cell><cell>110</cell><cell>174</cell><cell cols="2">250.2 340.0</cell></row><row><cell>SPANet [58]</cell><cell>59.4</cell><cell>92.5</cell><cell>6.5</cell><cell>9.9</cell><cell>-</cell><cell>-</cell><cell cols="2">232.6 311.7</cell></row><row><cell>Inception-v3</cell><cell>60.1</cell><cell>105.0</cell><cell>6.4</cell><cell>9.8</cell><cell>95.6</cell><cell cols="3">165.4 236.0 304.9</cell></row><row><cell>SGANet</cell><cell>58.0</cell><cell>100.4</cell><cell>6.3</cell><cell>10.6</cell><cell>89.1</cell><cell cols="3">150.6 224.6 314.6</cell></row><row><cell>SGANet + CL</cell><cell>57.6</cell><cell>101.1</cell><cell>6.6</cell><cell>10.2</cell><cell>87.6</cell><cell cols="3">152.5 221.9 289.8</cell></row><row><cell cols="4">shallow depth, DADNet [26] which employs the ideas of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">dilated convolution, attention map and deformable convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">in the framework, CANNet [55] which captures context-aware</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>feature by multiple branches, TEDNet [25] which also uses Inception-style modules, RANet [53] which uses an itera- tive distillation algorithm, ANF [57] which uses conditional random fields (CRFs) to aggregate multi-scale features, and SPANet [58].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>EFFECT OF CURRICULUM LEARNING IN DIFFERENT MODELS ON SHANGHAITECH PART A (THE SYMBOL ↓ MEANS THE ERROR DECREASES WITH THE USE OF CURRICULUM LOSS).</figDesc><table><row><cell>Model</cell><cell cols="4">Without CL MAE RMSE MAE With CL RMSE</cell></row><row><cell>MCNN</cell><cell>91.8</cell><cell>144.9</cell><cell cols="2">89.1 ↓ 142.3 ↓</cell></row><row><cell>CSRNet</cell><cell>67.2</cell><cell>110.5</cell><cell cols="2">66.7 ↓ 113.7</cell></row><row><cell>SANet</cell><cell>64.0</cell><cell>103.4</cell><cell cols="2">62.1 ↓ 100.3 ↓</cell></row><row><cell>CANNet</cell><cell>65.6</cell><cell>106.7</cell><cell cols="2">63.9 ↓ 103.9 ↓</cell></row><row><cell>DADNet</cell><cell>63.7</cell><cell>107.4</cell><cell>64.2</cell><cell>102.1 ↓</cell></row><row><cell cols="2">Inception-v3 60.1</cell><cell>105.0</cell><cell cols="2">58.2 ↓ 97.9 ↓</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>EFFECT OF IMAGE RESOLUTIONS IN THE PERFORMANCE OF SGANET ON UCF_QNRF DATASET.</figDesc><table><row><cell>Model</cell><cell cols="6">UCF_QNRF_512 UCF_QNRF_1024 UCF_QNRF_2048 MAE RMSE MAE RMSE MAE RMSE</cell></row><row><cell>SGANet</cell><cell>126.1</cell><cell>236.1</cell><cell>102.5</cell><cell>178.4</cell><cell>89.1</cell><cell>150.6</cell></row><row><cell>SGANet + CL</cell><cell>118.5</cell><cell>217.2</cell><cell>97.5</cell><cell>169.2</cell><cell>87.6</cell><cell>152.5</cell></row><row><cell>Performance gain</cell><cell>7.6</cell><cell>18.9</cell><cell>5.0</cell><cell>9.2</cell><cell>1.5</cell><cell>-1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RESULTS</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">OF DIFFERENT APPROACHES TO SEGMENTATION MAP</cell></row><row><cell></cell><cell cols="2">SUPERVISION.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">ShTechA MAE RMSE MAE RMSE UCF_QNRF</cell></row><row><cell>W/o Seg. map</cell><cell>60.1</cell><cell>105.0</cell><cell>95.6</cell><cell>165.4</cell></row><row><cell>W/ Seg. map as [44]</cell><cell>59.5</cell><cell>102.2</cell><cell>92.3</cell><cell>155.3</cell></row><row><cell cols="2">W/ Seg. map as SGANet 58.0</cell><cell>100.4</cell><cell>89.1</cell><cell>150.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Fig. 2. Visualization of estimated density and segmentation maps for five test images from ShanghaiTech part A. The numbers shown on the images in the second and third columns are the ground truth and estimated counts respectively.</figDesc><table><row><cell>Test image</cell><cell>Ground truth density map</cell><cell>Predicted density map</cell><cell>Predicted segmentation map</cell></row><row><cell></cell><cell>1068</cell><cell>1027</cell><cell></cell></row><row><cell></cell><cell>1210</cell><cell>1235</cell><cell></cell></row><row><cell></cell><cell>584</cell><cell>596</cell><cell></cell></row><row><cell></cell><cell>417</cell><cell>473</cell><cell></cell></row><row><cell></cell><cell>172</cell><cell>261</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowd counting with limited labeling through submodular frame selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1728" to="1738" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowd analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="345" to="357" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An evaluation of crowd counting methods, features and regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnnbased single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crowd density estimation using fusion of multi-layer features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counting and classification of highway vehicles by regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Tokuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2878" to="2888" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic car counting method for unmanned aerial vehicle images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moranduzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1635" to="1647" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to count leaves in rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leaf counting with deep convolutional and deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stavness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian human segmentation in crowded situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">459</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast crowd segmentation using shape indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zoghlami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counting people in the crowd using a generic head detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Subburaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Descamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carincotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1187" to="1190" />
			<date type="published" when="2006" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time, embedded scene invariant crowd counting using scale-normalized histogram of moving gradients (homg)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="532" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attention to head locations for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crowd counting and density estimation by trellis encoderdecoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6133" to="6142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dadnet: Dilated-attentiondeformable convnet for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8198" to="8207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4031" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving the learning of multi-column convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3343031.3350898</idno>
		<ptr target="http://doi.acm.org/10.1145/3343031.3350898" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, ser. MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, ser. MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1897" to="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crowd counting with deep structured scale integration network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1774" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ha-ccn: Hierarchical attention-based crowd counting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-level bottom-top and topbottom feature fusion for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3225" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting perspective information for efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7279" to="7288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Leveraging heterogeneous auxiliary tasks to assist crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inverse attention guided deep crowd counting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Counting with focus for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: The importance of starting small</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="99" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Point in, box out: Beyond counting persons in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6469" to="6478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-source multiscale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relational attention network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6788" to="6797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive density map generation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attentional neural fields for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5714" to="5723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6152" to="6161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structured prediction based selective pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">He received his PhD in machine learning from The University of Manchester in 2017, Master&apos;s degree in Biomedical Engineering and Bsc in Electronic Engineering in 2013 and 2010 respectively</title>
		<imprint>
			<pubPlace>Hefei</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Qian Wang is a research associate with department of computer science at Durham University, United Kingdom ; both from University of Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>His researches focus on deep learning and computer vision</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">His key research interests lie in the domain of computer vision and image processing and he leads a range of research activity in this area</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Toby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prof. Breckon holds a PhD in informatics</title>
		<imprint/>
		<respStmt>
			<orgName>Breckon is currently a Professor within the Departments of Engineering and Computer Science, Durham University (UK ; University of Edinburgh (UK</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">He has been a visiting member of faculty at the Ecole SupÃl&apos;rieure des Technologies Industrielles AvancÃl&apos;es (France)</title>
		<imprint>
			<pubPlace>Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northwestern Polytechnical University (China), Shanghai Jiao Tong University (China) and Waseda University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">In addition, he is an Accredited Senior Imaging Scientist and Fellow of the Royal Photographic Society. He led the development of image-based automatic threat detection for the 2008 UK MoD Grand Challenge winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prof</surname></persName>
		</author>
		<ptr target="http://www.durham.ac.uk/toby.breckon/" />
		<editor>R.J. Mitchell Trophy</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>British Computer Society</publisher>
		</imprint>
	</monogr>
	<note>His work is recognised as recipient of the Royal Photographic Society Selwyn Award for early-career contribution to imaging science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
