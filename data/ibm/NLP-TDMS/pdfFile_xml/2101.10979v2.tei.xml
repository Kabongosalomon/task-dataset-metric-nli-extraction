<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-training is a competitive approach in domain adaptive segmentation, which trains the network with the pseudo labels on the target domain. However inevitably, the pseudo labels are noisy and the target features are dispersed due to the discrepancy between source and target domains. In this paper, we rely on representative prototypes, the feature centroids of classes, to address the two issues for unsupervised domain adaptation. In particular, we take one step further and exploit the feature distances from prototypes that provide richer information than mere prototypes. Specifically, we use it to estimate the likelihood of pseudo labels to facilitate online correction in the course of training. Meanwhile, we align the prototypical assignments based on relative feature distances for two different views of the same target, producing a more compact target feature space. Moreover, we find that distilling the already learned knowledge to a self-supervised pretrained model further boosts the performance. Our method shows tremendous performance advantage over state-of-the-art methods. We will make the code publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the remarkable success of deep learning in computer vision, attaining high performance requires vast quantities of data. It is usually expensive to obtain labels for dense prediction tasks, e.g., semantic segmentation. Therefore, people think of leveraging abundant photo-realistic synthetic images with freely generated labels <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref>. However, deep neural networks are notoriously sensitive to the domain misalignment that any nuanced unrealism in rendered images will induce poor generalization to real data. To address this issue, domain adaption techniques aim to transfer the knowledge learned from the synthetic images (source domain) to real ones (target domain) with minimal performance loss. In this work, we focus on the challenging case, unsupervised domain adaptation (UDA), where there are no accessible labels in the target domain. Specifically, we solve the UDA problem for semantic segmentation.</p><p>Rather than explicitly aligning the distributions of the source and target domains as most predominant solutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b59">59]</ref>, self-training <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b76">76]</ref> has recently emerged as a simple yet competitive approach in the UDA task. This is achieved by iteratively generating a set of pseudo labels based on the most confident predictions on the target data and then relying on these pseudo labels to retrain the network. In this way, the network gradually learns the adaptation in the self-paced curriculum learning. However, the performance still lags far behind the supervised learning or semi-supervised learning using a few labeled samples, making unsupervised domain adaptation impractical in real scenarios.</p><p>After dissecting the self-training, we find two key ingredients are lacking in previous works. First, typical practice <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b76">76]</ref> suggests selecting the pseudo labels according to a strict confidence threshold, while high scores are not necessarily correct, making the network fail to learn reliable knowledge in the target domain. Second, due to the domain gap, the network is prone to produce dispersed features in the target domain. It is likely that for target data, the closer to the source distribution, the higher the confidence score. As a result, data lying far from the source distribution (i.e. low scores) will never be considered during the training.</p><p>In this paper, we propose to online denoise the pseudo labels and learn a compact target structure to address the above two issues respectively. We resort to prototypes, i.e., the class-wise feature centroids, to accomplish the two tasks. <ref type="bibr" target="#b0">(1)</ref> We rectify the pseudo labels by estimating the class-wise likelihoods according to its relative feature distances to all class prototypes. This depends on a practical assumption that the prototype lies closer to the true centroid of the underlying cluster, implying that false pseudo labels are in the minority. It is worth noting that the prototypes are computed on-the-fly, and thus the pseudo labels are progressively corrected throughout the training. <ref type="bibr" target="#b1">(2)</ref> We draw inspiration from the Deepcluster <ref type="bibr" target="#b3">[4]</ref> to learn the intrinsic structure of the target domain. Instead of directly learning from the cluster assignment, we propose to align soft prototypical assignments for different views of the same target, which produces a more compact target feature space. We refer to our method ProDA as we heavily rely on prototypes for domain adaption.</p><p>Supercharged with the above techniques, our ProDA can demonstrate clear superiority over prior works. Moreover, we find that the domain adaptation can also benefit from the task-agnostic pretraining -distilling the knowledge to a self-supervised model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> further boosts the performance to a record high. Our contributions can be summarized as follows:</p><p>• We propose to online correct the soft pseudo labels according to the relative feature distances to the prototypes, whereas the prototypes are also updated on-the-fly. The network thereby learns from denoised pseudo labels throughout the training. • We propose to rely on the soft prototypical assignment to teach the learning of an augmented view so that a compact target feature space can be obtained. • We show that distilling the already-learned knowledge to a self-supervised pretrained model further improves the performance significantly. • The proposed ProDA substantially outperforms state-ofthe-art. With the Deeplabv2 <ref type="bibr" target="#b7">[8]</ref> network, our method achieves the Cityscapes <ref type="bibr" target="#b13">[14]</ref> segmentation mIOU by 57.5 and 55.5 when adapting from the GTA5 <ref type="bibr" target="#b45">[45]</ref> and SYN-THIA <ref type="bibr" target="#b46">[46]</ref> datasets, improving the adaption gain 1 by 52.6% and 58.5% respectively over the prior leading approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised domain adaptation. As suggested by the theoretical analysis <ref type="bibr" target="#b2">[3]</ref>, domain alignment methods focus on reducing the distribution mismatch by optimizing some divergence <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref> or adopting adversarial training <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">41]</ref> at either the image level <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b64">64]</ref>, the intermediate feature level <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b59">59]</ref> or the output level <ref type="bibr" target="#b55">[55]</ref>. However, aligning global distribution cannot guarantee a small expected error on the target domain <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b72">72]</ref>. Recent approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">60]</ref> attempt to align distribution in a class-wise manner, aiming to promote fine-grained feature alignment. In fact, it is unnecessary to rigorously align the distribution as long as the features are well-separated. On the other hand, techniques originated from semisupervised learning (SSL) offer competitive performance. Entropy minimization and its variants <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b58">58]</ref> encourage the network to make sharp predictions on the unlabeled target data, and the resulting network is prone to be over-confident on false predictions. To address this, selftraining <ref type="bibr" target="#b75">[75]</ref> that leverages iteratively generated pseudo la- <ref type="bibr" target="#b0">1</ref> The mIoU gain relative to the model without domain adaption. bels has been proposed. However, the pseudo labels are inevitably noisy. Hence, <ref type="bibr" target="#b76">[76]</ref> adds confidence regularization terms to the network, while <ref type="bibr" target="#b73">[73]</ref> explicitly estimates a prediction confidence map to reduce the side-effect of unreliable labels. In <ref type="bibr" target="#b34">[35]</ref>, self-training and image translation are found mutually beneficial. A recent work <ref type="bibr" target="#b69">[69]</ref> generates pseudo labels based on categorical centroids and enforces feature alignment in category level. However, these selftraining approaches are optimized in an alternative manner -labels are fixed over the course of representation learning, and only get updated after the entire training stage. In contrast, we propose an online pseudo label updating scheme where the false predictions are rectified according to the prototypical context estimated in the target domain.</p><p>Unsupervised representation learning. A surge of research interest has been recently attracted to unsupervised learning <ref type="bibr" target="#b43">[43]</ref>. Early efforts dedicate to designing pretext tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b70">70]</ref>, which are proven beneficial for UDA when utilized as auxiliary tasks on target data <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b67">67]</ref>. The gap with supervised learning is considerably closed by a few prominent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> that build on contrastive learning. A series of recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">40]</ref> find that the network is able to learn rich semantic features as long as they are consistent under different augmented views. Yet these methods assume image-wise discrimination <ref type="bibr" target="#b65">[65]</ref>, making them unsuitable for learning pixel-level semantics for segmentation tasks. In this work, we find the marriage of consistent learning and cluster-based representation learning fits remarkably well with the UDA problem and learn a compact target feature space inspired from Deepcluster <ref type="bibr" target="#b3">[4]</ref>. Differently, we align relative feature distances rather than cluster assignments for different augmented views.</p><p>Learning from noisy labels. Self-training even with careful thresholding still gives noisy pseudo labels. Therefore, this work is also motivated by emerging techniques <ref type="bibr" target="#b53">[53]</ref> of learning from noisy labels. A straightforward way is to design robust losses <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b71">71]</ref>, but these methods fail to handle real-world noisy data. Self-label correction <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b74">74]</ref> is a more appealing approach. A typical manner <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref> under this category is to train two or multiple learners simultaneously and exploit their agreement of predictions to measure the label reliability. Our proposed pseudo label denoising is more close to <ref type="bibr" target="#b22">[23]</ref> which online corrects the incorrect labels according to the prototypes determined by some complex heuristics. In contrast, we are able to compute prototypes on-the-fly. On the other hand, knowledge distillation (KD) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b66">66]</ref> is proven effective to transfer clean knowledge from the teacher model to the student even when the network learns from itself <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref>. In this work, we demonstrate that the knowledge distillation to a self-supervised pretrained model further pushes the performance limit in our task.  <ref type="figure">Figure 1</ref>: We illustrate the existing issues of self-training by visualizing the feature space. (a) The decision boundary (dashed line) crosses the distribution of the target data and induces incorrect pseudo label predictions. This is because the network is unaware of the target distribution when generating pseudo labels. To solve this, we calculate the prototypes of each class on-the-fly and rely on these prototypes to online correct the false pseudo labels. (b) The network may induce dispersed feature distribution in the target domain which is hardly differentiated by a linear classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>Given the source dataset X s = {x s } ns j=1 with segmentation labels Y s = {y s } ns j=1 , we aim to train a segmentation network that learns the knowledge from the source and is expected to achieve low risk on the target dataset X t = {x t } nt j=1 without accessing its ground truth labels Y t , where Y s and Y t share the same K classes. Generally, the network h = g • f can be regarded as a composite of a feature extractor f and a classifier g.</p><p>Typically, the networks trained on the source data, i.e., the source model, cannot generalize well to the target data due to the domain gap. To transfer the knowledge, traditional self-training techniques <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b76">76]</ref> optimize the categorical cross-entropy (CE) with pseudo labelsŷ t :</p><formula xml:id="formula_0">t ce = − H×W i=1 K k=1ŷ (i,k) t log(p (i,k) t ),<label>(1)</label></formula><p>where p t = h(x t ) and p (i,k) t represents the softmax probability of pixel x (i) t belonging to the kth class. Typically, one can use the most probable class predicted by the source network as pseudo labels:</p><formula xml:id="formula_1">y (i,k) t = 1, if k = arg max k p (i,k ) t 0, otherwise<label>(2)</label></formula><p>Here we denote this conversion from the soft predictions to hard labels byŷ t = ξ(p t ). In practice, since the pseudo labels are noisy, only the pixels whose prediction confidence is higher than a given threshold are accounted for the pseudo label retraining. In this way, the network in the target domain is bootstrapped by learning from pseudo labels that only get update till convergence, and then the updated labels are employed for the next training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Prototypical pseudo label denoising</head><p>We conjecture that updating the pseudo label after one training stage is too late as the network has already overfitted the noisy labels. While simultaneously updating the pseudo labels and the network weights, on the other hand, is prone to give trivial solutions.</p><p>In this work, we propose a simple yet effective approach to online update the pseudo labels while avoiding trivial solutions. The key is to fix the soft pseudo labels and progressively weight them by class-wise probabilities, with the update in accordance with the freshly learned knowledge. Formally, we propose to use the weighted pseudo labels for self-training:ŷ</p><formula xml:id="formula_2">(i,k) t = ξ(ω (i,k) t p (i,k) t,0 ),<label>(3)</label></formula><p>where ω</p><formula xml:id="formula_3">(i,k) t</formula><p>is the weight we propose for modulating the probability and changes as the training proceeds. The p</p><formula xml:id="formula_4">(i,k)</formula><p>t,0 is initialized by the source model and remains fixed throughout the learning process, thus serving as a boilerplate for the subsequent refinement.</p><p>We exploit the distances from the prototypes to gradually rectify the pseudo labels. Let f (x t ) (i) represent the feature of x t at index i. If it is far from the prototype η (k) , the feature centroids of class k, it is more probable that the learned feature is an outlier, hence we downweight its probability of being classified into kth category. Concretely, we define the modulation weight in Equation 3 as the softmax over feature distances to prototypes:</p><formula xml:id="formula_5">ω (i,k) t = exp(− f (x t ) (i) − η (k) /τ ) k exp(− f (x t ) (i) − η (k ) /τ ) ,<label>(4)</label></formula><p>wheref denotes the momentum encoder <ref type="bibr" target="#b23">[24]</ref> of the feature extractor f , as we desire a reliable feature estimation for x t , and τ is the softmax temperature empirically set to τ = 1. In this equation, ω (i,k) t actually approximates the trust confidence of x (i) t belonging to the kth class. Note that this equation has a close formulation in <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b51">51]</ref> that shows promise in few-shot learning. Instead of relying on the prototypes for classifying new samples, we attempt to correct the mistakes in the pre-generated pseudo labels according to this prototypical context. Prototype computation. The proposed label updating scheme requires to compute the prototypes on-the-fly. At first, prototypes are initialized according to the predicted pseudo labelsŷ t for target domain images, which is</p><formula xml:id="formula_6">η (k) = xt∈Xt i f (x t ) (i) * 1(ŷ (i,k) t == 1) xt∈Xt i 1(ŷ (i,k) t == 1) ,<label>(5)</label></formula><p>where 1 is the indicator function. However, such prototype calculation is computational-intensive during training. To address this, we estimate the prototypes as the moving average of the cluster centroids in mini-batches, so that we can track the prototypes that slowly move. Specifically, in each iteration, the prototype is estimated as,</p><formula xml:id="formula_7">η (k) ← λη (k) + (1 − λ)η (k) ,<label>(6)</label></formula><p>where η (k) is the mean feature of class k calculated within the current training batch from the momentum encoder, and λ is the momentum coefficient which we set to 0.9999. Pseudo label training loss. With the online refined pseudo labels, we are able to train the network for target domain segmentation. Instead of using a standard cross-entropy, we adopt a more robust variant, symmetric cross-entropy (SCE) loss <ref type="bibr" target="#b63">[63]</ref>, to further enhance the noise-tolerance to stabilize the early training phase. Specifically, we enforce</p><formula xml:id="formula_8">t sce = α ce (p t ,ŷ t ) + β ce (ŷ t , p t ),<label>(7)</label></formula><p>where α and β are balancing coefficients and set to 0.1 and 1 respectively. Following <ref type="bibr" target="#b63">[63]</ref>, we clamp the one-hot label y t to [1e−4, 1], so as to avoid the numerical issue of log 0. Why are prototypes useful for label denoising? First, the prototypes are less sensitive to the outliers that are assumed to be the minority. Second, prototypes treat different classes equally regardless of their occurrence frequency, which is particularly useful to semantic segmentation as class imbalance poses a challenging issue. Experiments show that the proposed label denoising considerably improves the performance for hard classes. More importantly, prototypes help to gradually rectify the incorrect pseudo labels, which we illustrate using a toy example. As shown in <ref type="figure">Figure 1(a)</ref>, the classifier g, may still give a decision boundary crossing the target clusters and yields false pseudo labels. Training against them cannot further improve the classifier. The prototypes, on the other hand, downweight the false pseudo labels near the decision boundary of g as they are far from the prototypes. In this way, the network improves, and in turn, makes the prototypes closer to the true cluster centroid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structure learning by enforcing consistency</head><p>Pseudo labels can be denoised when the feature extractor f generates compact target features. However, due to the domain gap, the generated target distribution is more likely to be dispersed, as shown in <ref type="figure">Figure 1</ref>(b). In this case, the prototypes fail to rectify the labels of the data whose features lie in the far end of the cluster even when the target features from the source model are well-separated. A recent work <ref type="bibr" target="#b42">[42]</ref> has identified this issue in semi-supervised learning, but the issue becomes worse in domain adaptation since a few pseudo labeled data cannot cover the entire distribution in the target domain.</p><p>To this end, we aim to learn the underlying structure of target domain, and hope to obtain more compact features that are friendly to the pseudo label refinement. Motivated by the recent success of unsupervised learning, we perform simultaneously clustering and representation learning. As opposed to learning against the prototypical assignment, we use the prototypical assignment under weak augmentation to guide the learning for the strong augmented view. Specifically, let T (x t ) and T (x t ) respectively denote the weak and strong augmented views for x t . We make use of the momentum encoderf to generate a reliable prototypical assignment for T (x t ) which is,</p><formula xml:id="formula_9">z (i,k) T = exp(− f (T (x t )) (i) − η (k) /τ ) k exp(− f (T (x t )) (i) − η (k ) /τ ) ,<label>(8)</label></formula><p>where τ = 1 by default. Likewise, the soft assignment z T for T (x t ) can be obtained in the same manner except that we use the original trainable feature extractor f . Since z t is more reliable as the feature is given by a momentum encoderf and the input x t suffers from less distortion, we use it to teach f to produce consistent assignments for T (x t ).</p><p>Hence, we minimize the Kullback-Leibler (KL) divergence between the prototypical assignments under two views:</p><formula xml:id="formula_10">t kl = KL (z T z T ) .<label>(9)</label></formula><p>Intuitively, this equation enforces the network to give consistent prototypical labeling for the adjacent feature points, which results in more compact feature space in the target domain.</p><p>Similar to the previous works that simultaneously learn the clustering and representation, the proposed prototypical consistent learning may suffer from degeneration issue, i.e., one cluster becomes empty. To amend this, we use a regularization term from <ref type="bibr" target="#b76">[76]</ref>, which encourages the output to be evenly distributed to different classes,</p><formula xml:id="formula_11">t reg = − H×W i=1 K j=1 log p (i,k) t .<label>(10)</label></formula><p>Now equipped with the online label correction and the prototypical consistent learning, we train the domain adaptation network with the following total loss:</p><formula xml:id="formula_12">total = s ce + t sce + γ 1 t kl + γ 2 t reg .<label>(11)</label></formula><p>By default, the weighting coefficients γ 1 and γ 2 are set to 10, 0.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distillation to self-supervised model</head><p>After the training with Equation 11 converges, we further transfer knowledge from the learned target model to a student model with the same architecture but pretrained in a self-supervised manner. To be concrete, we initialize the feature extractor of the student model h † with Sim-CLRv2 <ref type="bibr" target="#b10">[11]</ref> pretrained weights, and we apply a knowledge distillation (KD) loss, which lets the student mimic the teacher by minimizing the KL divergence of their predictions on the unlabeled target images. Besides, following the self-training paradigm, we rely on the teacher model h to generate one-hot pseudo labels ξ(p t ) so as to teach the student model. To prevent the model from forgetting the source domain, the source images are also utilized. Altogether, we train the student model using the following loss,</p><formula xml:id="formula_13">KD = s ce (p s , y s ) + t ce (p † t , ξ(p t )) + βKL(p t p † t ),<label>(12)</label></formula><p>where p † t = h † (x t ) is the output of the student model, and we set β = 1. In practice, such self-distillation can be applied multiple times once the model converges, which helps the domain adaptation to achieve even higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>Training. We use the DeepLabv2 <ref type="bibr" target="#b7">[8]</ref> for segmentation with the backbone ResNet-101 <ref type="bibr" target="#b24">[25]</ref>. Following <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b73">73]</ref>, we utilize <ref type="bibr" target="#b56">[56]</ref> that applies adversarial training at the segmentation output as a warm-up. We apply the SGD solver with the initial learning rate as 1e-4 which is decayed by 0.9 every training epoch, and the training lasts 80 epochs. During the structure learning, the augmentation is composed of random crop, RandAugment <ref type="bibr" target="#b14">[15]</ref> and Cutout <ref type="bibr" target="#b15">[16]</ref>. For knowledge distillation, we utilize the pretrained SimCLRv2 model with the ResNet-101 backbone as well. An extra batch normalization (BN) layer is introduced after its feature extraction layer so as to accommodate the activation magnitude for our task, with the learning rate set to 6e-4 and 6e-3 respectively before and after this BN layer. During the distillation stage, we use hard pseudo labels with the selection threshold 0.95. Readers can refer to the supplementary material for more training details and algorithm flow. We conduct all the experiments on 4 Tesla V100 GPUs with PyTorch implementation.</p><p>Dataset. For evaluation, we adapt the segmentation from game scenes, GTA5 <ref type="bibr" target="#b45">[45]</ref> and SYNTHIA <ref type="bibr" target="#b46">[46]</ref> datasets, to real scene, the Cityscapes <ref type="bibr" target="#b13">[14]</ref> dataset. GTA5 contains 24,966 training images with the resolution of 1914×1052 and we use its 19 categories shared with Cityscapes. SYN-THIA dataset contains 9,400 1280×760 images and we use its 16 common categories with Cityscapes. We also report the results on 13 common categories on this dataset following the protocol of some methods. The Cityscapes dataset contains 2,975 training images and 500 images for validation with the resolution of 2048×1024. Since its testing set does not provide ground truth labeling, we conduct evaluations on its validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with state-of-the-art methods</head><p>We comprehensively compare our proposed method with the recent leading approaches. These methods could be divided into two categories: 1) domain alignment methods that align the distribution through adversarial training, which include AdaptSeg <ref type="bibr" target="#b55">[55]</ref>, CyCADA <ref type="bibr" target="#b26">[27]</ref>, CLAN <ref type="bibr" target="#b36">[37]</ref>, APODA <ref type="bibr" target="#b68">[68]</ref>, PatchAlign <ref type="bibr" target="#b57">[57]</ref>, ADVENT <ref type="bibr" target="#b58">[58]</ref>, BDL <ref type="bibr" target="#b34">[35]</ref> and FADA <ref type="bibr" target="#b61">[61]</ref>; 2) self-training approaches, including CBST <ref type="bibr" target="#b75">[75]</ref>, MRKLD <ref type="bibr" target="#b76">[76]</ref>, Seg-Uncertainty <ref type="bibr" target="#b73">[73]</ref>, CAG UDA <ref type="bibr" target="#b69">[69]</ref>. <ref type="table" target="#tab_2">Table 1</ref> shows the comparisons of GTA5 → Cicyscapes adaptation. our ProDA arrives at the state-of-the-art mIoU score 57.5, outperforming existing methods by a large margin. Among all the 19 categories, we achieve the best scores on 15 categories. ProDA shows evident advantage in hard classes, e.g., fence, terrace, motor, that cannot be well handled in previous works. Indeed, the performance improvement of ProDA mostly comes from these challenging cases, the small or rare objects, as we regard different categories equally thanks to the prototypes. Note that this is achieved without any heuristic class-balance strategies as <ref type="bibr" target="#b75">[75]</ref>. Compared with the non-adapted baseline (i.e., the model purely trained on the source), ProDA offers the mIoU gain by 20.9, outperforming the second-best method by 52.6%.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we show the adaptation results on SYNTHIA → Cityscapes, where ProDA also shows tremendous improvement. The proposed ProDA achieves the mIoU score by 55.5 and 62.0 over the 16 and 13 categories respectively. To be specific, we arrive at the best on 11 out of 16 categories, mostly the hard classes. Relative to the nonadaptive model, the segmentation model after our adaption sees the gain by 20.6, surpassing that of the prior leading approach by 58.4%. While the adaptation from SYNTHIA is more challenging than that from GTA5, our ProDA per-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion</head><p>The effectiveness of pseudo label denoising. In <ref type="table" target="#tab_2">Table 11</ref>, the non-adapted source model only gives 36.6 mIoU on the target domain. After the model warm-up, we get a 5.0 mIoU increase. Initialized with the warm-up, the baseline model, i.e., the vanilla self-training, which is trained using the offline pseudo labels, improves the mIoU to 45.2. Adding symmetric cross-entropy (SCE) brings +0.4 mIoU gain. By contrast, we are able to update the pseudo labels on-thefly and the training with the denoised pseudo labels significantly boosts the performance to 52.3. The model with this component alone sets the state-of-the-art, outperforming the prior best score by 2.0. <ref type="figure">Figure 2</ref> illustrates how the pseudo labels progress as the training proceeds. In the early phase, false pseudo labels are likely to occur and fail to recognize the tiny objects, e.g., the poles and traffic signs. As the training goes on, pseudo labels get denoised by virtue of the prototypes, and tiny objects can be gradually identified in the refined pseudo labels. The training after 40k iterations can already correct most of the incorrect labels. In <ref type="figure">Figure 3</ref>, we further quantify the mIoU score of the pseudo labels during the whole training process. While convention self-training only updates the pseudo labels after each stage and leads to step-wise mIoU increase, the pseudo labels given by ProDA can quickly attain a high quality, showing a distinct advantage over the   learning the feature and updating the labels during selftraining derives degenerate solutions. The key to avoiding such degeneration is to adopt a fixed soft label p t,0 as the boilerplate upon which we apply the weight ω t for rectification (Equation 3). To explain this, we investigate different variants for the online label updating in <ref type="figure">Figure 4</ref>, where the labels for self-training can be dynamic p t or fixed p t,0 , and the modulation weight can be the network prediction p t or the confidence w t estimated according to prototypes. In addition, we investigate the benefit of using hard labels (ξ(·)). <ref type="figure">Figure 4</ref> shows that the performance climbs up for a while and then starts to drop significantly when using non-fixed soft predictions (p t and its variants), whereas the learning with the fixed ones (p t,0 and its variants) steadily improves throughout the training and surpasses the conventional selftraining. We conjecture that fixing p t,0 makes the refined pseudo labels hardly deviate from this initial prediction, thus avoiding the trivial solution. Moreover, in contrast to using p t for reweighting, prototypical reweighting improves  <ref type="figure">Figure 5</ref>: The visualization of feature space, where we map features to 2D space with UMAP <ref type="bibr" target="#b37">[38]</ref>. For a clear illustration, we only show four categories, i.e., blue for building, gray for traffic sign, orange for pole, and green for vegetation.</p><p>the mIoU by more than 5.0, corroborating the importance of using prototypes for label denoising. Besides, we observe slight improvement (∼0.2) using hard labels over the soft ones.</p><p>The effectiveness of target structure learning. We propose to cultivate the intrinsic knowledge for the target domain and learns its underlying structure. As shown in <ref type="table" target="#tab_2">Table 11</ref>, without the label denoising, the structure learning improves the performance from 45.6 to 47.6. Note that this is also competitive among self-training approaches, but we do not need to carefully choose the threshold for selecting the confident pseudo labels. The target structure learning assists the pseudo label denoising by forming compact feature clusters and brings the performance gain by 1.4. The effectiveness of distilling to self-supervised model. At the second training stage, we apply knowledge distillation and transfer the dark knowledge of the 1st stage model to the current phase. <ref type="table" target="#tab_2">Table 11</ref> also compares different initialization strategies for this stage. Compared to resuming the last stage training, the initialization from a selfsupervised pretrained model (i.e., SimCLRv2) improves the mIoU by 0.6, whereas the initialization with the supervised pretraining degrades the performance. This is because the self-supervised pretraining possesses stronger transferability and can benefit a broad of downstream tasks. The initialization in this way helps the model escape from the local optima in the last stage. <ref type="table" target="#tab_2">Table 11</ref> also proves the effectiveness of the knowledge distillation: ablating this component drops the mIoU by 1.1. It is surprising to see that the third stage with the knowledge distillation further improves the performance by 0.6, attaining the 20.9 mIoU improvement relative to the model without domain adaptation.</p><p>The UMAP visualization of target features. To better develop intuition, we visualize the learned features for ProDA in <ref type="figure">Figure 5</ref>. The model before the domain adaptation mixes the features of the same class. The conventional self-training could produce more separated feature space, yet it is still hard for linear classification. When we apply prototypical pseudo label denoising, features among differ-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Parameter sensitivity analysis.</head><p>To showcase that ProDA is robust to the hyper-parameter selection, we analyze the impact of parameters. In <ref type="table" target="#tab_7">Table 4</ref>, we use different threshold values to select the pseudo labels and the performance is not sensitive to this threshold as opposed to the conventional self-training. Hence, ProDA does not apply thresholding for convenient usage. We also study the effect of momentum value when online computing the prototypes, and ProDA is robust to a wide numerical range as shown in <ref type="table" target="#tab_8">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose ProDA which resorts to prototypes to online denoise the pseudo labels and learn the compact feature space for the target domain. Knowledge distillation to a self-supervised pretrained model further boosts the performance. The proposed method outperforms stateof-the-art methods by a large margin, greatly reducing the gap with supervised learning. We will make the code publicly available. We believe the proposed method is a general amelioration to the self-training and we hope to explore its capability in other tasks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Influences of Design Choices</head><p>Prototype initialization strategy. In our implementation, the proposed ProDA initializes the prototypes of the target domain according to the pseudo predictions for the target images. Alternatively, the target prototypes can also be initialized according to the ground truth labeling in the source domain. However, both choices have their pros and cons: the former suffers from the noises in the pseudo labels whereas the latter suffers from domain gap as the prototypes of the two domains may not accurately align. <ref type="table">Table 6</ref> shows that the two initialization strategies induce comparable results, as the prototypes are online updated and can rapidly converge to the true cluster centroids. The quantitative performance is measured on the dataset GTA5 → Cityscapes, whereas the other dataset shows similar results. source ground truth target pseudo label mIoU 53.6 53.7 <ref type="table">Table 6</ref>: The performance of different target prototype initialization strategies. Here we only report the performance for the 1st training stage in the gta5 → Cityscapes task.</p><p>Strong augmentation. In the target structure learning, we take weak and strong augmentation views for the target image. We employ random crop for weak augmentation and explore the effects of different augmentation types for the strong augmented view. As shown in <ref type="table">Table 7</ref>, random crop only gives the mIoU score 52.7, whereas adding RandAugment <ref type="bibr" target="#b14">[15]</ref> and CutOut <ref type="bibr" target="#b15">[16]</ref> respectively improve the mIoU by 0.78 and 0.5. The strongest augmentation gives the best performance, indicating the importance of data augmentation when learning the compact feature space for the target domain.  <ref type="table">Table 7</ref>: The influence of various strong augmentations. Here we only report the performance for the 1st training stage in the gta5 → Cityscapes task.</p><p>Effect of temperature during prototypical denoising. We rely on the prototypical context to rectify the pseudo labels. We compute the softmax over feature distance to all the prototypes, and the softmax temperature τ influences the denoising effect and requires balancing: when τ → 0, only the nearest prototype dominates whereas τ → ∞ causes that all the prototypes are accounted equally. The influence of the temperature is shown in <ref type="table">Table 8</ref>. We empirically set τ = 1 in our experiments.  <ref type="table">Table 8</ref>: The effects of temperature during the prototypical denoising. Here we only report the performance for the 1st training stage in the gta5 → Cityscapes task.</p><p>Symmetric cross-entropy loss. We employ the symmetric cross-entropy loss (SCE) for robust learning to stabilize the early training phase. The SCE has coefficients α and β that balance the cross-entropy and the reverse cross-entropy. <ref type="table" target="#tab_12">Table 9</ref> shows that the final result is not sensitive to these hyper-parameters if β is not too small. Here, we follow the suggested setting as <ref type="bibr" target="#b63">[63]</ref>, i.e., α = 0.1, β = 1.</p><p>The effect of loss weight. <ref type="table" target="#tab_2">Table 10</ref> shows that the final result is not sensitive to the KL loss weight (γ 1 ) and the regularization loss weight (γ 2 ). In GTA5 → Cityscapes, we set γ 1 = 10 and γ 1 = 0.1, while in SYNTHIA → Cityscapes, we set γ 1 = 10 and γ 1 = 0.   <ref type="table" target="#tab_2">Table 10</ref>: The influence of the KL loss weight (γ 1 ) and the regularization loss weight (γ 2 ). Here we only report the performance for the 1st training stage in the gta5 → Cityscapes task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Detailed Ablation study</head><p>Here we show a detailed ablation study for all the 19 classes on GTA5 → Cityscapes.    AdaptSeg <ref type="bibr" target="#b56">[56]</ref> CBST <ref type="bibr" target="#b75">[75]</ref> BDL <ref type="bibr" target="#b34">[35]</ref> Seg-Uncertainty <ref type="bibr" target="#b73">[73]</ref> CAG UDA <ref type="bibr" target="#b69">[69]</ref> FADA <ref type="bibr" target="#b61">[61]</ref> ADVENT <ref type="bibr" target="#b58">[58]</ref> CLAN [37] ProDA <ref type="figure">Figure 8</ref>: Qualitative comparisons of different methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The mean accuracy and mean IoU score of the pseudo labels throughout the training. Comparing to the conventional self-training that updates pseudo labels only after the training stage, the pseudo labels in our method steadily improves as the training proceeds.vanilla self-training of multiple training stages. How to prevent degenerate solution? Simultaneously The training curves of different label reweighting schemes when online updating the labels. Adopting fixed p t,0 labels avoid the trivial solutions that plague the training with non-fixed pseudo labels p t . The dotted line denotes the performance of conventional self-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) w/o domain adaptation (b) conventional self-training (c) w/ prototypical denoising (1st stage) (d) our full model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.7 77.5 31.5 25.7 41.5 38.7 20.3 84.6 38.2 74.1 63.7 21.7 86.0 29.0 37.5 0.3 34.9 26.2 45.2 +8.6 87.7 36.8 78.2 30.9 24.8 41.5 40.0 23.2 83.0 35.3 72.9 64.1 24.6 85.9 32.9 36.5 2.0 31.0 35.0 45.6 +9.0 93.2 56.7 84.1 40.4 37.5 39.5 44.1 35.1 87.1 43.2 80.3 65.8 29.8 87.7 29.6 41.9 0.0 44.4 52.6 52.3 +15.7 89.0 38.6 80.7 37.1 27.2 42.8 41.5 20.7 85.8 42.4 74.8 64.8 17.8 87.6 30.8 39.4 0.0 41.0 34.6 47.6 +11.0 91.5 52.4 82.9 42.0 35.7 40.0 44.4 43.3 87.0 43.8 79.5 66.5 31.4 86.7 41.1 52.5 0.0 45.4 53.8 53.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 +0.0 AdaptSeg [55] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 +4.8 Uncertainty [73] 90.4 31.2 85.1 36.9 25.6 37.5 48.8 48.5 85.3 34.8 81.1 64.4 36.8 86.3 34.9 52.2 1.7 29.0 44.6 50.3 +13.7 ProDA 87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5 +20.9</figDesc><table><row><cell></cell><cell>road</cell><cell>sideway</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU gain</cell></row><row><cell cols="21">Source 75.8 CyCADA [27] 86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7 +6.1</cell></row><row><cell>CLAN [37]</cell><cell cols="20">87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 +6.6</cell></row><row><cell>APODA [68]</cell><cell cols="20">85.6 32.8 79.0 29.5 25.5 26.8 34.6 19.9 83.7 40.6 77.9 59.2 28.3 84.6 34.6 49.2 8.0 32.6 39.6 45.9 +9.3</cell></row><row><cell>PatchAlign [57]</cell><cell cols="20">92.3 51.9 82.1 29.2 25.1 24.5 33.8 33.0 82.4 32.8 82.2 58.6 27.2 84.3 33.4 46.3 2.2 29.5 32.3 46.5 +9.9</cell></row><row><cell>ADVENT [58]</cell><cell cols="20">89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5 +8.9</cell></row><row><cell>BDL [35]</cell><cell cols="20">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 +11.9</cell></row><row><cell>FADA [61]</cell><cell cols="20">91.0 50.6 86.0 43.4 29.8 36.8 43.4 25.0 86.8 38.3 87.4 64.0 38.0 85.2 31.6 46.1 6.5 25.4 37.1 50.1 +13.5</cell></row><row><cell>CBST [75]</cell><cell cols="20">91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 +9.3</cell></row><row><cell>MRKLD [76]</cell><cell cols="20">91.0 55.4 80.0 33.7 21.4 37.3 32.9 24.5 85.0 34.1 80.8 57.7 24.6 84.1 27.8 30.1 26.9 26.0 42.3 47.1 +10.5</cell></row><row><cell>CAG UDA [69]</cell><cell cols="20">90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2 +13.6</cell></row><row><cell>Seg-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of GTA5→Cityscapes adaptation in terms of mIoU. The best score for each column is highlighted. Uncertainty [73] 87.6 41.9 83.1 14.7 1.7 36.2 31.3 19.9 81.6 80.6 63.0 21.8 86.2 40.7 23.6 53.1 47.9 +13.0 54.9 +14.6 ProDA 87.8 45.7 84.6 37.1 0.6 44.0 54.6 37.0 88.1 84.4 74.2 24.3 88.2 51.1 40.5 45.6 55.5 +20.6 62.0 +21.7</figDesc><table><row><cell></cell><cell>road</cell><cell>sideway</cell><cell>building</cell><cell>wall*</cell><cell>fence*</cell><cell>pole*</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell>mIoU gain mIoU* gain*</cell></row><row><cell>Source</cell><cell cols="17">64.3 21.3 73.1 2.4 1.1 31.4 7.0 27.7 63.1 67.6 42.2 19.9 73.1 15.3 10.5 38.9 34.9 + 0.0 40.3</cell><cell>+0.0</cell></row><row><cell>AdaptSeg [55]</cell><cell cols="4">79.2 37.2 78.8 -</cell><cell>-</cell><cell>-</cell><cell cols="10">9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3</cell><cell>-</cell><cell>-</cell><cell>45.9</cell><cell>+5.6</cell></row><row><cell>PatchAlign [57]</cell><cell cols="17">82.4 38.0 78.6 8.7 0.6 26.0 3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6 19.3 31.7 40.0 +5.1 46.5</cell><cell>+6.2</cell></row><row><cell>CLAN [37]</cell><cell cols="4">81.3 37.0 80.1 -</cell><cell>-</cell><cell cols="11">-16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7</cell><cell>-</cell><cell>-</cell><cell>47.8</cell><cell>+7.5</cell></row><row><cell>APODA [68]</cell><cell cols="4">86.4 41.3 79.3 -</cell><cell>-</cell><cell cols="11">-22.6 17.3 80.3 81.6 56.9 21.0 84.1 49.1 24.6 45.7</cell><cell>-</cell><cell>-</cell><cell>53.1 +12.8</cell></row><row><cell>ADVENT [58]</cell><cell cols="17">85.6 42.2 79.7 8.7 0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2 +6.3 48.0</cell><cell>+7.7</cell></row><row><cell>BDL [35]</cell><cell cols="4">86.0 46.7 80.3 -</cell><cell>-</cell><cell cols="11">-14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell>-</cell><cell>51.4 +11.1</cell></row><row><cell>FADA [61]</cell><cell cols="17">84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2 +10.3 52.5 +12.2</cell></row><row><cell>CBST [75]</cell><cell cols="17">68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8 42.6 +7.7 48.9</cell><cell>+8.6</cell></row><row><cell>MRKLD [76]</cell><cell cols="17">67.7 32.2 73.9 10.7 1.6 37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0 19.4 45.3 43.8 +8.9 50.1</cell><cell>+9.8</cell></row><row><cell>CAG UDA [69]</cell><cell cols="17">84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3 44.5 +9.6 51.5 +11.2</cell></row><row><cell>Seg-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of SYNTHIA→Cityscapes adaptation in terms of mIoU. The best score for each column is highlighted. mIoU and mIoU* denote the averaged scores across 16 and 13 categories respectively.</figDesc><table /><note>forms equally well on both datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ProDA online refines the pseudo labels throughout the training. The first column is the segmentation input. The 2nd to 4th columns illustrate the pseudo labels after 1k, 10k, and 40k iterations.</figDesc><table><row><cell>road</cell><cell cols="2">sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell cols="3">traffic light traffic sign vegetation</cell><cell>n/a.</cell></row><row><cell>terrain</cell><cell cols="2">sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row><row><cell cols="4">Figure 2: components</cell><cell></cell><cell>mIoU gain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>initialization</cell><cell></cell><cell></cell><cell>source warm up</cell><cell></cell><cell>36.6 41.6 +5.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>self training</cell><cell>sce</cell><cell>prototypical denoising</cell><cell>structure learning</cell><cell>mIoU gain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.2 +8.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stage 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.6 +9.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.3 +15.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47.6 +11.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53.7 +17.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>self distill.</cell><cell>stage 1 init.</cell><cell>supervised init.</cell><cell>self-supervised init.</cell><cell>mIoU gain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stage 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.8 +19.2 56.3 +19.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.7 +19.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.9 +20.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stage 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.5 +20.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of each proposed component. The whole training involves three stages, where knowledge distillation can be applied in the last two stages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The effect of threshold in pseudo label selection during the prototypical label denoising.</figDesc><table><row><cell>momentum</cell><cell cols="4">0.99 0.999 0.9999 0.99999</cell></row><row><cell>mIoU</cell><cell>53.5</cell><cell>53.6</cell><cell>53.7</cell><cell>52.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The effect of prototype momentum during the prototype online update. ent classes are better separated, though the distribution is still dispersed. In comparison, the full model gives the most compact feature clusters that are amenable to classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>The influence of α and β in the symmetric cross-entropy (SCE) loss. Here we only report the performance for the 1st training stage in the gta5 → Cityscapes task.</figDesc><table><row><cell>γ1</cell><cell>γ2</cell><cell>0.02</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>2</cell><cell></cell><cell>52.9</cell><cell>53.7</cell><cell>53.5</cell></row><row><cell>10</cell><cell></cell><cell>53.2</cell><cell>53.7</cell><cell>53.4</cell></row><row><cell>20</cell><cell></cell><cell>53.4</cell><cell>53.6</cell><cell>52.1</cell></row><row><cell>50</cell><cell></cell><cell>53.6</cell><cell>52.0</cell><cell>52.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 warm up 86.7 34.2 79.3 26.6 21.6 38.4 33.</figDesc><table><row><cell></cell><cell>components</cell><cell>road</cell><cell>sideway</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU gain</cell></row><row><cell>init.</cell><cell>source</cell><cell>75.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>57.4 81.8 42.0 40.2 43.8 50.3 50.9 87.6 42.6 80.0 69.2 32.9 87.8 45.5 56.9 0.0 46.0 55.4 55.8 +19.2 91.4 53.3 83.4 41.3 37.8 43.9 53.0 47.9 88.3 46.1 79.9 70.5 33.2 89.0 48.4 54.6 0.0 50.5 56.7 56.3 +19.7 91.0 50.2 83.1 40.1 39.8 43.5 51.9 48.1 87.9 45.9 78.5 69.6 34.3 87.9 41.3 56.6 0.0 51.7 57.1 55.7 +19.1 89.4 56.5 81.3 46.3 42.7 45.1 52.2 51.3 88.5 47.0 82.9 69.3 36.5 87.4 46.0 57.5 0.6 45.9 54.5 56.9 +20.3 stage 3 87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5 +20.9</figDesc><table><row><cell>self-sup init.</cell><cell>mIoU gain</cell></row><row><cell>90.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Ablation study of each proposed component. The whole training involves three stages, where knowledge distillation can be applied in the last two stages. Here, ST stands for self-training, PD for prototypical denoising, and SL for structure learning.</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell cols="3">traffic light traffic sign vegetation</cell><cell>n/a.</cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell cols="2">Ground truth</cell><cell></cell><cell></cell><cell>Befor adaptation</cell><cell></cell></row><row><cell></cell><cell>AdaptSeg [56]</cell><cell></cell><cell></cell><cell cols="2">CBST [75]</cell><cell></cell><cell></cell><cell>BDL [35]</cell><cell></cell></row><row><cell></cell><cell cols="2">Seg-Uncertainty [73]</cell><cell></cell><cell cols="2">CAG UDA [69]</cell><cell></cell><cell></cell><cell>FADA [61]</cell><cell></cell></row><row><cell></cell><cell cols="2">ADVENT [58]</cell><cell></cell><cell cols="2">CLAN [37]</cell><cell></cell><cell></cell><cell>ProDA</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Figure 7: Qualitative comparisons of different methods.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This work is done during the first author's internship at Microsoft Research Asia.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Algorithm</head><p>The training procedure of our ProDA is summarized in Algorithm 1, which is composed of three stages. The first stage consists of prototypical pseudo label denoising and target structure learning. In the second and third stages, we apply knowledge distillation to a self-supervised model. For detailed equations and loss functions, please refer to our main paper. Calculate the denoising weight ω</p><p>Update the pseudo labelŷ</p><p>Train model h θ using loss t sce ; <ref type="bibr">14 15</ref> Calculate the soft label zT , z T ; <ref type="bibr" target="#b15">16</ref> Train the model h θ using loss t kl and t reg ; <ref type="bibr">17 18</ref> Calculate the batch prototype η c ;  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Heller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12551</idno>
		<title level="m">Keep it simple: Image statistics matching for domain adaptation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with crossdomain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10542" to="10552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep selflearning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahini</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9345" to="9356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Haris</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10285" to="10295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">João Paulo Papa, and Christoph Palm. Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Antonio De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="141" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">fgan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Guo-Jun Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11260</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8050" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Universal domain adaptation through self supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07953</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bringing old photos back to life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2747" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09222</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03788</idno>
		<title level="m">Proselflc: Progressive self label correction for training robust deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ace: adapting to changing environments for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Selfsupervised domain adaptation for computer vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156694" to="156706" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">On learning invariant representation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03773</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
