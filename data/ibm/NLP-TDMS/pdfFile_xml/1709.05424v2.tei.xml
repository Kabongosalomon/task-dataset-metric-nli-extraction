<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NIMA: Neural Image Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
						</author>
						<title level="a" type="main">NIMA: Neural Image Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image quality assessment</term>
					<term>No-reference quality assessment</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically learned quality assessment for images has recently become a hot topic due to its usefulness in a wide variety of applications such as evaluating image capture pipelines, storage techniques and sharing media. Despite the subjective nature of this problem, most existing methods only predict the mean opinion score provided by datasets such as AVA [1] and TID2013 <ref type="bibr" target="#b1">[2]</ref>. Our approach differs from others in that we predict the distribution of human opinion scores using a convolutional neural network. Our architecture also has the advantage of being significantly simpler than other methods with comparable performance. Our proposed approach relies on the success (and retraining) of proven, state-of-the-art deep object recognition networks. Our resulting network can be used to not only score images reliably and with high correlation to human perception, but also to assist with adaptation and optimization of photo editing/enhancement algorithms in a photographic pipeline. All this is done without need for a "golden" reference image, consequently allowing for single-image, semantic-and perceptually-aware, no-reference quality assessment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Quantification of image quality and aesthetics have been a long-standing problem in image processing and computer vision. While technical quality assessment deals with measuring low-level degradations such as noise, blur, compression artifacts, etc., aesthetic assessment quantifies semantic level characteristics associated with emotions and beauty in images. In general, image quality assessment can be categorized into full-reference and no-reference approaches. While availability of a reference image is assumed in the former (metrics such as PSNR, SSIM <ref type="bibr" target="#b2">[3]</ref>, etc.), typically blind (no-reference) approaches rely on a statistical model of distortions to predict image quality. The main goal of both categories is to predict a quality score that correlates well with human perception. Yet, the subjective nature of image quality remains the fundamental issue. Recently, more complex models such as deep convolutional neural networks (CNNs) have been used to address this problem <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Emergence of labeled data from human ratings has encouraged these efforts <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. In a typical deep CNN approach, weights are initialized by training on classification related datasets (e.g. ImageNet <ref type="bibr" target="#b14">[15]</ref>), and then fine tuned on annotated data for perceptual quality assessment tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Machine learning has shown promising success in predicting technical quality of images <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Kang et. al. <ref type="bibr" target="#b4">[5]</ref> show that extracting high level features using CNNs can result in stateof-the-art blind quality assessment performance. It appears H. <ref type="bibr">Talebi</ref>  that replacing hand-crafted features with an end-to-end feature learning system is the main advantage of using CNNs for pixel-level quality assessment tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The proposed method in <ref type="bibr" target="#b4">[5]</ref> is a shallow network with one convolutional layer and two fully-connected layers, and input patches are of size 32 × 32. Bosse et al. <ref type="bibr" target="#b5">[6]</ref> use a deep CNN with 12 layers to improve on image quality predictions of <ref type="bibr" target="#b4">[5]</ref>. Given the small input size (32 × 32 patch), both methods require score aggregation across the whole image. <ref type="bibr">Bianco et al.</ref> in <ref type="bibr" target="#b6">[7]</ref> propose a deep quality predictor based on AlexNet <ref type="bibr" target="#b14">[15]</ref>. Multiple CNN features are extracted from image crops of size 227 × 227, and then regressed to the human scores.</p><p>Success of CNNs on object recognition tasks has significantly benefited the research on aesthetic assessment. This seems natural, as semantic level qualities are directly related to image content. Recent CNN-based methods <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref> show a significant performance improvement compared to earlier works based on hand-crafted features <ref type="bibr" target="#b0">[1]</ref>. Murray et al. <ref type="bibr" target="#b0">[1]</ref> is the benchmark on aesthetic assessment. They introduce the AVA dataset and propose a technique to use manually designed features for style classification. Later, Lu et al. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref> show that deep CNNs are well suited to the aesthetic assessment task. Their double-column CNN <ref type="bibr" target="#b16">[17]</ref> consists of four convolutional and two fully-connected layers, and its inputs are the resized image and cropped windows of size 224 × 224. Predictions from these global and local image views are aggregated to an overall score by a fully-connected layer. Similar to Murray et al. <ref type="bibr" target="#b0">[1]</ref>, in <ref type="bibr" target="#b16">[17]</ref> images are also categorized to low and high aesthetics based on mean human ratings. A regression loss and an AlexNet inspired architecture is used in <ref type="bibr" target="#b8">[9]</ref> to predict the mean scores. In a similar approach to <ref type="bibr" target="#b8">[9]</ref>, Bin et al. <ref type="bibr" target="#b10">[11]</ref> fine-tune a VGG network <ref type="bibr" target="#b17">[18]</ref> to learn the human ratings of the AVA dataset. They use a regression framework to predict the histogram of ratings. A recent method by Zheng et al. <ref type="bibr" target="#b18">[19]</ref> retrains AlexNet and ResNet CNNs to predict quality of photos. More recently, <ref type="bibr" target="#b9">[10]</ref> uses an adaptive spatial pooling to allow for feeding multiple scales of the input image with fixed size aspect ratios to their CNN. This work presents a multi-net (each network a pre-trained VGG) approach which extracts features at multiple scales, and uses a scene aware aggregation layer to combine predictions of the sub-networks. Similarly, Ma et al. <ref type="bibr" target="#b19">[20]</ref> propose a layout-aware framework in which a saliency map is used to select patches with highest impact on predicted aesthetic score. Overall, none of these methods reported correlation of their predictions with respect to ground truth ratings. Recently, Kong et al. in <ref type="bibr" target="#b13">[14]</ref> proposed a method to aesthetically rank photos by training on AVA with a rank-based loss function. They trained an AlexNetbased CNN to learn the difference of the aesthetic scores from two input images, and as a result, indirectly optimize for rank correlation. To the best of our knowledge, <ref type="bibr" target="#b13">[14]</ref> is the only work that performed a correlation evaluation against AVA ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Contributions</head><p>In this work, we introduce a novel approach to predict both technical and aesthetic qualities of images. We show that models with the same CNN architecture, trained on different datasets, lead to state-of-the-art performance for both tasks. Since we aim for predictions with higher correlation with human ratings, instead of classifying images to low/high score or regressing to the mean score, the distribution of ratings are predicted as a histogram. To this end, we use the squared EMD (earth mover's distance) loss proposed in <ref type="bibr" target="#b20">[21]</ref>, which shows a performance boost in classification with ordered classes. Our experiments show that this approach also leads to more accurate prediction of the mean score. Also, as shown in aesthetic assessment case <ref type="bibr" target="#b0">[1]</ref>, non-conventionality of images is directly related to score standard deviations. Our proposed paradigm allows for predicting this metric as well.</p><p>It has recently been shown that perceptual quality predictors can be used as learning loss to train image enhancement models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Similarly, image quality predictors can be used to adjust parameters of enhancement techniques <ref type="bibr" target="#b23">[24]</ref>. In this work we use our quality assessment technique to effectively tune parameters of image denoising and tone enhancement operators to produce perceptually superior results.</p><p>This paper begins with reviewing three widely used datasets for quality assessment. Then, our proposed method is explained in more detail. Finally, performance of this work is quantified and compared to the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A Large-Scale Database for Aesthetic Visual Analysis (AVA) [1]</head><p>The AVA dataset contains about 255,000 images, rated based on aesthetic qualities by amateur photographers 1 . Each photo is scored by an average of 200 people in response to photography contests. Each image is associated to a single challenge theme, with nearly 900 different contests in the AVA. The image ratings range from 1 to 10, with 10 being the highest aesthetic score associated to an image. Histograms of AVA ratings are shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. As can be seen, mean ratings are concentrated around the overall mean score (≈5.5). Also, ratings of roughly half of the photos in AVA dataset have a standard deviation greater than 1.4. As pointed out in <ref type="bibr" target="#b0">[1]</ref>, presumably images with high score variance tend to be subject to interpretation, whereas images with low score variance seem to represent conventional styles or subject matter. A few examples with ratings associated with different levels of aesthetic quality and unconventionality are illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. It seems that aesthetic quality of a photograph can be represented by the mean score, and unconventionality of it closely correlates to the score deviation. Given the distribution of AVA scores, typically, training a model on AVA data results in predictions with small deviations around the overall mean (5.5).</p><p>It is worth mentioning that the joint histogram in <ref type="figure" target="#fig_1">Fig. 1</ref> shows higher deviations for very low/high ratings (compared to the overall mean 5.5, and mean standard deviation 1.43). In other words, divergence of opinion is more consistent in AVA images with extreme aesthetic qualities. As discussed in <ref type="bibr" target="#b0">[1]</ref>, distribution of ratings with mean value between 2 and 8 can be closely approximated by Gaussian functions, and highly skewed ratings can be modeled by Gamma distributions.</p><p>D. Tampere Image Database 2013 (TID2013) <ref type="bibr" target="#b1">[2]</ref> TID2013 is curated for evaluation of full-reference perceptual image quality. It contains 3000 images, from 25 reference (clean) images (Kodak images <ref type="bibr" target="#b24">[25]</ref>), 24 types of distortions with 5 levels for each distortion. This leads to 120 distorted images for each reference image; including different types of distortions such as compression artifacts, noise, blur and color artifacts.</p><p>Human ratings of TID2013 images are collected through a forced choice experiment, where observers select a better image between two distorted choices. Set up of the experiment allows raters to view the reference image while making a decision. In each experiment, every distorted image is used in 9 random pairwise comparisons. The selected image gets one point, and other image gets zero points. At the end of the experiment, sum of the points is used as the quality score associated with an image (this leads to scores ranging from 0 to 9). To obtain the overall mean scores, total of 985 experiments are carried out.</p><p>Mean and standard deviation of TID2013 ratings are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. As can be seen in <ref type="figure" target="#fig_3">Fig. 3(c)</ref>, the mean and score deviation values are weakly correlated. A few images from TID2013 are illustrated in <ref type="figure">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. LIVE In the Wild Image Quality Challenge Database [26]</head><p>LIVE dataset contains 1162 photos captured by mobile devices. Each image is rated by an average of 175 unique subjects. Mean and standard deviation of LIVE ratings are shown in <ref type="figure">Fig. 6</ref>. As can be seen in the joint histogram, images that are rated near overall mean score show higher standard deviation. A few images from LIVE dataset are illustrated in <ref type="figure">Fig. 7</ref>. It is worth noting that in this paper, LIVE scores are scaled to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Unlike AVA, which includes distribution of ratings for each image, TID2013 and LIVE only provide mean and standard deviation of the opinion scores. Since our proposed method requires training on score probabilities, the score distributions are approximated through maximum entropy optimization <ref type="bibr" target="#b26">[27]</ref>.   The rest of the paper is organized as follows. In Section II, a detailed explanation of the proposed method is described. Next, in SectionIII, applications of our algorithm in ranking photos and image enhancement are exemplified. We also provide details of our implementation. Finally, this paper is concluded in SectionIV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHOD</head><p>Our proposed quality and aesthetic predictor stands on image classifier architectures. More explicitly, we explore a few different classifier architectures such as VGG16 <ref type="bibr" target="#b17">[18]</ref>, Inception-v2 <ref type="bibr" target="#b29">[28]</ref>, and MobileNet <ref type="bibr" target="#b30">[29]</ref> for image quality assessment task. VGG16 consists of 13 convolutional and 3 fully-connected layers. Small convolution filters of size 3 × 3 are used in the deep VGG16 architecture <ref type="bibr" target="#b17">[18]</ref>. Inception-v2 <ref type="bibr" target="#b29">[28]</ref> is based on the Inception module <ref type="bibr" target="#b31">[30]</ref> which allows for parallel use of convolution and pooling operations. Also, in the Inception architecture, traditional fully-connected layers are replaced by average pooling, which leads to a significant reduction in number of parameters. MobileNet <ref type="bibr" target="#b30">[29]</ref> is an efficient deep CNN, mainly designed for mobile vision applications. In this architecture, dense convolutional filters are replaced by separable filters. This simplification results in smaller and faster CNN models.</p><p>We replaced the last layer of the baseline CNN with a fully-connected layer with 10 neurons followed by soft-max activations (shown in <ref type="figure" target="#fig_6">Fig. 8</ref>). Baseline CNN weights are initialized by training on the ImageNet dataset <ref type="bibr" target="#b14">[15]</ref>, and then an end-to-end training on quality assessment is performed. In this paper, we discuss performance of the proposed model with various baseline CNNs.    <ref type="figure">Fig. 7</ref>: Some example images from LIVE dataset <ref type="bibr" target="#b25">[26]</ref> with quality score µ(±σ), where µ and σ represent mean and standard deviation of score, respectively. Note that LIVE scores are scaled to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.  <ref type="bibr" target="#b14">[15]</ref>, and the added fully-connected weights are initialized randomly.</p><p>In training, input images are rescaled to 256 × 256, and then a crop of size 224 × 224 is randomly extracted. This lessens potential over-fitting issues, especially when training on relatively small datasets (e.g. TID2013). It is worth noting that we also tried training with random crops without rescal-ing. However, results were not compelling. This is due to the inevitable change in image composition. Another random data augmentation in our training process is horizontal flipping of the image crops.</p><p>Our goal is to predict the distribution of ratings for a given image. Ground truth distribution of human ratings of a given image can be expressed as an empirical probability mass function p = [p s1 , . . . , p s N ] with s 1 ≤ s i ≤ s N , where s i denotes the ith score bucket, and N denotes the total number of score buckets. In both AVA and TID2013 datasets N = 10, in AVA, s 1 = 1 and s N = 10, and in TID s 1 = 0 and s N = 9. Since N i=1 p si = 1, p si represents the probability of a quality score falling in the ith bucket. Given the distribution of ratings as p, mean quality score is defined as µ = N i=1 s i × p si , and standard deviation of the score is computed as σ = ( N i=1 (s i − µ) 2 × p si ) 1/2 . As discussed in the previous section, one can qualitatively compare images by mean and standard deviation of scores.</p><p>Each example in the dataset consists of an image and its ground truth (user) ratings p. Our objective is to find the probability mass function p that is an accurate estimate of p. Next, our training loss function is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Loss Function</head><p>Soft-max cross-entropy is widely used as training loss in classification tasks. This loss can be represented as N i=1 −p si log( p si ) (where p si denotes estimated probability of ith score bucket) to maximize predicted probability of the correct labels. However, in the case of ordered-classes (e.g. aesthetic and quality estimation), cross-entropy loss lacks the inter-class relationships between score buckets. One might argue that ordered-classes can be represented by a real number, and consequently, can be learned through a regression framework. Yet, it has been shown that for ordered classes, the classification frameworks can outperform regression models <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[31]</ref>. Hou et al. <ref type="bibr" target="#b20">[21]</ref> show that training on datasets with intrinsic ordering between classes can benefit from EMDbased losses. These loss functions penalize mis-classifications according to class distances.</p><p>For image quality ratings, classes are inherently ordered as s 1 &lt; · · · &lt; s N , and r−norm distance between classes is defined as s i − s j r , where 1 ≤ i, j ≤ N . EMD is defined as the minimum cost to move the mass of one distribution to another. Given the ground truth and estimated probability mass functions p and p, with N ordered classes of distance s i − s j r , the normalized Earth Mover's Distance can be expressed as <ref type="bibr" target="#b33">[32]</ref>:</p><formula xml:id="formula_0">EMD(p, p) = 1 N N k=1 |CDF p (k) − CDF p (k)| r 1/r<label>(1)</label></formula><p>where CDF p (k) is the cumulative distribution function as k i=1 p si . It is worth noting that this closed-form solution requires both distributions to have equal mass as N i=1 p si = N i=1 p si . As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, our predicted quality probabilities are fed to a soft-max function to guarantee that N i=1 p si = 1. Similar to <ref type="bibr" target="#b20">[21]</ref>, in our training framework, r is set as 2 to penalize the Euclidean distance between the CDFs. r = 2 allows easier optimization when working with gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>We train two separate models for aesthetics and technical quality assessment on AVA, TID2013, and LIVE. For each case, we split each dataset into train and test sets, such that 20% of the data is used for testing. In this section, performance of the proposed models on the test sets are discussed and compared to the existing methods. Then, applications of the proposed technique in photo ranking and image enhancement are explored. Before moving forward, details of our implementation are explained.</p><p>The CNNs presented in this paper are implemented using TensorFlow <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref>. The baseline CNN weights are initialized by training on ImageNet <ref type="bibr" target="#b14">[15]</ref>, and the last fully-connected layer is randomly initialized. The weight and bias momentums are set to 0.9, and a dropout rate of 0.75 is applied on the last layer of the baseline network. The learning rate of the baseline CNN layers and the last fully-connected layers are set as 3 × 10 −7 and 3 × 10 −6 , respectively. We observed that setting a low learning rate on baseline CNN layers results in easier and faster optimization when using stochastic gradient descent. Also, after every 10 epochs of training, an exponential decay with decay factor 0.95 is applied to all learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Comparisons</head><p>Accuracy, correlation and EMD values of our evaluations on the aesthetic assessment model on AVA are presented in <ref type="table" target="#tab_2">Table I</ref>. Most methods in <ref type="table" target="#tab_2">Table I</ref> are designed to perform binary classification on the aesthetic scores, and as a result, only accuracy evaluations of two-class quality categorization are reported. In this binary classification, predicted mean scores are compared to 5 as cut-off score. Images with predicted scores above the cut-off score are categorized as high quality. In two-class aesthetic categorization task, results from <ref type="bibr" target="#b19">[20]</ref>, and NIMA(Inception-v2) show the highest accuracy. Also, in terms of rank correlation, NIMA(VGG16) and NIMA(Inception-v2) outperform <ref type="bibr" target="#b13">[14]</ref>. NIMA is much cheaper: <ref type="bibr" target="#b19">[20]</ref> applies multiple VGG16 nets on image patches to generate a single quality score, whereas computational complexity of NIMA(Inception-v2) is roughly one pass of Inception-v2 (see <ref type="table" target="#tab_10">Table V)</ref>.</p><p>Our technical quality assessment model on TID2013 is compared to other existing methods in <ref type="table" target="#tab_2">Table II</ref>. While most of these methods regress to the mean opinion score, our proposed technique predicts the distribution of ratings, as well as mean opinion score. Correlation between ground truth and results of NIMA(VGG16) are close to the state-of-the-art results in <ref type="bibr" target="#b36">[35]</ref> and <ref type="bibr" target="#b6">[7]</ref>. It is worth highlighting that Bianco et al. <ref type="bibr" target="#b6">[7]</ref> feed multiple image crops to a deep CNN, whereas our method takes only the rescaled image.</p><p>The predicted distributions of AVA scores are presented in <ref type="figure">Fig. 9</ref>. We used NIMA(Inception-v2) model to predict the ground truth scores from our AVA test set. As can be seen, distribution of the ground truth mean scores is closely predicted by NIMA. However, predicting distribution of the ground truth standard deviations is a more challenging task. As we discussed previously, unconventionality of subject matter or style has a direct impact on score standard deviations.  <ref type="bibr" target="#b0">[1]</ref> compared to the state-ofthe-art. Reported accuracy values are based on classification of photos to two classes (column 2). LCC (linear correlation coefficient) and SRCC (Spearman's rank correlation coefficient) are computed between predicted and ground truth mean scores (column 3 and 4) and standard deviation of scores (column 5 and 6). EMD measures closeness of the predicted and ground truth rating distributions with r = 1 in Eq. 1. The accuracy, LCC, and SROC values are in ±0.3, ±0.005, and ±0.004 within 95% confidence, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy LCC SRCC LCC SRCC EMD (2 classes) (mean) (mean) (std.dev) (std.dev)</p><p>Murray et al. <ref type="bibr" target="#b0">[1]</ref> 66.70% -----Kao et al. <ref type="bibr" target="#b8">[9]</ref> 71.42% -----Lu et al. <ref type="bibr" target="#b37">[36]</ref> 74.46% -----Lu et al. <ref type="bibr" target="#b16">[17]</ref> 75.42% -----Kao et al. <ref type="bibr" target="#b38">[37]</ref> 76.58% -----Wang et al. <ref type="bibr" target="#b39">[38]</ref> 76.80% -----Mai et al. <ref type="bibr" target="#b9">[10]</ref> 77.10% -----Kong et al. <ref type="bibr" target="#b13">[14]</ref> 77.33% -0.558 ---Ma et al. <ref type="bibr" target="#b19">[20]</ref> 81.  <ref type="figure">Fig. 9</ref>: Histograms of the ground truth and predicted scores using NIMA(Inception-v2) applied on our AVA test set. Left: histograms of mean scores. Right: histograms of standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross Dataset Evaluation</head><p>As a cross validation test, performance of our trained models are measured on other datasets. These results are presented in <ref type="table" target="#tab_2">Table III and Table IV</ref>. We test NIMA(Inception-v2) model trained on AVA, TID2013 <ref type="bibr" target="#b1">[2]</ref> and LIVE <ref type="bibr" target="#b25">[26]</ref> across all three test sets. As can be seen, on average, training on AVA dataset shows the best performance. For instance, training on AVA and testing on LIVE results in 0.552 and 0.543 linear and rank correlations, respectively. However, training on LIVE and testing on AVA leads to 0.238 and 0.2 linear and rank correlation coefficients. We believe this observation shows that NIMA models trained on AVA can generalize to other test examples more effectively, whereas training on TID2013 results in poor performance on LIVE and AVA test sets. It is worth mentioning that AVA dataset contains roughly 250 times more examples (in comparison to the LIVE dataset), which allows training NIMA models without any significant overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Photo Ranking</head><p>Predicted mean scores can be used to rank photos, aesthetically. Some test photos from AVA dataset are ranked in <ref type="figure" target="#fig_1">Fig. 10</ref> and <ref type="figure" target="#fig_1">Fig. 11</ref>. Predicted NIMA scores and ground truth AVA scores are shown below each image. Results in <ref type="figure" target="#fig_1">Fig. 10</ref> suggest that in addition to image content, other factors such as tone, contrast and composition of photos are important aesthetic qualities. Also, as shown in <ref type="figure" target="#fig_1">Fig. 11</ref>, besides image semantics, framing and color palette are key qualities in these photos. These aesthetic attributes are closely predicted by our trained models on AVA.</p><p>Predicted mean scores are used to qualitatively rank photos in <ref type="figure" target="#fig_1">Fig. 12</ref>. These images are part of our TID2013 test set, which contain various types and levels of distortions. Comparing ground truth and predicted scores indicates that our trained model on TID2013 accurately ranks the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Enhancement</head><p>Quality and aesthetic scores can be used to perceptually tune image enhancement operators. In other words, maximizing NIMA score as a prior can increase the likelihood of enhancing perceptual quality of an image. Typically, parameters of enhancement operators such as image denoising and contrast enhancement are selected by extensive experiments under various photographic conditions. Perceptual tuning could be quite expensive and time consuming, especially when human opinion is required. In this section, our proposed models are (a) <ref type="bibr">6.38 (7.16)</ref> (b) 6.24 (6.79) (c) <ref type="bibr" target="#b5">6</ref>  Kim et al. <ref type="bibr" target="#b15">[16]</ref> 0.80 0.80 ---Moorthy et al. <ref type="bibr" target="#b40">[39]</ref> 0.89 0.88 ---Mittal et al. <ref type="bibr" target="#b41">[40]</ref> 0.92 0.89 ---Saad et al. <ref type="bibr" target="#b42">[41]</ref> 0.91 0.88 ---Kottayil et al. <ref type="bibr" target="#b43">[42]</ref> 0.89 0.88 ---Xu et al. <ref type="bibr" target="#b36">[35]</ref> 0.96 0.95 ---Bianco et al. <ref type="bibr" target="#b6">[7]</ref> 0 used to tune a tone enhancement method <ref type="bibr" target="#b44">[43]</ref>, and an image denoiser <ref type="bibr" target="#b45">[44]</ref>. A more detailed treatment is presented in <ref type="bibr" target="#b22">[23]</ref>.</p><p>The multi-layer Laplacian technique <ref type="bibr" target="#b44">[43]</ref> enhances local and global contrast of images. Parameters of this method control the amount of detail, shadow, and brightness of an image. <ref type="figure" target="#fig_1">Fig. 13</ref> shows a few examples of the multi-layer Laplacian with different sets of parameters. We observed that the predicted aesthetic ratings from training on the AVA dataset can be improved by contrast adjustments. Consequently, our model is able to guide the multi-layer Laplacian filter to find aesthetically near-optimal settings of its parameters. Examples of this type of image editing are represented in <ref type="figure" target="#fig_1">Fig. 14,</ref> where a combination of detail, shadow and brightness change is applied on each image. In each example, 6 levels of detail boost, 11 levels of shadow change, and 11 levels of brightness change account for a total of 726 variations. The aesthetic assessment model tends to prefer high contrast images with boosted details. This is consistent with the ground truth results from AVA illustrated in <ref type="figure" target="#fig_1">Fig. 10</ref>.</p><p>Turbo denoising <ref type="bibr" target="#b45">[44]</ref> is a technique which uses the domain transform <ref type="bibr" target="#b46">[45]</ref> as its core filter. Performance of Turbo denoising depends on spatial and range smoothing parameters, and consequently, proper tuning of these parameters can effectively boost performance of the denoiser. We observed that varying the spatial smoothing parameter makes the most significant perceptual difference, and as a result, we use our quality assessment model trained on TID2013 dataset to tune this denoiser. Application of our no-reference quality metric as a prior in image denoising is similar to the work of Zhu et al. <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>. Our results are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. Additive white Gaussian noise with standard deviation 30 is added to the clean image, and Turbo denoising with various spatial parameters is used to denoise the noisy image. To reduce the score deviation, 50 random crops are extracted from denoised image. These scores are averaged to obtain the plots illustrated in <ref type="figure" target="#fig_0">Fig. 15</ref>. As can be seen, although the same amount of noise is added to each image, maximum quality scores correspond to different denoising parameters in each example. For relatively smooth images such as (a) and (g), optimal spatial parameter of Turbo (a) <ref type="bibr" target="#b5">6</ref>.88 <ref type="bibr">(7.40)</ref> (b) <ref type="bibr" target="#b5">6</ref>.63 (6.89) (c) <ref type="bibr" target="#b5">6</ref>   denoising is higher (which implies stronger smoothing) than the textured image in (j). This is probably due to the relatively high signal-to-noise ratio of (j). In other words, the quality assessment model tends to respect textures and avoid oversmoothing of details. Effect of the denoising parameter can be visually inspected in <ref type="figure" target="#fig_1">Fig. 16</ref>. While the denoised result in <ref type="figure" target="#fig_1">Fig. 16 (a)</ref> is under-smoothed, (c), (e) and (f) show undesirable over-smoothing effects. The predicted quality scores validate this perceptual observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Costs</head><p>Computational complexity of NIMA models are compared in <ref type="table" target="#tab_10">Table V</ref>. Our inference TensorFlow implementation is tested on an Intel Xeon CPU @ 3.5 GHz with 32 GB memory and 12 cores, and NVIDIA Quadro K620 GPU. Timings of one pass of NIMA models on an image of size 224 × 224 × 3 are reported in <ref type="table" target="#tab_10">Table V</ref>. Evidently, NIMA(MobileNet) is significantly lighter and faster than other models. This comes at the expense of a slight performance drop (shown in <ref type="table" target="#tab_2">Table I</ref> and <ref type="table" target="#tab_2">Table II</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work we introduced a CNN-based image assessment method, which can be trained on both aesthetic and pixel-level quality datasets. Our models effectively predict the distribution of quality ratings, rather than just the mean scores. This leads to a more accurate quality prediction with higher correlation to the ground truth ratings. We trained two models for high level aesthetics and low level technical qualities, and utilized them to steer parameters of a few image enhancement operators. Our experiments suggest that these models are capable of guiding denoising and tone enhancement to produce perceptually superior results.</p><p>As part of our future work, we will exploit the trained models on other image enhancement applications. Our current experimental setup requires the enhancement operator to be evaluated multiple times. This limits real-time application of the proposed method. One might argue that in case of an enhancement operator with well-defined derivatives, using NIMA as the loss function is a more efficient approach.   <ref type="figure" target="#fig_0">Fig. 15</ref>: Tuning spatial parameter of Turbo denoising <ref type="bibr" target="#b45">[44]</ref> by using our proposed quality assessment model NIMA(VGG16). Standard deviation of the additive white Gaussian noise is set as 30. Denoised results are shown for maximum quality score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>and Fig. 5 .</head><label>5</label><figDesc>All five levels of JPEG compression artifacts and the respective ratings are illustrated inFig. 4. Evidently higher distortion level leads to lower mean score 2 . Effect of contrast compression/stretching distortion on the human ratings is demonstrated inFig. 5. Interestingly, stretch of contrast (Fig. 5(c) andFig. 5(e)) leads to relatively higher perceptual quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Histograms of ratings from AVA dataset<ref type="bibr" target="#b0">[1]</ref>. Left: Histogram of mean scores. Middle: Histogram of standard deviations. Right: Joint histogram of the mean and standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>(a) 6.36 (±1.04) (b)<ref type="bibr" target="#b6">7</ref>.84 (±2.08) (c) 2.62 (±2.15) (d) 3.12 (±1.28) Some example images from AVA dataset<ref type="bibr" target="#b0">[1]</ref> with quality score µ(±σ), where µ and σ represent mean and standard deviation of score, respectively.(a) high aesthetics and low unconventionality (challenge name: "Best of 2007", µ = 6.36, σ = 1.04), (b) high aesthetics and high unconventionality (challenge name: "Extreme super moon", µ = 7.84, σ = 2.08), (c) low aesthetics and high unconventionality (challenge name: "Travel", µ = 2.62, σ = 2.15), (d) low aesthetics and low unconventionality (challenge name: "Pieces of the human form", µ = 3.12, σ = 1.28).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Histograms of ratings from TID2013 dataset<ref type="bibr" target="#b1">[2]</ref>. Left: Histogram of mean scores. Middle: Histogram of standard deviations. Right: Joint histogram of the mean and standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>(a) clean image (b)<ref type="bibr" target="#b4">5</ref>.73 (±0.15) (c)5.47 (±0.11)    (d)<ref type="bibr" target="#b3">4</ref>.86 (±0.11) (e)<ref type="bibr" target="#b2">3</ref>.0 (±0.11) (f) 1.66 (±0.16) JPEG artifact example images from TID2013 dataset<ref type="bibr" target="#b1">[2]</ref> with quality score µ(±σ), where µ and σ represent mean and standard deviation of score, respectively. Clean image and 5 levels of JPEG compression artifacts are shown here.(a) clean image, (b) compression artifact level 1, µ = 5.73, σ = 0.15, (c) compression artifact level 2, µ = 5.47, σ = 0.11, (d) compression artifact level 3, µ = 4.86, σ = 0.11, (e) compression artifact level 4, µ = 3.0, σ = 0.11, (f) compression artifact level 5, µ = 1.66, σ = 0.16.(a) clean image (b) 5.67 (±0.10) (c) 6.80 (±0.18) (d) 4.83 (±0.16) (e) 6.69 (±0.29) (f) 3.88 (±0.18) Some example images from TID2013 dataset [2] with quality score µ(±σ), where µ and σ represent mean and standard deviation of score, respectively. Clean image and 5 levels of contrast change distortions are shown here. (a) clean image, (b) contrast change distortion of level 1, µ = 5.67, σ = 0.10, (c) contrast change distortion of level 2, µ = 6.80, σ = 0.18, (d) contrast change distortion of level 3, µ = 4.83, σ = 0.16, (e) contrast change distortion of level 4, µ = 6.69, σ = 0.29, (f) contrast change distortion of level 5, µ = 3.88, σ = 0.18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc>Fig. 6: Histograms of ratings from LIVE dataset [26]. Left: Histogram of mean scores. Middle: Histogram of standard deviations. Right: Joint histogram of the mean and standard deviation. Note that LIVE scores are scaled to [1,10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Modified baseline image classifier network used in our framework. Last layer of classifier network is replaced by a fully-connected layer to output 10 classes of quality scores. Baseline network weights are initialized by training on ImageNet dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>Predicted aesthetic score (NIMA(VGG16)) for various parameter settings of multi-layer Laplacian technique<ref type="bibr" target="#b44">[43]</ref>. Predicted aesthetic scores are shown below each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. 6: Histograms of ratings from LIVE dataset<ref type="bibr" target="#b25">[26]</ref>. Left: Histogram of mean scores. Middle: Histogram of standard deviations. Right: Joint histogram of the mean and standard deviation. Note that LIVE scores are scaled to<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.</figDesc><table /><note>(a) 9.99 (±1.22) (b) 9.35 (±1.49) (c) 8.29 (±1.99) (d) 3.50 (±1.69) (e) 2.33 (±1.51) (f) 1.95 (±1.39)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Performance of the proposed method with various architectures in predicting AVA quality ratings</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II</head><label>II</label><figDesc>EMD measures closeness of the predicted and ground truth rating distributions with r = 1 in Eq. 1. The LCC, and SROC values are in ±0.005, and ±0.004 within 95% confidence, respectively.</figDesc><table><row><cell></cell><cell cols="5">: Performance of the proposed method with vari-</cell></row><row><cell cols="6">ous architectures in predicting TID2013 quality ratings [2]</cell></row><row><cell cols="6">compared to the state-of-the-art. LCC (linear correlation co-</cell></row><row><cell cols="6">efficient) and SRCC (Spearman's rank correlation coefficient)</cell></row><row><cell cols="6">are computed between predicted and ground truth mean scores</cell></row><row><cell cols="6">(column 2 and 3) and standard deviation of scores (column 4</cell></row><row><cell>and 5). Model</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>EMD</cell></row><row><cell></cell><cell cols="4">(mean) (mean) (std.dev) (std.dev)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III :</head><label>III</label><figDesc>LCC (linear correlation coefficient) of NIMA(Inception-v2) model for training and testing on various datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Test Dataset</cell><cell></cell><cell></cell></row><row><cell cols="5">Train Dataset LIVE [26] TID2013 [2] AVA [1] Average</cell></row><row><cell>LIVE [26]</cell><cell>0.698</cell><cell>0.537</cell><cell>0.238</cell><cell>0.491</cell></row><row><cell>TID2013 [2]</cell><cell>0.178</cell><cell>0.827</cell><cell>0.101</cell><cell>0.369</cell></row><row><cell>AVA [1]</cell><cell>0.552</cell><cell>0.514</cell><cell>0.636</cell><cell>0.567</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">: SRCC (Spearman's rank correlation coefficient)</cell></row><row><cell cols="5">of NIMA(Inception-v2) model for training and testing on</cell></row><row><cell>various datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Test Dataset</cell><cell></cell><cell></cell></row><row><cell cols="5">Train Dataset LIVE [26] TID2013 [2] AVA [1] Average</cell></row><row><cell>LIVE [26]</cell><cell>0.637</cell><cell>0.327</cell><cell>0.200</cell><cell>0.388</cell></row><row><cell>TID2013 [2]</cell><cell>0.155</cell><cell>0.750</cell><cell>0.087</cell><cell>0.331</cell></row><row><cell>AVA [1]</cell><cell>0.543</cell><cell>0.432</cell><cell>0.612</cell><cell>0.529</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V :</head><label>V</label><figDesc>Comparison of the proposed quality assessment technique with various CNN architectures. Average timings are reported in ms for Xeon Intel CPU @ 3.5 GHz, and NVIDIA Quadro K620 GPU. Timings are reported for applying NIMA models on images of size 224 × 224 × 3.</figDesc><table><row><cell>(a) Noisy Input</cell><cell></cell><cell cols="3">(b) Optimized (denoising parameter=3.75)</cell><cell cols="2">(c) Quality score vs. denoising parameter</cell></row><row><cell>(d) Noisy Input</cell><cell></cell><cell cols="3">(e) Optimized (denoising parameter=2.25)</cell><cell cols="2">(f) Quality score vs. denoising parameter</cell></row><row><cell>(g) Noisy Input</cell><cell></cell><cell cols="3">(h) Optimized (denoising parameter=4.50)</cell><cell cols="2">(i) Quality score vs. denoising parameter</cell></row><row><cell>(j) Noisy Input</cell><cell></cell><cell cols="3">(k) Optimized (denoising parameter=1.25)</cell><cell cols="2">(l) Quality score vs. denoising parameter</cell></row><row><cell></cell><cell></cell><cell>Million</cell><cell cols="2">Billion CPU</cell><cell>GPU</cell></row><row><cell></cell><cell>Model</cell><cell cols="2">Parameters Flops</cell><cell cols="2">Timing (ms) Timing (ms)</cell><cell>farbspiel-</cell></row><row><cell>photo.com)</cell><cell>NIMA(MobileNet)</cell><cell>3.22</cell><cell>1.29</cell><cell>30.45</cell><cell>20.23</cell></row><row><cell></cell><cell cols="2">NIMA(Inception-v2) 10.16</cell><cell>4.37</cell><cell>70.49</cell><cell>39.11</cell></row><row><cell></cell><cell>NIMA(VGG16)</cell><cell>134.30</cell><cell>31.62</cell><cell>150.34</cell><cell>85.76</cell></row></table><note>Fig. 14: Tone enhancement by multi-layer Laplacian technique [43] along with our proposed aesthetic assessment model NIMA(VGG16). Predicted aesthetic scores are shown below each image. (Input photos are downloaded from www.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">AVA images are obtained from www.dpchallenge.com, which is an on-line community for amateur photographers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is a quite consistent trend for most of the other distortions too (namely noise, blur and color distortions). However, in case of the contrast change (Fig. 5), this trend is not obvious. This is due to the order of contrast compression/stretching from level 1 to level 5)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Dr. Pascal Getreuer for valuable discussions and helpful advice on approximation of score distributions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Color image database TID2013: Peculiarities and preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Information Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>4th European Workshop on. 1, 2, 3, 4, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep neural network for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3773" to="3777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the use of deep learning for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05531</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual aesthetic quality assessment with a regression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image aesthetic predictors based on weighted CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V O</forename><surname>Segovia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2291" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Live image quality assessment database release 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Most apparent distortion: fullreference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>1, 2, 6, 7</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rating image aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A probabilistic quality representation approach to deep blind image quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08190</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A-lamp: Adaptive layout-aware multipatch deep convolutional neural network for photo aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Squared earth mover&apos;s distancebased loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05916</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learned perceptual image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The analysis of image contrast: From quality assessment to automatic enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="284" to="297" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kodak lossless true color image suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kodak</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">John (a) denoising parameter=1.25, score=5.06 (b) denoising parameter=3.0, score=5.15 (c) denoising parameter=9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<idno>score=4.76</idno>
		<imprint>
			<biblScope unit="volume">75</biblScope>
		</imprint>
	</monogr>
	<note>Elements of information theory</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Effect of Turbo denoising [44] on our predicted quality scores. Input noisy images are shown in Fig</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fig</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-entropy vs. squared error training: a theoretical and experimental comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1756" to="1760" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance is the Mallows&apos; distance: Some insights from statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visual aesthetic quality assessment with multi-task deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04970</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dolcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04155</idno>
		<title level="m">Braininspired deep networks for image aesthetics assessment</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A color intensity invariant low-level feature optimization framework for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kottayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast multilayer Laplacian enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Turbo denoising for mobile photographic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Domain transform for edge-aware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A no-reference sharpness metric sensitive to blur and noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic parameter selection for denoising algorithms using a no-reference measure of image content</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3116" to="3132" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
