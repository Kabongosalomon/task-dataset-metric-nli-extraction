<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-term Temporal Convolutions for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
						</author>
						<title level="a" type="main">Long-term Temporal Convolutions for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action recognition</term>
					<term>video analysis</term>
					<term>representation learning</term>
					<term>spatio-temporal convolutions</term>
					<term>neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMAN actions and events can be seen as spatiotemporal objects. Such a view finds support both in psychology <ref type="bibr" target="#b0">[1]</ref> and in computer vision approaches to action recognition in video <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Successful methods for action recognition, indeed, share similar techniques with object recognition and represent actions by statistical models of local video descriptors. Differently to objects, however, actions are characterized by the temporal evolution of appearance governed by motion. Consistent with this fact, motion-based video descriptors such as HOF and MBH <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref> as well as recent CNN-based motion representations <ref type="bibr" target="#b5">[6]</ref> have shown most gains for action recognition in practice.</p><p>The recent rise of convolutional neural networks (CNNs) convincingly demonstrates the power of learning visual representations <ref type="bibr" target="#b6">[7]</ref>. Equipped with large-scale training datasets <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, CNNs have quickly taken over the majority of still-image recognition tasks such as object, scene and face recognition <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Extensions of CNNs to action recognition in video have been proposed in several recent works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Such methods, however, currently show only moderate improvements over earlier methods using hand-crafted video features <ref type="bibr" target="#b4">[5]</ref>.</p><p>Current CNN methods for action recognition often extend CNN architectures for static images <ref type="bibr" target="#b6">[7]</ref> and learn action representations for short video intervals ranging from 1 to 16 frames <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Yet, typical human actions such as hand-shaking and drinking, as well as cycles of repetitive actions such as walking and swimming often last several seconds and span tens or hundreds of video frames. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>  structure. Breaking this structure into short clips (see <ref type="figure">Figure</ref> 1(b),(d)) and aggregating video-level information by the simple average of clip scores <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref> or more sophisticated schemes such as LSTMs <ref type="bibr" target="#b13">[14]</ref> is likely to be suboptimal. In this work, we investigate the learning of long-term video representations. We consider space-time convolutional neural networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and study architectures with Long-term Temporal Convolutions (LTC), see <ref type="figure" target="#fig_2">Figure 2</ref>. To keep the complexity of networks tractable, we increase the temporal extent of representations at the cost of decreased spatial resolution. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields. Our experiments confirm the advantage of motion-based representations and highlight the importance of good quality motion estimation for learning efficient representations for human action recognition. We report state-of-the-art performance on two recent and challenging human action benchmarks: UCF101 and HMDB51.</p><p>The contributions of this work are twofold. We demonstrate (i) the advantages of long-term temporal convolutions and (ii) the importance of high-quality optical flow estimation for learning accurate video representations for human action recognition. In the remaining part of the paper we discuss related work in Section 2, describe space-time CNN architectures in Section 3 and present an extensive experimental study of our method in Section 4. Our implementation and pre-trained CNN models (compatible with Torch) are available on the project web page [17].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Action recognition in the last decade has been dominated by local video features <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> aggregated with Bagof-Features histograms <ref type="bibr" target="#b16">[18]</ref> or Fisher Vector representations <ref type="bibr" target="#b17">[19]</ref>. While typical pipelines resemble earlier methods for object recognition, the use of local motion features, in particular Motion Boundary Histograms <ref type="bibr" target="#b4">[5]</ref>, has been found important for action recognition in practice. Explicit representations of the temporal structure of actions have rarely beed used with some exceptions such as the recent work <ref type="bibr" target="#b18">[20]</ref>.</p><p>Learning visual representations with CNNs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b19">[21]</ref> has shown clear advantages over "hand-crafted" features for many recognition tasks in static images <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Extensions of CNN representations to action recognition in video have been proposed in several recent works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b23">[25]</ref>. Some of these methods encode single video frames with static CNN features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Extensions to short video clips where video frames are treated as multi-channel inputs to 2D CNNs have also been investigated in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b23">[25]</ref>.</p><p>Learning CNN representations for action recognition has been addressed for raw pixel inputs and for precomputed optical flow features. Consistent with previous results obtained with hand-crafted representations, motionbased CNNs typically outperform CNN representations learned for RGB inputs <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[23]</ref>. In this work we investigate multi-resolution representations of motion and appearance where for motion-based CNNs we demonstrate the importance of high-quality optical flow estimation. Similar findings have been recently confirmed by <ref type="bibr" target="#b24">[26]</ref>, where the authors transfer knowledge from high quality optical flow algorithms to motion vector encoding representation.</p><p>Most of the current CNN methods use architectures with 2D convolutions, enabling shift-invariant representations in the image plane. Meanwhile, the invariance to translations in time is also important for action recognition since the beginning and the end of actions is unknown in general. CNNs with 3D spatio-temporal convolutions address this issue and provide a natural extension of 2D CNNs to video. 3D CNNs have been investigated for action recognition in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. All of these methods, however, learn video representations for RGB input. Moreover, they typically consider very short video intervals, for example, 16-frame video clips are used in <ref type="bibr" target="#b12">[13]</ref> and 2, 7, 15 frames in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref> respectively. In this work we extend 3D CNNs to significantly longer temporal convolutions that enable action representation at their full temporal scale. We also explore the impact of optical flow input. Both of these extensions show clear advantages in our experimental comparison to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LONG-TERM TEMPORAL CONVOLUTIONS</head><p>In this section we first present the network architecture. We then specify the different inputs to networks used in this work. We finally provide details on learning and testing procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture</head><p>Our network architecture with long-term temporal convolutions is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. The network has 5 space-time convolutional layers with 64, 128, 256, 256 and 256 filter response maps, followed by 3 fully connected layers of sizes 2048, 2048 and the number of classes. Following <ref type="bibr" target="#b12">[13]</ref> we use 3 × 3 × 3 space-time filters for all convolutional layers. Each convolutional layer is followed by a rectified linear unit (ReLU) and a space-time max pooling layer. Max pooling filters are of size 2 × 2 × 2 except in the first layer, where it is 2 × 2 × 1. The size of convolution output is kept constant by padding 1 pixel in all three dimensions. Filter stride for all dimensions is 1 for convolution and 2 for pooling operations. We use dropout for the first two fully connected layers. Fully connected layers are followed by ReLU layers. Softmax layer at the end of the network outputs class scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network input</head><p>To investigate the impact of long-term temporal convolutions, we here study network inputs with different temporal <ref type="bibr">RGB</ref> MPEG flow <ref type="bibr" target="#b25">[27]</ref> Farneback <ref type="bibr" target="#b26">[28]</ref> Brox <ref type="bibr" target="#b27">[29]</ref> Input  extents. We depart from the recent C3D work <ref type="bibr" target="#b12">[13]</ref> and first compare inputs of 16 frames (16f) and 60 frames (60f). We then systematically analyze implications of the increased temporal and spatial resolutions for input signals in terms of motion and appearance. For the 16-frame network we crop input patches of size 112 × 112 × 16 from videos with spatial resolution 171 × 128 pixels. We choose this baseline architecture to enable direct comparison with <ref type="bibr" target="#b12">[13]</ref>.</p><p>For the 60-frames networks we decrease spatial resolution to preserve network complexity and use input patches of size 58 × 58 × 60 randomly cropped from videos rescaled to 89 × 67 spatial resolution. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, the temporal resolution in our 60f network corresponds to 60, 30, 15, 7 and 3 frames for each of the five convolutional layers. In comparison, the temporal resolution of the 16f network is reduced more drastically to 16, 8, 4, 2 and 1 frame at each convolutional layer. We believe that preserving the temporal resolution at higher convolutional layers should enable learning more complex temporal patterns. The space-time resolution for the outputs of the fifth convolutional layers is 3 × 3 × 1 and 1 × 1 × 3 for the 16f and 60f networks respectively. The two networks have a similar number of parameters in the fc6 layer and the same number of parameters in all other layers. For a systematic study of networks with different input resolutions we also evaluate the effect of increased temporal resolution t ∈ {20, 40, 60, 80, 100} and varying spatial resolution of {58 × 58, 71 × 71} pixels.</p><p>In addition to the input size, we experiment with different types of input modalities. First, as in <ref type="bibr" target="#b12">[13]</ref>, we use raw RGB values from video frames as input. To explicitly learn motion representations, we also use flow fields in x and y directions as input to our networks. Flow is computed for original videos. To maintain correct flow values for network inputs with reduced spatial resolution, the magnitude of the flow is scaled by the factor of spatial subsampling. In other words, if a point moves 2 pixels in a 320 × 240 video frame, its motion will be 1 pixel when the frame is resized to 160 × 120 resolution. Moreover, to center the input data, we follow <ref type="bibr" target="#b5">[6]</ref> and subtract the mean flow vector for each frame.</p><p>To investigate the dependency of action recognition on the quality of motion estimation, we experiment with three types of flow inputs obtained either directly from the video encoding, referred to as MPEG flow <ref type="bibr" target="#b25">[27]</ref>, or from two optical flow estimators, namely Farneback <ref type="bibr" target="#b26">[28]</ref> and Brox <ref type="bibr" target="#b27">[29]</ref>. <ref type="figure" target="#fig_3">Figure 3</ref> shows results for the three flow algorithms. MPEG flow is a fast substitute for optical flow which we obtain from the original video encoding. Such flow, however, has low spatial resolution. It also misses flow vectors at some frames (I-frames) which we interpolate from neighboring frames. Farneback flow is also relatively fast and obtains rather noisy flow estimates. The approach of Brox flow is the most sophisticated of the three and is known to perform well in various flow estimation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>We train our networks on the training set of each split independently for both UCF101 and HMDB51 datasets, which contain 9.5K and 3.7K videos, respectively. We use stochastic gradient descent applied to mini-batches with negative log likelihood criterion. For 16f networks we use a mini-batch size of 30 video clips. We reduce the batch size to 15 video clips for 60f networks, and 10 clips for 100f networks due to limitations of our GPUs. The initial learning rate for networks learned from scratch is 3 × 10 −3 and 3 × 10 −4 for networks fine-tuned from pre-trained models. For UCF101, the learning rate is decreased twice with a factor of 10 −1 . For 16f networks, the first decrease is after 80K iterations and the second one after 45K additional iterations. The optimization is completed after 20K more iterations. Convergence is faster for HMDB51, so the learning rate is decreased once after 60K iterations and completed after 10K more iterations. These numbers are doubled for 60f networks and tripled for 100f networks, since their batch sizes are twice and three times smaller compared to 16f nets. The above schedule is used together with 0.9 dropout ratio. Our experimental setups with 0.5 dropout ratio have less iterations due to faster convergence. The momentum is set to 0.9 and weight decay is initialized with 5 × 10 −3 and reduced by a factor of 10 −1 at every decrease of the learning rate.</p><p>Inspired by the random spatial cropping during training, we apply the corresponding augmentation to the temporal dimension as in <ref type="bibr" target="#b5">[6]</ref>, which we call random clipping. During training, given an input video, we randomly select a point (x, y, t) to sample a video clip of fixed size. A common alternative is to preprocess the data by using a sliding window approach to have pre-segmented clips of fixed size; however, this approach limits the amount of data when the windows are not overlapped as in <ref type="bibr" target="#b12">[13]</ref>. Another data augmentation method that we evaluate is to have a multiscale cropping similar to <ref type="bibr" target="#b21">[23]</ref>. For this, we randomly select a coefficient for width and height separately from (1.0, 0.875, 0.75, 0.66) and resize the cropped region to the size of the network input. Finally, we horizontally flip the input with 50% probability.</p><p>At test time, a video is divided into t-frame clips with a temporal stride of 4 frames. Each clip is further tested with 10 crops, namely the 4 corners and the center, together with their horizontal flips. The video score is obtained by averaging over clip scores and crop scores. If the number of frames in a video is less than the clip size, we pad the input by repeating the last frames to fill the missing volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We perform experiments on two widely used and challenging benchmarks for action recognition: UCF101 and HMDB51 (Sec. 4.1). We first examine the effect of network parameters (Sec. 4.2). We then compare to the state-of-the-art (Sec. 4.3) and present a visual analysis of the spatio-temporal filters (Sec. 4.4). Finally we report runtime analysis (Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation metrics</head><p>UCF101 <ref type="bibr" target="#b28">[30]</ref> is a widely-used benchmark for action recognition with 13K clips from YouTube videos lasting 7 seconds on average. The total number of frames is 2.4M distributed among 101 categories. The videos have spatial resolution of 320 × 240 pixels and 25 fps frame rate.</p><p>The HMDB51 dataset <ref type="bibr" target="#b29">[31]</ref> consists of 7K videos of 51 actions. The videos have 320 × 240 pixels spatial resolution and 30 fps frame rate. Although this dataset has been considered a large-scale benchmark for action recognition for the past few years, the amount of data for learning deep networks is limited.</p><p>We rely on two evaluation metrics. The first one measures per-clip accuracy, i.e. we assign each clip the class label with the maximum softmax output and measure the number of correctly assigned labels over all clips. The second metric measures video accuracy, i.e. the standard evaluation protocol. To obtain a video score we average the per-clip softmax scores and take the maximum value of this average as class label. We average over all videos to obtain video accuracy. We report our final results according to the standard evaluation protocol, which is the mean video accuracy across the three test splits. To evaluate the network parameters we use the first split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of LTC network parameters</head><p>In the following we first examine the impact of optical flow and data augmentation. We then evaluate gains provided by long-term temporal convolutions for the best flow and data augmentation techniques by comparing 16f and 60f networks. We also investigate the advantage of pre-training on one dataset (UCF101) and fine-tuning on a smaller dataset (HMDB51). Furthermore, we study the effect of systematically increased temporal resolution for flow and RGB inputs as well as the combination of networks.</p><p>Optical flow. The impact of the flow quality on action recognition and a comparison to RGB is shown in <ref type="figure" target="#fig_3">Figure 3</ref> for UCF101 (split 1). The network is trained from scratch and with a 60-frame video volume as input. We first observe that even the low-quality MPEG flow outperforms RGB. The increased quality of optical flow leads to further improvements. The use of Brox flow allows nearly 20% increase in performance. The improvements are consistent when classifying individual clips and full videos. This suggests that action recognition is easier to learn from motion compared to raw pixel values. While results in <ref type="figure" target="#fig_3">Figure 3</ref> were obtained for 60f networks, the same holds for 16f networks (see <ref type="table">Table 2</ref>). We also conclude that the high accuracy of optical flow estimation plays an important role for learning competitive video representations for action recognition. Given the results in <ref type="figure" target="#fig_3">Figure 3</ref>, we choose Brox flow for all remaining experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Clip accuracy Video accuracy give an improvement when used alone, the best performance is obtained when combined.</p><p>Data augmentation. <ref type="table" target="#tab_1">Table 1</ref> demonstrates the contribution of data augmentation when training a large CNN with limited amount of data. Our baseline uses sliding window clips with 75% overlap and a dropout of 0.5 during training. We gain 3.1% with random clipping, 1.6% with multiscale cropping and 2% with higher dropout ratio. When combined, the data augmentation and a higher dropout results in a 4% gain for video classification on UCF101 split 1. High dropout, multiscale cropping and random clipping are used in the remaining experiments, unless stated otherwise.</p><p>Comparison of 16f and 60f networks. Our 16-frame and 60-frame networks have similar complexity in terms of input sizes and the number of network parameters (see Section 3). Moreover, the 16-frame network resembles the C3D architecture and enables direct comparison with <ref type="bibr" target="#b12">[13]</ref>.</p><p>We therefore study the gains provided by the 60-frame inputs before analyzing performance with systematically increasing temporal resolution (from 20 to 100 frames by steps of 20) in the next paragraph. <ref type="table">Table 2</ref> compares the performance of 16f and 60f networks for RGB and flow inputs as well as for different data augmentation and dropout ratios for UCF101 split 1. We observe consistent and significant improvement of longterm temporal convolutions in 60f networks for all tested setups, when measured in terms of clip and video accuracies. Our 60f architecture significantly improves for both RGB and flow-based networks. As expected, the improvement is more prominent for clips since video evaluation aggregates information over the whole video.</p><p>We repeat similar experiments for the split 1 of HMDB51   and report results in <ref type="table">Table 3</ref>. Similar to UCF101, flow-based networks with long-term temporal convolutions lead to significant improvements over the 16f network, in terms of clip and video accuracies. Given the small size of HMDB51, we follow <ref type="bibr" target="#b5">[6]</ref> and also fine-tune networks that have been pretrained on UCF101. As illustrated in the 2nd row of <ref type="table">Table 3</ref>, such pre-training gives significant improvement. Moreover, our 60f flow networks significantly outperform results of the 2D CNN temporal stream ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="table">Table 2</ref>) evaluated in a comparable setup, both with and without pre-training.</p><p>Varying temporal and spatial resolutions. Given the benefits of long-term temporal convolutions above, it is interesting to study networks for increasing temporal extents and varying spatial resolutions systematically. In particular, we investigate if accuracy saturates for networks with larger temporal extents, if higher spatial resolution impacts the performance of long-term temporal convolutions and if LTC is equally beneficial for flow and RGB networks.</p><p>To study these questions, we evaluate networks with increasing temporal extent t ∈ {20, 40, 60, 80, 100} and two spatial resolutions {58 × 58, 71 × 71} for both RGB and flow. We also investigate combining RGB and flow by averaging their class scores. Preliminary experiments with alternative fusion techniques did not improve over such a late fusion.</p><p>Flow networks have our previous architecture as in <ref type="figure" target="#fig_2">Figure 2</ref>, except slightly more connections in fc6 for 71 × 71 resolution. For flow input, we train our networks from scratch. For RGB input, learning appears to be difficult from scratch. Even if we extend the temporal extent from 60 frames (see <ref type="table">Table 2</ref>) to 100 frames, we obtain 68.4% on UCF101 split 1, which is still below frame-based 2D convolution methods fine-tuned from ImageNet pre-training <ref type="bibr" target="#b5">[6]</ref>. Although longer extent boosts the performance significantly, we conclude that one needs to pre-train RGB network on larger data.</p><p>Given the large improvements provided by the pretraining of C3D RGB network on the large-scale Sports-1M dataset in <ref type="bibr" target="#b12">[13]</ref>, we use this 16-frame pre-trained network and extend it to longer temporal convolutions in 2 steps. <ref type="bibr" target="#b0">1</ref> The first step is fine-tuning the 16f C3D network. A randomly initialized fully connected (fc) layer of size 101 (number of classes) is added at the end of the network. Only the fc layers are fine-tuned by freezing the convolutional layers. We start with a learning rate of 3×10 −4 and decrease it to 3 × 10 −5 after 30K iterations for 1K more iterations. In the second step, we input longer clips to the network and fine-tune all the layers. Convolutional layers are applied to longer video clips of t frames. This results in outputs from conv5 layer with t/16 temporal resolution. To re-cycle pretrained fc layers of C3D, we max-pool conv5 outputs over time and pass results to fc6. We use a subset of the fc6 weights for inputs of lower spatial resolution. For this phase, we run for same number of iterations, but we decrease the learning rate from 3 × 10 −5 to 3 × 10 −6 . We keep dropout ratio 0.5 as in the pre-trained network. <ref type="figure" target="#fig_4">Figure 4</ref>(a)(b) illustrates results of networks with varying temporal and spatial resolutions for clips and videos of UCF101, split 1. We observe significant improvements over t for LTC networks using flow (trained from scratch), RGB (with pre-training on Sports-1M), as well as combination of both modalities. Networks with higher spatial resolutions give better results for lower t, however, the gain of increased spatial resolution is lower for networks with long temporal extents. Given the large number of parameters in highresolution networks, such behavior can be explained by the overfitting due to the insufficient amount of training data in UCF101. We believe that larger training sets could lead to further improvements. Moreover, flow benefits more from the averaging of clip scores than RGB. This could be an indication of static RGB information over different time intervals of the video, whereas flow is dynamic. <ref type="figure" target="#fig_4">Figure 4</ref>(c) presents results of LTC for a few action classes demonstrating a variety of accuracy patterns over different temporal extents. Out of all 101 classes, no action has monotonic decrease with the increasing temporal extent, <ref type="bibr" target="#b0">1</ref>. We have also tried to pre-train our flow-based networks on Sports-1M but did not obtain significant improvements.  <ref type="figure">Fig. 5</ref>. The highest improvement of long-term temporal convolutions in terms of class accuracy is for JavelinThrow. For 16-frame network, it is mostly confused with the FloorGymnastics class. Here, we visualize sample videos with 7 frames extracted at every 8 frames. The intuitive explanation is that both classes start by running for a few seconds and then the actual action takes place. LTC can capture this interval, whereas 16-frame networks fail to recognize such long-term activities.</p><p>whereas the performance of 25 action classes increased monotonically. PushUps, YoYo and ShavingBeard are examples of classes with high, medium and low performance that all benefit from larger temporal extents. Shotput is an example of a class with lower performance for longer temporal extents. A possible explanation is that samples of the Shotput class are relatively short and have 90 frames on average (we pad short clips). Two additional examples with a significant gain for larger temporal extents are Floor-Gymnastics and JavelinThrow, see <ref type="figure">Figure 5</ref> for sample frames from these two classes. We observe that both actions are composed of running followed by throwing a javelin or the actual gymnastics action. Short-term networks, thus, easily confuse the two actions, while LTC can capture such long and complex actions. For both classes, we provide snapshots at every 8th frame. It is clear that one needs to look at more than 16 frames to distinguish these actions.</p><p>Let the performance of class c for temporal extent t be p c (t). A set of classes with the maximum performance at t is then M (t) := {c | t ∈ arg max t (p c (t ))}. <ref type="figure" target="#fig_4">Figure 4</ref>(d) plots |M (t)| with respect to t. The majority of classes (64 out of 101) obtain maximum performance when trained with 100f networks. To further check if there exists an "ideal temporal extent" for different actions, <ref type="figure" target="#fig_4">Figure 4</ref>(e) illustrates the average performance increase d(t):</p><formula xml:id="formula_0">d(t) := 1 |M (t)| M (t) max t (p c (t )) − min t (p c (t ))<label>(1)</label></formula><p>We can observe that values of d(t) are lower for shorter extents and larger for longer extents. That means actions scoring best at short extents score similar at all scales, so we cannot conclude that certain actions favor certain extents.</p><p>Most actions favor long extents as the difference is largest for 100f. A possible explanation is that making the interval too long for short actions does not have much impact, whereas making the interval too short for long actions does impact the performance, see <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining networks of varying temporal resolutions.</head><p>We evaluate combining different networks with late fusion. For final results on flow, 58 × 58 spatial resolution and 0.9 dropout are used for both UCF101 and HMDB51 datasets.  The flow networks are learned from scratch for UCF101 and fine-tuned for HMDB51. For final results on UCF101 with RGB input, we use 71 × 71 spatial resolution networks finetuned from C3D network <ref type="bibr" target="#b12">[13]</ref>. However, we do not further fine-tune it for HMDB51 because of overfitting, and use C3D network as a feature extractor in combination with SVM for obtaining RGB scores. Our implementation of C3D as a feature extractor and a SVM classifier achieved 80.2% and 49.7% average performance on 3 splits of UCF101 and HMDB51, respectively. We get similar result when finetuning C3D on 16-frames (80.5% on UCF101). <ref type="figure" target="#fig_6">Figure 6</ref> (left) shows results for combining outputs of flow networks with different temporal extents. The combination is performed by averaging video-level class scores produced by each network. We observe that combinations of two networks with different temporal extents provides significant improvement for flow. The gains of combining more than two resolutions appear to be marginal. For final results, we report combining 60f and 100f networks for both flow and RGB, with the exception of HMDB51 RGB scores for which we use a SVM classifier on 16f feature extractor. <ref type="figure" target="#fig_6">Figure 6</ref> (right) shows results for combining multiscale networks of different modalities together with the IDT+FV baseline classifier <ref type="bibr" target="#b4">[5]</ref> on split 1 of both datasets. We observe complementarity of different networks and IDT+FV where the best result is obtained by combining all classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the-state-of-the-art</head><p>In <ref type="table" target="#tab_6">Table 4</ref>, we compare to the state-of-the-art on HMDB51 and UCF101 datasets. Note that the numbers do not directly match with previous tables and figures, which are reported only on first splits. Different methods are grouped together according to being hand-crafted, using only RGB or optical flow input to CNNs and combining any of these. Trajectory features perform already well, especially with higher-order encodings. CNNs on RGB perform very poor if trained from scratch, but strongly benefits from static image pre-training such as ImageNet. Recently <ref type="bibr" target="#b12">[13]</ref> trained space-time filters from a large collection of videos; however, their method is not end-to-end, given that one has to train a SVM on top of the CNN features. Although we fine-tune LTC RGB based on a network learned with a short temporal span and we reduce the spatial resolution, we are able to improve by 2.2% on UCF101 (80.2% versus 82.4%) by extending the pretrained network to 100 frames.</p><p>We observe that LTC outperforms 2D convolutions on both datasets. Moreover, LTC F low outperforms LTC RGB despite no pre-training. Our results using LTC F low+RGB with average fusion significantly outperform the Twostream average fusion baseline <ref type="bibr" target="#b5">[6]</ref>    <ref type="bibr" target="#b12">[13]</ref> and is clip-based, therefore not directly comparable. <ref type="bibr" target="#b1">2</ref> We use C3D+SVM scores (49.7%) for HMDB51.</p><p>on UCF101 and HMDB51 datasets, respectively. Moreover, the SVM fusion baseline in <ref type="bibr" target="#b5">[6]</ref> is still significantly below LTC F low+RGB . Overall, the combination of our best networks LTC F low+RGB together with the IDT method 2 provides best results on both UCF101 (92.7%) and HMDB51 (67.2%) datasets. Notably both of these results outperform previously published results on these datasets, except <ref type="bibr" target="#b23">[25]</ref> which studies best ways to combine RGB and flow streams, hence complementary to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of the 3D spatio-temporal filters</head><p>First layer weights. In order to have an intuition of what an LTC network learns, we visualize the first layer spacetime convolutional filters in the vector-field form. Filters learned on 2-channel optical flow vectors have dimension 2 × 3 × 3 × 3 in terms of channels, width, height and time. For each filter, we take the two channels in each 3 × 3 × 3 volume and visualize them as vectors using xand y-components. <ref type="figure">Figure 7</ref> shows 18 example filters out of the 64 from a network learned on UCF101 with 60 frames flow input. Since our filters are spatio-temporal, they have a third dimension in time. We find it convenient to show them as vectors concatenated one after the other with regard to the time steps. We denote each time step with different colors and see that the filters learned by long-term temporal convolutions are able to represent complex motions in local neighborhoods, which enables to incorporate even more complex patterns in later stages of the network. High-layer filter activations. We further investigate filters from higher convolutional layers by examining their highest activations. For a given layer and a chosen filter, we record the maximum activation value for all test videos 3 for that 2. Our implementation of IDT+FV <ref type="bibr" target="#b4">[5]</ref> obtained 84.5% and 57.3% for UCF101 and HMDB51, respectively.</p><p>3. UCF101 videos are obtained by clipping different parts (video) from a longer video (group). We take one video per group assuming that videos from the same group would have similar activations and would avoid a proper analysis. In total, there are 7 test groups per class; therefore there can be at most 7 videos belonging to a class. <ref type="figure">Fig. 7</ref>. Spatio-temporal filters from the first layer of the network learned with 2-channel, Brox optical flow and 60 frames on UCF101. 18 out of 64 filters are presented. Each cell in the grid represents two 3 × 3 × 3 filters for 2-channel flow input (one for x and one for y). x and y intensities are converted into vectors in 2D. Third dimension (time) is denoted by putting vectors one after the other in different colors for better visualization (t=1 blue, t=2 red, t=3 green). We see that LTC is able to learn complex motion patterns for video representation. Better viewed in color.</p><p>filter. We then sort test videos according to the activation values and select the top 7 videos. This procedure is similar to <ref type="bibr" target="#b33">[35]</ref>. We can expect that a filter should be activated by similar action classes especially at the higher network layers. Given longer video clips available to the LTC networks, we also expect better grouping of actions from the same class by filter activations of LTC. We illustrate action classes for 30 filters (x-axis) and their top 7 activations (y-axis) for the 100f and 16f networks in <ref type="figure" target="#fig_8">Figure 8(a)</ref>. Each action class is represented by a unique color. The filters are sorted by their purity, i.e. the frequency of the dominating class. We assign each video the color of its ground truth class label. We see that the clustering of videos from the same class becomes more clear in higher layers in the network for both 16f and 100f networks. However, it is evident that 100f filters have more purity than 16f even in L4 and L3. Note that 16f network is trained with high resolution (112×112) flow and 100f network with low resolution (58 × 58) flow.</p><p>Example frames from top-scoring videos for a set of selected filters f are shown in <ref type="figure" target="#fig_8">Figure 8</ref>(b) for 16f and 100f flow networks. We also provide a video on our project web page [17] to show which videos activate for these filters. We can observe that for filters f maximizing the homogeneity of returned class labels, the top activations for filters of the 100f network result in videos with similar action classes. The grouping of videos by classes is less prominent for activations of the 16f network. This result indicates that the LTC networks have higher level of abstraction at corresponding convolution layers when compared to networks with smaller temporal extents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Runtime</head><p>Training on UCF101 takes 1.9 day for 100f (58x58) networks and 1.1 day for 16f (112x112) networks with 0.5 dropout. At test time (without flow computation), the 100f and 16f networks run at 4452fps and 1128fps respectively on a Titan X GPU and 8 CPU cores for parallel data loading. Although it takes more time (roughly 1.6 times) to compute one forward pass for 100f, a larger number of frames are processed per second. C3D <ref type="bibr" target="#b12">[13]</ref> reports 313fps for a 16f network while using a larger number of parameters. Our proposed solution is therefore computationally efficient.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This paper introduces and evaluates long-term temporal convolutions (LTC) and shows that they can significantly improve the performance. Using space-time convolutions over a large number of video frames, we obtain state of the art performance on two action recognition datasets UCF101 and HMDB51. We also demonstrate the impact of the optical flow quality. In the presence of limited training data, using flow improves over RGB and the quality of the flow impacts the results significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a),(c), actions often contain characteristic patterns with specific spatial as well as long-term temporal • Gül Varol and Ivan Laptev are with Inria, WILLOW projectteam, Département d'Informatique de l'École Normale Supérieure, ENS/Inria/CNRS UMR 8548, Paris, France. E-mail: gul.varol@inria.fr • Cordelia Schmid is with Inria, Thoth project-team, Inria Grenoble Rhône-Alpes, Laboratoire Jean Kuntzmann, France.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Video patches for two classes of swimming actions. (a),(c): Actions often contain characteristic, class-specific space-time patterns that last for several seconds. (b),(d): Splitting videos into short temporal intervals is likely to destroy such patterns making recognition more difficult. Our neural network with Long-term Temporal Convolutions (LTC) learns video representations over extended periods of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Network architecture. Spatio-temporal convolutions with 3x3x3 filters are applied in the first 5 layers of the network. Max pooling and ReLU are applied in between all convolutional layers. Network input channels C1...Ck are defined for different temporal resolutions t ∈ {20, 40, 60, 80, 100} and either two-channel motion (flow-x, flow-y) or three-channel appearance(R,G,B). The spatio-temporal resolution of convolution layers decreases with the pooling operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the three optical flow methods and comparison of corresponding recognition performance. From left to right: original image, MPEG, Farneback and Brox optical flow. The color coding indicates the orientation of the flow. The table on the right presents accuracy of action recognition in UCF101 (split 1) for different inputs. Results are obtained with 60f networks and training from scratch (see text for more details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Results for the split 1 of UCF101 using LTC networks of i. varying temporal extents t, ii. varying spatial resolutions [high (H), low (L)] and iii. different input modalities (RGB pre-trained on Sports-1M, flow trained from scratch). For faster convergence all networks were trained using 0.5 dropout and a fixed batch size of 10. Classification results are shown for clips (a) and videos (b) computed over all classes and presented for a subset of individual classes for flow input of low spatial resolution (c). The average number of frames in the training set for a class is denoted in parenthesis. (d) shows a distribution of action classes over the optimal temporal extent and (e) indicates correspondnig improvements (see text for details). With the exception of a few classes, most of the classes benefit from larger temporal extents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Results for network combinations. (Left): Combination of LTC flow networks with different temporal extents on UCF101-split 1. (Right): Combination of flow and RGB networks together with IDT features on UCF101 and HMDB51-splits 1. For UCF101, flow is trained from scratch and RGB is pre-trained on Sports-1M. For HMDB51, flow is pre-trained on UCF101 and RGB scores are obtained using C3D feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Top activations of filters at conv3-conv5 layers. Each row is another layer, indicated by L3-L5. Left is for 100 frames and right is for 16 frames networks. Colors indicate different action classes. Each color plot illustrates distribution of classes for seven top activations of 30 selected filters. Rows are maximum responding test videos and columns are filters. Frames corresponding to videos with top activations at conv4 and conv5 layers. Circles indicate the spatial location of the maximum response. The visualized frames correspond to the maximum response in time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of 100f and 16f networks by looking at the top activations of filters. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Data augmentations on UCF101 (split 1). All results are with 60-frame Brox flow and training from scratch. All three modifications (random clipping, multiscale cropping and high dropout)</figDesc><table><row><cell>Baseline augmentation</cell><cell>71.6</cell><cell>76.5</cell></row><row><cell>Random clipping</cell><cell>74.8</cell><cell>79.6</cell></row><row><cell>Multiscale cropping</cell><cell>72.5</cell><cell>78.1</cell></row><row><cell>High dropout (0.9)</cell><cell>74.4</cell><cell>78.5</cell></row><row><cell>Combined</cell><cell>76.3</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 TABLE 3 Results</head><label>23</label><figDesc>Results for networks with different temporal resolutions and under variation of data augmentation (MS: multiscale cropping) and dropout (D) for UCF101 (split 1), trained from scratch. Random clipping is used in all experiments. Evaluations are on individual clips and on full videos.</figDesc><table><row><cell>Pre-training</cell><cell>Test</cell><cell>16f</cell><cell>60f</cell><cell>gain</cell><cell>2D CNN [6]</cell></row><row><cell>x</cell><cell>Clip</cell><cell>37.0</cell><cell>52.6</cell><cell>+ 15.6</cell><cell></cell></row><row><cell></cell><cell>Video</cell><cell>43.9</cell><cell>52.9</cell><cell>+ 9.0</cell><cell>46.6</cell></row><row><cell></cell><cell>Clip</cell><cell>40.6</cell><cell>56.1</cell><cell>+ 15.5</cell><cell></cell></row><row><cell></cell><cell>Video</cell><cell>48.3</cell><cell>57.1</cell><cell>+ 8.8</cell><cell>49.0</cell></row></table><note>for networks with different temporal resolutions for HMDB51 (split 1) with or without pre-training on UCF101. Flow input, random clipping, multiscale cropping and 0.9 dropout are used in all setups.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Comparison with the state-of-the-art on UCF101 and HMDB51 (mean accuracy across 3 splits).<ref type="bibr" target="#b0">1</ref> This number is read from the plot infigure 2</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the ERC starting grant ACTIVIA, the ERC advanced grant ALLEGRO, Google and Facebook Research Awards and the MSR-Inria joint lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On bodies and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zacks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Imitative Mind</title>
		<editor>A. Meltzoff and W. Prinz</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schüldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding, and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR- 12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond Gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Actions˜transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
