<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Distillation for Learning Video Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Attention Distillation for Learning Video Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the challenging problem of learning motion representations using deep models for video recognition. To this end, we make use of attention modules that learn to highlight regions in the video and aggregate features for recognition. Specifically, we propose to leverage output attention maps as a vehicle to transfer the learned representation from a flow network to an RGB network. We systematically study the design of attention modules, develop a novel method for attention distillation, and evaluate our method on major action recognition benchmarks. Our results suggest that our method improves the performance of the baseline RGB network by a significant margin while maintains similar efficiency. Moreover, we demonstrate that attention serves a more robust tool for knowledge distillation in video domain. We believe our method provides a step forward towards learning motion-aware representations in deep models and valuable insights for knowledge distillation. Our project page is available at https://aptx4869lm.github.io/AttentionDistillation/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action recognition in videos has emerged as a key challenge for deep models. This task requires the understanding of both spatial and temporal cues and the best methods for extracting and fusing them. The two-stream architecture <ref type="bibr" target="#b39">[40]</ref>, exemplified by the I3D model <ref type="bibr" target="#b2">[3]</ref>, has proven to be a effective framework for addressing these challenges. Fusing two modalities of appearance and motion is conceptually appealing, yet it is computationally expensive. A two-stream model can be 100 times slower than its single RGB stream version <ref type="bibr" target="#b4">[5]</ref>. Moreover, learning motion-aware video features from RGB frames remains a challenging problem <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this context, we address the following research questions: Does a deep model need an explicit flow channel to capture motion patterns? How can we bridge the gap between an c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1904.03249v2 [cs.CV] 14 Aug 2020 RGB stream network and its two stream version without incurring the extra computational cost? Several previous works have addressed the challenge of learning a video representation that encodes motion information using a single RGB stream <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>. Our work shares the same motivation, but pursues a very different approach.</p><p>We present a novel video representation learning method called attention distillation. Our method makes use of an explicit probabilistic attention model, and leverages motion information available at training time to predict the motion-sensitive attention features from a single RGB stream. In addition to their utility in visualizing and understanding learned feature representations, we argue that attention models provide an attractive vehicle for mapping between sensing modalities in a task-sensitive way. Once learned, our model requires only RGB frames as inference inputs, and jointly predicts appearance and motion attention maps for action recognition. We conduct extensive experiments and demonstrate that our attention distillation enables more accurate action recognition across several video datasets, while remaining very efficient.</p><p>Our main contributions are summarized as follows:</p><p>• We propose a novel method for learning motion-aware video representations from RGB frames. Our method distills motion knowledge into an RGB network by mimicking the attention map of a reference flow network.</p><p>• Our method achieves consistent improvements of ∼ 1% across major datasets, including UCF101 <ref type="bibr" target="#b40">[41]</ref>, HMDB51 <ref type="bibr" target="#b22">[23]</ref>, EGTEA <ref type="bibr" target="#b23">[24]</ref>, and 20BN-V2 <ref type="bibr" target="#b30">[31]</ref>, with almost no extra computational cost.</p><p>• We study different choices of attention modules for action recognition, and demonstrate that attention serves as a more robust vehicle for knowledge distillation in comparison to previous feature distillation methods.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Action Recognition</head><p>Action recognition is well studied in computer vision <ref type="bibr" target="#b36">[37]</ref>. Recent efforts focus on developing novel deep models for action recognition. For example, recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44]</ref> proposed to make use of 3D convolutional networks to capture spatio-temporal features beyond a single frame. However, their performance using video frames alone falls far behind their two stream versions <ref type="bibr" target="#b2">[3]</ref>. Our work seeks to address the problem of recovering the motion cues encoded in videos from RGB frames alone. There are several recent attempts in this direction. Bilen et al. <ref type="bibr" target="#b0">[1]</ref> proposed a dynamic image network that makes use of the parameters of a ranking machine that captures the temporal evolution of the video frames. Ng et al. <ref type="bibr" target="#b32">[33]</ref> proposed to jointly predict action labels and flow maps from video frames using multi-task learning. This idea is extended by Fan et al. <ref type="bibr" target="#b6">[7]</ref>, where they fold the TV-L1 flow estimation <ref type="bibr" target="#b34">[35]</ref> into their TVNet. Without using flow, Tran et al. <ref type="bibr" target="#b44">[45]</ref> demonstrated that factorized 3D convolutions (2D spatial convolution and 1D temporal convolution) can facilitate the learning of spatio-temporal features. A similar finding was also presented by Xie et al. <ref type="bibr" target="#b48">[49]</ref>. Our method shares the same motivation as these approaches, yet takes a vastly different route. We explore attention mechanisms for video recognition, and propose to distill the predicted attention from a flow network to an RGB network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>Our attention distillation method is inspired by knowledge distillation, first proposed by <ref type="bibr" target="#b1">[2]</ref> for model compression and further popularized by <ref type="bibr" target="#b15">[16]</ref>. Some recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref> explored knowledge distillation across modalities. The most relevant works are <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>. They both addressed the challenge of video representation learning via knowledge distillation. They assume that the reference flow network has better performance than the RGB stream network, and seek to regularize the learning of the RGB stream by distilling features from the flow network to the RGB stream. However, for recently developed large scale video datasets (e.g., Kinetics <ref type="bibr" target="#b21">[22]</ref>, Charades <ref type="bibr" target="#b38">[39]</ref>) and egocentric video datasets (e.g., EGTEA <ref type="bibr" target="#b23">[24]</ref> and EPIC-Kitchens <ref type="bibr" target="#b5">[6]</ref>), the RGB stream has better performance than the flow stream. Feature distillation methods also suffer from the potential threat of "overwriting" the features from RGB stream with the features from flow stream. This is related to the pitfall of catastrophic forgetting <ref type="bibr" target="#b8">[9]</ref>. In contrast, we propose to distill attention maps-which are indicators of important regions for recognition. This design choice stems from the key challenge of video representation learning-motion is substantially different from appearance and both modalities are important for recognition. Our experimental results in Sec. 4 demonstrate that our method can overcome the disadvantages of previous feature distillation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention for Recognition</head><p>Attention has been widely used for visual recognition. We focus on selective visual attention that highlights discriminative regions. This is very different from the recent efforts on selfattention, i.e., self-similarity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>. Recently, selective attention has been explored in deep models for object recognition <ref type="bibr" target="#b31">[32]</ref> and image captioning <ref type="bibr" target="#b49">[50]</ref>. Attention enables these models to "fixate" on image regions, where the decision is made based on a sequence of fixations. Several attention mechanisms are proposed for deep models. For example, <ref type="bibr" target="#b37">[38]</ref> integrated soft attention in LSTMs for action recognition. <ref type="bibr" target="#b25">[26]</ref> further extends <ref type="bibr" target="#b49">[50]</ref> into videos. Specifically, they combined LSTMs with motion-based attention to infer the location of the actions. <ref type="bibr" target="#b10">[11]</ref> modeled top-down and bottom-up attention using bilinear pooling. <ref type="bibr" target="#b46">[47]</ref> proposed a residual architecture for soft attentions. Our previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> considered attention as a probabilistic distribution for egocentric action recognition. Our recent work <ref type="bibr" target="#b26">[27]</ref> made use of motor attention for action anticipation. In this paper, we demonstrate that a useful probabilistic attention model can be obtained without access to a prior distribution from human gaze data. We also provide a systematical study of the utility of probabilistic attention model in action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distilling Motion Attention for Actions</head><p>In this section, we present our method of attention distillation. We start with an overview of the key ideas, followed by a detailed description of the components in our method. Finally, we describe our network architecture and discuss the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>For simplicity, we consider an input video with a fixed length of T frames. Our method can easily generalize to multiple videos, e.g., for mini-batch training. We denote the input video  <ref type="figure">Figure 1</ref>: Overview of our method. Our model (c) takes multiple RGB frames as inputs and adopts a 3D convolutional network as the backbone. It outputs two attention maps using the attention module (b), based on which the action labels are predicted. The motion map is learned by mimicking the attention from a reference flow network (a). The appearance map is learned to highlight discriminative regions for recognition. These two maps are used to create spatio-temporal feature representations from video frames for action recognition. as x = {x 1 , x 2 , ..., x T }, where x t is a frame of resolution H × W with t as the frame number. Given x, our goal is to predict a video-level action label y. We leverage the intermediate output of a 3D convolutional network φ to represent x. This is given by a 4D tensor φ (x) of the size</p><formula xml:id="formula_0">T φ × H φ ×W φ ×C φ . C φ is the feature dimension of 3D grids T φ × H φ ×W φ from the video x.</formula><p>Our method consists of three key components:</p><p>• Attention Generation. Our model first predicts an attention map A based on φ (x) using the attention mapping function F A . A is a 3D tensor of size T φ × H φ ×W φ . Moreover, A is normalized within each temporal slice, i.e., ∑ w,h A(t, w, h) = 1. A is thus a sequence of 2D attention maps A(t) defined over T φ steps.</p><p>• Attention Guided Recognition. Based on the attention map A and the feature map φ (x), our model further applies a recognition module F R to predict the action label y. Specifically, this module uses A to selectively pool features from φ (x), followed by a classifier that maps the result feature vectors to the action label y.</p><p>• Attention Distillation. To regularize the learning, we assume that A will receive supervision from a teacher model that outputs a reference attention mapÃ. The teacher model comes from the flow stream and is equipped with the same attention module for recognition. <ref type="figure">Fig. 1</ref> presents an overview of our method. Our model takes multiple video frames x as inputs, and learns to predict two attention maps based on φ (x): A M for motion attention and A A for appearance attention. Based on these two maps, the model further aggregates visual features that will be passed into the final recognition sub-network. During training, we match A M to the attention mapÃ M from the reference flow network. For testing, only the input video is required for recognition. Our model also outputs two attention maps that can be used to visualize and diagnose recognition performance. We now detail the design of our key components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention Generation</head><p>We explore two different approaches for generating an attention map from the features φ (x), including soft attention <ref type="bibr" target="#b46">[47]</ref> and its probabilistic version <ref type="bibr" target="#b23">[24]</ref>. Soft Attention. Attention maps can be created by a linear function of w a ∈ R C φ over the feature map φ (x),</p><formula xml:id="formula_1">F A (φ (x)) = so f tmax(w a * φ (x)),<label>(1)</label></formula><p>where * is the 1x1 convolution on 3D feature grids. Softmax is applied on every time slice to normalize each 2D map. Probabilistic Soft Attention. An alternative approach is to further model the distribution of linear mapping outputs as discussed in <ref type="bibr" target="#b23">[24]</ref>, namely</p><formula xml:id="formula_2">A ∼ p(A) = so f tmax(w a * φ (x))<label>(2)</label></formula><p>where we model the distribution of A. During training, an attention map can be sampled from p(A) using Gumbel Softmax trick <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. We follow <ref type="bibr" target="#b23">[24]</ref> to regularize the learning by adding additional loss term of</p><formula xml:id="formula_3">L R = ∑ t KL [A(t)||U] ,<label>(3)</label></formula><p>where KL[·] is the Kullback-Leibler divergence and U is the 2D uniform distribution (H φ × W φ ). This term matches each time slice of the attention map to the prior distribution. It is derived from variational learning and accounts for (1) the prior of attention maps and <ref type="formula" target="#formula_2">(2)</ref> additional regularization by spatial dropout <ref type="bibr" target="#b23">[24]</ref>. During testing, we directly plug in p(A) (the expected value of A) for approximate inference. Note that for both approaches, we restrict F A to a linear mapping without a bias term. In practice, this linear mapping avoids the trivial solution of generating a uniform attention map by setting w to all zeros. This all-zero solution almost never arises during training when using a proper initialization of w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Guided Recognition</head><p>Our recognition module makes use of an attention map A to select features from φ (x). Again, we consider two different models for the attention guided recognition. Attention Pooling. Inspired by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>, we design the function F R as</p><formula xml:id="formula_4">y = F R (φ (x), A) = so f tmax W T r (A ⊗ φ (x))<label>(4)</label></formula><p>where ⊗ denotes the tilted multiplication</p><formula xml:id="formula_5">A ⊗ φ (x) = ∑ t,h,w A(t, h, w)φ (x) t,h,w,c .</formula><p>This operation is equivalent to weighted average pooling with the weights shared across all channels. Residual Connection. Using the attention map to re-weight features helps to filter out background noise, yet may also increase the potential risk of missing important foreground features. This drawback was discussed in <ref type="bibr" target="#b46">[47]</ref>. We follow their solution of using a residual connection to the attention map, given bỹ</p><formula xml:id="formula_6">y = F R (φ (x), A) = so f tmax W T r ((A + I) ⊗ φ (x)) ,<label>(5)</label></formula><p>where I is a 3D tensor of all ones. Intuitively, this operation further adds average pooled features to the representation before the linear classifier. By adding the residual term, the features learned by the network are preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention Distillation</head><p>The key to our approach lies in the use of attention distillation during training. Specifically, we assume that a reference flow network is given as the teacher network. The teacher model also uses an attention mechanism for recognition. Moreover, its motion attention mapÃ M is used as additional supervisory signal for training our RGB network. This RGB network is thus the student model that mimics the motion attention map. With probabilistic attention modeling, the imitation of the attention maps is enforced by using the loss</p><formula xml:id="formula_7">L A = ∑ t KL A M (t)||Ã M (t) .<label>(6)</label></formula><p>This loss minimizes the distance between the attention maps at every time step t. In our implementation, our teacher flow network is trained with the same attention mechanism. Once trained, the weights of the teacher model remain fixed during the learning of the student model. At testing time, only the student model (RGB network) is used for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Our Full Model</head><p>Putting everything together, we summarize our full model with probabilistic soft attention and attention distillation. Specifically, our model estimates the two probabilistic attention</p><formula xml:id="formula_8">maps A M ∼ F M A (φ (x)) (motion) and A A ∼ F A A (φ (x)) (appearance)</formula><p>. These maps are further used to predict the action labels. This is given bỹ</p><formula xml:id="formula_9">y = F M R (φ (x), A M ) + F A R (φ (x), A A )<label>(7)</label></formula><p>where each F R follows Eq 4. We use equal weighting for F M R and F A R . We found that tuning the weights has negligible effect on the performance in practice. Loss Function. Our training loss is defined as</p><formula xml:id="formula_10">L = CE(ỹ, y) + λ 1 ∑ t KL A M (t)||Ã M (t) + λ 2 ∑ t KL A A (t)||U ,<label>(8)</label></formula><p>where CE is the cross entropy loss between the predicted labelsỹ and the ground-truth y. Thus, the loss consists of three terms. The first cross entropy term is to minimize the error for classification. The second KL term (from Eq. 6) enforces that the motion attention A M should mimic the attention mapÃ M from the reference flow network. Finally, the third KL term (from Eq. 3) regularizes the learning of the appearance attention. The coefficients λ 1 and λ 2 are used to balance the three terms. We choose λ 1 = 1 and</p><formula xml:id="formula_11">λ 2 = 1/(T φ ×W φ × H φ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head><p>Network Architecture. Our model uses I3D network <ref type="bibr" target="#b2">[3]</ref> as the backbone. I3D has five 3D convolution blocks, and three of them are composed of multiple Inception Modules. For all attention modules, the intermediate feature φ is obtained from the outputs of the 4th convolutional block. The attention map is used to select the final network feature from the last Inception module of the 5th convolutional block. Data Preparation. We down-sample all frames to 320 × 256 with a frame rate of 24 Hz. For training, we compute optical flow using TV-L1 <ref type="bibr" target="#b34">[35]</ref>. We apply several data augmentation techniques, including random flipping, cropping and color perturbation to prevent overfitting. Our model takes 24 consecutive frames as inputs, and all input frames are cropped to 224×224 for training. For testing, we evaluate our model on full resolution clips (320×256) and aggregate scores from all clips to produce the video-level results.</p><p>Training and Inference Details. All of our models are trained using SGD with momentum of 0.9. The weights are initialized from Kinetics pre-trained models provided by the authors of <ref type="bibr" target="#b2">[3]</ref>. Our models are trained with a batch size of 64 on 4 GPUs. The initial learning rate is 0.01 with a decay rate of 10 when the loss starts to saturate. We set weight decay to 4e-5 and enable batch norm <ref type="bibr" target="#b18">[19]</ref>. We also adopt dropout rate 0.7. At inference time our model does not need optical flow, and runs at the same speed as the RGB network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now present our experiments and results. We start with a systematical evaluation of attention guided action recognition, followed by our main results on several public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We make use of four action recognition datasets for our experiments: UCF101, HMDB51, EGTEA Gaze+ (egocentric videos) and 20BN-V2. UCF101 <ref type="bibr" target="#b40">[41]</ref> has 13,320 videos from 101 action categories. HMDB51 <ref type="bibr" target="#b22">[23]</ref> includes 6,766 videos from 51 action categories. EGTEA <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> contains 10,321 videos from 106 action categories. We evaluate mean class accuracy and report the results using the first split of these three datasets. 20BN-V2 <ref type="bibr" target="#b30">[31]</ref> has over 220K videos from 174 fine-grained action categories. We use their training and validation split, and report top-1/top-5 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention Guided Action Recognition</head><p>We start from an ablation study of attention-guided action recognition. Specifically, we evaluate different combinations of attention modules and compare their results to those from models without attention. Our experiments show that the proper design of the attention mechanism can consistently improve the performance of action recognition across multiple datasets. We now present our baselines and results. Baselines. We consider the different combinations of how the model generates attention maps (Soft vs. Probabilistic Attention) and how the attention maps are used for recognition (Attention Pooling vs. Residual Connection). In addition, we also show how the approach to combining motion attention and appearance attention affects the recognition performance. The valid combinations include the following:</p><p>• Soft-Atten combines soft attention and attention pooling for recognition similar to <ref type="bibr" target="#b27">[28]</ref>.</p><p>• Soft-Res is the residual attention in <ref type="bibr" target="#b46">[47]</ref> that adds residual connection to Soft-Atten.</p><p>• Prob-Atten combines probabilistic attention with attention pooling as in <ref type="bibr" target="#b23">[24]</ref>.</p><p>We note that the combination of Prob+Res is invalid, as it violates the probabilistic modeling of attention. In practice, we also found its training to be unstable. Therefore, we report the results of three valid designs for both RGB and flow stream and the vanilla I3D models (our backbone) using the same input sequence length (24 frames) in <ref type="table" target="#tab_1">Table 1</ref>. Adding attention to the backbone recognition network almost always improves the performance. Importantly, Soft-Res decreases the performance of RGB stream on HMDB51 and Soft-Atten decreases the performance of RGB stream on HMDB51 and UCF101. More interestingly, Prob-Atten   <ref type="table">Table 2</ref>: Action recognition results on UCF101 and HMDB51 datasets. We compare the results of our model with previous works. Our model outperforms state-of-the-art methods that use only RGB stream and the same input sequence length by ∼ 1%. *For fair comparison, we report results of I3D models that use 24 frames as inputs-the same as our model.</p><p>is the most robust design choice, despite the lack of human gaze as a supervisory signal as in <ref type="bibr" target="#b23">[24]</ref>. Across all of the modalities and datasets, Prob-Atten can consistently improve the recognition accuracy (+0.3%/0.4%/1.8%) for the RGB stream and (+0.9%/0.5%/2.1%) for the flow stream. The performance boost from the attention module is larger for the flow stream in comparison to the RGB stream. Moreover, attention modules provide more significant boost for egocentric actions (EGTEA). We conjecture that the explicit modeling of attention helps to suppress background objects in first person video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attention Distillation for Action Recognition</head><p>We now evaluate our method of attention distillation. In this setting, we assume a reference flow network with attention module is given at training time. We attach motion and appearance attention modules to our RGB backbone. Both attention heads follow the same attention module design as the reference network. The flow attention is asked to mimic the motion attention map from the reference flow network. During testing our model does not need optical flow, and runs at the same speed as the RGB network (about 100 times faster than a two stream network <ref type="bibr" target="#b4">[5]</ref>). We present our results for action recognition, and contrast our method with feature distillation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">51]</ref>. Impact of Attention Distillation. <ref type="table">Table 2</ref> compares our results with previous methods on UCF101/HMDB51. We denote our models using Prob-Atten for distillation as Prob-Distill. Prob-Distill outperforms all previous state-of-the-art methods of motion represen-  <ref type="table">Table 3</ref>: Action recognition results on on 20BN-V2 dataset <ref type="bibr" target="#b30">[31]</ref>. Our model achieves the best performance among networks that uses RGB frames. Fusing our model with a flow network also outperforms two stream baseline by a significant margin. tation learning. Specifically, our results are at least 1.2% better than previous state-of-theart methods for learning motion-aware video representations from RGB frames, including Dynamic Image <ref type="bibr" target="#b0">[1]</ref>, ActionFlowNet <ref type="bibr" target="#b32">[33]</ref> and TVNet <ref type="bibr" target="#b6">[7]</ref>. Our model also outperforms MARS <ref type="bibr" target="#b4">[5]</ref>, our direct competitor, by 0.9% on UCF101 and performs on-par with MARS on HMDB51 when using a similar sequence length, despite the fact that MARS uses a stronger backbone network. It is worth noting that this performance boost is significant for action recognition. In contrast, with 50 more layers, ResNet101 is only 0.7% better than ResNet50 on HMDB51 <ref type="bibr" target="#b14">[15]</ref>. Moreover, Prob-Distill also outperforms another feature distillation method -FeatMatch <ref type="bibr" target="#b50">[51]</ref> by a significant margin (+1.4%/1.3% on UCF101/HMDB51). These results support our argument that distilling attention maps is more robust than distilling network features for motion representation learning. Finally, a late fusion of our model with a reference flow network helps to further boost the performance. <ref type="table">Table 3</ref> presents our results on a large scale dataset-20BN-V2. With 1/5 of the temporal receptive field as TRN <ref type="bibr" target="#b52">[53]</ref>, our model with RGB frames outperforms TRN RGB by 1.1%/1.5% in top-1/top-5 accuracy. And our method improves the backbone by 2.6%/3.0% in top-1/top-5 accuracy. Further fusion of our model with a flow network improves the results by a large margin (+4.7%), again outperforming the two stream baseline. Learning from a Weak Flow Network. Crasto et al. <ref type="bibr" target="#b4">[5]</ref> pointed out that their model ran into a failure mode when the reference flow network has worse performance than the RGB network. To support our claim that attention distillation can leverage a flow-based teacher network even when the flow network does not provide strong baseline performance, we report the results of our model on the EGTEA Gaze+ dataset. Due to severe ego-motion, flowbased models are less effective than RGB models on this dataset. For instance, I3D Flow is 9% worse than I3D RGB (38.3% vs. 47.3%). Despite a much weaker teacher model, Prob-Distill achieves 49.5%, outperforming the best attention-based I3D models for both RGB (Prob-Atten 49.1%) and Flow (Prob-Atten 40.4%). This indicates that even with a weak teacher model, our proposed method is a robust approach to video representation learning. Distillation without Forgetting. Feature distillation might "overwrite" the features from RGB stream with the features from flow stream. This is evidenced by the result that fusing MARS with reference flow stream network lags behind the two stream version of the network (MARS + Flow ResNeXt vs. Two Stream ResNeXt in <ref type="table">Table 2</ref>). In contrast, fusing our Prob-distill model with a reference flow model (Prob-Distill + Flow I3D in <ref type="table">Table 2</ref>) further improves the accuracy and outperforms the two-stream I3D model (+0.5% on UCF101, +0.7% on HMDB51 and +0.9% on 20BN-V2). These results indicate that our attention distillation model does not simply copy the feature from the reference flow network, as the distilled RGB model can still preserve meaningful appearance features.</p><p>Note that our single-stream (Prob-Distill) results still lag behind the two stream networks when using the same input sequence length (Two Stream I3D*). This gap reveals that our model does not fully capture the concepts of motion that are encoded in the two stream networks. Nonetheless, we believe that our model provides a key step forward for learning motion-aware representations from RGB frames. Note that some most recent works achieved better performance on the benchmark datasets using more advanced network structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>, additional features <ref type="bibr" target="#b3">[4]</ref>, or a longer temporal footprint <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. In this context, our work provides a novel method for learning video representations and a robust strategy for knowledge distillation. In supplementary materials, we provide additional analysis to show how the learned attention maps help to localize the spatial extent of actions. We also demonstrate how motion information is encoded in the distilled model. Visualization of Attention Maps. To better understand our model, we visualize both motion and appearance attention maps from our model. We also compare these maps with attention maps created by our Soft-Atten models from RGB and flow streams in <ref type="figure" target="#fig_1">Fig 2.</ref> Notice that these two attention maps are qualitatively different across all methods. The appearance attention is likely to cover foreground objects or actors, while the motion attention focuses on the moving parts. Moreover, the appearance attention from our model can better localize the foreground regions of actions than those of Soft-Atten from the RGB stream, while the motion attention from our model remains similar to the Soft-Atten from the flow stream. We also find that the attention maps from our model are more "diffused". This is because the regularization by a uniform distribution in Prob-Atten leads to smoother attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented a novel method of attention distillation for action recognition. We provided extensive experiments to evaluate our method. Our results demonstrate that proper design of the attention module helps to improve recognition performance. In addition, attention maps from RGB and flow networks are qualitatively different, suggesting that these networks capture different aspects of the video. We also showed that our method achieves competitive results for action recognition across datasets, and that attention distillation is more robust for learning a motion-aware video representation. We believe our work provides valuable insights into attention based recognition, and a step towards learning spatio-temporal video features via knowledge distillation. This is the supplementary material for our paper in BMVC 2020, titled 'Attention Distillation for Learning Video Representations". In this document, we introduce the implementation details. Moreover, we investigate the predicted attention maps and the learned features of our model and provide further analysis of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>We detail the network architecture of our full model (Prob-Atten) in <ref type="table">Table 6</ref>. Specifically, our model adopts I3D network <ref type="bibr" target="#b2">[3]</ref> as the backbone. We attached the attention modules to the last Inception Module of the fourth convolution block. The appearance and motion attention maps, predicted by attention modules, are further used to pool features from the last Inception Module of the fifth convolution block for classification. And their results are fused at the end for final recognition. The network takes 24 frames as inputs with dimension 24 × 224 × 224 × 3 (RGB). And the network outputs (a) a downsampled attention map with size 3 × 7 × 7 (three temporal slices with spatial resolution of 7 × 7); and (b) the action scores for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Attention Distillation</head><p>We provide extensive analysis to understand what has been learned by our model. We show that these attention maps help to locate the spatial extents of actions. And, we study different approaches to evaluate whether the learned representation is sensitive to motion. Finally, we provide more visualization of the attention maps of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does the attention help to localize actions?</head><p>We evaluate our output attention for action localization using THUMOS'13 localization dataset <ref type="bibr" target="#b17">[18]</ref>-a subset of UCF101 with bounding box annotations for actions. We present our evaluation metric and discuss our results.</p><p>• Evaluation Metric. We consider action localization as binary labeling of pixels and report the F1 score from Precision-Recall (PR) curve. Specifically, we first rescale both attention maps and video frames into a fixed resolution (56 × 56). We then enumerate all thresholds and binarize the attention map. Each threshold defines a point on the PR curve. Given a binary attention map, a positive pixel is considered as a true positive if it is inside the bounding box, or it is within 10-pixel "tolerance zone" of the box. This tolerance is added to compensate for the reduced resolution of the attention map, as in <ref type="bibr" target="#b33">[34]</ref>. We report the best F1 score on the curve and its corresponding precision and recall.</p><p>• Results. We compare attention maps from our model to a set of baseline methods, including a fixed Gaussian distribution (center prior), a latest deep saliency model (DSS <ref type="bibr" target="#b16">[17]</ref>), and our Soft-Atten (RGB/Flow). The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Our appearance attention beats the baselines of center prior and Soft-Atten (RGB), but is worse than Soft-Atten (flow). Our motion attention achieves the highest score among all methods that only receive action labels as supervision, and only under-performs DSS. We have to emphasis that directly comparing our results to DSS is unfair. DSS is trained with pixel-level annotations using external data and runs at the original video resolution, while our attention maps are trained using clip-level action labels and down-sampled both spatially (32x) and   <ref type="bibr" target="#b17">[18]</ref>. We report the best F1 score and its precision and recall. Our motion attention outperforms all baselines that are trained with only action labels.  temporally (8x). These results suggest that our attention maps help to locate the spatial extent of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does our method learn better motion representation?</head><p>We further study how the temporal order of the input video frames will affect the recognition performance. We conduct an experiment of classifying reverted videos as in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53]</ref>. Specifically, we invert the frame order for all testing videos of UCF101 and HMDB51. We compare their recognition results with those from normal temporal order. If a model truly rely on motion representation for the recognition, this inversion will significantly decrease the recognition performance. We test the vanilla I3D RGB and flow models, as well as our model. And the results are presented in <ref type="table" target="#tab_6">Table 5</ref>. Not surprisingly, I3D flow model has the largest performance drop. In contrast, I3D RGB is barely affected by the reverted arrow of time. Our model has a performance drop that is larger than I3D RGB yet much smaller than I3D flow. This is consistent with our results on action recognition. Our model does not capture the same level of motion information as the flow network.</p><p>How is the motion encoded? It is also possible that our model simply copies the motion attention map without encoding motion in the network. To eliminate this hypothesis, we experimented with training an RGB network that directly combines a reference motion attention map and its own appearance attention map for action recognition. The reference motion attention is produced by a flow network during both training and testing. And the rest of this network follows exactly the same architecture as our model. This model has an accuracy of 95.1%/71.6% on UCF101/HMDB51, under-performing our model by -0.6%/-0.4% on UCF101/HMDB51. These results indicate that the distillation process not only generates motion attention maps, but also learns motion-aware representation.</p><p>Additional Visualizations. We provide additional visualization of attention maps in <ref type="figure" target="#fig_2">Fig 3.</ref> The figure follows the same format as <ref type="figure" target="#fig_1">Fig. 2</ref> in our paper. These results further verify that (1) the appearance and motion attention maps are qualitatively different and (2) these attention map at good at localizing the actions, e.g., the actors or the moving regions.</p><p>What has been learned? Our visualization and action localization experiment suggest that our model learns to locate moving regions from video frames. However, when we invert the temporal order of frames, our learned features are not as sensitive as those from flow network. These results illustrate a key challenge for learning motion-aware representations. How the model learns to identify moving regions is not necessarily the right representation to encode motion. This is the same pitfall faced by our work and many previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>.    <ref type="table" target="#tab_1">Convolution  7x7x7  2x2x2 12x112x112x64 1  2  Max Pool  1x3x3  1x2x2 12x56x56x64  0  3  Convolution  1x1x1  1x1x1 12x56x56x64  1  4  Convolution  3x3x3  1x1x1 12x56x56x192  1  5  Max Pool  1x3x3  1x2x2 12x28x28x192  0  6  Inception 3a  12x28x28x256  2  7  Inception 3b  12x28x28x480  2  8</ref> Max Pool 3x3x3 2x2x2 6x14x14x480 0 9</p><p>Inception 4a 6x14x14x512 2 10</p><p>Inception 4b 6x14x14x512 2 11</p><p>Inception 4c 6x14x14x512 2 12</p><p>Inception 4d 6x14x14x528 2 13</p><p>Inception <ref type="formula" target="#formula_4">4e</ref>   <ref type="table">Table 6</ref>: Network Architecture of our full model (Prob-Atten). The network attaches appearance and motion attention moduels to a backbone I3D network. We list details of all operations in the network, as well as where the loss functions are attached. Note that the predicted scores from Motion Action Branch and Appearance Action Branch are fused at the end for final recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of attention maps (Ours vs. Soft-Atten using the same I3D backbone). For each video clip, we re-interpolate the attention maps and plot them on the first and last frame. Red regions indicate higher value of attention. Our model produces appearance and motion attention maps that are qualitatively different and index key action regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of attention maps from our full model and Soft-Atten. For each 24 frames video clip, we plot the attention heatmap over the first frame and last frame. Our model produces qualitatively different appearance and motion attention maps. And these attention maps are better at localizing the actions when compared to vanilla soft attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluations of attention modules. We compared 3 different design choices with RGB/flow stream on three datasets. Prob-Atten provides a consistent performance boost on both streams and across datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">UCF101 HMDB51</cell></row><row><cell>Dynamic Image [1]</cell><cell>90.6</cell><cell>61.3</cell></row><row><cell>ActionFlowNet [33]</cell><cell>83.9</cell><cell>56.4</cell></row><row><cell>TVNet [7]</cell><cell>94.5</cell><cell>71.0</cell></row><row><cell>I3D RGB* [3]</cell><cell>94.8</cell><cell>70.9</cell></row><row><cell>FeatMatch [51]</cell><cell>94.3</cell><cell>70.7</cell></row><row><cell>MARS [5]</cell><cell>94.6</cell><cell>72.3</cell></row><row><cell>Ours (Prob-Distill)</cell><cell>95.7</cell><cell>72.0</cell></row><row><cell>Two Stream ResNeXt [5]</cell><cell>95.6</cell><cell>74.0</cell></row><row><cell>MARS+Flow ResNeXt [5]</cell><cell>94.9</cell><cell>74.5</cell></row><row><cell>Two Stream I3D*</cell><cell>96.7</cell><cell>74.8</cell></row><row><cell>Prob-Distill+Flow I3D*</cell><cell>97.4</cell><cell>75.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of action localization using attention maps on THUMOS'13 localization test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Inverting the arrow of time for action recognition. We train the models on normal samples, yet test them on videos with reversed temporal order. A large performance drop indicates that the model has to rely on motion information for the recognition.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>And this challenge remains open.</figDesc><table><row><cell></cell><cell>Our Full Model</cell><cell>Playing Dhol</cell><cell>Soft Attention</cell><cell>Our Full Model</cell><cell>Playing Cello</cell><cell>Soft Attention</cell></row><row><cell>Appearance</cell><cell>Attention</cell><cell></cell><cell cols="2">Appearance Attention</cell><cell></cell></row><row><cell cols="2">Motion Attention</cell><cell></cell><cell cols="2">Motion Attention</cell><cell></cell></row><row><cell></cell><cell>Our Full Model</cell><cell>Drumming</cell><cell>Soft Attention</cell><cell>Our Full Model</cell><cell>Playing Flute</cell><cell>Soft Attention</cell></row><row><cell>Appearance</cell><cell>Attention</cell><cell></cell><cell>Appearance</cell><cell>Attention</cell><cell></cell></row><row><cell cols="2">Motion Attention</cell><cell></cell><cell cols="2">Motion Attention</cell><cell></cell></row><row><cell></cell><cell>Our Full Model</cell><cell>Soccer Penalty</cell><cell>Soft Attention</cell><cell>Our Full Model</cell><cell>Playing Sitar</cell><cell>Soft Attention</cell></row><row><cell>Appearance</cell><cell>Attention</cell><cell></cell><cell>Appearance</cell><cell>Attention</cell><cell></cell></row><row><cell cols="2">Motion Attention</cell><cell></cell><cell cols="2">Motion Attention</cell><cell></cell></row><row><cell></cell><cell>Our Full Model</cell><cell>Archery</cell><cell>Soft Attention</cell><cell>Our Full Model</cell><cell>Apply Lipstick</cell><cell>Soft Attention</cell></row><row><cell>Appearance</cell><cell>Attention</cell><cell></cell><cell>Appearance</cell><cell>Attention</cell><cell></cell></row><row><cell cols="2">Motion Attention</cell><cell></cell><cell cols="2">Motion Attention</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Portions of this research were supported in part by National Science Foundation Award 1936970 and a gift from Facebook. YL acknowledges the support from the Wisconsin Alumni Research Foundation. XC acknowledges the support from Midea Emerging Technology Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Buciluçő</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MARS: Motion-Augmented RGB Stream for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">Ermon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What have we learned from deep representations for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modality distillation with multiple stream networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rahul Sukthankar, and Mubarak Shah. The THUMOS challenge on action recognition for videos âȂIJin the wildâȂİ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbelsoftmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00626</idno>
		<title level="m">the eye of the beholder: Gaze and actions in first person video</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Forecasting human object interaction: Joint prediction of motor attention and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-grained video classification and captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waseem</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pérez</forename></persName>
		</author>
		<title level="m">Enric Meinhardt-Llopis, and Gabriele Facciolo. TV-L1 optical flow estimation. IPOL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
