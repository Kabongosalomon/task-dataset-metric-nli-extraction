<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Precise End-to-end Weakly Supervised Object Detection Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
							<email>yangke13@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Precise End-to-end Weakly Supervised Object Detection Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is challenging for weakly supervised object detection network to precisely predict the positions of the objects, since there are no instance-level category annotations. Most existing methods tend to solve this problem by using a two-phase learning procedure, i.e., multiple instance learning detector followed by a fully supervised learning detector with bounding-box regression. Based on our observation, this procedure may lead to local minima for some object categories. In this paper, we propose to jointly train the two phases in an end-to-end manner to tackle this problem. Specifically, we design a single network with both multiple instance learning and bounding-box regression branches that share the same backbone. Meanwhile, a guided attention module using classification loss is added to the backbone for effectively extracting the implicit location information in the features. Experimental results on public datasets show that our method achieves state-ofthe-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, Convolutional Neural Networks (CNN) approaches have achieved great success in computer vision field, due to its ability to learn generic visual features that can be applied in many tasks such as image classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>, object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> and semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>. Fully supervised object detection has been widely studied and achieved promising results. There are also plenty of public datasets which provide precise location and category annotations of the objects. However, precise object-level annotations are always expensive in human resource and huge data volume is required by training accurate object detection models. In this paper, we focus on Weakly Supervised Object Detection (WSOD) problem, which uses only image-level category labels so that significant cost of preparing training data can be saved. Due to the lack of accurate annotations, this problem has not been well handled and the performance is still far from the fully supervised methods.  <ref type="figure">Figure 1</ref>: The learning strategy comparison of existing weakly supervised object detection methods (above the blue solid line) and our proposed method (below the blue solid line).</p><p>Recent WSOD methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18]</ref> usually follows a two-phase learning procedure as shown in the top part of <ref type="figure">Figure 1</ref>. In the first phase, the Multiple Instance Learning (MIL) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref> like weakly learning pipeline is used, which trains a MIL detector by using CNN as feature extractor. In the second phase, a fully supervised detector, e.g. Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> or Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>, is trained to further refine object location by using the selected proposals of the first phase as supervision. The main functionality of the second phase is to regress the object locations more precisely. However, we observed that the two-phase learning is easy to get stuck into local minima if the selected proposals of the first phase are too far from real Ground Truth (GT). As shown in the top part of <ref type="figure">Figure 1</ref>, in some categories, the MIL detector tends to focus on the local dis-  criminative parts of the objects, such as the head of a cat, so that the wrong proposals are used as pseudo GT for the second phase. In this case, the accurate location of the object can hardly be learned in the regression process of the second phase, as the MIL detector has already over-fitted seriously to the discriminate parts, as shown in the middle part of <ref type="figure" target="#fig_0">Figure 2</ref>. We further observed that the MIL detector does not select the most discriminative parts at the beginning of the training, but gradually over-fits to these parts, as shown in the left part of <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Taking into account the above observations, we propose to jointly train the MIL detector and the boundingbox regressor together in an end-to-end manner, as shown in the bottom part of <ref type="figure">Figure 1</ref>. In this manner, the regressor is able to start to adjust the predicted boxes before the MIL detector focuses seriously to small discriminative parts, as shown in the right part of <ref type="figure" target="#fig_0">Figure 2</ref>. Specifically, we use MIL detection scheme <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> as baseline and integrate fully supervised RoI-based classification and bounding-box regression branch similar to Fast R-CNN, which shares the same backbone with MIL detector. MIL detector is a weakly learning process, which selects object predictions from the region proposals, e.g. generated by Selective Search Windows (SSW) <ref type="bibr" target="#b35">[36]</ref> method, according to classification scores. These selected proposals are then used as the pseudo GT supervision of the classification and regression branch.</p><p>In order to further enhance the localization ability of the proposed network, we propose to use a guided attention module using image-level classification loss in the backbone. To our best knowledge, the well trained classification network contains rich object location information. Therefore, we add this attention branch which is guided by imagelevel classification loss. Fully considering the global characteristics of the objects, the attention branch can improve the discriminative ability of the network as well as detection accuracy.</p><p>It is worth noting that though jointly learning of classification and boxes regression has already been shown to be beneficial for fully supervised object detection, for weakly supervised object detection it is still non-trivial and needs innovative idea and insight on this task. Although Our method is conceptually simple in form, it significantly alleviates the weak detector over-fitting to discriminate parts and substantially surpasses previous methods. Our contributions can be summarized as follows.</p><p>• We design a single end-to-end weakly supervised object detection network that can jointly optimize the region classification and regression, which boosts performance significantly.</p><p>• We design a classification guided attention module to enhance the localization ability of feature learning, which also leads to a noteworthy improvement.</p><p>• Our proposed network significantly outperforms previous state-of-the-art weakly supervised object detection approaches on PASCAL VOC 2007 and 2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Feature Extraction</head><p>After the success of using CNNs for image classification task <ref type="bibr" target="#b19">[20]</ref>, a research stream based on CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> shows significant improvements in detection performance. These methods use convolutional layers to extract features from each region proposal. To speed up the the detection, SPP-Net <ref type="bibr" target="#b10">[11]</ref> and Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> firstly extract regionindependent feature maps at the full-image level, and then pool region-wise features via spatial extents of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weakly Supervised Object Detection</head><p>Most existing methods formulate weakly-supervised detection as a multiple instance learning problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. These approaches divided training images into positive and negative parts, where each image is considered as a bag of candidate object instances. If an image is annotated as a positive sample of a specific object class, at least one proposal instance of the image belongs to this class. The main task of MIL-based detectors is to learn the discriminative representation of the object instances and then select them from positive images to train a detector. Previous works on applying MIL to WSOD can be roughly categorized into multi-phase learning approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41]</ref> and end-to-end learning approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. End-to-end learning approaches combine CNNs and MIL into a unified network to address weakly supervised object detection task. Diba et al. <ref type="bibr" target="#b4">[5]</ref> proposed an endto-end cascaded convolutional network to perform weakly supervised object detection and segmentation in cascaded manner. Bilen et al. <ref type="bibr" target="#b0">[1]</ref> developed a two-stream weakly supervised deep detection network (WSDDN), which selected the positive samples by aggregating the score of classification stream and detection stream. Based on WSDDN, Kantorov et al. <ref type="bibr" target="#b18">[19]</ref> proposed to learn a context-aware CNN with contrast-based contextual modeling. Also based on WSDDN, Tang et al. <ref type="bibr" target="#b33">[34]</ref> designed an online instance classifier refinement (OICR) algorithm to alleviate the local optimum problem. Tang et al. <ref type="bibr" target="#b32">[33]</ref> also proposed Proposal Cluster Learning (PCL) to improve the performance of OICR. Following the inspiration of <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b4">[5]</ref>, Wei et al. <ref type="bibr" target="#b38">[39]</ref> proposed a tight box mining method that leverages surrounding segmentation context derived from weaklysupervised segmentation to suppress low quality distracting candidates and boost the high-quality ones. Recently, Tang et al. <ref type="bibr" target="#b34">[35]</ref> proposed a weakly supervised region proposal network to generate more precise proposals for detection. Positive object instances often focus on the most discriminative parts of an object (e.g. the head of a cat, etc.) but not the whole object, which leads to inferior performance of weakly supervised detectors. Multi-phase learning approaches first employ MIL to select the best object candidate proposals, then use these selected proposals as pseudo GT annotations for learning the fully supervised object detector such as R-CNN <ref type="bibr" target="#b9">[10]</ref> or Fast(er) R-CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed classification adaptation to fine-tune the network to collect class specific object proposals, and detection adaptation was used to optimize the representations for the target domain by the confident object candidates. Cinbis et al. <ref type="bibr" target="#b3">[4]</ref> proposed a multi-fold MIL detector by re-labeling proposals and retraining the object classifier iteratively to prevent the detector from being locked into wrong object locations. Jie et al. <ref type="bibr" target="#b17">[18]</ref> proposed a self-taught learning approach to progressively harvest high-quality positive instances. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed pseudo ground-truth excavation (PGE) algorithm and pseudo groundtruth adaptation (PGA) algorithm to refine the pseudo ground-truth obtained by <ref type="bibr" target="#b33">[34]</ref>. Wan et al. <ref type="bibr" target="#b37">[38]</ref> proposed a min-entropy latent model (MELM) and recurrent learning algorithm for weakly supervised object detection. Ge et al. <ref type="bibr" target="#b7">[8]</ref> proposed to fuse and filter object instances from different techniques and perform pixel labeling with uncertainty and they used the resulting pixelwise labels to generate groundtruth bounding boxes for object detection and attention maps for multi-label classification. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> proposed a Multi-view Learning Localization Network (ML-LocNet) by incorporating multiview learning into a two-phase WSOD model. However, multiphase learning WSOD is a non-convex optimization problem, which makes such approaches trapped in local optima.</p><p>In this paper, we consider the MIL (positive object candidates mining) and regression (object candidates localization refinement) problems simultaneously. We follow the MIL pipeline and combine the two-stream WSDDN <ref type="bibr" target="#b0">[1]</ref> and OICR/PCL algorithms <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref> to implement our basic MIL branch and refine the detected boxes with a regression branch in an online manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attention Module</head><p>Attention modules were first used in the natural language processing field and then introduced to the computer vision area. Attention can be seen as a method of biasing the allocation of available computational resources towards the most informative components of a signal <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>The current attention modules can be divided into two categories: spatial attention and channel-wise attention. Spatial attention is to assign different weights to different spatial regions depending on their feature content. It automatically predicts the weighted heat map to enhance the relevant features and suppress the irrelevant features during the training process of a specific task. Spatial attention has been used in image captioning <ref type="bibr" target="#b39">[40]</ref>, multi-label classification <ref type="bibr" target="#b44">[45]</ref>, pose estimation <ref type="bibr" target="#b2">[3]</ref> and so on. Hu et al. <ref type="bibr" target="#b13">[14]</ref> proposed an Squeeze-and-Excitation block which models channel-wise attention in a computationally efficient manner. In this paper, we use a combination of spatial and channel-wise attention, and our attention module is guided by object category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we introduce proposed weakly supervised object detection network, which consists of three major components: guided attention module (GAM), MIL branch and regression branch. The overall architecture of proposed network is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Given an input image, an en-  hanced feature map is first extracted from the CNN network with GAM. Region features generated by ROI pooling are then sent to MIL branch and regression branch. The object locations and categories proposed by MIL branch are taken as pseudo GT of the regression branch for location regression and classification. The remainder of this section discusses the three components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Guided Attention Module</head><p>First, we describe the conventional spatial neural attention structure. Given a feature map X ∈ R H×W ×D extracted from a ConvNet, the attention module takes it as input and outputs a spatial-normalized attention weight map A ∈ R H×W via a 1×1 convolutional layer. Attention map is then multiplied to X to get attended feature X a ∈ R H×W ×D . X a is added to X to get the enhanced feature mapX. After that,X is fed to subsequent modules. Attention map A acts as a spatial regularizer to enhance the relevant regions and suppress the non-relevant regions for feature X.</p><p>Formally, attention module consists of a convolutional layer, a non-linear activation layer and a spatial normalization as follows:</p><formula xml:id="formula_0">z i,j = F w T x i,j + b ,<label>(1)</label></formula><formula xml:id="formula_1">a i,j = z i,j i,j z i,j ,<label>(2)</label></formula><p>where F is non-linear activation function. w and b are the parameters of the attention module, which is a 1 × 1 convolutional layer. The attended featurex i,j can be calculated</p><formula xml:id="formula_2">by:x i,j = (1 + a i,j )x i,j .<label>(3)</label></formula><p>The conventional attention map is class-agnostic. We hope it can learn some foreground/background information to help figure out the position of the objects, because it has been proved that CNNs are not only effective at predicting the class label of an image, but also localizing the image regions relevant to this label <ref type="bibr" target="#b43">[44]</ref>.</p><p>We add the classification loss to guide the learning of the attention weights. To achieve this, we expand spatial attention to both spatial and channel attention. Specifically, attention map are changed from A ∈ R H×W to A ∈ R H×W ×D . The attention module can be formalized as:</p><formula xml:id="formula_3">z c i,j = F w T c x i,j + b c ,<label>(4)</label></formula><formula xml:id="formula_4">a c i,j = z c i,j 1 + exp (−z c i,j ) ,<label>(5)</label></formula><p>where c denotes the value of the c-th channel. The attended featurex c i,j can be calculated by:</p><formula xml:id="formula_5">x c i,j = (1 + a c i,j )x c i,j .<label>(6)</label></formula><p>To introduce classification supervision to attention weights learning, attention map A is also fed to another convolutional layer and a Global Average Pooling (GAP) layer to get the classification score vector. Then the attention map can be supervised by the standard multi-label classification loss. The enhanced feature mapX is fed to subsequent components for detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MIL Branch</head><p>We only have image-level labels indicating whether an object category appears. To train a standard object detector with regression, it is necessary to mine instance-level supervision such as bounding-box annotations. Therefore, we need to introduce a MIL branch to initialize the pseudo GT annotations. There are a couple of possible choices such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34]</ref>. We choose to adopt OICR network <ref type="bibr" target="#b33">[34]</ref> which is based on WSDDN <ref type="bibr" target="#b0">[1]</ref> for its effectiveness and end-toend training. WSDNN employed a two streams network: the classification and detection data streams. By aggregating these two streams, instance-level predictions can be achieved.</p><p>Specifically, given an image I with only image-level label Y = [y 1 , y 2 , ..., y C ] ∈ R C×1 , where y c = 1 or 0 indicates the presence or absence of an object class c. For each input image I, the object proposals R = (R 1 , R 2 , ..., R n ) are generated by the selective search windows method <ref type="bibr" target="#b35">[36]</ref>. The features of each proposal are extracted through a Con-vNet pre-trained on ImageNet <ref type="bibr" target="#b27">[28]</ref> and RoI Pooling, then are branched into two streams to produce two matrices x cls , x det ∈ R C×|R| by two FC layers, where |R| denotes the number of proposals and C denotes the number of image classes. These two matrices are passed through a softmax layer with different dimensions and the outputs are two matrices with the same shape: σ(x det ) and σ(x cls ).</p><p>After that, the scores of all proposals are generated by element-wise product x R = σ(x det ) σ(x cls ). Finally, the c-th class prediction score at the image-level can be obtained by summing up the scores over all proposals:</p><formula xml:id="formula_6">p c = |R| r=1 x R c,r .</formula><p>During the training stage, the loss function can be formulated as follows:</p><formula xml:id="formula_7">L mil = − C c=1 {y c log p c + (1 − y c ) log(1 − p c )}. (7)</formula><p>Since the performance of WSDDN is unsatisfactory, we adopt the OICR <ref type="bibr" target="#b33">[34]</ref> and its upgraded version Proposal Cluster Learning (PCL) <ref type="bibr" target="#b32">[33]</ref> to refine the proposal classification results of WSDDN.</p><p>After several times classifier refinement, the classifier tends to select the tight boxes as positive instances, which can be used as pseudo GT annotations for our online boxes regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Task Branch</head><p>After pseudo GT annotations are generated, a multi-task branch can operate fully supervised classification and regression as Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>. The detection branch has two sibling branches. The first branch predicts a discrete probability distribution (per RoI), p ∈ R (C+1)×1 , over C+1 categories, which is computed by a softmax over the C+1 outputs of a FC layer. The second sibling branch outputs bounding-box regression offsets, t c = (t c x , t c y , t c w , t c h ) for each of the C object classes, indexed by c.</p><p>Since we get the instance annotations from MIL branch as introduced in Section 3.2, each RoI now has a GT bounding-box regression target v and GT classification target u. We use a multi-task loss L det of all labeled RoIs for classification and bounding-box regression:</p><formula xml:id="formula_8">L det = L cls + λL loc ,<label>(8)</label></formula><p>where L cls is classification loss, and L loc is regression loss. λ controls the balance between two losses. For L loc , smooth L 1 loss is used. For L cls , since the pseudo GT annotations are noisy, we add a weight w r with respect to RoI r:</p><formula xml:id="formula_9">L cls = − 1 |R| |R| r=1 C+1 c=1 w r u r c log p r c ,<label>(9)</label></formula><p>where |R| is the number of proposals. The weight w r is calculated following the weights calculation method in <ref type="bibr" target="#b33">[34]</ref> when refining the classifiers. - -  - -  -</p><formula xml:id="formula_10">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_11">- - - - - - - - - - - - - - - - - - - 48.0 PCL-</formula><formula xml:id="formula_12">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_13">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_14">MELM[8] - - - - - - - - - - - - - - - - - - - - 42.4 OICR-Ens.+FRCNN[34] - - - - - - - - - - - - - - - - - - - - 42.</formula><formula xml:id="formula_15">- - - - - - - - - - - - - - - - - - - 44</formula><p>.0 PCL-Ens.+FRCNN <ref type="bibr" target="#b32">[33]</ref> 69.0 71. -  The overall network is trained by optimizing the following composite loss functions from the four components using stochastic gradient descent:</p><formula xml:id="formula_16">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_17">L = L img cls + L mil + L ref ine + L det ,<label>(10)</label></formula><p>where L img cls is the multi-label classification loss of GAM; L mil is the multi-label classification loss of WSDDN; L ref ine is the classifier refinement loss; and L det is multitask loss of the detection sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the evaluation datasets and the implementation details of our approach. Then we explore the contributions of each proposed module by the ablation experiments. Finally, we compare the performance of our method with the-state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We evaluate our method on the popular PASCAL VOC 2007 and 2012 datasets <ref type="bibr" target="#b5">[6]</ref> which have 9963 and 22531 im- -  -  - -  </p><formula xml:id="formula_18">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_19">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_20">- - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_21">- - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_22">- - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the object proposals generated by selective search windows <ref type="bibr" target="#b35">[36]</ref> and adopt VGG16 <ref type="bibr" target="#b30">[31]</ref> pre-trained on ImageNet <ref type="bibr" target="#b27">[28]</ref> as the backbone of our proposed network.</p><p>For the newly added layers, the parameters are randomly initialized with a Gaussian distribution N (µ, δ)(µ = 0, δ = 0.01) and 10 times learning rate. During training, we adopt a mini-batch size of 2 images, and set the learning rate to 0.001 for the first 40K iterations and then decrease it to 0.0001 in the following 30K iterations. The momentum and weight decay are set to 0.9 and 0.0005, respectively. We use five image scales , i.e., {480, 576, 688, 864, 1200}, and horizontal flips for both training and testing data augmentation. During testing, we use the mean output of the regression branch, including classificaiton scores and bounding boxes, as the final results. Our experiments are based on the deep learning framework of Caffe <ref type="bibr" target="#b16">[17]</ref>. All of the experiments run on NVIDIA GTX 1080Ti GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We conduct ablation experiments on PASCAL VOC 2007 to prove the effectiveness of our proposed network. We validate the contribution of each component including GAM and regression branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Baseline</head><p>The baseline is the MIL detector without GAM and regression branch that we introduced in Section 3.1, which is the same as OICR <ref type="bibr" target="#b33">[34]</ref>. We re-run the experiment and get a slightly higher result of 41.3% mAP (41.2% mAP in <ref type="bibr" target="#b33">[34]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Guided Attention Module</head><p>To verify the effect of GAM, we conduct experiments with and w/o GAM. We denote the network with GAM as MIL+GAM, which does not include regression branch. From <ref type="table" target="#tab_3">Table 1</ref>, we can conclude that GAM does help the detector learn better features and improves the accuracy of MIL detector by 2.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Joint Optimization</head><p>To optimize proposal classification and regression jointly, we propose to use bounding-box regression in an online manner together with MIL detection. To verify the effect of online regression, we conduct control experiments under two setting: 1) our joint optimization of MIL detector and regressor, which we denote as MIL+REG; 2) we train a MIL detector first, then use the pseudo GT from the MIL detector to train a fully supervised Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>. We denote this setting as MIL+FRCN. The experimental results are summarized in <ref type="table" target="#tab_3">Table 1</ref>. From the results, we can see the performance of our MIL+REG is much higher than MIL+FRCN. We attribute the improvements to joint optimization. Separate optimization of MIL detector and regressor result in sub-optimal results. It easily gets stuck in local minima if the pseudo GTs are not accurate. This can be seen from the results of the object category cat and dog. The two object classes are much easier to over-fit to the discriminate parts in the MIL detection. Our joint optimization strategy can alleviate this problem as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. More visualization results are shown in the supplementary file. We also carry the exploration study on the CorLoc metric, as reported in <ref type="table" target="#tab_4">Table 2</ref>. From these results, we can draw the same conclusion. In <ref type="figure" target="#fig_4">Figure 5</ref>, we show more qualitative results in the same way to supplement <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art</head><p>To fully compare with other methods, we report the results for both "single end-to-end network" and "multiphase approaches or ensemble model". The results on VOC 2007 and VOC 2012 are shown in <ref type="table" target="#tab_8">Table 3</ref>, <ref type="table" target="#tab_17">Table  5</ref>, <ref type="table" target="#tab_14">Table 4 and Table 6</ref>. From the tables, we can see that our method achieves the highest performance, outperforming the state-of-the-arts for both cases. It is worth noting that our single model results are even much better than the ensemble models results of most methods which ensemble the results of multiple CNN networks. For example, compared with OICR <ref type="bibr" target="#b33">[34]</ref>, which we use as baseline, our single model outperforms the ensemble models of OICR significantly while keeping much lower complexity (47.0% mAP Versus 48.6% mAP; 60.6% CorLoc Versus 66.8% CorLoc on VOC 2007). In <ref type="figure" target="#fig_2">Figure 4</ref>, we also illus-trate some detection results by our network as compared to those by our baseline method, i.e., OICR+FRCN. It can be concluded from the illustration that our joint training strategy significantly alleviates the detector focusing on the most discriminative parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>C-WSL <ref type="bibr" target="#b6">[7]</ref> also explored bounding box regression in weakly supervised object detection network. We list the relationship and some differences below. Relationship: We both use bounding box regression in an online manner. However, there are key differences in network architecture between the two, which lead to the performance of C-WSL being much lower than ours, even though they use additional object count labels. Differences: The network structure is different. We use bounding box regression after several box classifier refinements and use only once. C-WSL <ref type="bibr" target="#b6">[7]</ref> uses a box regressor together with each box classifier refinement after the MIL branch. Their structure brings two problems. First, a single MIL branch's classification performance is very poor, it is not wise to directly use the box regressor to refine the box location after the MIL branch. The second problem is that the bounding box regression is used in a cascade manner for each refinement without re-extracting features for the RoIs. Specifically, the subsequent box regression branch should take the refined box locations from the previous box regression branch to update RoIs and re-extracting RoIs features for the classifier and regressor. Because of the above problems, after deducting the improvement of extra label information, their network only improves 1.5% compared with OICR as shown in <ref type="bibr" target="#b6">[7]</ref> while our network has increased by 6% compared with OICR (Please note that we use the same set of code released by the authors of OICR). In addition, <ref type="bibr" target="#b6">[7]</ref> does not solve the problem of local minima. On the two categories that most affected by the local minima problem, <ref type="bibr" target="#b6">[7]</ref> drops 4% in the dog category and improves 3% in the cat category while our method improves 16.3% and 38.6% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel framework for weakly supervised object detection. Different from traditional approaches in this field, our method jointly optimize the MIL detection and regression in an end-to-end manner. Meanwhile, a guided attention module is also added for better feature learning. Experiments show substantial and consistent improvements by our method. Our learning algorithm is potential to be applied in many other weakly supervised visual learning tasks. MIL Detector with Regressor (Ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration=10k Iteration=30k Iteration=70k</head><p>Iteration=10k Iteration=20k Iteration=40k Iteration=10k Iteration=30k Iteration=70k </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Detection results of MIL detector (left part), Fast R-CNN with pseudo GT from MIL detector (middle part) and our jointly training network (right part) at different training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of our proposed network. (1) Generate discriminate features using attention mechanism. (2) Generate the RoI features from enhanced feature map. (3) MIL branch: Feed the extracted RoI features into a MIL network for pseudo GT boxes annotation initialization. (4) Regression branch: Feed the extracted RoI features and generated pseudo GT to the regression branch for RoI classification and regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative detection results of our method and the baseline (OICR+FRCN).The results of baseline are shown in the odd columns. The results of our method are shown in even columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Detection results of MIL detector (left part), Fast R-CNN with pseudo GT from MIL detector (middle part) and our jointly training network (right part) at different training iterations .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pseudo GT Boxes Regressor MIL Detector Fast(er) R-CNN MIL detector Training images Training images Training images Testing images Testing images Testing images Supervision</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>arXiv:1911.12148v1 [cs.CV] 27 Nov 2019</figDesc><table><row><cell>MIL Detector</cell><cell>MIL Detector + Fast R-CNN</cell><cell>MIL Detector with Regressor (Ours)</cell></row><row><cell>Iteration=10k Iteration=30k Iteration=70k</cell><cell>Iteration=10k Iteration=20k Iteration=40k</cell><cell>Iteration=10k Iteration=30k Iteration=70k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP MIL 56.2 62.1 39.4 21.8 10.3 63.6 60.6 31.8 24.8 45.9 35.3 24.1 36.7 63.3 13.1 23.1 39.4 49.1 64.7 60.3 41.3 MIL+GAM 55.2 62.5 42.6 23.0 12.7 66.2 62.0 39.2 26.1 48.9 37.7 26.1 45.3 64.5 12.8 24.4 42.3 46.4 65.9 62.4 43.3 MIL+FRCN 60.2 65.0 50.9 24.9 11.9 71.6 68.0 34.6 27.2 61.2 40.8 17.6 47.1 65.6 13.0 22.8 51.0 57.6 66.5 60.5 45.9 MIL+REG 56.5 63.4 38.8 28.3 15.3 68.2 66.6 68.0 23.7 51.6 46.0 32.4 53.8 63.9 12.1 23.5 47.2 56.3 65.2 64.9 47.3 MIL+GAM+REG 55.2 66.5 40.1 31.1 16.9 69.8 64.3 67.8 27.8 52.9 47.0 33.0 60.8 64.4 13.8 26.0 44.0 55.7 68.9 65.5 48.6 Ablation study: AP performance (%) on PASCAL VOC 2007 test Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean MIL 82.5 76.5 61.0 47.3 30.2 80.7 82.4 44.8 42.1 78.1 45.2 32.8 57.1 89.6 16.6 57.9 73.2 61.8 79.1 73.5 60.6 MIL+GAM 82.1 78.4 64.3 48.9 32.4 81.2 82.9 48.5 43.4 79.5 43.7 34.9 61.9 89.2 16.6 57.5 71.1 56.2 78.7 77.4 61.4 MIL+FRCN 83.8 81.2 65.2 48.4 34.4 84.3 84.6 49.4 44.8 82.9 48.7 37.7 67.0 90.0 21.4 60.1 76.3 66.4 82.5 80.6 64.5 MIL+REG 82.1 79.2 61.6 52.7 33.2 82.7 85.8 77.3 39.2 82.2 47.5 42.3 75.2 92.0 19.3 58.6 79.4 65.6 77.2 83.9 65.8 MIL+GAM+REG 81.7 81.2 58.9 54.3 37.8 83.2 86.2 77.0 42.1 83.6 51.3 44.9 78.2 90.8 20.5 56.8 74.2 66.1 81.0 86.0 66.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablation study: CorLoc performance (%) on PASCAL VOC 2007 trainval</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP WSDDN[1] 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8 ContextLocNet[19] 57.1 52.0 31.5 7.6 11.5 55.0 53.1 34.1 1.7 33.1 49.2 42.0 47.3 56.6 15.3 12.8 24.8 48.9 44.4 47.8 36.3 OICR[34] 58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2 Self-taught[18] 52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8 6.6 18.8 51.9 43.6 53.6 62.4 41.7 WCCN[5] 49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9 64.1 13.8 23.5 45.9 54.1 60.8 54.5 42.8 TS2C[39] 59.3 57.5 43.7 27.3 13.5 63.9 61.7 59.9 24.1 46.9 36.7 45.6 39.9 62.6 10.3 23.6 41.7 52.4 58.7 56.6 44.</figDesc><table><row><cell></cell><cell></cell><cell>3</cell></row><row><cell>WSRPN[35]</cell><cell>57.9 70.5 37.8 5.7 21.0 66.1 69.2 59.4 3.4 57.1 57.3 35.2 64.2 68.6</cell><cell>32.8 28.6 50.8 49.5 41.1 30.0 45.3</cell></row><row><cell>PCL[33]</cell><cell>54.4 69.0 39.3 19.2 15.7 62.9 64.4 30.0 25.1 52.5 44.4 19.6 39.3 67.7</cell><cell>17.8 22.9 46.6 57.5 58.6 63.0 43.5</cell></row><row><cell cols="2">MIL-OICR+GAM+REG(Ours) 55.2 66.5 40.1 31.1 16.9 69.8 64.3 67.8 27.8 52.9 47.0 33.0 60.8 64.4</cell><cell>13.8 26.0 44.0 55.7 68.9 65.5 48.6</cell></row><row><cell cols="2">MIL-PCL+GAM+REG(Ours) 57.6 70.8 50.7 28.3 27.2 72.5 69.1 65.0 26.9 64.5 47.4 47.7 53.5 66.9</cell><cell>13.7 29.3 56.0 54.9 63.4 65.2 51.5</cell></row><row><cell>PDA[22]</cell><cell>54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8</cell><cell>16.2 29.9 40.7 15.9 55.3 40.2 39.5</cell></row><row><cell>WSDDN-Ens.[1]</cell><cell>46.4 58.3 35.5 25.9 14.0 66.7 53.0 39.2 8.9 41.8 26.6 38.6 44.7 59.0</cell><cell>10.8 17.3 40.7 49.6 56.9 50.8 39.3</cell></row><row><cell>OICR-Ens.+FRCNN[34]</cell><cell>65.5 67.2 47.2 21.6 22.1 68.0 68.5 35.9 5.7 63.1 49.5 30.3 64.7 66.1</cell><cell>13.0 25.6 50.0 57.1 60.2 59.0 47.0</cell></row><row><cell>WCCN+FRCNN[5]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>tv mAP</cell></row></table><note>Comparison of AP performance (%) on PASCAL VOC 2007 test. The upper part shows results by single end-to-end model. The lower part shows results by multi-phase approaches or ensemble model.Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 4 :</head><label>4</label><figDesc>Comparison of AP performance (%) on PASCAL VOC 2012 test. The upper part shows results by single end-to-end model. The lower part shows results by multi-phase approaches or ensemble model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP WSDDN[1] 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5 ContextLocNet[19] 83.3 68.6 54.7 23.4 18.3 73.6 74.1 54.1 8.6 65.1 47.1 59.5 67.0 83.5 35.3 39.9 67.0 49.7 63.5 65.2 55.1 OICR[34] 81.7 80.4 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5 88.8 21.8 57.9 76.3 59.9 75.3 81.4 60.6 Self-taught[18] 72.7 55.3 53.0 27.8 35.2 68.6 81.9 60.7 11.6 71.6 29.7 54.3 64.3 88.2 22.2 53.7 72.2 52.6 68.9 75.5 56.1 WCCN[5] 83.9 72.8 64.5 44.1 40.1 65.7 82.5 58.9 33.7 72.5 25.6 53.7 67.4 77.4 26.8 49.1 68.1 27.9 64.5 55.7 56.7 TS2C[39] 84.2 74.1 61.3 52.1 32.1 76.7 82.9 66.6 42.3 70.6 39.5 57.0 61.</figDesc><table><row><cell></cell><cell>2 88.4</cell><cell>9.3</cell><cell>54.6 72.2 60.0 65.0 70.3 61.0</cell></row><row><cell>WSRPN[35]</cell><cell>77.5 81.2 55.3 19.7 44.3 80.2 86.6 69.5 10.1 87.7 68.4 52.1 84.4 91.6</cell><cell cols="2">57.4 63.4 77.3 58.1 57.0 53.8 63.8</cell></row><row><cell>PCL[33]</cell><cell>79.6 85.5 62.2 47.9 37.0 83.8 83.4 43.0 38.3 80.1 50.6 30.9 57.8 90.8</cell><cell cols="2">27.0 58.2 75.3 68.5 75.7 78.9 62.7</cell></row><row><cell cols="2">MIL-OICR+GAM+REG(Ours) 81.7 81.2 58.9 54.3 37.8 83.2 86.2 77.0 42.1 83.6 51.3 44.9 78.2 90.8</cell><cell cols="2">20.5 56.8 74.2 66.1 81.0 86.0 66.8</cell></row><row><cell cols="2">MIL-PCL+GAM+REG(Ours) 80.0 83.9 74.2 53.2 48.5 82.7 86.2 69.5 39.3 82.9 53.6 61.4 72.4 91.2</cell><cell cols="2">22.4 57.5 83.5 64.8 75.7 77.1 68.0</cell></row><row><cell>PDA [22]</cell><cell>78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1 73.5</cell><cell cols="2">21.4 47.4 64.6 22.3 60.9 52.3 52.4</cell></row><row><cell>WSDDN-Ens. [1]</cell><cell>68.9 68.7 65.2 42.5 40.6 72.6 75.2 53.7 29.7 68.1 33.5 45.6 65.9 86.1</cell><cell cols="2">27.5 44.9 76.0 62.4 66.3 66.8 58.0</cell></row><row><cell>OICR-Ens.+FRCNN [34]</cell><cell>85.8 82.7 62.8 45.2 43.5 84.8 87.0 46.8 15.7 82.2 51.0 45.6 83.7 91.2</cell><cell cols="2">22.2 59.7 75.3 65.1 76.8 78.1 64.3</cell></row><row><cell>GAL-fWSD [30]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="3">aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</cell></row><row><cell>ContextLocNet[19]</cell><cell>78.3 70.8 52.5 34.7 36.6 80.0 58.7 38.6 27.7 71.2 32.3 48.7 76.2 77.4</cell><cell cols="2">16.0 48.4 69.9 47.5 66.9 62.9 54.8</cell></row><row><cell>OICR[34]</cell><cell>86.2 84.2 68.7 55.4 46.5 82.8 74.9 32.2 46.7 82.8 42.9 41.0 68.1 89.6</cell><cell>9.2</cell><cell>53.9 81.0 52.9 59.5 83.2 62.1</cell></row><row><cell>Self-taught[18]</cell><cell>82.4 68.1 54.5 38.9 35.9 84.7 73.1 4.8 17.1 78.3 22.5 57.0 70.8 86.6</cell><cell cols="2">18.7 49.7 80.7 45.3 70.1 77.3 58.8</cell></row><row><cell>TS2C[39]</cell><cell>79.1 83.9 64.6 50.6 37.8 87.4 74.0 74.1 40.4 80.6 42.6 53.6 66.5 88.8</cell><cell cols="2">18.8 54.9 80.4 60.4 70.7 79.3 64.4</cell></row><row><cell>WSRPN[35]</cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison of correct localization (CorLoc) (%) on PASCAL VOC 2007 trainval. The upper part shows results by single end-to-end model. The lower part shows results by multi-phase approaches or ensemble model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 6 :</head><label>6</label><figDesc>Comparison of correct localization (CorLoc) (%) on PASCAL VOC 2012 trainval. The upper part shows results by single end-to-end model. The lower part shows results by multi-phase approaches or ensemble model.</figDesc><table><row><cell>ages for 20 object classes, respectively. These two datasets</cell></row><row><cell>are split into train, validation, and test sets. We use the</cell></row><row><cell>trainval set (5011 images for 2007 and 11540 for 2012) for</cell></row><row><cell>training. As we focus on weakly supervised detection, only</cell></row><row><cell>image-level labels are utilized during training. Average Pre-</cell></row><row><cell>cision (AP) and the mean of AP (mAP) are taken as the</cell></row><row><cell>evaluation metrics to test our model on the testing set. Cor-</cell></row><row><cell>rect localization (CorLoc) is also used to evaluate our model</cell></row><row><cell>on the trainval set to measure the localization accuracy [1].</cell></row><row><cell>Both metrics are evaluated on the PASCAL criteria, i.e., IoU</cell></row><row><cell>&gt; 0.5 between ground truths boxes and predicted boxes.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multifold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Mohammad</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">C-wsl: Count-guided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2883" to="2891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4700" to="4719" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised localization of novel objects using appearance transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4315" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Michaël Mathieu, Rob Fergus, and Yann LeCun</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative adversarial learning towards fast weakly supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PCL: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ts2c: tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="454" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4262" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ml-locnet: Improving object localization with multi-view learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="240" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">W2f: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2027" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
