<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentai</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Brain tumor segmentation</term>
					<term>magnetic resonance imaging</term>
					<term>class imbalance</term>
					<term>convolutional neural networks</term>
					<term>multi- task learning</term>
					<term>channel attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy, a popular scheme, significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws in the MC approach, we propose in this paper a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation for brain tumor segmentation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks, which enables us to design a cross-task guided attention (CGA) module. By following the guidance of the prediction results provided by the previous task, CGA can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results of the proposed attention network. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy, a popular scheme, significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws in the MC approach, we propose in this paper a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation for brain tumor segmentation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks, which enables us to design a cross-task guided attention (CGA) module. By following the guidance of the prediction results provided by the previous task, CGA can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results of the proposed attention network. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.</p><p>Index Terms-Brain tumor segmentation, magnetic resonance imaging, class imbalance, convolutional neural networks, multitask learning, channel attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>B RAIN tumors are one of the most deadly cancers worldwide. Among these tumors, glioma is the most common type <ref type="bibr" target="#b0">[1]</ref>. The average survival time for glioblastoma patients is less than 14 months <ref type="bibr" target="#b1">[2]</ref>. Timely diagnosis of brain tumors is thus vital to ensuring appropriate treatment planning, surgery, and follow-up visits <ref type="bibr" target="#b2">[3]</ref>. As a popular non-invasive technique, Magnetic Resonance Imaging (MRI) produces markedly different types of tissue contrast and has thus been widely used by radiologists to diagnose brain tumors <ref type="bibr" target="#b3">[4]</ref>. However, the manual segmentation of brain tumors from MRI images is both subjective and time-consuming <ref type="bibr" target="#b4">[5]</ref>. Therefore, it is highly desirable to design automatic and robust brain tumor segmentation tools.</p><p>Recently, deep learning-based methods such as convolutional neural networks (CNNs) <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b17">[18]</ref>, have become increasingly popular and achieved significant progress in brain tumor segmentation tasks. Unfortunately, a severe class imbalance problem usually emerges between healthy tissue and tumor tissue, as well as between intra-tumoral classes. This problem causes the healthy tissue to be dominant during the training phase and degrades the optimization quality of the model. To handle the class imbalance problem, many recent studies have employed the Model Cascade (MC) strategy <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b26">[27]</ref>. More specifically, the MC strategy decomposes medical image segmentation into two or more tasks, each of which is achieved by an individual model. The most common MC framework for segmentation tasks <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b24">[25]</ref> incorporates two models, where the first one detects regions of interest (ROIs) via coarse segmentation, while the second conducts fine segmentation within the ROIs. Therefore, MC can effectively alleviate class imbalance via coarse-to-fine segmentation. In spite of its effectiveness, however, MC has several disadvantages. First, it usually requires multiple deep models to be trained, which substantially increases both the system complexity and the storage space consumption. Second, each model is trained separately using its own training data, which ignores the correlation between the deep models. Third, MC runs the deep models one by one, which leads to alternate GPU-CPU computations and a lack of online interactions between tasks.</p><p>Here, we propose adopting multi-task learning to overcome the shortcomings of MC. In more detail, we aim to decompose multi-class brain tumor segmentation into three separate yet interconnected tasks. While MC trains one individual network for each task, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we incorporate the three tasks into a single model and propose a One-pass Multi-task Network (OM-Net). The proposed OM-Net not only makes use of the relevance of these tasks to each other during the training stage, but also simplifies the prediction stage by implementing one-pass computation, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Furthermore, an effective training scheme inspired by curriculum learning is also designed: instead of training the three tasks together all the time, we gradually add the tasks to OM-Net in an increasing order of difficulty; this is beneficial for improving the convergence quality of the model. Moreover, the fact that OM-Net integrates three tasks provides the possibility of online interaction between these tasks, which produces more benefits. First, the online training data transfer strategy we propose here enables the three tasks to share training data. Therefore, certain tasks obtain more training data, and the overall optimization quality can be improved. Second, we construct a novel channel attention module, named Cross-task Guided Attention (CGA), by sharing prediction results between tasks. In CGA, the prediction results of a preceding task can guide the following task to obtain categoryspecific statistics for each channel beforehand. This categoryspecific information further enables CGA to predict channelwise dependencies with regard to a specific category of voxels. By contrast, existing self-attention models such as the popular 'squeeze &amp; excitation' (SE) block <ref type="bibr" target="#b27">[28]</ref>, do not make use of such external guidance. Without external guidance, SE blocks only predict a single weight for each channel. However, there are usually multiple categories in a patch, and the importance of each channel varies for different categories. The proposed CGA module handles this problem by predicting the categoryspecific channel attention.</p><p>To further refine the segmentation results of OM-Net, we also propose a new post-processing scheme. The efficacy of the proposed methods is systematically evaluated on three popular brain tumor segmentation datasets, namely BraTS 2015, 2017, and 2018. Experimental results indicate that OM-Net outperforms MC, despite having only one-third of the model parameters of MC. The CGA module further promotes the performance of OM-Net by a significant margin.</p><p>A preliminary version of this paper has previously been published in <ref type="bibr" target="#b28">[29]</ref>. Compared with the conference version, this version proposes the novel CGA module, improves the postprocessing method, and includes more experimental investigation. The remainder of this paper is organized as follows. We briefly review the related works for brain tumor segmentation in Section II, then provide the details of the OM-Net model in Section III. Experimental settings and datasets are detailed in Section IV, while the experimental results and analysis are presented in Section V. Finally, we conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we briefly review approaches in two domains related to our proposed model: namely, brain tumor segmentation and attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brain Tumor Segmentation</head><p>In recent years, deep learning-based methods such as CNNs have dominated the field of automatic brain tumor segmentation. The architectures of deep models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b17">[18]</ref> have developed rapidly from single-label prediction (classifying only the central voxel of the input patch) to dense-prediction (making predictions for all voxels in the input patch simultaneously). For instance, Pereira et al. <ref type="bibr" target="#b4">[5]</ref> designed a deep model equipped with small convolutional kernels to classify the central voxel of the input 2D patch. Moreover, Havaei et al. <ref type="bibr" target="#b5">[6]</ref> introduced a novel 2D two-pathway deep model to explore additional contextual information. The above methods make predictions based on 2D patches, ignoring the 3D contextual information. To handle this problem, Kamnitsas et al. <ref type="bibr" target="#b6">[7]</ref> introduced the DeepMedic model, which extracts information from 3D patches using 3D convolutional kernels. The abovementioned methods make predictions only for a single voxel or a set of central voxels within the input patch, meaning that they are slow in the inference stage. To promote efficiency, encoderdecoder architectures such as fully convolutional networks (FCNs) <ref type="bibr" target="#b7">[8]</ref> and U-Net <ref type="bibr" target="#b8">[9]</ref> have been widely adopted to realize dense prediction. For instance, Chen et al. <ref type="bibr" target="#b9">[10]</ref> designed a voxelwise residual network (VoxResNet) to make predictions for all voxels within the input 3D patch. Zhao et al. <ref type="bibr" target="#b10">[11]</ref> introduced a unified framework integrating FCNs and conditional random fields (CRFs) <ref type="bibr" target="#b29">[30]</ref>. This framework realizes end-toend dense prediction with appearance and spatial consistency.</p><p>The issue of class imbalance is commonly encountered in medical image segmentation, especially brain tumor segmentation. To address this problem, many recent studies have adopted the MC strategy <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b26">[27]</ref> to perform coarse-to-fine segmentation. In particular, the common two-model cascaded framework has been widely adopted in many applications, including renal segmentation in dynamic contrast-enhanced MRI (DCE-MRI) images <ref type="bibr" target="#b18">[19]</ref>, cancer cell detection in phase contrast microscopy images <ref type="bibr" target="#b19">[20]</ref>, liver and lesion segmentation <ref type="bibr" target="#b20">[21]</ref>, volumetric pancreas segmentation in CT images <ref type="bibr" target="#b21">[22]</ref>, calcium scoring in low-dose chest CT images <ref type="bibr" target="#b22">[23]</ref>, etc. Moreover, MC can incorporate more stages to achieve better segmentation performance. For instance, Wang et al. <ref type="bibr" target="#b26">[27]</ref> divided the brain tumor segmentation into three successive binary segmentation problems: namely, the segmentation of the complete tumor, tumor core, and enhancing tumor areas in MRI images. Since MC effectively alleviates class imbalance, its results are very encouraging.</p><p>Despite its effectiveness, however, MC is cumbersome in terms of system complexity; moreover, it ignores the correlation among tasks. Accordingly, in this paper, we adopt multitask learning to overcome the disadvantages inherent in MC. By implementing the sharing of model parameters and training data, our proposed OM-Net outperforms MC using only onethird of the model parameters of MC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Attention is a popular tool in deep learning that highlights useful information in feature maps while suppressing irrelevant information. The majority of existing studies <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b35">[36]</ref> belong to the category of self-attention models, meaning that they infer attentions based only on feature maps. These models can be roughly categorized into three types, namely hard regional attention, soft spatial attention, and channel attention. Moreover, some studies combine two or more types of attention in one unified model <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. The spatial transformer network (STN) <ref type="bibr" target="#b30">[31]</ref> is a representative example of a hard attention model. STN selects and reshapes important regions in feature maps to a canonical pose to simplify inference. STN performs at the coarse region-level while neglecting the fine pixel-level saliency <ref type="bibr" target="#b31">[32]</ref>. In comparison, soft spatial attention models aim to evaluate pixel-wise importance in the spatial dimension. For instance, Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed a residual attention learning method that adds soft weights to feature maps via a residual unit, in order to refine the feature maps.</p><p>Complementary to spatial attention, channel attention aims to recalibrate channel-wise feature responses. The 'squeeze &amp; excitation' (SE) block <ref type="bibr" target="#b27">[28]</ref> is one of the most popular channel attention models due to its simplicity and efficiency. However, this model was originally proposed for image classification and object detection tasks, but may not be optimal for image segmentation tasks; this is because SE blocks are based on the average response of all voxels in each channel and recalibrate each channel with a single weight, regardless of which category the voxels belong to. However, there are usually multiple categories of voxels in one patch, and the importance of each channel varies between the different categories. Several existing studies have already tried to alleviate the abovementioned problems experienced by SE blocks during segmentation tasks. For example, Pereira et al. <ref type="bibr" target="#b34">[35]</ref> designed a segmentation SE (SegSE) block that produces a channel descriptor for each voxel in feature maps; consequently, the obtained channel attention map is of the same size as the feature maps. Voxelwise multiplication between the feature maps and the attention map produces the re-weighted features.</p><p>Furthermore, the proposed CGA module also aims to solve the problems of SE blocks for segmentation. Unlike the existing self-attention models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b35">[36]</ref>, we make use of the special structure of OM-Net to provide cross-task guidance for the learning of category-specific channel attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we first present a strong segmentation baseline based on MC, then introduce the model structure and training strategy of OM-Net. Next, we further explain the principles of OM-Net from the perspective of the attention mechanism, and subsequently propose the CGA module that promotes the performance of OM-Net by predicting robust channel attention. Finally, we propose a simple but effective post-processing method in order to refine the segmentation results of the attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Strong Baseline Based on Model Cascade</head><p>According to <ref type="bibr" target="#b2">[3]</ref>, tumors can be divided into the following classes: edema (ED), necrotic (NCR), non-enhancing tumor (NET), and enhancing tumor (ET). Following <ref type="bibr" target="#b36">[37]</ref>, we consistently merge NCR and NET into one class. Performance is evaluated on three re-defined tumor regions: complete tumor (including all tumor classes), tumor core (including all tumor classes except edema), and enhancing tumor (including only the enhancing tumor class). Under this definition, three regions satisfy the hierarchical structure of tumor subregions, each of which completely covers the subsequent one. Based on this observation, brain tumor segmentation can be decomposed into three separate yet interconnected tasks. In the following, we design an MC model that includes three independent networks and forms a strong baseline for OM-Net. Each network is trained for a specific task. These three tasks are detailed below.</p><p>1) Coarse segmentation of the complete tumor. We utilize the first network to detect the complete tumor region as an ROI. We randomly sample training patches within the brain and train the network as a five-class segmentation task: three tumor classes, normal tissue, and background. In the testing stage, we simply sum the predicted probabilities of all tumor classes to obtain a coarse tumor mask. 2) Refined segmentation for the complete tumor and its intra-tumoral classes. We dilate the above coarse tumor mask by 5 voxels in order to reduce false negatives. Next, the labels of all voxels in the dilated region are predicted again as a five-class segmentation task by the second network. Training data are sampled randomly within the dilated ground-truth complete tumor area. 3) Precise segmentation for the enhancing tumor. Due to the extreme class imbalance, it is very difficult to conduct precise segmentation of the enhancing tumor. To handle this problem, a third network is introduced specially for the segmentation of enhancing tumor. Similarly, the training patches for this task are randomly sampled within the ground-truth tumor core area. It should be noted here that training patches for the three tasks are sampled independently, even if the sampling areas for the three tasks are nested hierarchically.</p><p>The network structures for the above three tasks are the same except for the final classification layer. The adopted structure is a 3D variant of the FusionNet <ref type="bibr" target="#b37">[38]</ref>, as shown in <ref type="figure">Fig. 2</ref>. We crop the MRI images to patches of size 32 × 32 × 16 × 4 voxels as input for the network. Here, the first three numbers correspond to the input volume, while the last number (4) denotes the four MRI modalities: FLAIR, T1-weighted (T1), T1 with gadolinium enhancing contrast (T1c), and T2-weighted (T2). Due to the lack of contextual information, the segmentation results of the boundary voxels in the patch may be inaccurate. Accordingly, we adopt the overlap-tile strategy proposed in <ref type="bibr" target="#b8">[9]</ref> during inference. In brief, we sample 3D patches with a stride of 20 × 20 × 5 voxels within the MRI image. For each patch, we retain only the predictions of voxels within the central region (20 × 20 × 5 voxels) and abandon the predictions of the boundary voxels. The predictions of all central regions from the sampled patches are stitched together to constitute the segmentation results of the whole brain. This strategy is also utilized in the following  <ref type="figure">Fig. 2</ref>. The network structure for each task, which is composed of five basic building blocks. Each block is represented by a different type of colored cube. The number below each cube refers to the number of feature maps. C equals to 5, 5, and 2 for the first, second, and third task respectively. SoftmaxWithLoss is adopted as the loss function and applied to the output of each task. (Best viewed in color) models.</p><p>During the inference stage of MC, the three networks have to be run one by one since the ROI of one task is obtained by considering the results of all preceding tasks. More specifically, we employ the first network to generate a coarse mask for the complete tumor; subsequently, all voxels within the dilated area of the mask are classified by the second network, from which the precise complete tumor and tumor core areas can be obtained. Finally, the third network is utilized to scan all voxels in the tumor core region in order to determine the precise enhancing tumor area. Thus, there are three alternate GPU-CPU computations carried out during the MC inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. One-pass Multi-task Network</head><p>Despite its promising performance, MC not only encounters disadvantages due to its system complexity, but also neglects the relevance among tasks. We can observe that the essential difference among these tasks lies in the training data rather than the model architecture. Therefore, we propose a multitask learning model that integrates the three tasks involved in MC into one network. Each task in this model has its own training data that is exactly the same as the data in MC. Moreover, each task has an independent convolutional layer, a classification layer, and a loss layer. The other parameters are shared to make use of the correlation among the tasks. Thanks to the multi-task model, we can obtain the prediction results of the three classifiers simultaneously through one-pass computation. As a result, we name the proposed model the One-pass Multi-task Network, or OM-Net.</p><p>As the difficulty levels of these three tasks progressively increase, we propose to train OM-Net more effectively by employing curriculum learning <ref type="bibr" target="#b38">[39]</ref>, which is useful for improving the convergence quality of machine learning models. More specifically, instead of training the three tasks together all the time, we gradually introduce the tasks to the model in an order of increasing difficulty. The model structure and training strategy of OM-Net are illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. First, OM-Net is trained with only the first task in order to learn the basic knowledge required to differentiate between tumors and normal tissue. This training process lasts until the loss curve displays a flattening trend.</p><p>The second task is then added to OM-Net; this means that the first and the second tasks are trained together. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, we concatenate Data-1 and Data-2 along the batch dimension to form the input for OM-Net. We split the features generated by the shared backbone model along the batch dimension; here, the splitting position in the batch dimension is the same as the concatenation position of training data. We then obtain task-specific features and use the sliced features to optimize task-specific parameters. Moreover, we argue that not only knowledge (model parameters) but also learning material (training data) can be transferred from the easier course (task) to the more difficult course (task) during curriculum learning. Since the sampling areas for the three tasks are hierarchically nested, we propose the following online training data transfer strategy. The training patches in Data-1 that satisfy the following sampling condition can be transferred to assist the training of the second task:</p><formula xml:id="formula_0">N i=1 1 {l i ∈ C complete } N ≥ 0.4,<label>(1)</label></formula><p>where l i is the label of the i-th voxel in the patch, C complete denotes the set of all tumor classes, N is the number of voxels in the input patch, and 0.4 is set to meet the patch sampling condition of the second task. We thus concatenate the features of these patches in Data-1 with Feature-2, then compute the loss for the second task. The training process in this step continues until the loss curve of the second task displays a flattening trend. Finally, the third task and its training data are introduced to OM-Net, meaning that these three tasks are trained together. The concatenation and slicing operations are similar to those in the second step. The training patches from Data-1 and Data-2 that satisfy the following sampling condition can be transferred Output-1 <ref type="bibr">32 5</ref> Classifier-1 <ref type="bibr">32 5</ref> Output-2  to the third task:</p><formula xml:id="formula_1">N i=1 1 {l i ∈ C core } N ≥ 0.5,<label>(2)</label></formula><p>where C core indicates the tumor classes belonging to the tumor core. Similarly, 0.5 is chosen to meet the patch sampling condition of the third task. The threshold in Ineq. <ref type="formula" target="#formula_0">(1)</ref> is smaller than that in Ineq. <ref type="bibr" target="#b1">(2)</ref>; this is because the center points of training patches for the second task are sampled within the dilated area of the complete tumor. By comparison, the center points of training patches for the third task are sampled within the ground-truth area of the tumor core. This means that one training patch for the second task may include more than 50% non-tumor voxels, while one training patch for the third task includes at most 50% non-core voxels. Therefore, we set the thresholds in Ineq. (1) and Ineq.</p><p>(2) to 0.4 and 0.5, respectively. The three tasks are trained together until convergence occurs. In conclusion, the OM-Net equipped with the curriculum learning-based training strategy has three main components: 1) a deep model based on multi-task learning that realizes coarse-to-fine segmentation via one-pass computation; 2) a stepwise training scheme that progresses from easy to difficult; 3) the transfer of training data from the easier task to the more difficult task.</p><p>In the inference stage, the data concatenation, feature slicing, and data transfer operations in <ref type="figure" target="#fig_2">Fig. 3</ref> are removed, and the 3D patches of an MRI image are fed into the shared backbone model. Feature-1, Feature-2, and Feature-3 are now the same for each patch. The prediction results of the three tasks can be obtained simultaneously by OM-Net. These results are fused in exactly the same way as in the MC baseline. Moreover, OM-Net is different from the existing multi-task learning models for brain tumor segmentation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The principle behind these models <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> involves the provision of multiple supervisions for the same training data. By contrast, OM-Net aims to achieve coarse-to-fine segmentation by integrating tasks with their own training data into a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-task Guided Attention</head><p>The coarse-to-fine segmentation strategy adopted by OM-Net can be regarded as a type of cascaded spatial attention, since the segmentation results of one task determine the ROI for the following task. In the following, we further enhance the performance of OM-Net from the perspective of channel attention. In particular, we propose a novel and effective channel attention model that makes use of cross-task guidance to solve the problems experienced by the popular SE block for the segmentation task.</p><p>As explained in Section II, the global average pooling (GAP) operation in the SE block ignores the dramatic variation in volume of each class within the input patch. We solve this problem by computing statistics in category-specific regions rather than in a whole patch. However, the category-specific regions for common CNNs are unknown until we reach the final classification layer; therefore, this is a chicken-and-egg problem. Fortunately, OM-Net allows us to estimate categoryspecific regions beforehand by sharing the prediction results between tasks. More specifically, in the training stage, we let Feature-2 and Feature-3 (shown in <ref type="figure" target="#fig_2">Fig. 3</ref>) pass through the first and second task of OM-Net, respectively. In this way, we obtain the coarse segmentation results for the second and third tasks respectively. It is worth noting that this strategy introduces only negligible additional computation in the training stage and no extra computation in the testing stage. This is because Feature-1, Feature-2, and Feature-3 are exactly the same in the testing stage, as the concatenation and slicing operations in <ref type="figure" target="#fig_2">Fig. 3</ref> are removed during testing. Since we introduce cross-task guidance for the proposed channel attention block, we refer to it as Cross-task Guided Attention (CGA). We also rename Classifier-2 and Classifier-3 in <ref type="figure" target="#fig_2">Fig.  3</ref> once equipped with CGA as CGA-tumor and CGA-core, respectively. The overall architecture of OM-Net equipped with CGA modules is illustrated in <ref type="figure">Fig. 4</ref>. Note that cross-task guidance takes place only in the forward pass, meaning that the back-propagations of the three tasks are still independent.</p><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, we take CGA-tumor as an example to illustrate the structure of the CGA module. This module  <ref type="figure">Fig. 4</ref>. The overall architecture of OM-Net equipped with CGA modules in the training stage. D-i, F-i, and O-i are abbreviations for Data-i, Featurei, and Output-i, which denote the training data, features, and outputs for the i-th task, respectively. Feature-2 and Feature-3 pass through the first and second task respectively to obtain their own coarse prediction results beforehand. We refer to these two operations as cross-task forward flow for Data-2 and Data-3, represented by the green and red dotted lines respectively. These coarse predictions are utilized as cross-task guidance to help generate category-specific channel attention. SoftmaxWithLoss is adopted as the loss function and applied to the output of each task. The training data transfer strategy described in <ref type="figure" target="#fig_2">Fig. 3</ref> is omitted for clarity in this figure.</p><p>is composed of two blocks: (a) a category-specific channel importance (CSCI) block, and (b) a complementary segmentation (CompSeg) block. In <ref type="figure" target="#fig_4">Fig. 5</ref>, all 4D feature maps and probability maps are simplified into 3D cubes to enable better visualization. In more detail, the height of one cube denotes the number of channels in feature maps. P ∈ R W ×H×L×5 denotes a probability tensor predicted by the preceding task and is represented as a pink cube. The number 5 next to the cube for P refers to the number of classes, which we have explained in the MC baseline. The grey cube F ∈ R W ×H×L×32 denotes the input feature maps of the current task for the CGA module. Moreover, the number 32 next to the cube for F denotes its number of channels. 1) CSCI Block: As illustrated in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, the CSCI block utilizes both P estimated by the preceding task and F to estimate the importance of each channel for the segmentation of tumor and non-tumor categories, respectively. More specifically, we first compute P t and P n ; each of these values refers to the probability of one voxel belonging to the tumor and non-tumor categories, respectively:</p><formula xml:id="formula_2">P t (i, j, k) = c∈Ctumor P(i, j, k, c),<label>(3)</label></formula><formula xml:id="formula_3">P n (i, j, k) = c∈Cnon−tumor P(i, j, k, c),<label>(4)</label></formula><p>where {P t , P n } ∈ R W ×H×L×1 . C tumor and C non−tumor refer to the sets of classes that belong to the tumor and non-tumor categories, respectively. C tumor includes all tumor classes, while C non−tumor contains normal tissue and background. We then reshape P t and P n to R N ×1 , respectively, where N is equal to W × H × L and denotes the number of voxels in a patch. Similarly, F is reshaped to R N ×32 . Subsequently, we perform matrix multiplication between the reshaped F and the reshaped P t , then apply L 1 normalization for the obtained vector:</p><formula xml:id="formula_4">m t (i) = Re(F) T i,: · Re(P t ) 32 k=1 Re(F) T k,: · Re(P t ) ,<label>(5)</label></formula><p>where m t ∈ R 32×1 , while Re(·) denotes the reshape operation. Similarly,</p><formula xml:id="formula_5">m n (i) = Re(F) T i,: · Re(P n ) 32 k=1</formula><p>Re(F) T k,: · Re(P n )</p><p>.</p><p>Elements in m t and m n describe the importance of each channel for the segmentation of tumor and non-tumor categories, respectively. Compared with the popular SE block, which squeezes the global information of each channel into a single value in order to describe its importance, CGA makes use of finer category-specific statistics to evaluate the importance of each channel for one specific category.</p><p>2) CompSeg Block: Inspired by <ref type="bibr" target="#b41">[42]</ref>, we further propose a complementary segmentation (CompSeg) block that performs segmentation via two complementary pathways. These two pathways can make full use of the category-specific channel importance information (m t and m n ) to improve segmentation performance. As shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, the two pathways focus on the segmentation of the tumor and non-tumor voxels respectively. The CompSeg block can be described in more detail as follows. First, m t and m n are used to recalibrate each channel in F, respectively:</p><formula xml:id="formula_7">U t = F scale (m t , F) = m 1 t f 1 , m 2 t f 2 , · · ·, m 32 t f 32 ,<label>(7)</label></formula><formula xml:id="formula_8">U n = F scale (m n , F) = m 1 n f 1 , m 2 n f 2 , · · ·, m 32 n f 32 , (8) where f i ∈ R W ×H×L is the i-th channel in F. m i t and m i n</formula><p>are the i-th elements in m t and m n , respectively. The recalibrated feature maps U t and U n highlight the more important channels and suppress the less important ones for tumors and non-tumors, respectively. These maps are then individually fed into a 1 × 1 × 1 convolutional classification layer to produce their own score maps, S t and S n . {S t , S n } ∈ R W ×H×L×C , where C refers to the number of classes for the current task.</p><p>The two score maps are more sensitive to tumor and nontumor classes respectively. Therefore, we merge the two score maps via weighted averaging:</p><formula xml:id="formula_9">S(i, j, k, c) = P t (i, j, k)·S t (i, j, k, c)+P n (i, j, k)·S n (i, j, k, c),<label>(9)</label></formula><p>whereS ∈ R W ×H×L×C . Finally, we feedS into another 1 × 1 × 1 convolutional layer to obtain the ultimate prediction results S for the current task.</p><p>As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, P t and P n are used twice in the CGA module. The first time, we use P t and P n in the CSCI block to provide category-specific probabilities, by which we calculate m t and m n ; these two vectors embed the interdependencies between channels with regard to different categories. The second time, we use them in the CompSeg block as soft spatial masks to merge two score maps by means of weighted averaging and produce the final segmentation results. Because all tasks are integrated in OM-Net, we are able to obtain P t and P n for use as cross-task guidance to compute category-specific statistics for each channel, thereby obtaining improved channel attentions. By comparison, the popular SE block ignores category-specific statistics and reweights each channel with a single weight. In the experiment section, we justify the effectiveness of both the CSCI block and the CompSeg block.</p><p>The model structures of the CGA-tumor and CGA-core modules are almost the same; there are only two trivial and intuitive differences. As illustrated in <ref type="figure">Fig. 4</ref>, the first difference lies in the position where we introduce the cross-task guidance; for the CGA-core module, it is introduced from the second task of OM-Net. Second, the counterparts of P t and P n in CGA-core indicate the probability of each voxel belonging to the core and non-core tumor categories, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post-processing</head><p>We can observe from many prior studies [5]- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> that post-processing is an efficient way to improve segmentation performance by refining the results of CNNs. For example, some small clusters of the predicted tumors are removed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In addition, conditional random field (CRF) is commonly used as a post-processing step in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In particular, Zhao et al. <ref type="bibr" target="#b10">[11]</ref> proposed a post-processing method comprising six steps to boost the segmentation performance by a large margin.</p><p>In this paper, we introduce a simple and flexible postprocessing method in order to refine the predictions of the proposed networks. Our method is mainly inspired by <ref type="bibr" target="#b10">[11]</ref>, but consists of fewer steps and adopts K-means clustering to achieve automatic classification rather than defining the thresholds of voxel intensities in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Step 1: We remove isolated small clusters with volumes smaller than a threshold τ V OL . τ V OL = min(2000, 0.1 × V max ), where V max denotes the volume of the largest 3D connected tumor area predicted by the proposed model. This step can slightly improve the Dice score for the complete tumor, as false positives are removed.</p><p>Step 2: It is observed that non-enhancing voxels are likely to be misclassified as edema if the predicted enhancing tumor area is small <ref type="bibr" target="#b10">[11]</ref>. Accordingly, we propose a K-means-based method to handle this problem, as follows.</p><p>Let vol e and vol t denote the volumes of enhancing tumor and complete tumor in the predicted results, respectively. Moreover, vol e (n) and vol t (n) refer to the volumes of enhancing tumor and complete tumor in the n-th 3D connected tumor area, respectively. If vol e /vol t &lt; 0.1, vol e (n)/vol t (n) &lt; 0.05, and vol e (n) &lt; 1000, the K-means clustering algorithm is employed. Based on their intensity values in the MRI images, the segmented edema voxels in the n-th connected component are clustered into two groups. Finally, the average intensity of each group in the T1c channel is computed. We convert the labels of voxels in the group with the lower averaged intensity to the non-enhancing class. The labels of the voxels in the other group remain unchanged.</p><p>In the experiment section, we show that the second step significantly improves the Dice score of the tumor core. As this step only changes the labels of the voxels predicted as edema, it will not affect the segmentation results of the complete tumor or enhancing tumor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, we provide the datasets used to validate our approaches, the evaluation metrics, and the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To demonstrate the effectiveness of the proposed methods, we conduct experiments on the BraTS 2018 <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b47">[48]</ref>, BraTS 2017 <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>, and BraTS 2015 <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b48">[49]</ref> datasets. There are four modalities for each MRI image. All images in the three datasets have been co-registered, interpolated and skull-stripped. The dimensions of all images are 240 × 240 × 155 voxels.</p><p>The BraTS 2018 dataset contains three subsets: the training set, testing set, and validation set. The training set comprises 210 cases of high-grade gliomas (HGG) and 75 cases of lowgrade gliomas (LGG). The testing and validation sets contain 191 cases and 66 cases respectively with hidden ground-truth. The evaluation metrics of the testing and validation sets are computed using an online evaluation platform <ref type="bibr" target="#b49">[50]</ref>.</p><p>The BraTS 2017 dataset shares an identical training set with BraTS 2018. Compared with BraTS 2018, it has a smaller validation set comprising 46 cases. The evaluation of the validation set is conducted online <ref type="bibr" target="#b49">[50]</ref>.</p><p>The BraTS 2015 dataset consists of a training set including 274 MRI images and a testing set including 110 MRI images. Performance evaluation of the testing set is also conducted using an online evaluation platform <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>We follow the official evaluation metrics for each dataset. There are a number of different metrics, namely the Dice score, Positive Predictive Value (PPV), Sensitivity, and Hausdorff distance, each of which is defined below:</p><formula xml:id="formula_10">Dice = 2T P F P + 2T P + F N ,<label>(10)</label></formula><formula xml:id="formula_11">P P V = T P F P + T P ,<label>(11)</label></formula><formula xml:id="formula_12">Sensitivity = T P T P + F N ,<label>(12)</label></formula><formula xml:id="formula_13">Haus(T, P ) = max{sup t∈T inf p∈P d(t, p), sup p∈P inf t∈T d(t, p)},<label>(13)</label></formula><p>where the number of false negative, true negative, true positive, and false positive voxels are denoted as FN, TN, TP, and FP, respectively. sup represents the supremum and inf denotes the infimum, while t and p denote the points on surface T of the ground-truth regions and surface P of the predicted regions, respectively. d(·, ·) is the function that computes the distance between points t and p. Dice score, PPV, and Sensitivity measure the voxel-wise overlap between the ground-truth and the predicted results <ref type="bibr" target="#b2">[3]</ref>. The Hausdorff distance evaluates the distance between the surface of the ground-truth regions and that of the predicted regions. Moreover, Hausdorff95 is a metric of Hausdorff distance used to measure the 95% quantile of the surface distance. As the Dice score is the overall evaluation metric, adopted consistently across all the BraTS challenges, we adopt it as the main metric for evaluation in line with existing works <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>During pre-processing, we normalize the voxel intensities within the brain area to have zero mean and unit variance for each MRI modality. The numbers of training patches are around 400,000, 400,000, and 200,000 for the first, second, and third task respectively. SoftmaxWithLoss is adopted as the loss function consistently. All implementations are based on the C3D 1 <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref> package, which is a modified 3D version of Caffe <ref type="bibr" target="#b53">[54]</ref>. The models are trained using stochastic gradient 1 https://github.com/facebook/C3D descent with a momentum of 0.99 and a batchsize of 20 for each task. The learning rate of all networks is initially set to 0.001, which is divided by 2 after every four epochs. We train each network in MC for 20 epochs. Similarly, we train OM-Net for 1 epoch, 1 epoch, and 18 epochs for its three steps, respectively. Therefore, the three tasks in OM-Net are trained for 20, 19, and 18 epochs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>We first carry out ablation studies to demonstrate the validity of each contribution proposed in this paper. We then compare the performance of the proposed methods with state-of-theart brain tumor segmentation approaches on the BraTS 2015, 2017, and 2018 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Studies</head><p>The training set of BraTS 2018 is randomly divided into two subsets to enable convenient evaluation. These two subsets are a training subset and a local validation subset, which consist of 260 and 25 MRI images respectively. Quantitative results of this local validation subset are presented in <ref type="table" target="#tab_2">Table I</ref>. Here, the one-model, two-model, and three-model cascades are denoted as MC1, MC2, and MC3, respectively. In the below, we conduct a series of experiments to prove the effectiveness of each component in the proposed approach.</p><p>1) Effectiveness of Model Cascade Strategy: From <ref type="table" target="#tab_2">Table  I</ref>, we can observe that with the increase of model number in MC, Dice scores steadily improve. These results prove the contribution of each deep network in MC. Unfortunately, the number of parameters increases along with the number of models, leading to increased storage consumption and system complexity.</p><p>2) Effectiveness of One-pass Multi-task Network: We compare the performance of OM-Net with that of the MC strategy in <ref type="table" target="#tab_2">Table I</ref>. Despite having only one-third of the parameters of MC3, OM-Net consistently obtains better segmentation performance, especially for Dice scores on the tumor core and enhancing tumor. Moreover, we additionally train OM-Net 0 (a naive multi-task learning model without stepwise training or training data transfer) and OM-Net d (a multi-task learning model without stepwise training, but with training data transfer). It can be seen that OM-Net outperforms both OM-Net 0 and OM-Net d ; this demonstrates the effectiveness of the data transfer strategy and the curriculum learning-based training strategy.</p><p>3) Effectiveness of Cross-task Guided Attention: To compare the performance of the CGA module with that of the SE block, we further test the OM-Net + SE model where an SE block is inserted before each Classifier-i (1 ≤ i ≤ 3) module of OM-Net in <ref type="figure" target="#fig_2">Fig. 3</ref>. Experimental results in <ref type="table" target="#tab_2">Table  I</ref> show that OM-Net + CGA outperforms both OM-Net and OM-Net + SE. In particular, it outperforms OM-Net by as much as 2.28% on Dice score for the tumor core region. Moreover, there is a slight performance drop for enhancing tumor of 0.14%. However, on much larger data sets (see <ref type="table" target="#tab_2">Table  II</ref>, III, IV) where the experimental results are more stable, we observe that CGA consistently improves the Dice score on enhancing tumor. In comparison, there is no clear difference in performance between OM-Net and OM-Net + SE. This can be explained from two perspectives. First, the GAP operation in the SE block ignores category-specific statistics; second, recalibrating each channel with the same weight for all categories is suboptimal for segmentation. The proposed CGA module effectively handles the above two problems, and therefore achieves better performance than the SE block. In addition, the model size of OM-Net + CGA is smaller than both OM-Net and OM-Net + SE. We can thus safely attribute the performance gains to the CGA module rather than to the additional parameters.</p><p>Next, we conduct additional experimental investigation into the CGA module in order to prove the validity of both the CSCI block and the CompSeg block.</p><p>• To justify the effectiveness of the CSCI block, we visualize some feature maps produced by the shared backbone model, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. It should be noted that, in the interests of intuitive and clear visualization, we choose to visualize the feature maps of a complete 2D slice rather than those of a 3D patch. To achieve this, we stitch F-2 and O-1 of 3D patches in <ref type="figure">Fig. 4</ref> respectively to form the whole feature maps and probability maps. We then select the feature maps F and probability map P of a certain slice and calculate m t and m n corresponding to this slice, according to Eqs. 5 and 6, respectively. The channels corresponding to the five largest and five smallest values in m t are presented in <ref type="figure" target="#fig_5">Fig. 6</ref>(c) and <ref type="figure" target="#fig_5">Fig. 6(d)</ref>, respectively. Similarly, the channels with the five largest and five smallest values in m n are presented in <ref type="figure" target="#fig_5">Fig.  6</ref>(e) and <ref type="figure" target="#fig_5">Fig. 6(f)</ref>, respectively. As deconvolution layers are used in the model, there are inevitable checkboard artifacts; however, these do not affect the observations.</p><p>It is clear that the feature maps shown in <ref type="figure" target="#fig_5">Fig. 6</ref>(c) do indeed have strong responses for the tumor region, which should be highlighted for the segmentation of the tumor region. In contrast, the feature maps in <ref type="figure" target="#fig_5">Fig. 6(d)</ref> have weak responses for the tumor region, but strong responses for the non-tumor region; therefore, they will be suppressed in CGA for the segmentation of the tumor region. Similarly, consistent observations can be found in <ref type="figure" target="#fig_5">Fig. 6</ref>(e) and <ref type="figure" target="#fig_5">Fig. 6(f)</ref>. The above analysis proves the validity of the CSCI block in generating the categoryspecific channel dependence. • Furthermore, we also justify the effectiveness of the CompSeg block, where P t and P n are used a second time. We additionally train a model without using P t and P n , which simply performs an element-wise addition between S t and S n in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, denoted as OM-Net + CGA − . Experimental results in <ref type="table" target="#tab_2">Table I</ref> reveal that OM-Net + CGA significantly outperforms OM-Net + CGA − ; this is because OM-Net + CGA employs soft spatial masks (P t and P n ) to fuse two complementary prediction results. This performance comparison proves the validity of P t and P n used in the CompSeg block.</p><p>In addition, we also test another two models: OM-Net + CGA t is a model whose CompSeg block only includes the upper branch in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, while OM-Net + CGA n is a model whose CompSeg block only incorporates the lower branch in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>. The experimental results are reported in <ref type="table" target="#tab_2">Table I</ref>. From the table, it can be seen that OM-Net + CGA outperforms both OM-Net + CGA t and OM-Net + CGA n . Accordingly, we can conclude that the two pathways in the CompSeg block can make full use of the complementary information, which is beneficial for the segmentation task. 4) Effectiveness of Post-processing: To justify the effectiveness of the proposed post-processing operation, we apply it to refining the segmentation results of both OM-Net and OM-Net + CGA, denoted as OM-Net p and OM-Net + CGA p respectively in <ref type="table" target="#tab_2">Table I</ref>. First, compared with OM-Net, it is shown that OM-Net p can slightly improve the Dice score for the complete tumor due to false positives having been removed in the first post-processing step; meanwhile, it significantly improves the Dice score of tumor core by 2.6% because of the second post-processing step. Moreover, the performance comparison between OM-Net + CGA and OM-Net + CGA p shows that post-processing operation consistently brings about performance improvement for the complete tumor and tumor core regions.</p><p>In conclusion, the above experimental results justify the effectiveness of the proposed techniques. Qualitative comparisons between MC3, OM-Net, OM-Net + CGA, and OM-Net + CGA p are also provided in <ref type="figure" target="#fig_6">Fig. 7</ref>. It is clear that the proposed methods steadily improve the quality of brain tumor segmentation. This is consistent with the quantitative comparisons in <ref type="table" target="#tab_2">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Comparison on BraTS 2015 Testing Set</head><p>In this experiment, the performance of MC3, OM-Net, and OM-Net + CGA is evaluated on the testing set of the BraTS 2015 dataset. Each of these models is trained using the entire training set of the dataset. Experimental results are tabulated in <ref type="table" target="#tab_2">Table II</ref>. From this table, we can make the following observations. First, we compare the segmentation performance of MC3, OM-Net, OM-Net + CGA, and OM-Net + CGA p . It is evident that OM-Net has clear advantages over MC3, with 1% higher Dice scores on both tumor core and enhancing tumor. OM-Net + CGA further promotes the Dice score of OM-Net by 1% on enhancing tumor. Moreover, following refinement by the proposed post-processing method, the Dice scores of OM-Net + CGA improve significantly by 1% and 4% on complete tumor and tumor core, respectively. These results are consistent with comparisons on the local validation subset of BraTS 2018.</p><p>Second, we compare the performance of OM-Net + CGA p with state-of-the-art methods. Our results exhibit clear advantages over those obtained by the comparison methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b54">[55]</ref>. In particular, our results outperform the popular DeepMedic model <ref type="bibr" target="#b6">[7]</ref> by 2%, 8%, and 2% in terms of Dice scores on complete tumor, tumor core, and enhancing tumor, respectively. Furthermore, OM-Net + CGA p also outperforms the method in <ref type="bibr" target="#b10">[11]</ref> which adopts more pre-processing and postprocessing operations. At the time of this submission, OM-Net + CGA p ranks first on the online leaderboard of BraTS 2015, demonstrating the effectiveness of the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison on BraTS 2017 Validation Set</head><p>Since access to the testing set of BraTS 2017 was closed after the challenge, we evaluate the proposed methods on the online validation set and compare them with other participants in <ref type="table" target="#tab_2">Table III</ref>.</p><p>First, we train the MC3, OM-Net, OM-Net + SE, and OM-Net + CGA models using the training subset of 260 MRI images. The following observations can be made. OM-Net outperforms MC3, especially for the Dice scores on enhancing tumor. The SE block cannot improve the performance of OM-Net in terms of Dice scores; in fact, it reduces the Dice score of OM-Net by 1.18% on enhancing tumor. In comparison, OM-Net + CGA outperforms both OM-Net and OM-Net + SE by a significant margin. Specifically, its Dice scores are 1.88% and 2.09% higher than those of OM-Net on tumor core and enhancing tumor respectively; this again proves the effectiveness of the CGA module. In addition, OM-Net + CGA p considerably improves the Dice score on tumor core, demonstrating the effectiveness of the proposed postprocessing method.</p><p>Second, to further boost the performance of OM-Net + CGA, we divide the training data into ten folds and obtain an ensemble system using the following scheme: we train one model using nine folds of training data and pick the best snapshot on the rest one fold. We repeat this process and obtain 10 models as an ensemble (denoted as OM-Net + CGA ). A similar strategy was used in <ref type="bibr" target="#b42">[43]</ref>. <ref type="table" target="#tab_2">Table III</ref> shows that OM-Net + CGA consistently obtains higher Dice scores than OM-Net + CGA. We further apply the proposed post-processing method to OM-Net + CGA , denoted as OM-Net + CGA p . It can clearly be seen that the proposed post-processing method improves the Dice score of OM-Net + CGA by as much as 1.48% for tumor core.</p><p>Third, we present comparisons between OM-Net + CGA p and some state-of-the-art methods on the online validation leaderboard, which comprises more than 60 entries. It is clear that OM-Net + CGA p outperforms all other methods in terms of Dice scores. It is further worth noting that other top entries, such as Kamnitsas et al. <ref type="bibr" target="#b11">[12]</ref>, also combined multiple models to boost performance; moreover, Wang et al. <ref type="bibr" target="#b26">[27]</ref> integrated nine single-view models from three orthogonal views to achieve excellent performance. The above comparisons demonstrate the superiority of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparison on BraTS 2018 Dataset</head><p>We also make additional comparisons on the BraTS 2018 dataset. As BraTS 2018 and 2017 share the same training dataset, we directly evaluate the same models in the previous experiments on the validation set of BraTS 2018. The BraTS 2018 Challenge is intensely competitive, with more than 100 entries displayed on the online validation leaderboard. Therefore, we only present comparisons between our methods and the top entries in <ref type="table" target="#tab_2">Table IV</ref>. Our observations are as follows.</p><p>First, comparison results between MC3, OM-Net, OM-Net + SE, and OM-Net + CGA are consistent with those in <ref type="table" target="#tab_2">Table  III</ref>. We can see that OM-Net achieves better performance than MC3 on enhancing tumor with a visible margin. Moreover, despite having only a single model and without post-processing, OM-Net is able to outperform more than 70% of entries on the leaderboard. In addition, OM-Net + CGA outperforms OM-Net by 1.26% and 1.45% in terms of Dice scores on tumor core and enhancing tumor, respectively. By comparison, SE cannot improve the performance of OM-Net in terms of Dice scores.</p><p>Second, by implementing the model ensemble and the postprocessing operation, OM-Net + CGA p obtains higher Dice scores as expected, achieving very competitive performance on the leaderboard. It is worth noting that the model ensemble strategy was also applied in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The approach described in <ref type="bibr" target="#b14">[15]</ref> also decomposes the multi-class brain tumor segmentation into three tasks. It achieves top performance by training 10 models as an ensemble, the inputs of which are  large patches of size 160 × 192 × 128 voxels. Large input patches lead to considerable memory consumption; therefore, 32GB GPUs are employed in <ref type="bibr" target="#b14">[15]</ref> to train this model. In comparison, OM-Net utilizes small patches of size 32×32×16 voxels, making it memory-efficient and capable of being trained or deployed on low-cost GPU devices. We can thus conclude that OM-Net is very competitive and has its own advantages. More impressively, benefitting from the techniques proposed in this paper, we obtained the joint third position among 64 teams on the testing set of the BraTS 2018 Challenge 2 . More detailed results of this challenge are introduced in <ref type="bibr" target="#b47">[48]</ref>. In conclusion, the effectiveness of the proposed methods has been demonstrated through comparisons on the above three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a novel model, OM-Net, for brain tumor segmentation that is tailored to handle the class 2 https://www.med.upenn.edu/sbia/brats2018/rankings.html imbalance problem. Unlike the popular MC framework, OM-Net requires only one-pass computation to perform coarseto-fine segmentation. OM-Net is superior to MC because it not only significantly reduces the model size and system complexity, but also thoroughly exploits the correlation between the tasks through sharing parameters, training data, and even prediction results. In particular, we propose a CGA module that makes use of cross-task guidance information to learn category-specific channel attention, enabling it to significantly outperform the popular SE block. In addition, we introduce a novel and effective post-processing method for use in refining the segmentation results in order to achieve better accuracy. Extensive experiments were conducted on three popular datasets; the results of these experiments prove the effectiveness of the proposed OM-Net model, and further demonstrate that OM-Net has clear advantages over existing</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustrations of (a) a three-model cascade pipeline and (b) our proposed OM-Net. The model cascade pipeline contains three networks that segment different tumor regions sequentially. OM-Net is a novel end-to-end deep model that simplifies prediction using one-pass computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Network structure of OM-Net in the training stage. For the i-th task, its training data, feature maps, and outputs of the classification layer are denoted as Data-i, Feature-i, and Output-i, respectively. The light blue rectangles marked with 'Concat' and 'Split' represent the concatenation and splitting operations, respectively, while the blue arrows represent the batch dimension. SoftmaxWithLoss is adopted as the loss function and applied to the output of each task. The shared backbone model refers to the network layers outlined by the yellow dashed line inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Structure of the CGA-tumor module. (a) Category-specific channel importance (CSCI) block; (b) complementary segmentation (CompSeg) block. The elements in m t and mn describe the importance of each channel for one category of voxels within the patch, which are utilized to recalibrate the channel-wise feature responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of the feature maps output from the shared backbone model in OM-Net + CGA. We present the feature maps of a complete 2D slice for intuitive and clear visualization. (a) The Flair modality on the 75 th slice of the sample BraTS18 2013 21 1 in the BraTS 2018 training set. (b) Its corresponding ground truth. (c) Heat maps corresponding to the channels with the five largest values in m t . (d) Heat maps corresponding to the channels with the five smallest values in m t . (e) Heat maps corresponding to the channels with the five largest values in mn. (f) Heat maps corresponding to the channels with the five smallest values in mn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Example segmentation results on the local validation subset of BraTS 2018. From left to right: Ground truth, MC3, OM-Net, OM-Net + CGA, and OM-Net + CGA p results overlaid on FLAIR image; edema (green), necrosis and non-enhancing (blue), and enhancing(red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDIES ON THE LOCAL VALIDATION SUBSET OF BRATS 2018</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dice (%)</cell><cell></cell></row><row><cell>Method</cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell></row><row><cell>MC1</cell><cell>13.813 M</cell><cell>90.41</cell><cell>78.48</cell><cell>72.91</cell></row><row><cell>MC2</cell><cell>27.626 M</cell><cell>91.08</cell><cell>79.11</cell><cell>75.14</cell></row><row><cell>MC3</cell><cell>41.439 M</cell><cell>91.08</cell><cell>79.11</cell><cell>79.53</cell></row><row><cell>OM-Net</cell><cell>13.869 M</cell><cell>91.10</cell><cell>79.87</cell><cell>80.87</cell></row><row><cell>OM-Net 0</cell><cell>13.869 M</cell><cell>90.40</cell><cell>79.41</cell><cell>79.96</cell></row><row><cell>OM-Net d</cell><cell>13.869 M</cell><cell>91.11</cell><cell>79.93</cell><cell>80.26</cell></row><row><cell>OM-Net + SE</cell><cell>13.870 M</cell><cell>91.03</cell><cell>80.20</cell><cell>80.72</cell></row><row><cell>OM-Net + CGA</cell><cell>13.814 M</cell><cell>91.34</cell><cell>82.15</cell><cell>80.73</cell></row><row><cell>OM-Net + CGA −</cell><cell>13.814 M</cell><cell>91.06</cell><cell>80.28</cell><cell>80.78</cell></row><row><cell>OM-Net + CGA t</cell><cell>13.814 M</cell><cell>90.65</cell><cell>80.27</cell><cell>80.10</cell></row><row><cell>OM-Net + CGA n</cell><cell>13.814 M</cell><cell>89.75</cell><cell>79.87</cell><cell>76.00</cell></row><row><cell>OM-Net p</cell><cell>13.869 M</cell><cell>91.28</cell><cell>82.50</cell><cell>80.84</cell></row><row><cell>OM-Net + CGA p</cell><cell>13.814 M</cell><cell>91.59</cell><cell>82.74</cell><cell>80.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>ON BRATS 2015 TESTING SET (%)</figDesc><table><row><cell></cell><cell>Dice</cell><cell></cell><cell cols="3">Positive Predictive Value</cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III MEAN</head><label>III</label><figDesc>VALUES OF DICE AND HAUSDORFF95 METRICS ON BRATS 2017 VALIDATION SET</figDesc><table><row><cell></cell><cell></cell><cell>Dice</cell><cell></cell><cell cols="3">Hausdorff95 (mm)</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Enh.</cell><cell>Whole</cell><cell>Core</cell><cell>Enh.</cell><cell>Whole</cell><cell>Core</cell></row><row><cell>MC3</cell><cell>0.7424</cell><cell>0.8991</cell><cell>0.7937</cell><cell>4.9901</cell><cell>4.6085</cell><cell>8.5537</cell></row><row><cell>OM-Net</cell><cell>0.7534</cell><cell>0.9007</cell><cell>0.7934</cell><cell>3.6547</cell><cell>7.2524</cell><cell>8.4676</cell></row><row><cell>OM-Net + SE</cell><cell>0.7416</cell><cell>0.8997</cell><cell>0.7938</cell><cell>3.5115</cell><cell>6.2859</cell><cell>7.0154</cell></row><row><cell>OM-Net + CGA</cell><cell>0.7743</cell><cell>0.8988</cell><cell>0.8122</cell><cell>3.8820</cell><cell>4.8380</cell><cell>6.7953</cell></row><row><cell>OM-Net + CGA p</cell><cell>0.7743</cell><cell>0.9016</cell><cell>0.8320</cell><cell>3.8820</cell><cell>4.6663</cell><cell>6.7312</cell></row><row><cell>OM-Net + CGA</cell><cell>0.7852</cell><cell>0.9065</cell><cell>0.8274</cell><cell>3.2991</cell><cell>4.4886</cell><cell>6.9896</cell></row><row><cell>OM-Net + CGA p</cell><cell>0.7852</cell><cell>0.9071</cell><cell>0.8422</cell><cell>3.2991</cell><cell>4.3815</cell><cell>7.5614</cell></row><row><cell>Wang et al. [27]</cell><cell>0.7859</cell><cell>0.9050</cell><cell>0.8378</cell><cell>3.2821</cell><cell>3.8901</cell><cell>6.4790</cell></row><row><cell>MIC DKFZ</cell><cell>0.7756</cell><cell>0.9027</cell><cell>0.8194</cell><cell>3.1626</cell><cell>6.7673</cell><cell>8.6419</cell></row><row><cell>inpm</cell><cell>0.7723</cell><cell>0.8998</cell><cell>0.8085</cell><cell>4.7852</cell><cell>9.0029</cell><cell>7.2359</cell></row><row><cell>xfeng</cell><cell>0.7511</cell><cell>0.8922</cell><cell>0.7991</cell><cell>4.7547</cell><cell>16.3018</cell><cell>8.6847</cell></row><row><cell>Kamnitsas et al. [12]</cell><cell>0.738</cell><cell>0.901</cell><cell>0.797</cell><cell>4.50</cell><cell>4.23</cell><cell>6.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV MEAN</head><label>IV</label><figDesc>VALUES OF DICE AND HAUSDORFF95 METRICS ON BRATS 2018 VALIDATION SETTo facilitate fair comparison, we report the performance of<ref type="bibr" target="#b42">[43]</ref> without private training data.</figDesc><table><row><cell></cell><cell></cell><cell>Dice</cell><cell></cell><cell cols="3">Hausdorff95 (mm)</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Enh.</cell><cell>Whole</cell><cell>Core</cell><cell>Enh.</cell><cell>Whole</cell><cell>Core</cell></row><row><cell>MC3</cell><cell>0.7732</cell><cell>0.9015</cell><cell>0.8233</cell><cell>4.1624</cell><cell>4.7198</cell><cell>7.6082</cell></row><row><cell>OM-Net</cell><cell>0.7882</cell><cell>0.9034</cell><cell>0.8273</cell><cell>3.1003</cell><cell>6.5218</cell><cell>7.1974</cell></row><row><cell>OM-Net + SE</cell><cell>0.7791</cell><cell>0.9034</cell><cell>0.8259</cell><cell>2.9950</cell><cell>5.7685</cell><cell>6.4289</cell></row><row><cell>OM-Net + CGA</cell><cell>0.8027</cell><cell>0.9033</cell><cell>0.8399</cell><cell>3.4437</cell><cell>4.7609</cell><cell>6.4339</cell></row><row><cell>OM-Net + CGA p</cell><cell>0.8027</cell><cell>0.9052</cell><cell>0.8536</cell><cell>3.4437</cell><cell>4.6236</cell><cell>6.3892</cell></row><row><cell>OM-Net + CGA</cell><cell>0.8112</cell><cell>0.9074</cell><cell>0.8461</cell><cell>2.8697</cell><cell>4.9105</cell><cell>6.6243</cell></row><row><cell>OM-Net + CGA p</cell><cell>0.8111</cell><cell>0.9078</cell><cell>0.8575</cell><cell>2.8810</cell><cell>4.8840</cell><cell>6.9322</cell></row><row><cell>Myronenko [15]</cell><cell>0.8233</cell><cell>0.9100</cell><cell>0.8668</cell><cell>3.9257</cell><cell>4.5160</cell><cell>6.8545</cell></row><row><cell>SHealth</cell><cell>0.8154</cell><cell>0.9120</cell><cell>0.8565</cell><cell>4.0461</cell><cell>4.2362</cell><cell>7.2181</cell></row><row><cell>Isensee et al. [43]  †</cell><cell>0.8048</cell><cell>0.9072</cell><cell>0.8514</cell><cell>2.81</cell><cell>5.23</cell><cell>7.23</cell></row><row><cell>MedAI</cell><cell>0.8053</cell><cell>0.9104</cell><cell>0.8545</cell><cell>3.6695</cell><cell>4.1369</cell><cell>5.9821</cell></row><row><cell>BIGS2</cell><cell>0.8054</cell><cell>0.9104</cell><cell>0.8506</cell><cell>2.7543</cell><cell>4.8444</cell><cell>7.4548</cell></row><row><cell>SCAN</cell><cell>0.7925</cell><cell>0.9008</cell><cell>0.8474</cell><cell>3.6035</cell><cell>4.0626</cell><cell>4.9885</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review of MRI-based brain tumor image segmentation using deep learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Işın</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Direkoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="317" to="324" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exciting new advances in neuro-oncology: the avenue to a cure for malignant glioma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Van Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Hadjipanayis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Norden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: a cancer journal for clinicians</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="166" to="193" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of MRIbased medical image analysis for brain tumor studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="97" to="129" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in MRI images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VoxResNet: Deep voxelwise residual networks for brain segmentation from 3D MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="446" to="455" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deep learning model integrating FCNNs and CRFs for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. Anal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="98" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensembles of multiple models and architectures for robust brain tumour segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</title>
		<meeting>Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Her2Net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bottom-up approach for pancreas segmentation using cascaded superpixels and (deep) image patch labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="386" to="399" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D MRI brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian polytrees with learned deep features for multi-class cell segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic segmentation of retinal layer in OCT images with choroidal neovascularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5880" to="5891" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning contextual and attentive information for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic renal segmentation in DCE-MRI using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurugol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symposium on Biomed. Imag. (ISBI)</title>
		<meeting>IEEE Int. Symposium on Biomed. Imag. (ISBI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1534" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection and recognition for life state of cell cancer using two-stage cascade CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinf</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armbruster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danastasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A 3D coarseto-fine framework for automatic pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00201</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic calcium scoring in low-dose chest CT using deep neural networks with dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zreik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="615" to="625" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting anatomical landmarks from limited medical imaging data using two-stage task-oriented deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4753" to="4764" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A framework for classification and segmentation of branch retinal artery occlusion in SD-OCT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3518" to="3527" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene text detection and segmentation based on cascaded convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1509" to="1520" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic brain tumor segmentation using cascaded anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</title>
		<meeting>Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One-pass multi-task convolutional neural networks for efficient brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel squeeze &amp; excitation in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive feature recombination and recalibration for semantic segmentation: application to brain tumor segmentation in MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="706" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention couplenet: Fully convolutional attention coupling network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Section for Biomedical Image Analysis, MICCAI BraTS</title>
		<ptr target="https://www.med.upenn.edu/sbia/brats2018/data.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fusionnet: A deep fully residual convolutional neural network for image segmentation in connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Hilderbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task fully convolutional network for brain tumour segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annu. Conf. Med. Imag. Understand. Anal. (MIUA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Boundary-aware fully convolutional network for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="433" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CompNet: Complementary segmentation network for brain MRI extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="628" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">No new-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Focus, segment and erase: An efficient network for multi-label brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="674" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Segmentation labels and radiomic features for the pre-operative scans of the TCGA-LGG collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging Arch</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segmentation labels and radiomic features for the pre-operative scans of the TCGA-GBM collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging Arch</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The virtual skeleton database: an open access repository for biomedical research and collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonaretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfahrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Büchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A web accessible platform for imaging analytics</title>
		<ptr target="https://ipp.cbica.upenn.edu/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Center for Biomedical Image Computing and Analytics, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virtualskeleton</surname></persName>
		</author>
		<ptr target="https://www.virtualskeleton.ch/BRATS/Start2015" />
	</analytic>
	<monogr>
		<title level="j">BRATS</title>
		<imprint>
			<date type="published" when="2013-09-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation and radiomics survival prediction: Contribution to the BRATS 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</title>
		<meeting>Int. Conf. Med. Image Comput. Comput.-Assist. Intervent Brainlesion Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
