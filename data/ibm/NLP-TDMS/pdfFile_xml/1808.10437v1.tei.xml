<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-30">30 Aug 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
							<email>chengao@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech Virginia</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
							<email>ylzou@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech Virginia</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<email>jbhuang@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech Virginia</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-30">30 Aug 2018</date>
						</imprint>
					</monogr>
					<note>GAO ET AL.: INSTANCE-CENTRIC ATTENTION NETWORK 1 2 GAO ET AL.: INSTANCE-CENTRIC ATTENTION NETWORK</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person Person Cake Chair Knife Wine glass Person, cut obj, cake Person, cut instr, knife Person, hold, knife Person, smile</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Object detection HOI detection <ref type="figure">Figure 1</ref>: Human-object interaction detection. Given an input image (left) and the detected object instances in the image (middle), our method detects and recognizes the interactions between each person and the objects they are interacting with (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent years have witnessed rapid progress in detecting and recognizing individual object instances. To understand the situation in a scene, however, computers need to recognize how humans interact with surrounding objects. In this paper, we tackle the challenging task of detecting human-object interactions (HOI). Our core idea is that the appearance of a person or an object instance contains informative cues on which relevant parts of an image to attend to for facilitating interaction prediction. To exploit these cues, we propose an instance-centric attention module that learns to dynamically highlight regions in an image conditioned on the appearance of each instance. Such an attentionbased network allows us to selectively aggregate features relevant for recognizing HOIs. We validate the efficacy of the proposed network on the Verb in COCO and HICO-DET datasets and show that our approach compares favorably with the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, there has been rapid progress in visual recognition tasks, including object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>, and action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>. However, understanding a scene requires not only detecting individual object instances but also recognizing the visual relationship between object pairs. One particularly important class of visual relationship detection is detecting and recognizing how each person interacts with the surrounding objects. This task, known as Human-Object Interactions (HOI) detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, aims to localize a person, an object, as well as identify the interaction between the person and the object. In <ref type="figure">Figure 1</ref>, we show an example of the HOI detection problem. Given an input image and the detected instances from an object detector, we aim to identify all the triplets human, verb, object .</p><p>Why HOI? Detecting and recognizing HOI is an essential step towards a deeper understanding of the scene. Instead of "What is where?" (i.e., localizing object instances in an image), the goal of HOI detection is to answer the question "What is happening?". Studying the HOI detection problem also provides important cues for other related high-level vision tasks, such as pose estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref>, image captioning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref>, and image retrieval <ref type="bibr" target="#b20">[21]</ref>.</p><p>Why attention? Driven by the progress in object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>, several recent efforts have been devoted to detecting HOI in images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>. Most existing approaches infer interactions using appearance features of a person and an object as well as their spatial relationship. In addition to using only appearance features from a person, recent action recognition algorithms exploit contextual cues from an image. As shown in <ref type="figure">Figure 2</ref>, examples of encoding context include selecting a secondary box <ref type="bibr" target="#b12">[13]</ref>, using the union of the human and object bounding boxes <ref type="bibr" target="#b28">[29]</ref>, extracting features around human pose keypoints <ref type="bibr" target="#b5">[6]</ref>, or exploiting global context from the whole image <ref type="bibr" target="#b30">[31]</ref>. While incorporating context generally helps improve performance, these hand-designed attention regions may not always be relevant for recognizing actions/interactions. For examples, attending to human poses may help identify actions like 'ride' and 'throw', attending to the point of interaction may help recognize actions involving hand-object interaction such as 'drinking with cup' and 'eat with spoon', and attending to the background may help distinguish between 'hit with tennis racket' and 'hit with baseball ball bat'. To address this limitation, recent works leverage end-to-end trainable attention modules for action recognition <ref type="bibr" target="#b8">[9]</ref> or image classification <ref type="bibr" target="#b19">[20]</ref>. These methods, however, are designed for image-level classification tasks.</p><p>Our work. In this paper, we propose an end-to-end trainable instance-centric attention module that learns to highlight informative regions using the appearance of a person or an object instance. Our intuition is that the appearance of an instance (either human or an object) provides cues on where in the image we should pay attention to. For example, to better determine whether a person is carrying an object, one should direct its attention to the region around the person's hands. On the other hand, given a bicycle in an image, attending to the pose of the person nearby helps to disambiguate the potential interactions involved with object instance (e.g., riding or carrying a bike). The proposed instance-centric attention network (iCAN) dynamically produces an attentional map for each detected person or object instance highlighting regions relevant to the task. We validate the efficacy of our network design on two large public benchmarks on HOI detection: Verbs in COCO (V-COCO) <ref type="bibr" target="#b15">[16]</ref> and Humans Interacting with Common Objects (HICO-DET) <ref type="bibr" target="#b3">[4]</ref> datasets. Our results show that the proposed iCAN compares favorably against the state-of-the-art with around 10% relative improvement on V-COCO and 49% on HICO-DET with respect to the existing best-performing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAP softmax</head><p>Secondary regions <ref type="bibr" target="#b12">[13]</ref> Bottom-up attentional feature <ref type="bibr" target="#b8">[9]</ref> Conv feature * b</p><p>Conv feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAP softmax</head><p>Human pose <ref type="bibr" target="#b5">[6]</ref> Instance-centric attentional feature (ours) <ref type="figure">Figure 2</ref>: Examples of contextual features. Different ways of capturing contextual cues from an image in addition to using the bounding boxes of persons and objects.</p><p>Our contributions. We make the following four contributions.</p><p>• We introduce an instance-centric attention module that allows the network to dynamically highlight informative regions for improving HOI detection. • We establish new state-of-the-art performance on two large-scale HOI benchmark datasets. • We conduct detailed ablation study and error analysis to identify the relative contributions of the individual components and quantify different types of errors. • We release our source code and pre-trained models to facilitate future research. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection. Object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> is an essential building block for scene understanding. Our work uses the off-the-shelf Faster R-CNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> to localize persons and object instances. Given the detected instances, our method aims to recognize interactions (if any) between all pairs of person and object instances.</p><p>Visual relationship detection. A number of recent work addresses the problem of detecting visual relationship <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> and generating scene graph <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. Several papers leverage some forms of language prior <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref> to help overcome the problem of large numbers of the relationship subject-predicate-object triplets and limited data samples. Our work focuses on one particular class of visual relationship detection problems: detecting human-object interactions. HOI detection poses additional challenges over visual relationship detection. With human as a subject, the interactions (i.e., the predicate) with objects are a lot more fine-grained and diverse than other generic objects.</p><p>Attention. Extensive efforts have been made to incorporate attention in action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> and human-object interaction tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. These methods often use handdesigned attention regions to extract contextual features. Very recently, end-to-end trainable attention-based methods have been proposed to improve the performance of action recognition <ref type="bibr" target="#b8">[9]</ref> or image classification <ref type="bibr" target="#b19">[20]</ref>. However, these methods are designed for image-level classification task. Our work builds upon the recent advances of attention-based techniques and extends them to address instance-level HOI recognition tasks.</p><p>Human-object interactions. Detecting HOI provides a deeper understanding of the situation in a scene. Gupta and Malik <ref type="bibr" target="#b15">[16]</ref> first tackle the HOI detection problem -detecting  <ref type="figure">Figure 3</ref>: Overview of the proposed model. The proposed model consists of following three major streams: (1) a human stream for detecting interaction based on human appearance; (2) an object steam that predicts the interaction based on object appearance; (3) a pairwise stream for encoding the spatial layouts between the human and object bounding boxes. Given the detected object instances by the off-the-shelf Faster R-CNN, we generate the HOI hypothesis using all the human-object pairs. The action scores from individual streams are then fused to produce the final prediction as shown on the right. people doing actions and the object instances they are interacting with. Associating objects in a scene with various semantic roles leads to a finer-grained understanding of the current state of activity. Very recently, Gkioxari et al. <ref type="bibr" target="#b13">[14]</ref> extend the method in <ref type="bibr" target="#b15">[16]</ref> by introducing an action-specific density map over target object locations based on the appearance of a detected person. Significantly improved results have also been shown by replacing feature backbone with ResNet-50 <ref type="bibr" target="#b16">[17]</ref> and the Feature Pyramid Network <ref type="bibr" target="#b26">[27]</ref>. In addition to using object instance appearances, Chao et al. <ref type="bibr" target="#b3">[4]</ref> also encode the relative spatial relationship between a person and the object with a CNN. Our work builds upon these recent advances in HOI detection, but with a key differentiator. Existing work recognizes interactions based on individual cues (either human appearance, object appearance, or spatial relationship between a human-object pair). Our key observation is that such predictions inevitably suffer from the lack of contextual information. The proposed instance-centric attention module extracts contextual features complementary to the appearance features of the localized regions (e.g., humans/object boxes) to facilitate HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instance-Centric Attention Network</head><p>In this section, we present our Instance-centric Attention Network for HOI detection <ref type="figure">(Figure 3</ref>). We start with an overview of our approach (Section 3.1) and then introduce the instance-centric attention module (Section 3.2). Next, we outline the details of the three main streams for feature extraction (Section 3.3): the human stream, the object stream, and the pairwise stream. Finally, we describe our inference procedure (Section 3.4). inst (for human) or x o inst (for object) as well as the features from the instance-centric attentional map. For computing the attentional map, we measure the similarity in the embedding space with a bottleneck of 512 channels <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Specifically, we embed the image feature using a 1 × 1 convolution and the instance appearance feature x h inst with a fully connected layer. Here res5 denotes the fifth residual block, GAP denotes a global average pooling layer, and FC denotes a fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm overview</head><p>Our approach to human-object interaction detection consists of two main steps: 1) object detection and 2) HOI prediction. First, given an input image we use Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> from Detectron <ref type="bibr" target="#b11">[12]</ref> to detect all the person/object instances. We denote b h as the detected bounding box for a person and b o for an object instance. We use s h and s o to denote the confidence scores for a detected person and an object, respectively. Second, we evaluate all the human-object bounding box pairs through the proposed instance-centric attention network to predict the interaction score. <ref type="figure">Figure 3</ref> shows an overview of the model.</p><p>Inference. We predict HOI scores in a similar manner to the existing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. For each human-object bounding box pair (b h , b o ), we predict the score S a h,o for each action a ∈ {1, · · · , A}, where A denotes the total number of possible actions. The score S a h,o depends on (1) the confidence for the individual object detections (s h and s o ), (2) the interaction prediction based on the appearance of the person s a h and the object s a h , and (3) the score prediction based on the spatial relationship between the person and the object s a sp . Specifically, our HOI score S a h,o for the human-object bounding box pair (b h , b o ) has the form:</p><formula xml:id="formula_0">S a h,o = s h · s o · (s a h + s a o ) · s a sp .<label>(1)</label></formula><p>For some of the action classes that do not involve any objects (e.g., walk, smile), we use the action score s a h from the human stream only. For those actions, our final scores are s h · s a h . Training. As a person can concurrently perform different actions to one or multiple target objects, e.g., a person can 'hit with' and 'hold' a tennis racket at the same time, HOI detection is thus a multi-label classification problem, where each interaction class is independent and not mutually exclusive. We apply a binary sigmoid classifier for each action category, and then minimize the cross-entropy loss between action score s a h , s a o , or s a sp and the groundtruth action label for each action category. In the following, we introduce our instancecentric attention module for extracting informative features from an image and then describe a multi-stream network architecture for computing the action scores s a h , s a o , and s a sp .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-centric attention module</head><p>In this section, we introduce the instance-centric attention module for extracting contextual features from an image. <ref type="figure" target="#fig_2">Figure 4</ref> shows the detailed procedure using human as an instance for clarity. Using an object as an instance is straightforward. We first extract the instance-level appearance feature x h inst using the standard process, e.g., applying ROI pooling, passing through a residual block, followed by the global average pooling. Next, our goal is to dynamically generate an attention map conditioned on the object instance of interest. To do so, we embed both the instance-level appearance feature x h inst and the convolutional feature map onto a 512-dimensional space and measure the similarity in this embedding space using vector dot product. We can then obtain the instance-centric attentional map by applying softmax. The attentional map highlights relevant regions in an image that may be helpful for recognizing HOI associated with the given human/object instance. Using the attentional map, we can extract the contextual feature x h context by computing the weighted average of the convolutional features. The final output of our iCAN module is a concatenation of instance-level appearance feature x h inst and the attention-based contextual feature x h context . Our iCAN module offers several advantages over existing approaches. First, unlike hand-designed contextual features based on pose <ref type="bibr" target="#b5">[6]</ref>, the entire image <ref type="bibr" target="#b30">[31]</ref>, or secondary regions <ref type="bibr" target="#b12">[13]</ref>, our attention map are automatically learned and jointly trained with the rest of the networks for improving the performance. Second, when compared with attention modules designed for image-level classification, our instance-centric attention map provides greater flexibility as it allows attending to different regions in an image depending on different object instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-stream network</head><p>As shown in <ref type="figure">Figure 3</ref>, our network uses three streams to compute the action scores based on human appearance s a h , object appearance s a o , and their spatial relationship s a sp . Human/object stream. For human and object stream, we extract both 1) the instance-level appearance feature x h inst for a person or x o inst for an object and 2) the contextual features x h context (or x o context ) based on the attentional map following the steps outlined in Section 3.2 and <ref type="figure" target="#fig_2">Figure 4</ref>. With the two feature vectors, we then concatenate them and pass it through two fully connected layers to produce the action scores s a h and s a o . The score s a h from the human stream also allows us to detect actions that do not involve any objects, e.g., walk, smile.</p><p>Pairwise stream. While the human and object appearance features contain strong cues for recognizing the interaction, using appearance features alone often leads to plausible but incorrect predictions. To encode the spatial relationship between the person and object, we adopt the two-channel binary image representation in <ref type="bibr" target="#b3">[4]</ref> to characterize the interaction patterns. Specifically, we take the union of these two boxes as the reference box and construct a binary image with two channels within it. The first channel has value 1 within the human bounding box and value 0 elsewhere; the second channel has value 1 within the object bounding box and value 0 elsewhere. We then use a CNN to extract spatial features from this two-channel binary image. However, we found that this feature by itself cannot produce accurate action prediction due to the coarse spatial information (only two bounding boxes). To address this, we concatenate the spatial feature with the human appearance feature x h inst . Our intuition is that the appearance of the person can greatly help disambiguate different actions with similar spatial layouts, e.g., riding vs. walking a bicycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient inference</head><p>Following Gkioxari et al. <ref type="bibr" target="#b13">[14]</ref> we compute the scores for the triplets in a cascade fashion. We first compute the scores from the human and the object stream action classification head, for each box b h and b o , respectively. This first step has a complexity of O(n) for n human/object instances. The second step involves computing scores of all possible human-object pairs. While the second step has a complexity of O(n 2 ), computing the scores S a h,o , however, is very efficient as it involves summing a pair of scores from the human stream s a h and object stream s a o (which are already computed and cached in the first step). Late vs. early fusion. We refer to our approach using the pairwise summing scores method as late fusion (because the action scores are independently predicted from the human/object streams first and then summed later). We also implement a variant of iCAN with early fusion. Specifically, we first concatenate all the features from human iCAN, object iCAN, and the pairwise stream and use two fully connected layers to predict the action score. Unlike late fusion, the early fusion approach needs to evaluate the scores from all human-object pairs and thus have slower inference speed and does not scale well for scenes with many objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the performance of our proposed iCAN model and compare with the state-ofthe-art on two large-scale HOI benchmark datasets. Additional results including detailed class-wise performance and error diagnosis can be found in the supplementary material. The source code and the pre-trained models are available on our project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets. V-COCO <ref type="bibr" target="#b15">[16]</ref> is a subset of the COCO dataset <ref type="bibr" target="#b25">[26]</ref> that provides HOI annotations. V-COCO includes a total of 10,346 images containing 16,199 human instances. Each person is annotated with a binary label vector for 26 different actions (where each entry indicates whether the person is performing a certain action). Each person can perform multiple actions at the same time, e.g., holding a cup while sitting on a chair. HICO-DET <ref type="bibr" target="#b2">[3]</ref> is a subset of the HICO dataset <ref type="bibr" target="#b2">[3]</ref>. HICO-DET contains 600 HOI categories over 80 object categories (same as <ref type="bibr" target="#b25">[26]</ref>), and provides more than 150K annotated instances of human-object pairs. Evaluation metrics. We evaluate the HOI detection performance using the commonly used role mean average precision (role mAP) <ref type="bibr" target="#b15">[16]</ref> for both V-COCO and HICO datasets. The goal is to detect the agent and the objects in the various roles for the action, denoted as the human, verb, object triplet. A detected triplet is considered as a true positive if it has the correct action label, and both the predicted human and object bounding boxes b h and b o have IoUs ≥ 0.5 w.r.t the ground truth annotations. Implementation details. We use Detectron <ref type="bibr" target="#b11">[12]</ref> with a feature backbone of ResNet-50-FPN <ref type="bibr" target="#b26">[27]</ref> to generate human and object bounding boxes. We keep human boxes with scores s h higher than 0.8 and object boxes with scores s o higher than 0.4. We implement our network based on Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> with a ResNet-50 <ref type="bibr" target="#b16">[17]</ref> feature backbone. <ref type="bibr" target="#b1">2</ref> We train our network for 300K iterations on the V-COCO trainval set with a learning rate of 0.001, a weight decay of 0.0001, and a momentum of 0.9. Training our network on V-COCO takes 16 hours on a <ref type="table">Table 1</ref>: Performance comparison with the state-of-the-arts on the V-COCO test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Feature backbone AP role Model C of <ref type="bibr" target="#b15">[16]</ref> (implemented by <ref type="bibr" target="#b13">[14]</ref>) ResNet-50-FPN 31.8 InteractNet <ref type="bibr" target="#b13">[14]</ref> ResNet-50-FPN 40.0 BAR-CNN <ref type="bibr" target="#b21">[22]</ref> Inception-ResNet <ref type="bibr">[</ref>  single NVIDIA P100 GPU. For HICO-DET, training the network on the train set takes 72 hours. Using a single NVIDIA P100 GPU, our method (with late score fusion) takes less than 75ms to process an image of size 480 × 640 (including ResNet-50 feature extraction, multistream network, attention-based feature extraction, and HOI recognition). We apply the same training and inference procedures for both V-COCO and HICO-DET datasets. Please refer to the supplementary material for additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative evaluation</head><p>We present the overall quantitative results in terms of AP role on V-COCO in <ref type="table">Table 1</ref> and HICO-DET in <ref type="table" target="#tab_1">Table 2</ref>. For V-COCO, the proposed instance-centric attention network achieves sizable performance boost over competing approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. For HICO-DET, we also demonstrate that our method compares favorably against existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>. Following the evaluation protocol <ref type="bibr" target="#b3">[4]</ref>, we report the quantitative evaluation of all, rare, and non-rare interactions with two different settings: 'Default' and 'Known Object'. Compared to <ref type="bibr" target="#b13">[14]</ref>, we achieve an absolute gain of 4.90 points over the best-performing model (InteractNet) <ref type="bibr" target="#b13">[14]</ref> under the full category of the 'Default' setting. This amounts to a relative improvement of 49.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative evaluation</head><p>HOI detection results. Here we show sample HOI detection results on the V-COCO dataset and the HICO-DET dataset. We highlight the detected human and object with red and blue bounding boxes, respectively. <ref type="figure">Figure 5</ref> shows that our model can predict HOIs in a wide variety of different situations. <ref type="figure" target="#fig_3">Figure 6</ref> shows that our model is capable of predicting different action with the objects from the same category. <ref type="figure" target="#fig_4">Figure 7</ref> presents two examples of detecting a person interacting with different objects.</p><p>Attention map visualization. <ref type="figure" target="#fig_5">Figure 8</ref> visualizes the human-centric and object-centric attention maps. The human-centric attention map often focuses on the surrounding objects that help disambiguate action prediction for the detected person. The object-centric atten-  tion map, on the other hand, highlights the informative human body part, e.g., in the first image, the attention map highlights the right hand of the person even though he was not holding anything with his right hand. We also show an example of two detected persons doing different actions. We show the cropped 100 × 100 patches centered at the peaks of the generated human-centric attentional maps. The highlighted regions roughly correspond to the objects they are interacting with.   contextual features, we investigate several different approaches for incorporating contextual information of the image, including bottom-up attention map <ref type="bibr" target="#b8">[9]</ref>, convolutional features from the entire image <ref type="bibr" target="#b30">[31]</ref>, and the proposed instance-centric attention map. <ref type="table" target="#tab_3">Table 3</ref>(a) shows that incorporating contextual features generally helps improve the HOI detection performance.</p><p>Our approach provides a larger boost over methods that use features without conditioning on the instance-of-interest.</p><p>Human-centric vs. object-centric.  <ref type="table" target="#tab_3">Table 3</ref>(c) characterizes the variants of the proposed iCAN using their trade-off in terms of mAP, inference time, and memory usage. Our model with early fusion achieves the best performance on V-COCO dataset. However, this comes at the cost of expensive evaluation of all possible human-object pairs in an image based on their appearance features, and slower training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose an instance-centric attention module for HOI detection. Our core idea is to learn to highlight informative regions from an image using the appearance of a person and an object instance, which allow us to gather relevant contextual information facilitating HOI detection. We validate the effectiveness of our approach and show a sizable performance boost compared to the state-of-the-arts on two HOI benchmark datasets. In this work we consider class-agnostic instance-centric attention. We believe that the classdependent instance-centric attention is a promising future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>iCAN module. Given the convolutional features of the image (shown in gray) and a human/object bounding box (shown in red), the iCAN module extracts the appearance features of the instance x h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Sample HOI detections on the HICO-DET test set. Our model detects different types of interactions with objects from the same category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Detecting multiple actions. Our model detects an individual taking multiple actions and interacting with different objects, e.g., the person sitting on the couch is reading a book while working on a laptop.hold cupHuman-centric att. Object-centric att. talk on cellphone Human-centric att. Object-centric att.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Attention map visualization. (Top) Examples of human/object-centric attention maps. (Bottom) 100 × 100 patches centered at the peaks of the human-centric attentional maps generated by the two persons. Our model learns to attend to objects (e.g., bicycle, skateboard) and the human poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with the state-of-the-arts on HICO-DET test set. The results from our model are from late fusion.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell cols="2">Known Object</cell></row><row><cell>Method</cell><cell>Feature backbone</cell><cell>Full</cell><cell cols="2">Rare Non Rare</cell><cell>Full</cell><cell cols="2">Rare Non Rare</cell></row><row><cell>Shen et al. [35]</cell><cell>VGG-19</cell><cell>6.46</cell><cell>4.24</cell><cell>7.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HO-RCNN [4]</cell><cell>CaffeNet</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell><cell cols="2">10.41 8.94</cell><cell>10.85</cell></row><row><cell cols="2">InteractNet [14] ResNet-50-FPN</cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iCAN (ours)</cell><cell>ResNet-50</cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell cols="2">16.26 11.33</cell><cell>17.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Sample HOI detections on the V-COCO test set. Our model detects various forms of HOIs in everyday photos. For actions 'ride', 'eat', 'lay' and 'drink', our model detects a diverse set of objects that the persons are interacting with in different situations.</figDesc><table><row><cell>ride elephant</cell><cell>ride motorcycle</cell><cell>ride boat</cell><cell>ride horse</cell><cell>ride bike</cell><cell>ride truck</cell></row><row><cell>eat banana</cell><cell>eat hot dog</cell><cell>eat carrot</cell><cell>eat sandwich</cell><cell>eat donut</cell><cell>eat pizza</cell></row><row><cell>lay on couch</cell><cell>lay on bench</cell><cell>lay on bed</cell><cell>drink w/ wineglass</cell><cell>drink w/ cup</cell><cell>drink w/ bottle</cell></row><row><cell>Figure 5: hold motorcycle</cell><cell>inspect motorcycle</cell><cell>jump motorcycle</cell><cell>race motorcycle</cell><cell>turn motorcycle</cell><cell>straddle motorcycle</cell></row><row><cell>feed elephant</cell><cell>hose elephant</cell><cell>kiss elephant</cell><cell>pet elephant</cell><cell>ride elephant</cell><cell>walk elephant</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the V-COCO test dataset</figDesc><table><row><cell></cell><cell>AP role</cell><cell cols="2">Human Object</cell><cell>AP role</cell></row><row><cell>None</cell><cell>42.5</cell><cell>-</cell><cell>-</cell><cell>42.5</cell></row><row><cell>Full image [31]</cell><cell>42.9</cell><cell></cell><cell>-</cell><cell>44.4</cell></row><row><cell>Bottom-up att. [9]</cell><cell>43.2</cell><cell>-</cell><cell></cell><cell>44.3</cell></row><row><cell>Inst-centric att. (ours)</cell><cell>44.7</cell><cell></cell><cell></cell><cell>44.7</cell></row><row><cell>(a) Scene feature</cell><cell></cell><cell cols="3">(b) Human/Object stream</cell><cell>(c) mAP vs. time/model size</cell></row><row><cell cols="2">4.4 Ablation study</cell><cell></cell><cell></cell><cell></cell></row></table><note>Contextual feature. Recognizing the correct actions using only human and object appear- ance features remains challenging. Building upon a strong baseline that does not use any</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>(b) validates the importance of leveraging both human-centric and object-centric attentional maps. mAP vs. time vs. memory.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Project webpage: https://gaochen315.github.io/iCAN/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We believe that using ResNet-50-FPN based on the Detectron framework for jointly training object detection and HOI detection could lead to improved performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by NSF under Grant No. (1755785). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to detect humanobject interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilhem</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Detecting visual relationships using box attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao&amp;apos;ou</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">ViP-CNN: Visual phrase guided convolutional neural network. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Does human action recognition benefit from pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining randomization and discrimination for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural Motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PPR-FCN: Weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards context-aware interaction recognition for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
