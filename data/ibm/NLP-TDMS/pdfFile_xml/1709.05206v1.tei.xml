<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LSTM Fully Convolutional Networks for Time Series Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fazle</forename><surname>Karim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houshang</forename><surname>Darabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Shun</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">LSTM Fully Convolutional Networks for Time Series Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Long Short Term Memory Recurrent Neural Network</term>
					<term>Time Series Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fully convolutional neural networks (FCN) have been shown to achieve state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared to others. We also explore the usage of attention mechanism to improve time series classification with the Attention Long Short Term Memory Fully Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose fine-tuning as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared to other techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Over the past decade, there has been an increased interest in time series classification. Time series data is ubiquitous, existing in weather readings, financial recordings, industrial observations, and psychological signals <ref type="bibr" target="#b0">[1]</ref>. In this paper two deep learning models to classify time series datasets are proposed, both of which outperform existing state-of-the-art models.</p><p>A plethora of research have been done using feature-based approaches or methods to extract a set of features that represent time series patterns. Bag-of-Words (BoW) <ref type="bibr" target="#b1">[2]</ref>, Bag-of-features (TSBF) <ref type="bibr" target="#b2">[3]</ref>, Bag-of-SFA-Symbols (BOSS) <ref type="bibr" target="#b3">[4]</ref>, BOSSVS <ref type="bibr" target="#b4">[5]</ref>, Word ExtrAction for time Series cLassification (WEASEL) <ref type="bibr" target="#b5">[6]</ref>, have obtained promising results in the field. Bag-of-words quantizes the extracted features and feeds the BoW into a classifier. TSBF extracts multiple subsequences of random local information, which a supervised learner condenses into a cookbook used to predict time series labels. BOSS introduces a combination of a distance based classifier and histograms. The histograms represent substructures of a time series that are created using a symbolic Fourier approximation. BOSSVS extends this method by proposing a vector space model to reduce time complexity while maintaining performance.</p><p>WEASEL converts time series into feature vectors using a sliding window. Machine learning algorithms utilize these feature vectors to detect and classify the time series. All these classifiers require heavy feature extraction and feature engineering.</p><p>Ensemble algorithms also yield state-of-the-art performance with time series classification problems. Three of the most successful ensemble algorithms that integrate various features of a time series are Elastic Ensemble (PROP) <ref type="bibr" target="#b6">[7]</ref>, a model that integrates 11 time series classifiers using a weighted ensemble method, Shapelet ensemble (SE) <ref type="bibr" target="#b7">[8]</ref>, a model that applies a heterogeneous ensemble onto transformed shapelets, and a flat collective of transform based ensembles (COTE) <ref type="bibr" target="#b7">[8]</ref>, a model that fuses 35 various classifiers into a single classifier.</p><p>Recently, deep neural networks have been employed for time series classification tasks. Multi-scale convolutional neural network (MCNN) <ref type="bibr" target="#b8">[9]</ref>, fully convolutional network (FCN) <ref type="bibr" target="#b9">[10]</ref>, and residual network (ResNet) <ref type="bibr" target="#b9">[10]</ref> are deep learning approaches that take advantage of convolutional neural networks (CNN) for end-to-end classification of univariate time series. MCNN uses down-sampling, skip sampling and sliding window to preprocess the data. The performance of the MCNN classifier is highly dependent on the preprocessing applied to the dataset and the tuning of a large set of hyperparameters of that model. On the other hand, FCN and ResNet do not require any heavy preprocessing on the data or feature engineering. In this paper, we improve the performance of FCN by augmenting the FCN module with either a Long Short Term Recurrent Neural Network (LSTM RNN) sub-module , called LSTM-FCN, or a LSTM RNN with attention, called ALSTM-FCN. Similar to FCN, both proposed models can be used to visualize the Class Activation Maps (CAM) of the convolutional layers to detect regions that contribute to the class label. In addition, the Attention LSTM can also be used detect regions of the input sequence that contribute to the class label through the context vector of the Attention LSTM cells. A major advantages of the LSTM-FCN and ALSTM-FCN models is that it does not require heavy preprocessing or feature engineering. Results indicate the new proposed models, LSTM-FCN and ALSTM-FCN, dramatically improve performance on the University of California Riverside (UCR) Benchmark datasets <ref type="bibr" target="#b10">[11]</ref>. LSTM-FCN and ALSTM-FCN produce better results than several state-of-the-art ensemble algorithms on a majority of the UCR Benchmark datasets. This paper proposes two deep learning models for endto-end time series classification. The proposed models do not require heavy preprocessing on the data or feature engineering. Both the models are tested on all 85 UCR time series benchmarks and outperform most of the state-of-the-art models. The remainder of the paper is organized as follows. Section II reviews the background work. Section III presents the architecture of the proposed models. Section IV analyzes and discusses the experiments performed. Finally, conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND WORKS A. Temporal Convolutions</head><p>The input to a Temporal Convolutional Network is generally a time series signal. As stated in Lea et al. <ref type="bibr" target="#b11">[12]</ref>, let X t ∈ R F0 be the input feature vector of length F 0 for time step t for 0 &lt; t ≤ T . Note that the time T may vary for each sequence, and we denote the number of time steps in each layer as T l . The true action label for each frame is given by y t ∈ {1, ..., C}, where C is the number of classes.</p><p>Consider L convolutional layers. We apply a set of 1D filters on each of these layers that capture how the input signals evolve over the course of an action. According to Lea et al. <ref type="bibr" target="#b11">[12]</ref>, the filters for each layer are parameterized by tensor W (l) ∈ R F l ×d×F l−1 and biases b (l) ∈ R F l , where l ∈ {1, ..., L} is the layer index and d is the filter duration. For the l-th layer, the i-th component of the (unnormalized) activationÊ</p><formula xml:id="formula_0">(l) t ∈ R F l is a function of the incoming (normal- ized) activation matrix E (l−1) ∈ R F l−1 ×T l−1 from the previous layerÊ (l) i,t = f b (l) i + d t =1 W (l) i,t ,. , E (l−1) .,t+d−t (1) for each time t where f (·) is a Rectified Linear Unit.</formula><p>We use Temporal Convolutional Networks as a feature extraction module in a Fully Convolutional Network (FCN) branch. A basic convolution block consists of a convolution layer, followed by batch normalization <ref type="bibr" target="#b12">[13]</ref>, followed by an activation function, which can be either a Rectified Linear Unit or a Parametric Rectified Linear Unit <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recurrent Neural Networks</head><p>Recurrent Neural Networks, often shortened to RNNs, are a class of neural networks which exhibit temporal behaviour due to directed connections between units of an individual layer. As reported by Pascanu et al. <ref type="bibr" target="#b14">[15]</ref>, recurrent neural networks maintain a hidden vector h, which is updated at time step t as follows:</p><formula xml:id="formula_1">h t = tanh(Wh t−1 + Ix t ),<label>(2)</label></formula><p>tanh is the hyperbolic tangent function, W is the recurrent weight matrix and I is a projection matrix. The hidden state h is used to make a prediction</p><formula xml:id="formula_2">y t = softmax(Wh t−1 ),<label>(3)</label></formula><p>softmax provides a normalized probability distribution over the possible classes, σ is the logistic sigmoid function and W is a weight matrix. By using h as the input to another RNN, we can stack RNNs, creating deeper architectures</p><formula xml:id="formula_3">h l t = σ(Wh l t−1 + Ih l−1 t ).<label>(4)</label></formula><p>C. Long Short-Term Memory RNNs Long short-term memory recurrent neural networks are an improvement over the general recurrent neural networks, which possess a vanishing gradient problem. As stated in Hochreiter et al. <ref type="bibr">[16]</ref>, LSTM RNNs address the vanishing gradient problem commonly found in ordinary recurrent neural networks by incorporating gating functions into their state dynamics. At each time step, an LSTM maintains a hidden vector h and a memory vector m responsible for controlling state updates and outputs. More concretely, Graves et al. <ref type="bibr" target="#b16">[17]</ref> define the computation at time step t as follows :</p><formula xml:id="formula_4">g u = σ(W u h t−1 + I u x t ) g f = σ(W f h t−1 + I f x t ) g o = σ(W o h t−1 + I o x t ) g c = tanh(W c h t−1 + I c x t ) m t = g f m t−1 + g u g c h t = tanh(g o m t )<label>(5)</label></formula><p>where σ is the logistic sigmoid function, represents elementwise multiplication, W u , W f , W o , W c are recurrent weight matrices and I u , I f , I o , I c are projection matrices.</p><p>While LSTMs possess the ability to learn temporal dependencies in sequences, they have difficulty with long term dependencies in long sequences. The attention mechanism proposed by Bahdanau et al. <ref type="bibr" target="#b17">[18]</ref> can help the LSTM RNN learn these dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention Mechanism</head><p>The attention mechanism is a technique often used in neural translation of text, where a context vector C is conditioned on the target sequence y. As discussed in Bahdanau et al. <ref type="bibr" target="#b17">[18]</ref>, the context vector c i depends on a sequence of annotations (h 1 , ..., h Tx ) to which an encoder maps the input sequence. Each annotation h i contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence.</p><p>The context vector c i is then computed as a weighted sum of these annotations h i :</p><formula xml:id="formula_5">c i = Tx j=1 α ij h j .<label>(6)</label></formula><p>The weight α ij of each annotation h j is computed by :</p><formula xml:id="formula_6">α ij = exp(e ij ) Tx k=1 exp(e ik )<label>(7)</label></formula><p>where e ij = a(s i−1 , h j ) is an alignment model, which scores how well the input around position j and the output at position i match. The score is based on the RNN hidden state s i1 and the j-th annotation h j of the input sentence.</p><p>Bahdanau et al. <ref type="bibr" target="#b17">[18]</ref> parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the model. The alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LSTM FULLY CONVOLUTIONAL NETWORK A. Network Architecture</head><p>Temporal convolutions have proven to be an effective learning model for time series classification problems <ref type="bibr" target="#b9">[10]</ref>. Fully Convolutional Networks comprised of temporal convolutions are typically used as feature extractors, and global average pooling <ref type="bibr" target="#b18">[19]</ref> is used to reduce the number of parameters in the model prior to classification. In the proposed models, the fully convolutional block is augmented by an LSTM block followed by dropout <ref type="bibr" target="#b19">[20]</ref>, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>.</p><p>The fully convolutional block consists of three stacked temporal convolutional blocks with filter sizes of 128, 256, and 128 respectively. Each convolutional block is identical to the convolution block in the CNN architecture proposed by Wang et al. <ref type="bibr" target="#b9">[10]</ref>. Each block consists of a temporal convolutional layer, which is accompanied by batch normalization <ref type="bibr" target="#b12">[13]</ref> (momentum of 0.99, epsilon of 0.001) followed by a ReLU activation function. Finally, global average pooling is applied following the final convolution block.</p><p>Simultaneously, the time series input is conveyed into a dimension shuffle layer (explained more in Section III-B). The transformed time series from the dimension shuffle is then passed into the LSTM block. The LSTM block comprises of either a general LSTM layer or an Attention LSTM layer, followed by a dropout. The output of the global pooling layer and the LSTM block is concatenated and passed onto a softmax classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Input</head><p>The fully convolutional block and LSTM block perceive the same time series input in two different views. The fully convolutional block views the time series as a univariate time series with multiple time steps. If there is a time series of length N , the fully convolutional block will receive the data in N time steps.</p><p>Contrarily, the LSTM block in the proposed architecture receives the input time series as a multivariate time series with a single time step. This is accomplished by the dimension shuffle layer, which transposes the temporal dimension of the time series. A univariate time series of length N , after transformation, will be viewed as a multivariate time series (having N variables) with a single time step.</p><p>This approach is key to the enhanced performance of the proposed architecture. In contrast, when the LSTM block received the univariate time series with N time steps, the performance was significantly reduced due to rapid overfitting on small short-sequence UCR datasets and a failure to learn long term dependencies in the larger long-sequence UCR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fine-Tuning of Models</head><p>Transfer learning is a technique wherein the knowledge gained from training a model on a dataset can be reused when training the model on another dataset, such that the domain of the new dataset has some similarity with the prior domain <ref type="bibr" target="#b20">[21]</ref>. Similarly, fine-tuning can be described as transfer learning on the same dataset.</p><p>The training procedure can thus be split into two distinct phases. In the initial phase, the optimal hyperparameters for the model are selected for a given dataset. The model is then trained on the given dataset with these hyperparameter settings. In the second step, we apply fine-tuning to this initial model. The procedure of transfer learning is iterated over in the fine-tuning phase, using the original dataset. Each repetition is initialized using the model weight of the previous iteration. At each iteration the learning rate is halved. Furthermore, the batch size is halved once every alternate iteration. This is done until the initial learning rate is 1e−4 and batch size is 32. The procedure is repeated K times, where K is an arbitrary constant, generally set as 5. initial model weights ← model weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>i ← i + 1 <ref type="bibr">6:</ref> initial lr ← updateLearningRate(initial lr, i) <ref type="bibr">7:</ref> batchsize ← updateBatchsize(batchsize, i)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed models have been tested on all 85 UCR time series datasets <ref type="bibr" target="#b10">[11]</ref>. The FCN block was kept constant throughout all experiments. The optimal number of LSTM cells was found by hyperparameter search over a range of 8 cells to 128 cells. The number of training epochs was generally kept constant at 2000 epochs, but was increased for datasets where the algorithm required a longer time to converge. Initial batch size of 128 was used, and halved for each successive iteration of the fine-tuning algorithm. A high dropout rate of 80% was used after the LSTM or Attention LSTM layer to combat overfitting. Class imbalance was handled via a class weighing scheme inspired by King et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>All models were trained via the Adam optimizer <ref type="bibr" target="#b22">[23]</ref>, with an initial learning rate of 1e−3 and a final learning rate of 1e−4. All convolution kernels were initialized with the initialization proposed by He et al. <ref type="bibr" target="#b23">[24]</ref>. The learning rate was reduced by a factor of 1/ 3 √ 2 every 100 epochs of no improvement in the validation score, until the final learning rate was reached. No additional preprocessing was done on the UCR datasets as they have close to zero mean and unit variance. All models were fine-tuned, and scores stated in <ref type="table" target="#tab_0">Table  I</ref> refer to the scores obtained by models prior to and after finetuning. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>In this paper, the proposed model was evaluated using accuracy, rank based statistics, and the mean per class error as stated by Wang et al. <ref type="bibr" target="#b9">[10]</ref>.</p><p>The rank-based evaluations used are the arithmetic rank, geometric rank, and the Wilcoxon signed rank test. The arithmetic rank is the arithmetic mean of the rank of dataset. The geometric rank is the geometric mean of the rank of each dataset. The Wilcoxson signed rank test is used to compare the median rank of the proposed model and the existing state-ofthe-art models. The null hypothesis and alternative hypothesis are as follows:    <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The performance of the proposed models on the UCR datasets are summarized in <ref type="table" target="#tab_0">Table I</ref>. The colored cells are cells that outperform the state-of-the-art model for that dataset. Both proposed models, the ALSTM-FCN model and the LSTM-FCN model, with both phases, without fine-tuning (Phase 1) and with fine-tuning (Phase 2), outperforms the state-of-the-art models in at least 43 datasets. The average arithmetic rank in <ref type="figure" target="#fig_4">Fig. 3</ref> indicates the superiority of our proposed models over the existing state-of-the-art models. This is further validated using the Wilcoxon signed rank test, where the p-value of each of the proposed models are less than 0.05 when compared to existing state-of-the-art models, <ref type="table" target="#tab_0">Table II.</ref> The Wilcoxon Signed Test also provides evidence that finetuning maintains or improves the overall accuracy on each of the proposed models. The MPCE of the LSTM-FCN and ALSTM-FCN models was found to reduce by 0.0035 and 0.0007 respectively when fine-tuning was applied. Fine-tuning improves the accuracy of the LSTM-FCN models on a greater number of datasets as compared to the ALSTM-FCN models. We postulate that this discrepancy is due to the fact that the LSTM-FCN model contains fewer total parameters than the ALSTM-FCN model. This indicates a lower rate of overfitting on the UCR datasets. As a consequence, fine-tuning is more effective on the LSTM-FCN models for the UCR datasets.</p><p>A significant drawback of fine-tuning is that it requires more training time due to the added computational complexity of retraining the model using smaller batch sizes. The disadvantages of fine-tuning are mitigated when using the ALSTM-FCN within Phase 1. At the end of Phase 1, the ALSTM-FCN    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION &amp; FUTURE WORK</head><p>With the proposed models, we achieve a potent improvement in the current state-of-the-art for time series classification using deep neural networks. Our baseline models, with and without fine-tuning, are trainable end-to-end with nominal preprocessing and are able to achieve significantly improved performance. LSTM-FCNs are able to augment FCN models, appreciably increasing their performance with a nominal increase in the number of parameters. ALSTM-FCNs provide one with the ability to visually inspect the decision process of the LSTM RNN and provide a strong baseline on their own. Fine-tuning can be applied as a general procedure to a model to further elevate its performance. The strong increase in performance in comparison to the FCN models shows that LSTM RNNs can beneficially supplement the performance of FCN modules for time series classification. An overall analysis of the performance of our model is provided and compared to other techniques.</p><p>There is further research to be done on understanding why the attention LSTM cell is unsuccessful in matching the performance of the general LSTM cell on some of the datasets. Furthermore, extension of the proposed models to multivariate time series is elementary, but has not been explored in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The LSTM-FCN architecture. LSTM cells can be replaced by Attention LSTM cells to construct the ALSTM-FCN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of context vector on CBF dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 Fine-tuning 1 : 3 :</head><label>113</label><figDesc>for i &lt; K do 2: model weights ← initial model weights Train(model, initial lr, batchsize) 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>H</head><label></label><figDesc>o : M edian proposed model = M edian state-of-the-art model H a : M edian proposed model = M edian state-of-the-art model Mean Per Class Error (MPCE) is defined as the arithmetic mean of the per class error (PCE),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Critical difference diagram of the arithmetic means of the ranks B. Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2</head><label>2</label><figDesc>is an example of the visual representation of the Attention LSTM cell on the "CBF" dataset. The points in the figure where the sequences are "squeezed" together are points at which all the classes have the same weight. These are the points in the time series at which the Attention LSTM can correctly identify the class. This is further supported by visual inspection of the actual time series. The squeeze points are points where each of the classes can be distinguished from each other, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>III: Summary of advantages of the proposed models Advantage LSTM-FCN F-t LSTM-FCN ALSTM-FCN F-t ALSTM-FCN Performance Visualization model outperforms the Phase 1 LSTM-FCN model. One of the major advantage of using the Attention LSTM cell is it provides a visual representation of the attention vector. The Attention LSTM also benefits from fine-tuning, but the effect is less significant as compared to the general LSTM model. A summary of the performance of each model type on certain characteristics is provided onTable III.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison of proposed models with the rest.</figDesc><table><row><cell>Dataset</cell><cell>Existing</cell><cell>LSTM-FCN</cell><cell>F-t</cell><cell>ALSTM-FCN</cell><cell>F-t</cell></row><row><cell></cell><cell>SOTA [6, 10]</cell><cell></cell><cell>LSTM-FCN</cell><cell></cell><cell>ALSTM-FCN</cell></row><row><cell>Adiac</cell><cell>0.8570</cell><cell>0.8593</cell><cell>0.8849</cell><cell>0.8670</cell><cell>0.8900*</cell></row><row><cell>ArrowHead</cell><cell>0.8800</cell><cell>0.9086</cell><cell>0.9029</cell><cell>0.9257*</cell><cell>0.9200</cell></row><row><cell>Beef</cell><cell>0.9000</cell><cell>0.9000</cell><cell>0.9330</cell><cell>0.9333*</cell><cell>0.9333*</cell></row><row><cell>BeetleFly</cell><cell>0.9500</cell><cell>0.9500</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>BirdChicken</cell><cell>0.9500</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>Car</cell><cell>0.9330</cell><cell>0.9500</cell><cell>0.9670</cell><cell>0.9667</cell><cell>0.9833*</cell></row><row><cell>CBF</cell><cell>1.0000</cell><cell>0.9978</cell><cell>1.0000*</cell><cell>0.9967</cell><cell>0.9967</cell></row><row><cell>ChloConc</cell><cell>0.8720</cell><cell>0.8099</cell><cell>1.0000*</cell><cell>0.8070</cell><cell>0.8070</cell></row><row><cell>CinC ECG</cell><cell>0.9949</cell><cell>0.8862</cell><cell>0.9094</cell><cell>0.9058</cell><cell>0.9058</cell></row><row><cell>Coffee</cell><cell>1.0000</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>Computers</cell><cell>0.8480</cell><cell>0.8600</cell><cell>0.8600</cell><cell>0.8640*</cell><cell>0.8640*</cell></row><row><cell>Cricket X</cell><cell>0.8210</cell><cell>0.8077</cell><cell>0.8256*</cell><cell>0.8051</cell><cell>0.8051</cell></row><row><cell>Cricket Y</cell><cell>0.8256</cell><cell>0.8179</cell><cell>0.8256*</cell><cell>0.8205</cell><cell>0.8205</cell></row><row><cell>Cricket Z</cell><cell>0.8154</cell><cell>0.8103</cell><cell>0.8257</cell><cell>0.8308</cell><cell>0.8333*</cell></row><row><cell>DiaSizeRed</cell><cell>0.9670</cell><cell>0.9673</cell><cell>0.9771*</cell><cell>0.9739</cell><cell>0.9739</cell></row><row><cell>DistPhxAgeGp</cell><cell>0.8350</cell><cell>0.8600</cell><cell>0.8600</cell><cell>0.8625*</cell><cell>0.8600</cell></row><row><cell>DistPhxCorr</cell><cell>0.8200</cell><cell>0.8250</cell><cell>0.8217</cell><cell>0.8417*</cell><cell>0.8383</cell></row><row><cell>DistPhxTW</cell><cell>0.7900</cell><cell>0.8175</cell><cell>0.8100</cell><cell>0.8175</cell><cell>0.8200*</cell></row><row><cell>Earthquakes</cell><cell>0.8010</cell><cell>0.8354*</cell><cell>0.8261</cell><cell>0.8292</cell><cell>0.8292</cell></row><row><cell>ECG200</cell><cell>0.9200</cell><cell>0.9000</cell><cell>0.9200*</cell><cell>0.9100</cell><cell>0.9200</cell></row><row><cell>ECG5000</cell><cell>0.9482</cell><cell>0.9473</cell><cell>0.9478</cell><cell>0.9484</cell><cell>0.9496*</cell></row><row><cell>ECGFiveDays</cell><cell>1.0000</cell><cell>0.9919</cell><cell>0.9942</cell><cell>0.9954</cell><cell>0.9954</cell></row><row><cell>ElectricDevices</cell><cell>0.7993</cell><cell>0.7681</cell><cell>0.7633</cell><cell>0.7672</cell><cell>0.7672</cell></row><row><cell>FaceAll</cell><cell>0.9290</cell><cell>0.9402</cell><cell>0.9680</cell><cell>0.9657</cell><cell>0.9728*</cell></row><row><cell>FaceFour</cell><cell>1.0000</cell><cell>0.9432</cell><cell>0.9772</cell><cell>0.9432</cell><cell>0.9432</cell></row><row><cell>FacesUCR</cell><cell>0.9580</cell><cell>0.9293</cell><cell>0.9898*</cell><cell>0.9434</cell><cell>0.9434</cell></row><row><cell>FiftyWords</cell><cell>0.8198</cell><cell>0.8044</cell><cell>0.8066</cell><cell>0.8242</cell><cell>0.8286*</cell></row><row><cell>Fish</cell><cell>0.9890</cell><cell>0.9829</cell><cell>0.9886</cell><cell>0.9771</cell><cell>0.9771</cell></row><row><cell>FordA</cell><cell>0.9727</cell><cell>0.9272</cell><cell>0.9733*</cell><cell>0.9267</cell><cell>0.9267</cell></row><row><cell>FordB</cell><cell>0.9173</cell><cell>0.9180</cell><cell>0.9186*</cell><cell>0.9158</cell><cell>0.9158</cell></row><row><cell>Gun Point</cell><cell>1.0000</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>Ham</cell><cell>0.7810</cell><cell>0.7714</cell><cell>0.8000</cell><cell>0.8381*</cell><cell>0.8000</cell></row><row><cell>HandOutlines</cell><cell>0.9487</cell><cell>0.8930</cell><cell>0.8870</cell><cell>0.9030</cell><cell>0.9030</cell></row><row><cell>Haptics</cell><cell>0.5510</cell><cell>0.5747*</cell><cell>0.5584</cell><cell>0.5649</cell><cell>0.5584</cell></row><row><cell>Herring</cell><cell>0.7030</cell><cell>0.7656*</cell><cell>0.7188</cell><cell>0.7500</cell><cell>0.7656*</cell></row><row><cell>InlineSkate</cell><cell>0.6127</cell><cell>0.4655</cell><cell>0.5000</cell><cell>0.4927</cell><cell>0.4927</cell></row><row><cell>InsWngSnd</cell><cell>0.6525</cell><cell>0.6616</cell><cell>0.6696</cell><cell>0.6823*</cell><cell>0.6818</cell></row><row><cell>ItPwDmd</cell><cell>0.9700</cell><cell>0.9631</cell><cell>0.9699</cell><cell>0.9602</cell><cell>0.9708*</cell></row><row><cell>LrgKitApp</cell><cell>0.8960</cell><cell>0.9200*</cell><cell>0.9200*</cell><cell>0.9067</cell><cell>0.9120</cell></row><row><cell>Lighting2</cell><cell>0.8853</cell><cell>0.8033</cell><cell>0.8197</cell><cell>0.7869</cell><cell>0.7869</cell></row><row><cell>Lighting7</cell><cell>0.8630</cell><cell>0.8356</cell><cell>0.9178*</cell><cell>0.8219</cell><cell>0.9178*</cell></row><row><cell>Mallat</cell><cell>0.9800</cell><cell>0.9808</cell><cell>0.9834</cell><cell>0.9838</cell><cell>0.9842*</cell></row><row><cell>Meat</cell><cell>1.0000</cell><cell>0.9167</cell><cell>1.0000*</cell><cell>0.9833</cell><cell>1.0000*</cell></row><row><cell>MedicalImages</cell><cell>0.7920</cell><cell>0.8013</cell><cell>0.8066*</cell><cell>0.7961</cell><cell>0.7961</cell></row><row><cell>MidPhxAgeGp</cell><cell>0.8144</cell><cell>0.8125</cell><cell>0.8150</cell><cell>0.8175*</cell><cell>0.8075</cell></row><row><cell>MidPhxCorr</cell><cell>0.8076</cell><cell>0.8217</cell><cell>0.8333</cell><cell>0.8400</cell><cell>0.8433*</cell></row><row><cell>MidPhxTW</cell><cell>0.6120</cell><cell>0.6165</cell><cell>0.6466</cell><cell>0.6466*</cell><cell>0.6316</cell></row><row><cell>MoteStrain</cell><cell>0.9500</cell><cell>0.9393</cell><cell>0.9569*</cell><cell>0.9361</cell><cell>0.9361</cell></row><row><cell>NonInv Thor1</cell><cell>0.9610</cell><cell>0.9654</cell><cell>0.9657</cell><cell>0.9751</cell><cell>0.9756*</cell></row><row><cell>NonInv Thor2</cell><cell>0.9550</cell><cell>0.9623</cell><cell>0.9613</cell><cell>0.9664</cell><cell>0.9674*</cell></row><row><cell>OliveOil</cell><cell>0.9333</cell><cell>0.8667</cell><cell>0.9333</cell><cell>0.9333</cell><cell>0.9667*</cell></row><row><cell>OSULeaf</cell><cell>0.9880</cell><cell>0.9959*</cell><cell>0.9959*</cell><cell>0.9959*</cell><cell>0.9917</cell></row><row><cell>PhalCorr</cell><cell>0.8300</cell><cell>0.8368</cell><cell>0.8392*</cell><cell>0.8380</cell><cell>0.8357</cell></row><row><cell>Phoneme</cell><cell>0.3492</cell><cell>0.3776*</cell><cell>0.3602</cell><cell>0.3671</cell><cell>0.3623</cell></row><row><cell>Plane</cell><cell>1.0000</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>ProxPhxAgeGp</cell><cell>0.8832</cell><cell>0.8927*</cell><cell>0.8878</cell><cell>0.8878</cell><cell>0.8927*</cell></row><row><cell>ProxPhxCorr</cell><cell>0.9180</cell><cell>0.9450*</cell><cell>0.9313</cell><cell>0.9313</cell><cell>0.9381</cell></row><row><cell>ProxPhxTW</cell><cell>0.8150</cell><cell>0.8350</cell><cell>0.8275</cell><cell>0.8375*</cell><cell>0.8375*</cell></row><row><cell>RefDev</cell><cell>0.5813</cell><cell>0.5813</cell><cell>0.5947*</cell><cell>0.5840</cell><cell>0.5840</cell></row><row><cell>ScreenType</cell><cell>0.7070</cell><cell>0.6693</cell><cell>0.7073</cell><cell>0.6907</cell><cell>0.6907</cell></row><row><cell>ShapeletSim</cell><cell>1.0000</cell><cell>0.9722</cell><cell>1.0000*</cell><cell>0.9833</cell><cell>0.9833</cell></row><row><cell>ShapesAll</cell><cell>0.9183</cell><cell>0.9017</cell><cell>0.9150</cell><cell>0.9183</cell><cell>0.9217*</cell></row><row><cell>SmlKitApp</cell><cell>0.8030</cell><cell>0.8080</cell><cell>0.8133*</cell><cell>0.7947</cell><cell>0.8133*</cell></row><row><cell>SonyAIBOI</cell><cell>0.9850</cell><cell>0.9817</cell><cell>0.9967</cell><cell>0.9700</cell><cell>0.9983*</cell></row><row><cell>SonyAIBOII</cell><cell>0.9620</cell><cell>0.9780</cell><cell>0.9822*</cell><cell>0.9748</cell><cell>0.9790</cell></row><row><cell>StarlightCurves</cell><cell>0.9796</cell><cell>0.9756</cell><cell>0.9763</cell><cell>0.9767</cell><cell>0.9767</cell></row><row><cell>Strawberry</cell><cell>0.9760</cell><cell>0.9838</cell><cell>0.9864</cell><cell>0.9838</cell><cell>0.9865*</cell></row><row><cell>SwedishLeaf</cell><cell>0.9664</cell><cell>0.9792</cell><cell>0.9840</cell><cell>0.9856*</cell><cell>0.9856*</cell></row><row><cell>Symbols</cell><cell>0.9668</cell><cell>0.9839</cell><cell>0.9849</cell><cell>0.9869</cell><cell>0.9889*</cell></row><row><cell>Synth Cntr</cell><cell>1.0000</cell><cell>0.9933</cell><cell>1.0000*</cell><cell>0.9900</cell><cell>0.9900</cell></row><row><cell>ToeSeg1</cell><cell>0.9737</cell><cell>0.9825</cell><cell>0.9912*</cell><cell>0.9868</cell><cell>0.9868</cell></row><row><cell>ToeSeg2</cell><cell>0.9615</cell><cell>0.9308</cell><cell>0.9462</cell><cell>0.9308</cell><cell>0.9308</cell></row><row><cell>Trace</cell><cell>1.0000</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell><cell>1.0000*</cell></row><row><cell>Two Patterns</cell><cell>1.0000</cell><cell>0.9968</cell><cell>0.9973</cell><cell>0.9968</cell><cell>0.9968</cell></row><row><cell>TwoLeadECG</cell><cell>1.0000</cell><cell>0.9991</cell><cell>1.0000*</cell><cell>0.9991</cell><cell>1.0000*</cell></row><row><cell>uWavGest X</cell><cell>0.8308</cell><cell>0.8490</cell><cell>0.8498</cell><cell>0.8481</cell><cell>0.8504*</cell></row><row><cell>uWavGest Y</cell><cell>0.7585</cell><cell>0.7672*</cell><cell>0.7661</cell><cell>0.7658</cell><cell>0.7644</cell></row><row><cell>uWavGest Z</cell><cell>0.7725</cell><cell>0.7973</cell><cell>0.7993</cell><cell>0.7982</cell><cell>0.8007*</cell></row><row><cell>uWavGestAll</cell><cell>0.9685</cell><cell>0.9618</cell><cell>0.9609</cell><cell>0.9626</cell><cell>0.9626</cell></row><row><cell>Wafer</cell><cell>1.0000</cell><cell>0.9992</cell><cell>1.0000*</cell><cell>0.9981</cell><cell>0.9981</cell></row><row><cell>Wine</cell><cell>0.8890</cell><cell>0.8704</cell><cell>0.8890</cell><cell>0.9074*</cell><cell>0.9074*</cell></row><row><cell>WordsSynonyms</cell><cell>0.7790</cell><cell>0.6708</cell><cell>0.6991</cell><cell>0.6677</cell><cell>0.6677</cell></row><row><cell>Worms</cell><cell>0.8052</cell><cell>0.6685</cell><cell>0.6851</cell><cell>0.6575</cell><cell>0.6575</cell></row><row><cell>WormsTwoClass</cell><cell>0.8312</cell><cell>0.7956</cell><cell>0.8066</cell><cell>0.8011</cell><cell>0.8011</cell></row><row><cell>yoga</cell><cell>0.9183</cell><cell>0.9177</cell><cell>0.9163</cell><cell>0.9190</cell><cell>0.9237*</cell></row><row><cell>Count</cell><cell>-</cell><cell>43</cell><cell>65</cell><cell>51</cell><cell>57</cell></row><row><cell>MPCE</cell><cell>-</cell><cell>0.0318</cell><cell>0.0283</cell><cell>0.0301</cell><cell>0.0294</cell></row><row><cell>Arith. Mean</cell><cell>-</cell><cell>-</cell><cell>2.1529</cell><cell>-</cell><cell>2.5647</cell></row><row><cell>Geom. Mean</cell><cell>-</cell><cell>-</cell><cell>1.8046</cell><cell>-</cell><cell>1.8506</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Wilcoxon Signed Rank Test comparison of each Model</figDesc><table><row><cell></cell><cell>WEASEL</cell><cell>1-NN DTW CV</cell><cell>1-NN DTW</cell><cell>BOSS</cell><cell>Learning Shapelet</cell><cell>TSBF</cell><cell>ST</cell><cell>EE</cell><cell>COTE</cell><cell>MLP</cell><cell>CNN</cell><cell>ResNet</cell><cell>LSTM-FCN</cell><cell>F-t LSTM-FCN</cell><cell>ALSTM-FCN</cell></row><row><cell>WEASEL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1-NN DTW CV</cell><cell>2.39E-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1-NN DTW</cell><cell>2.53E-12</cell><cell>7.20E-04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BOSS</cell><cell>4.27E-03</cell><cell>1.82E-07</cell><cell>5.31E-11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning Shapelet</cell><cell>2.00E-04</cell><cell>2.53E-02</cell><cell>2.33E-04</cell><cell>1.94E-02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSBF</cell><cell>2.18E-05</cell><cell>1.59E-01</cell><cell>2.49E-03</cell><cell>4.36E-03</cell><cell>4.73E-01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ST</cell><cell>1.29E-01</cell><cell>1.05E-07</cell><cell>9.64E-11</cell><cell>2.39E-01</cell><cell>1.61E-03</cell><cell>3.60E-04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EE</cell><cell>4.51E-05</cell><cell>3.45E-07</cell><cell>1.31E-10</cell><cell>1.37E-02</cell><cell>6.13E-01</cell><cell>2.02E-01</cell><cell>1.39E-03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>COTE</cell><cell>5.44E-01</cell><cell>3.05E-14</cell><cell>3.03E-16</cell><cell>6.21E-04</cell><cell>4.76E-07</cell><cell>1.13E-06</cell><cell>4.24E-03</cell><cell>3.54E-11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell>2.56E-07</cell><cell>5.21E-01</cell><cell>3.41E-01</cell><cell>6.89E-05</cell><cell>1.44E-02</cell><cell>8.37E-02</cell><cell>6.76E-06</cell><cell>4.88E-03</cell><cell>2.84E-08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN</cell><cell>2.77E-01</cell><cell>1.84E-10</cell><cell>2.14E-15</cell><cell>1.03E-03</cell><cell>3.65E-06</cell><cell>1.54E-06</cell><cell>8.85E-03</cell><cell>6.07E-06</cell><cell>4.82E-01</cell><cell>2.79E-09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet</cell><cell>5.67E-01</cell><cell>1.82E-10</cell><cell>5.95E-15</cell><cell>4.38E-03</cell><cell>1.32E-05</cell><cell>3.56E-06</cell><cell>2.47E-02</cell><cell>1.09E-05</cell><cell>9.61E-01</cell><cell>4.64E-08</cell><cell>2.52E-01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM-FCN</cell><cell>4.92E-06</cell><cell>1.92E-17</cell><cell>8.59E-21</cell><cell>3.00E-11</cell><cell>2.65E-12</cell><cell>4.04E-12</cell><cell>9.93E-13</cell><cell>5.14E-13</cell><cell>1.60E-07</cell><cell>1.61E-14</cell><cell>1.05E-07</cell><cell>4.91E-10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F-t LSTM-FCN</cell><cell>1.23E-08</cell><cell>5.17E-19</cell><cell>5.77E-22</cell><cell>3.35E-13</cell><cell>2.20E-13</cell><cell>1.12E-13</cell><cell>3.44E-14</cell><cell>1.25E-14</cell><cell>2.81E-10</cell><cell>5.09E-16</cell><cell>3.35E-12</cell><cell>4.58E-15</cell><cell>7.53E-05</cell><cell></cell><cell></cell></row><row><cell>ALSTM-FCN</cell><cell>1.34E-07</cell><cell>2.74E-18</cell><cell>5.14E-21</cell><cell>1.34E-12</cell><cell>3.38E-12</cell><cell>7.48E-13</cell><cell>7.11E-14</cell><cell>1.26E-13</cell><cell>1.30E-08</cell><cell>1.70E-15</cell><cell>3.74E-09</cell><cell>1.33E-11</cell><cell>8.53E-04</cell><cell>3.06E-02</cell><cell></cell></row><row><cell>F-t ALSTM-FCN</cell><cell>4.58E-08</cell><cell>1.01E-18</cell><cell>1.18E-21</cell><cell>1.44E-12</cell><cell>2.41E-12</cell><cell>4.63E-13</cell><cell>3.96E-14</cell><cell>4.12E-14</cell><cell>2.56E-09</cell><cell>1.87E-15</cell><cell>2.60E-10</cell><cell>1.12E-12</cell><cell>5.96E-05</cell><cell>1.89E-01</cell><cell>5.40E-02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The codes and weights of each models are available at https://github.com/houshd/LSTM-FCN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Green cells designate instances where our performance matches or exceeds state-of-the-art results. * denotes model with best performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Kadous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New South Wales</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experiencing SAX: A Novel Symbolic Representation of Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="144" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Bag-of-Features Framework to Classify Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tuv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2796" to="2802" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The BOSS is Concerned with Time Series Classification in the Presence of Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1530" />
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast and Accurate Time Series Classification with WEASEL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07681</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Time Series Classification with Ensembles of Elastic Distance Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="592" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Time-Series Classification with COTE: The Collective of Transformation-Based Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2522" to="2535" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06995</idno>
		<title level="m">Multi-Scale Convolutional Neural Networks for Time Series Classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<ptr target="www.cs.ucr.edu/∼eamonn/timeseriesdata/" />
		<title level="m">The UCR Time Series Classification Archive</title>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal Convolutional Networks: A Unified Approach to Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giguère</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.09332" />
		<title level="m">Parametric Exponential Linear Unit for Deep Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How to Construct Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">Network in Network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How Transferable are Features in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Logistic Regression in Rare Events Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="163" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
