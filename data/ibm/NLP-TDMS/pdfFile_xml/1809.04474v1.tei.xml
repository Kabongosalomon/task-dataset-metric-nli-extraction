<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Deep Reinforcement Learning with PopArt</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
						</author>
						<title level="a" type="main">Multi-task Deep Reinforcement Learning with PopArt</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequentialdecision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent's updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy -with a single set of weights -that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, the field of deep reinforcement learning (RL) has enjoyed many successes. Deep RL agents have been applied to board games such as <ref type="bibr">Go (Silver et al. 2016</ref>) and chess , continuous control <ref type="bibr" target="#b3">Duan et al. 2016)</ref>, classic video-games such as Atari <ref type="bibr" target="#b10">(Mnih et al. 2015;</ref><ref type="bibr" target="#b4">Gruslys et al. 2018;</ref><ref type="bibr" target="#b15">Schulman et al. 2015;</ref><ref type="bibr" target="#b15">Schulman et al. 2017;</ref><ref type="bibr" target="#b0">Bacon, Harb, and Precup 2017)</ref>, and 3D first person environments <ref type="bibr" target="#b5">Jaderberg et al. 2016)</ref>. While the results are impressive, they were achieved on one task at the time, each task requiring to train a new agent instance from scratch.</p><p>Multi-task and transfer learning remain important open problems in deep RL. There are at least four different strains Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. of multi-task reinforcement learning that have been explored in the literature: off-policy learning of many predictions about the same stream of experience <ref type="bibr" target="#b14">(Schmidhuber 1990;</ref><ref type="bibr" target="#b16">Sutton et al. 2011;</ref><ref type="bibr" target="#b5">Jaderberg et al. 2016)</ref>, continual learning in a sequence of tasks <ref type="bibr" target="#b12">(Ring 1994;</ref><ref type="bibr" target="#b19">Thrun 1996;</ref><ref type="bibr" target="#b20">Thrun 2012;</ref>, distillation of task-specific experts into a single shared model <ref type="bibr" target="#b11">(Parisotto, Ba, and Salakhutdinov 2015;</ref><ref type="bibr">Rusu et al. 2015;</ref><ref type="bibr" target="#b15">Schmitt et al. 2018;</ref><ref type="bibr" target="#b18">Teh et al. 2017)</ref>, and parallel learning of multiple tasks at once <ref type="bibr" target="#b15">(Sharma and Ravindran 2017;</ref><ref type="bibr">Caruana 1998)</ref>. We will focus on the latter.</p><p>Parallel multi-task learning has recently achieved remarkable success in enabling a single system to learn a large number of diverse tasks. The Importance Weighted Actor-Learner Architecture, henceforth IMPALA <ref type="bibr" target="#b3">(Espeholt et al. 2018)</ref>, achieved a 59.7% median human normalised score across 57 Atari games, and a 49.4% mean human normalised score across 30 DeepMind Lab levels. These results are state of the art for multi-task RL, but they are far from the humanlevel performance demonstrated by deep RL agents on the same domains, when trained on each task individually.</p><p>Part of why multi-task learning is much harder than single task learning is that a balance must be found between the needs of multiple tasks, that compete for the limited resources of a single learning system (for instance, for its limited representation capacity). We observed that the naive transposition of common RL algorithms to the multi-task setting may not perform well in this respect. More specifically, the saliency of a task for the agent increases with the scale of the returns observed in that task, and these may differ arbitrarily across tasks. This affects value-based algorithms such as Q-learning <ref type="bibr" target="#b22">(Watkins 1989)</ref>, as well as policybased algorithms such as REINFORCE <ref type="bibr" target="#b22">(Williams 1992)</ref>.</p><p>The problem of scaling individual rewards appropriately is not novel, and has often been addressed through reward clipping <ref type="bibr" target="#b10">(Mnih et al. 2015)</ref>. This heuristic changes the agent's objective, e.g., if all rewards are non-negative the algorithm optimises frequency of rewards rather than their cumulative sum. If the two objectives are sufficiently well aligned, clipping can be effective. However, the scale of returns also depends on the rewards' sparsity. This implies that, even with reward clipping, in a multi-task setting the magnitude of updates can still differ significantly between tasks, causing some tasks to have a larger impact on the learning dynamics than other equally important ones.</p><p>Note that both the sparsity and the magnitude of rewards collected in an environment are inherently non-stationary, because the agent is learning to actively maximise the total amount of rewards it can collect. These non-stationary learning dynamics make it impossible to normalise the learning updates a priori, even if we would be willing to pour significant domain knowledge into the design of the algorithm.</p><p>To summarise, in IMPALA the magnitude of updates resulting from experience gathered in each environment depends on: 1) the scale of rewards, 2) the sparsity of rewards, 3) the competence of the agent. In this paper we use PopArt normalisation  to derive an actorcritic update invariant to these factors, enabling large performance improvements in parallel multi-task agents. We demonstrated this on the Atari-57 benchmark, where a single agent achieved a median normalised score of 110% and on DmLab-30, where it achieved a mean score of 72.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Reinforcement learning (RL) is a framework for learning and decision-making under uncertainty <ref type="bibr" target="#b15">(Sutton and Barto 2018)</ref>. A learning system -the agent -must learn to interact with the environment it is embedded in, so as to maximise a scalar reward signal. The RL problem is often formalised as a Markov decision process (Bellman 1957): a tuple (S, A, p, γ), where S, A are finite sets of states and actions, p denotes the dynamics, such that p(r, s | s, a) is the probability of observing reward r and state s when executing action a in state s, and γ ∈ [0, 1] discounts future rewards. The policy maps states s ∈ S to probability distributions over actions π(A|S = s), thus specifying the behaviour of the agent. The return G t = R t+1 +γR t+2 +. . . is the γ-discounted sum of rewards collected by an agent from state S t onward under policy π. We define action values and state values as q π (s, a) = E π [G t | S t = s, A t = a] and v π (s) = E π [G t | S t = s], respectively. The agent's objective is to find a policy to maximise such values.</p><p>In multi-task reinforcement learning, a single agent must learn to master N different environments</p><formula xml:id="formula_0">T = {D i = (S i , A i , p i , γ)} N i=1</formula><p>, each with its own distinct dynamics (Brunskill and Li 2013). Particularly interesting is the case in which the action space and transition dynamics are at least partially shared. For instance, the environments might follow the same physical rules, while the set of interconnected states and obtainable rewards differ. We may formalise this as a single larger MDP, whose state space is</p><formula xml:id="formula_1">S = {{(s j , i)} sj ∈Si } N i=1 .</formula><p>The task index i may be latent, or may be exposed to the agent's policy. In this paper, we use the task index at training time, for the value estimates used to compute the policy updates, but not at testing time: our algorithm will return a single general policy π(A|S) which is only function of the individual environment's state S and not conditioned directly on task index i. This is more challenging than the standard multi-task learning setup, which typically allows conditioning the model on the task index even at evaluation <ref type="bibr" target="#b13">(Romera-Paredes et al. 2013;</ref><ref type="bibr" target="#b2">Collobert and Weston 2008)</ref>, because our agents will need to infer what task to solve purely from the stream of raw observations and/or early rewards in the episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actor-critic</head><p>In our experiments, we use an actor-critic algorithm to learn a policy π η (A|S) and a value estimate v θ (s), which are both outputs of a deep neural network. We update the agent's policy by using REINFORCE-style stochastic gradi-</p><formula xml:id="formula_2">ent (G t − v θ (S t ))∇ η log π(A t |S t ) (Williams 1992), where v θ (S t )</formula><p>is used as a baseline to reduce variance. In addition we use a multi-step return G v t that bootstraps on the value estimates after a limited number of transitions, both to reduce variance further and to allow us to update the policy before G t fully resolves at the end of an episode. The value function v θ (S) is instead updated to minimise the squared loss with respect to the (truncated and bootstrapped) return:</p><formula xml:id="formula_3">∆θ ∝ −∇ θ (G v t −v θ (S t )) 2 = (G v t −v θ (S t ))∇ θ v θ (S t ) (1) ∆η ∝ (G π t − v θ (S t ))∇ η log(π η (A t |S t )) ,<label>(2)</label></formula><p>where G v t and G π t are stochastic estimates of v π (S t ) and q π (S t , A t ), respectively. Note how both updates depend linearly on the scale of returns, which, as previously argued, depend on scale/sparsity of rewards, and agent's competence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient multi-task learning in simulation</head><p>We use the IMPALA agent architecture <ref type="bibr" target="#b3">(Espeholt et al. 2018)</ref>, proposed for reinforcement learning in simulated environments. In IMPALA the agent is distributed across multiple threads, processes or machines. Several actors run on CPU generating rollouts of experience, consisting of a fixed number of interactions (100 in our experiments) with their own copy of the environment, and then enqueue the rollouts in a shared queue. Actors receive the latest copy of the network's parameters from the learner before each rollout. A single GPU learner processes rollouts from all actors, in batches, and updates a deep network. The network is a deep convolutional ResNet <ref type="bibr" target="#b4">(He et al. 2015)</ref>, followed by a LSTM recurrent layer <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref>. Policy and values are all linear functions of the LSTM's output.</p><p>Despite the large network used for estimating policy π η and values v θ , the decoupled nature of the agent enables to process data very efficiently: in the order of hundreds of thousands frames per second <ref type="bibr" target="#b3">(Espeholt et al. 2018)</ref>. The setup easily supports the multi-task setting by simply assigning different environments to each of the actors and then running the single policy π(S|A) on each of them. The data in the queue can also be easily labelled with the task id, if useful at training time. Note that an efficient implementation of IMPALA is available open-source 1 , and that, while we use this agent for our experiments, our approach can be applied to other data parallel multi-task agents (e.g. A3C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Off-policy corrections</head><p>Because we use a distributed queue-based learning setup, the data consumed by the learning algorithm might be slightly off-policy, as the policy parameters change between acting and learning. We can use importance sampling corrections ρ t = π(A t |S t )/µ(A t |S t ) to compensate for this <ref type="bibr" target="#b11">(Precup, Sutton, and Singh 2000)</ref>. In particular, we can write the nstep return as</p><formula xml:id="formula_4">G t = R t+1 + γR t+2 + . . . + γ n v(S t+n ) = v(S t ) + t+n−1 k=t γ k−t δ k , where δ t = R t+1 + γv(S t+1 ) − v(S t )</formula><p>, and then apply appropriate importance sampling corrections to each error term <ref type="bibr" target="#b17">(Sutton et al. 2014)</ref> </p><formula xml:id="formula_5">to get G t = v(S t )+ t+n−1 k=t γ k−t ( k i=t ρ i )δ k .</formula><p>This is unbiased, but has high variance. To reduce variance, we can further clip most of the importance-sampling ratios, e.g., as c t = min(1, ρ t ). This leads to the v-trace return <ref type="bibr" target="#b3">(Espeholt et al. 2018</ref>)</p><formula xml:id="formula_6">G v−trace t = v(S t ) + t+n−1 k=t γ k−t k i=t c i δ k<label>(3)</label></formula><p>A very similar target was proposed for the ABQ(ζ) algorithm (Mahmood 2017), where the product ρ t λ t was considered and then the trace parameter λ t was chosen to be adaptive to lead to exactly the same behaviour that c t = ρ t λ t = min(1, ρ t ). This shows that this form of clipping does not impair the validity of the off-policy corrections, in the same sense that bootstrapping in general does not change the semantics of a return. The returns used by the value and policy updates defined in Equation 1 and 2 are then</p><formula xml:id="formula_7">G v t = G v−trace t and G π t = R t+1 + γG v-trace t+1 .</formula><p>(4) This is the same algorithm as used by <ref type="bibr" target="#b3">Espeholt et al. (2018)</ref> in the experiments on the IMPALA architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive normalisation</head><p>In this section we use PopArt normalisation , which was introduced for value-based RL, to derive a scale invariant algorithm for actor-critic agents. For simplicity, we first consider the single-task setting, then we extend it to the multi-task setting (the focus of this work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale invariant updates</head><p>In order to normalise both baseline and policy gradient updates, we first parameterise the value estimate v µ,σ,θ (S) as the linear transformation of a suitably normalised value prediction n θ (S). We further assume that the normalised value prediction is itself the output of a linear function, for instance the last fully connected layer of a deep neural net:</p><formula xml:id="formula_8">v µ,σ,θ (s) = σ · n θ (s) + µ = σ · (w f θ\{w,b} (s) + b = n θ (s) ) + µ .</formula><p>(5) As proposed by <ref type="bibr">van Hasselt et al.,</ref> µ and σ can be updated so as to track mean and standard deviation of the values. First and second moments of can be estimated online as</p><formula xml:id="formula_9">µ t = (1−β)µ t−1 +βG v t , ν t = (1−β)ν t−1 +β(G v t ) 2 ,<label>(6)</label></formula><p>and then used to derive the estimated standard deviation as σ t = ν t − µ 2 t . Note that the fixed decay rate β determines the horizon used to compute the statistics. We can then use the normalised value estimate n θ (S) and the statistics µ and σ to normalise the actor-critic loss, both in its value and policy component; this results in the scale-invariant updates:</p><formula xml:id="formula_10">∆θ ∝ G v t − µ σ − n θ (S t ) ∇ θ n θ (S t ) ,<label>(7)</label></formula><formula xml:id="formula_11">∆η ∝ G π t − µ σ − n θ (S t ) ∇ η log π η (A t |S t ) .<label>(8)</label></formula><p>If we optimise the new objective naively, we are at risk of making the problem harder: the normalised targets for values are non-stationary, since they depend on statistics µ and σ. The PopArt normalisation algorithm prevents this, by updating the last layer of the normalised value network to preserve unnormalised value estimates v µ,σ,θ , under any change in the statistics µ → µ and σ → σ :</p><formula xml:id="formula_12">w = σ σ w , b = σb + µ − µ σ .<label>(9)</label></formula><p>This extends PopArt's scale-invariant updates to the actorcritic setting, and can help to make tuning hyperparameters easier, but it is not sufficient to tackle the challenging multitask RL setting that we are interested in this paper. For this, a single pair of normalisation statistics is not sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale invariant updates for multi-task learning</head><formula xml:id="formula_13">Let D i be an environment in some finite set T = {D i } N i=1</formula><p>, and let π(S|A) be a task-agnostic policy, that takes a state S from any of the environments D i , and maps it to a probability distribution onto the shared action space A. Consider now a multi-task value function v(S) with N outputs, one for each task. We can use for v the same parametrisation as in Equation 5, but with vectors of statistics µ, σ ∈ R N , and a vector-valued function n θ (s) = (n 1</p><formula xml:id="formula_14">θ (s), . . . , n N θ (s)) v µ,σ,θ (S) = σ n θ (S)+µ = σ (Wf θ\{W,b} (S)+b)+µ<label>(</label></formula><p>10) where W and b denote the parameters of the last fully connected layer in n θ (s). Given a rollout {S i,k , A k , R i,k } t+n k=t , generated under task-agnostic policy π η (A|S) in environment D i , we can adapt the updates in Equation 7 and 8 to provide scale invariant updates also in the multi-task setting:</p><formula xml:id="formula_15">∆θ ∝ G v,i t − µ i σ i − n i θ (S t ) ∇ θ n i θ (S t ) ,<label>(11)</label></formula><formula xml:id="formula_16">∆η ∝ G π,i t − µ i σ i − n i θ (S t ) ∇ η log π η (A t |S t ) . (12)</formula><p>Where the targets G ·,i t use the value estimates for environment D i for bootstrapping. For each rollout, only the i th head in the value net is updated, while the same policy network is updated irrespectively of the task, using the appropriate rescaling for updates to parameters η. As in the single-task case, when updating the statistics µ and σ we also need to update W and b to preserve unnormalised outputs,</p><formula xml:id="formula_17">w i = σ i σ i w i , b i = σ i b i + µ i − µ i σ i ,<label>(13)</label></formula><p>where w i is the i th row of matrix W, and µ i , σ i , b i are the i th elements of the corresponding parameter vectors. Note that in all updates only the values, but not the policy, are conditioned on the task index, which ensures that the resulting agent can then be run in a fully task agnostic way, since values are only used to reduce the variance of the policy updates at training time but not needed for action selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluated our approach in two challenging multi-task benchmarks, Atari-57 and DmLab-30, based on Atari and DeepMind Lab respectively, and introduced by Espeholt et al. We also consider a new benchmark, consisting of the same games as Atari-57, but with the original unclipped rewards. We demonstrate state of the art performance on all benchmarks. To aggregate scores across many tasks, we normalise the scores on each task based on the scores of a human player and of a random agent on that same task ( . Multi-task learning on this platform has not been as successful due to large number of environments, inconsistent dynamics and very different reward structure. Prior work on multi-task RL in the ALE has therefore focused on smaller subsets of games <ref type="bibr">(Rusu et al. 2015;</ref><ref type="bibr" target="#b15">Sharma and Ravindran 2017)</ref>. Atari has a particularly diverse reward structure. Consequently, it is a perfect domain to fully assess how well can our agents deal with extreme differences in the scale of returns. Thus, we train all agents both with and without reward clipping, to compare performance degradation as returns get more diverse in the unclipped version of the environment. In both cases, at the end of training, we test agents both with random-starts <ref type="bibr" target="#b10">(Mnih et al. 2015)</ref> and human-starts (Nair et al. 2015); aggregate results are reported in <ref type="table" target="#tab_0">Table 1</ref> accordingly. DmLab-30 is a collection of 30 visually rich, partially observable RL environments . This benchmark has strong internal consistency (all levels are played with a first person camera in a 3D environment with consistent dynamics). Howevere, the tasks themselves are quite diverse, and were designed to test distinct skills in RL agents: among these navigation, memory, planning, laser-tagging, and language grounding. The levels can also differ visually in non-trivial ways, as they include both natural environments and maze-like levels. Two levels (rooms collect good objects and rooms exploit deferred effects) have held out test versions, therefore <ref type="table" target="#tab_0">Table 1</ref> reports both train and test aggregate scores. We observed that the original IMPALA agent suffers from an artificial bottleneck in performance, due to the fact that some of the tasks cannot be solved with the action set available to the agent. As first step, we thus fix this issue by equipping it with a larger action set, resulting in a stronger IMPALA baseline than reported in the original paper. We also run multiple independent PBT experiments, to assess the variability of results across multiple replications. <ref type="figure">Figures 1 and 2</ref> show the median human normalised performance across the entire set of 57 Atari games in the ALE, when training agent with and without reward clipping, respectively. The curves are plotted as function of the total number of frames seen by each agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atari-57 results</head><p>PopArt-IMPALA (orange line) achieves a median performance of 110% with reward clipping and a median performance of 101% in the unclipped version of Atari-57. Recall that here we are measuring the median performance of a single trained agent across all games, rather than the median over the performance of a set of individually trained agents as it has been more common in the Atari domain. To our knowledge, both agents are the first to surpass median human performance across the entire set of 57 Atari games.</p><p>The IMPALA agent (blue line) performs much worse. The baseline barely reaches 60% with reward clipping, and the median performance is close to 0% in the unclipped setup. The large decrease in the performance of the baseline IM-PALA agent once clipping is removed is in stark contrast with what we observed for PopArt-IMPALA, that achieved almost the same performance in the two training regimes.</p><p>Since the level-specific value predictions used by multitask PopArt effectively increase the capacity of the network, we also ran an additional experiment to disentangle the contribution of the increased network capacity from the contribution of the adaptive normalisation. For this purpose, we train a second baseline, that uses level specific value predictions, but does not use PopArt to adaptively normalise the learning updates. The experiments show that such MultiHead-IMPALA agent (pink line) actually performs slightly worse than the original IMPALA both with PopArt-IMPALA MultiHead-IMPALA IMPALA <ref type="figure">Figure 1</ref>: Atari-57 (reward clipping). Median human normalised score across all Atari levels, as function of the total number of frames seen by the agents across all levels. We compare PopArt-IMPALA to IMPALA and to an additional baseline, MultiHead-IMPALA, that uses task-specific value predictions but no adaptive normalisation. All three agent are trained with the clipped reward scheme.</p><p>and without clipping, confirming that the performance boost of PopArt-IMPALA is indeed due to the adaptive rescaling. We highlight that in our experiments a single instance of multi-task PopArt-IMPALA has processed the same amount of frames as a collection of 57 expert DQN agents (57 × 200 M = 1.14 × 10 10 ), while achieving better performance. Despite the large CPU requirements, on a cloud service, training multi-task PopArt-IMPALA can also be competitive in terms of costs, since it exceeds the performance of a vanilla-DQN in just 2.5 days, with a smaller GPU footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalisation statistics</head><p>It is insightful to observe the different normalisation statistics across games, and how they adapt during training.  <ref type="figure" target="#fig_4">Figure 3</ref> shows the corresponding agent's undiscounted episode return: it follows the same patterns as the statistics (with differences in magnitude due to discounting). Finally note how the statistics can even track the instabilities in the agent's performance, as in qbert. <ref type="figure" target="#fig_5">Figure 4</ref> shows, as a function of the total number of frames processed by each agent, the mean human normalised performance across all 30 DeepMind Lab levels, where each level's score is capped at 100% . For all agents, we ran three PopArt-IMPALA MultiHead-IMPALA IMPALA <ref type="figure">Figure 2</ref>: Atari-57 (unclipped): Median human normalised score across all Atari levels, as a function of the total number of frames seen by the agents across all levels. We here compare the same set of agents as in <ref type="figure">Figure 1</ref>, but now all agents are trained without using reward clipping. The approximately flat lines corresponding to the baselines mean no learning at all on at least 50% of the games.  independent PBT experiments. In <ref type="figure" target="#fig_5">Figure 4</ref> we plot the learning curves for each experiment and, for each agent, fill in the area between best and worse experiment. Compared to the original paper, our IMPALA baseline uses a richer action set, that includes more possible horizontal rotations, and vertical rotations (details in Appendix). Fine-grained horizontal control is useful on lasertag levels, while vertical rotations are necessary for a few psychlab levels. Note that this new baseline (solid blue in <ref type="figure" target="#fig_5">Figure 4</ref>) performs much better than the original IM-PALA agent, which we also train and report for completeness (dashed blue). Including PopArt normalisation (in orange) on top of our baseline results in largely improved scores. Note how agents achieve clearly separated performance levels, with the new action set dominating the original paper's one, and with PopArt-IMPALA dominating IM-PALA for all three replications of the experiment. For reference, we also plot the performance of IMPALA with the limited action set from the original paper (dashed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DmLab-30 results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extensions</head><p>In this section, we explore the combination of the proposed PopArt-IMPALA agent with pixel control (Jaderberg et al. 2016), to further improve data efficiency, and make training IMPALA-like agents on large multi-task benchmarks cheaper and more practical. Pixel control is an unsupervised auxiliary task introduced to help learning good state representations. As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, the combination of PopArt-IMPALA with pixel control (red line) allows to match the final performance of the vanilla PopArt-IMPALA (orange line) with a fraction of the data (∼ 2B frames). This is on top of the large improvement in data efficiency already provided by PopArt, meaning that the pixel control augmented PopArt-IMPALA needs less than 1/10-th of the data to match our own IMPALA baseline's performance (and 1/30-th of the frames to match the original published IMPALA). Importantly, since both PopArt and Pixel Control only add a very small computational cost, this improvement in data efficiency directly translates in a large reduction of the cost of training IMPALA agents on large multi-task benchmarks. Note, finally, that other orthogonal advances in deep RL could also be combined to further improve performance, similarly to what was done by <ref type="bibr">Rainbow (Hessel et al. 2018)</ref>, in the context of value-based reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation notes</head><p>We implemented all agents in TensorFlow. For each batch of rollouts processed by the learner, we average the G v t targets within a rollout, and for each rollout in the batch we perform one online update of PopArt's normalisation statistics with decay β = 3 × 10 −4 . Note that β didn't require any tuning. To prevent numerical issues, we clip the scale σ in the range [0.0001, 1e6]. We do not back-propagate gradients into µ and σ, exclusively updated as in Equation 6. The weights W of the last layer of the value function are updated according to Equation 13 and 11. Note that we first apply the actor-critic updates (11), then update the statistics (6), finally apply output preserving updates (13). For more just-in-time rescaling of updates we can invert this order, but this wasn't necessary. As anticipated, in all experiments we used population-based training (PBT) to adapt hyperparameters during training <ref type="bibr" target="#b5">(Jaderberg et al. 2017)</ref>. As in the IMPALA paper, we use PBT to tune learning rate, entropy cost, the optimiser's epsilon, and-in the Atari experiments-the max gradient norm. In Atari-57 we used populations of 24 instances, in DmLab-30 just 8 instances. All hyperparameters are reported in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this paper we propose a scale-invariant actor-critic algorithm that enables significantly improved performance in multi-task reinforcement learning settings. Being able to acquire knowledge about a wide range of facts and skills has been long considered an essential feature for an RL agent to demonstrate intelligent behaviour <ref type="bibr" target="#b16">(Sutton et al. 2011;</ref><ref type="bibr" target="#b2">Degris and Modayil 2012;</ref><ref type="bibr" target="#b7">Legg and Hutter 2007)</ref>. To ask our algorithms to master multiple tasks is therefore a natural step as we progress towards increasingly powerful agents. The wide-spread adoption of deep learning in RL is quite timely in this regard, since sharing parts of a neural network across multiple tasks is also a powerful way of building robust representations. This is particularly important for RL, because rewards on individual tasks can be sparse, and therefore sharing representations across tasks can be vital to bootstrap learning. Several agents <ref type="bibr" target="#b5">(Jaderberg et al. 2016;</ref><ref type="bibr" target="#b6">Lample and Chaplot 2016;</ref><ref type="bibr" target="#b15">Shelhamer et al. 2016;</ref><ref type="bibr" target="#b9">Mirowski et al. 2016</ref>) demonstrated this by improving performance on a single external task by learning off-policy about auxiliary tasks defined on the same stream of experience (e.g. pixel control, immediate reward prediction or auto-encoding).</p><p>Multi-task learning, as considered in this paper, where we get to execute, in parallel, the policies learned for each task, has potential additional benefits, including deep exploration <ref type="bibr" target="#b10">(Osband et al. 2016)</ref>, and policy composition <ref type="bibr" target="#b9">(Mankowitz et al. 2018;</ref><ref type="bibr" target="#b20">Todorov 2009</ref>). By learning on-policy about tasks, it may also be easier to scale to much more diverse tasks: if we only learn about some task off-policy from experience generated pursuing a very different one, we might never observe any reward. A limitation of our approach is that it can be complicated to implement parallel learning outside of simulation, but recent work on parallel training of robots  suggests that this is not necessarily an insurmountable obstacle if sufficient resources are available.</p><p>Adoption of parallel multi-task RL has up to now been fairly limited. That the scaling issues considered in this paper, may have been a factor in the limited adoption is indicated by the wider use of this kind of learning in supervised settings <ref type="bibr" target="#b6">(Johnson et al. 2017;</ref><ref type="bibr" target="#b9">Lu et al. 2016;</ref><ref type="bibr">Misra et al. 2016;</ref><ref type="bibr" target="#b4">Hashimoto et al. 2016)</ref>, where loss functions are naturally well scaled (e.g. cross entropy), or can be easily scaled thanks to the stationarity of the training distribution. We therefore hope and believe that the work presented here can enable more research on multi-task RL.</p><p>We also believe that PopArt's adaptive normalisation can be combined with other research in multi-task reinforcement learning, that previously did not scale as effectively to large numbers of diverse tasks. We highlight as potential candidates policy distillation <ref type="bibr" target="#b11">(Parisotto, Ba, and Salakhutdinov 2015;</ref><ref type="bibr">Rusu et al. 2015;</ref><ref type="bibr" target="#b15">Schmitt et al. 2018;</ref><ref type="bibr" target="#b18">Teh et al. 2017</ref>) and active sampling of the task distribution the agent trains on (Sharma and Ravindran 2017). The combination of PopArt-IMPALA with active sampling might be particularly promising since it may allow a more efficient use of the parallel data generation, by focusing it on the task most amenable for learning. Elastic weight consolidation ) and other work from the continual learning literature <ref type="bibr" target="#b12">(Ring 1994;</ref><ref type="bibr" target="#b9">Mcclelland, Mcnaughton, and O'Reilly 1995)</ref> might also be adapted to parallel learning setups to reduce interference (French 1999) among tasks.</p><p>In our experiments we used Population-Based Training (PBT) to tune hyper-parameters. In our DmLab-30 experiments, however, we used smaller populations than in the original IMPALA paper. For completeness, we also report here the results of running PopArt-IMPALA and IMPALA with the larger population size used by Espeholt et al. Due to the increased cost of using larger populations, in this case we will only report one PBT tuning experiment per agent, rather than the 3 reported in the main text.</p><p>The learning curves for both IMPALA and PopArt-IMPALA are shown in <ref type="figure" target="#fig_8">Figure 6</ref>, together with horizontal dashed lines marking average final performance of the agents trained with the smaller population of just 8 instances. The performance of both IMPALA and PopArt-IMPALA agents at the end of training is very similar whether hyperparameters are tuned with 8 or 24 PBT instances, suggesting that the large populations used for hyperparameter tuning by Espeholt et al. may not be necessary.</p><p>Note, however, that we have observed larger discrepancies between experiments where small and large population size are used for tuning hyper-parameter, when training the less performing IMPALA agent that used a more limited action set, as presented in the original IMPALA paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Control</head><p>Pixel control <ref type="bibr" target="#b5">(Jaderberg et al. 2016</ref>) is an unsupervised auxiliary task introduced to help learning good state representations. We report here also the performance of combining Pixel Control with IMPALA without also using PopArt. As shown in <ref type="figure" target="#fig_9">Figure 7</ref>, pixel control increases the performance of both the PopArt-IMPALA agent as well as that of the IM-PALA baseline. PopArt still guarantees a noticeable boost in performance, with the median human normalized score of Pixel-PopArt-IMPALA (red line) exceeding the score of Pixel-IMPALA (green line) by approximately 10 points.</p><p>We implemented the pixel control task as described in the original paper, only adapting the scheme to the rectangular observations used in DmLab-30. We split the (72 × 96) observations into a 18×24 grid of 4×4 cells. For each location in the grid we define a distinct pseudo-rewardr i,j , equal to the absolute value of the difference between pixel intensities in consecutive frames, averaged across the 16 pixels of cell c i,j . For each cell, we train action values with multi-step Qlearning, accumulating rewards until the end of a rollout and then bootstrapping. We use a discount γ = 0.9. Learning is fully off-policy on experience generated by the actors, that follow the main policy π as usual.</p><p>We use a deep deconvolutional network for the action value predictions associated to each pseudo-rewardr i,j . First, we feed the LSTM's output to a fully connected layer, reshape the output tensor as 6 × 9 × 32, and apply a deconvolution with 3 × 3 kernels that outputs a 8 × 11 × 32 tensor. From this, we compute a spatial grid of Q-values using a dueling network architecture: we use a deconvolution with 1 output channel for the state values across the grid and a deconvolution with |A| channels for the advantage estimates of each cell. Output deconvolutions use 4 × 4 kernels with stride 2. The additional head is only evaluated on the learner, actors do not execute it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atari-57 Score breakdown</head><p>In this section we use barplots to report the final performance of the agents on each of the levels in the Atari-57 multi-task benchmark. In order to compute these scores we take the final trained agent and evaluate it with a frozen policy on each of the levels for 200 episodes. The same trained policy is evaluated in all the levels, and the policy is not provided information about the task it's being evaluated on. For Atari, we compare PopArt-IMPALA, with and without reward clipping, to an IMPALA baseline. In all cases the height of the bars in the plot denote human normalised score. For the Atari results we additionally rescale logarithmically the x-axis, because in this domain games may differ in their normalised performance by several orders of magnitude. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DmLab-30 Score breakdown</head><p>In this section we use barplots to report the final performance of the agents on each of the levels in the DmLab-30 multi-task benchmark. In order to compute these scores we take the final trained agent and evaluate it with a frozen policy on each of the levels for 500 episodes. We perform the evaluation over a higher number of episodes (compared to Atari) because the variance of the mean episode return is typically higher in DeepMind Lab. As before, the same trained policy is evaluated on all levels, and the policy is not provided information about the task it's being evaluated on. Also in DmLab-30 we perform a three-way comparison. We compare PopArt-IMPALA to our improved IMPALA baseline, and, for completeness, to the original paper's IMPALA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepMind Lab action discretisation</head><p>DeepMind Lab's native action space is a 7-dimensional continuous space, whose dimensions correspond to rotating horizontally/vertically, strafing left/right, moving forward/backward, tagging, crouching, and jumping. Despite the native action space being continuous, previous work on this platform has however typically relied on a coarse discretisation of the action space. We therefore follow the same approach also in our experiments.</p><p>Below we list the discretisations used by the agents considered in our experiments. This includes the discretisation used by IMPALA, as well as the one we introduce in this paper in order to unlock some levels in DmLab-30 which just can't be solved under the original IMPALA discretisation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Native DmLab Action Forward <ref type="bibr">(FW)</ref> [ 0, 0, 0, 1, 0, 0, 0] Backward (BW) [ 0, 0, 0, -1, 0, 0, 0] Strafe Left [ 0, 0, -1, 0, 0, 0, 0] Strafe Right [ 0, 0, 1, 0, 0, 0, 0] Look Left <ref type="bibr">(LL)</ref> [-20, 0, 0, 0, 0, 0, 0] Look Right (LR) [ 20, 0, 0, 0, 0, 0, 0] FW + LL [-20, 0, 0, 1, 0, 0, 0] FW + LR [ 20, 0, 0, 1, 0, 0, 0] Fire [ 0, 0, 0, 0, 1, 0, 0] Action Native DmLab Action FW [ 0, 0, 0, 1, 0, 0, 0] BW [ 0, 0, 0, -1, 0, 0, 0] Strafe Left [ 0, 0, -1, 0, 0, 0, 0] Strafe Right [ 0, 0, 1, 0, 0, 0, 0] Small LL [-10, 0, 0, 0, 0, 0, 0] Small LR [ 10, 0, 0, 0, 0, 0, 0] Large LL [-60, 0, 0, 0, 0, 0, 0] Large LR [ 60, 0, 0, 0, 0, 0, 0] Look Down [ 0, 10, 0, 0, 0, 0, 0] Look Up [ 0,-10, 0, 0, 0, 0, 0] FW + Small LL [-10, 0, 0, 1, 0, 0, 0] FW + Small LR [ 10, 0, 0, 1, 0, 0, 0] FW + Large LL [-60, 0, 0, 1, 0, 0, 0] FW + Large LR [ 60, 0, 0, 1, 0, 0, 0] Fire [ 0, 0, 0, 0, 1, 0, 0]    Network Architecture  Mean capped human normalised score (cap=100) <ref type="table">Table 9</ref>: hyperparameters tuned with population based training are listed below: note that these are the same used by all baseline agents we compare to, to ensure fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixed Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter distribution Entropy cost</head><p>Log-uniform on [5e-5, 1e-2] Learning rate</p><p>Log-uniform on [5e-6, 5e-3]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSProp epsilon</head><p>Categorical on [1e-1, 1e-3, 1e-5, 1e-7] Max Grad Norm Uniform on <ref type="bibr">[10,</ref><ref type="bibr">100]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>All experiments use population-based training (PBT) to tune hyperparameters (Jaderberg et al. 2017). As in Espeholt et al., we report learning curves as function of the number of frames processed by one instance of the tuning population, summed across tasks.DomainsAtari-57 is a collection of 57 classic Atari 2600 games. The ALE<ref type="bibr" target="#b1">(Bellemare et al. 2013)</ref>, exposes them as RL environments. Most prior work has focused on training agents for individual games<ref type="bibr" target="#b10">(Mnih et al. 2015;</ref><ref type="bibr" target="#b4">Gruslys et al. 2018;</ref><ref type="bibr" target="#b15">Schulman et al. 2015;</ref><ref type="bibr" target="#b15">Schulman et al. 2017;</ref><ref type="bibr" target="#b0">Bacon, Harb, and Precup 2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3 (top row) plots the shift µ for a selection of Atari games, in the unclipped training regime. The scale σ is visualised in the same figure by shading the area in the range [µ − σ, µ + σ]. The statistics differ by orders of magnitude across games: in crazy climber the shift exceeds 2500, while in bowling it never goes above 15. The adaptivity of the proposed normalisation emerges clearly in crazy climber and qbert, where the statistics span multiple orders of magnitude during training. The bottom row in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Normalisation statistics: Top: learned statistics, without reward clipping, for four distinct Atari games. The shaded region is [µ−σ, µ+σ]. Bottom: undiscounted returns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>. Mean capped human normalised score of IMPALA (blue) and PopArt-IMPALA (orange), across the DmLab-30 benchmark as function of the number of frames (summed across all levels). Shaded region is bounded by best and worse run among 3 PBT experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>. Mean capped human normalised score of PopArt-IMPALA with pixel control (red), across the DmLab-30 benchmark as function of the total number of frames across all tasks. Shaded region is bounded by best and worse run among 3 PBT experiments. Dotted lines mark the point where Pixel-PopArt-IMPALA matches PopArt-IMPALA and the two IMPALA baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Larger populations: mean capped human normalised score across the DmLab-30 benchmark as a function of the total number of frames seen by the agents across all levels. The solid lines plot the performance of IMPALA (blue) and PopArt-IMPALA (orange) when tuning hyperparameters with a large PBT population of 24 instances. Dashed lines correspond to the final performance of these same agents, after 10B frames, in the previous experiments where hyper-parameters were tuned with a population of 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Pixel Control: mean capped human normalised score across the DmLab-30 benchmark as a function of the total number of frames (summed across levels). Solid lines plot the performance PopArt-IMPALA (red) and IM-PALA (green), after augmenting both with pixel control. Dashed lines mark the point at which Pixel-PopArt-IMPALA matches the final performance of previous agents. Note how, thanks to the improved data efficiency, we train for 2B frames, compared to 10B in previous experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Atari-57 breakdown: human normalised score for IMPALA and PopArt-IMPALA, as measured in a separate evaluation phase at the end of training, broken down for the 57 games. For PopArt-IMPALA we report the scores both with and without reward clipping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>DmLab-30 breakdown: human normalised score for the original paper's IMPALA, our improved IMPALA baseline, and PopArt-IMPALA, as measured at the end of training, broken down for the 30 tasks; they all used 8 instances for population based training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of results: aggregate scores for IMPALA and PopArt-IMPALA. We report median human normalised score for Atari-57, and mean capped human normalised score for DmLab-30. In Atari, Random and Human refer to whether the trained agent is evaluated with random or human starts. In DmLab-30 the test score includes evaluation on the held-out levels.</figDesc><table><row><cell></cell><cell cols="2">Atari-57</cell><cell cols="2">Atari-57 (unclipped)</cell><cell cols="2">DmLab-30</cell></row><row><cell>Agent</cell><cell>Random</cell><cell>Human</cell><cell>Random</cell><cell>Human</cell><cell>Train</cell><cell>Test</cell></row><row><cell>IMPALA</cell><cell>59.7%</cell><cell>28.5%</cell><cell>0.3%</cell><cell>1.0%</cell><cell cols="2">60.6% 58.4%</cell></row><row><cell>PopArt-IMPALA</cell><cell cols="2">110.7% 101.5%</cell><cell>107.0%</cell><cell>93.7%</cell><cell cols="2">73.5% 72.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Action discretisation used by IMPALA: we report below the discretisation of DeepMind Lab's action space, as used by the original IMPALA agent in Espeholt et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Action discretisation of DeepMind Lab's action space, as used by our version of IMPALA and by PopArt-IMPALA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>PopArt specific hyperparameters: these are held fixed during training and were only very lightly tuned. The lower bound is used to avoid numerical issues when rewards are extremely sparse.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell cols="2">Statistics learning rate 0.0003</cell></row><row><cell>Scale lower bound</cell><cell>0.0001</cell></row><row><cell>Scale upper bound</cell><cell>1e6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>DeepMind Lab preprocessing. As in previous work on DeepMind Lab, we render the observation with a resolution of[72, 96], as well as use 4 action repeats. We also employ the optimistic asymmetric rescaling (OAR) of rewards, that was introduced in Espeholt et al. for exploration.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Image Height</cell><cell>72</cell></row><row><cell>Image Width</cell><cell>96</cell></row><row><cell>Number of action repeats</cell><cell>4</cell></row><row><cell>Reward Rescaling</cell><cell>-0.3min(tanh(r),0)+</cell></row><row><cell></cell><cell>5max(tanh(r),0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Atari preprocessing. The standard Ataripreprocessing is used in the Atari experiments. Since the introduction of DQN these setting have become a standard practice when training deep RL agent on Atari. Note however, that we report experiments training agents both with and without reward clipping.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Image Height</cell><cell>84</cell></row><row><cell>Image Width</cell><cell>84</cell></row><row><cell>Grey scaling</cell><cell>True</cell></row><row><cell>Max-pooling 2 consecutive frames</cell><cell>True</cell></row><row><cell>Frame Stacking</cell><cell>4</cell></row><row><cell>End of episode on life loss</cell><cell>True</cell></row><row><cell>Reward Clipping (if used)</cell><cell>[-1, 1]</cell></row><row><cell>Number of action repeats</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Other agent hyperparameters: These hyperparameters are the same used by Espeholt et al.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Unroll length</cell><cell>20 (Atari), 100 (DmLab)</cell></row><row><cell>Discount γ</cell><cell>0.99</cell></row><row><cell>Baseline loss weight γ</cell><cell>0.5</cell></row><row><cell>Batch size</cell><cell>32</cell></row><row><cell>Optimiser</cell><cell>RMSProp</cell></row><row><cell>RMSProp momentum</cell><cell>0.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Network hyperparameters. The network architecture is described in details in Espeholt et al., For completeness, we also report in the Table below the complete specification of the network. Convolutional layers are specified according to the pattern (num layers, kernel size, stride).</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Convolutional Stack</cell><cell></cell></row><row><cell>-Number of sections</cell><cell>3</cell></row><row><cell>-Channels per section</cell><cell>[16, 32, 32]</cell></row><row><cell>-Activation Function</cell><cell>ReLU</cell></row><row><cell>ResNet section</cell><cell></cell></row><row><cell>-Conv</cell><cell>1 / 3x3 / 1)</cell></row><row><cell>-Max-Pool</cell><cell>1 / 3x3 / 2</cell></row><row><cell>-Conv</cell><cell>2 / 3x3 / 1</cell></row><row><cell>-Skip</cell><cell>Identity</cell></row><row><cell>-Conv</cell><cell>2 / 3x3 / 1</cell></row><row><cell>-Skip</cell><cell>Identity</cell></row><row><cell>Language preprocessing</cell><cell></cell></row><row><cell>-Word embeddings</cell><cell>20</cell></row><row><cell>-Sentence embedding</cell><cell>LSTM / 64</cell></row><row><cell>Fully connected layer</cell><cell>256</cell></row><row><cell>LSTM (DmLab-only)</cell><cell>256</cell></row><row><cell>Network Heads</cell><cell></cell></row><row><cell>-Value</cell><cell>Linear</cell></row><row><cell>-Policy</cell><cell>Linear+softmax</cell></row><row><cell>Population Based Training</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Population Based Training: we use PBT for tuning hyper-parameters, as described in Espeholt et al., with population size and fitness function as defined below.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Population Size (Atari)</cell><cell>24</cell></row><row><cell>Population Size (DmLab)</cell><cell>8</cell></row><row><cell>Fitness</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.github.com/deepmind/scalable agent</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this Appendix we report additional details about the results presented in the main text, as well as present additional experiments on the DmLab-30 benchmark. We also report the breakdown per level of the scores of IMPALA and PopArt-IMPALA on the Atari-57 and DmLab-30 benchmarks. Finally, we report the hyperparameters used to train the baseline agents as well as PopArt-IMPALA. These hyperparameters are mostly the same as in Espeholt et al., but we report them for completeness and to ease reproducibility.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harb</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<title level="m">The option-critic architecture. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents. JAIR. [Bellman 1957] Bellman, R. 1957. A markovian decision process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beattie</surname></persName>
		</author>
		<idno>abs/1309.6821</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Mechanics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Sample complexity of multi-task reinforcement learning. Caruana 1998] Caruana, R. 1998. Multitask learning. In Learning to learn</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Designing Intelligent Robots</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>French</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>ICML. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gruslys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
	</analytic>
	<monogr>
		<title level="m">Rainbow: Combining improvements in deep reinforcement learning. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaderberg</surname></persName>
		</author>
		<idno>abs/1611.05397</idno>
		<ptr target="CoRRabs/1711.09846" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2016" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
	<note>Overcoming catastrophic forgetting in neural networks. Playing FPS games with deep reinforcement learning. CoRR abs/1609.05521</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<idno>Levine et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Universal intelligence: A definition of machine intelligence. Minds Mach</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory</title>
		<idno>abs/1611.03673</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. UAlberta</note>
	<note>Learning to navigate in complex environments. Misra et al. 2016] Misra, I.; Shrivastava, A.; Gupta, A.; and Hebert, M. 2016. Cross-stitch networks for multi-task learning. CoRR abs/1604.03539</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<idno type="arXiv">arXiv:1507.04296</idno>
	</analytic>
	<monogr>
		<title level="m">Massively parallel methods for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actor-mimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ba</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salakhutdinov ; Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<idno>abs/1511.06342</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Sutton, and Singh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Eligibility traces for off-policy policy evaluation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Continual learning in reinforcement environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilinear multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romera-Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An on-line algorithm for dynamic reinforcement learning and planning in reactive environments</title>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>CoRR abs/1606.04671. [Schmidhuber</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmitt</surname></persName>
		</author>
		<idno>abs/1712.01815</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note>Reinforcement Learning: An Introduction</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new q(λ) with interim forward view and Monte Carlo equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distral: Robust multitask reinforcement learning</title>
		<idno>abs/1707.04175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explanation-based neural network learning: A lifelong learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Compositionality of optimal control laws</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning values across many orders of magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guez</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silver ; Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Cambridge, England; Williams</addrLine></address></meeting>
		<imprint>
			<publisher>Mach. Learning</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Deep reinforcement learning with double Qlearning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
