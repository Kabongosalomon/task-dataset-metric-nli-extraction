<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Workshop track -ICLR 2017 AUDIO SUPER-RESOLUTION USING NEURAL NETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
							<email>kuleshov@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Zayd</forename><surname>Enam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Workshop track -ICLR 2017 AUDIO SUPER-RESOLUTION USING NEURAL NETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new audio processing technique that increases the sampling rate of signals such as speech or music using deep convolutional neural networks. Our model is trained on pairs of low and high-quality audio examples; at test-time, it predicts missing samples within a low-resolution signal in an interpolation process similar to image super-resolution. Our method is simple and does not involve specialized audio processing techniques; in our experiments, it outperforms baselines on standard speech and music benchmarks at upscaling ratios of 2×, 4×, and 6×. The method has practical applications in telephony, compression, and text-tospeech generation; it demonstrates the effectiveness of convolutional architectures on an audio generation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The generative modeling of audio signals is a fundamental problem at the intersection of signal processing and machine learning; recent learning-based algorithms have enabled advances in speech recognition <ref type="bibr" target="#b12">(Hinton et al., 2012)</ref>, audio synthesis (van den <ref type="bibr" target="#b26">Oord et al., 2016;</ref><ref type="bibr" target="#b20">Mehri et al., 2016)</ref>, music recommendation systems <ref type="bibr" target="#b6">(Coviello et al., 2012;</ref><ref type="bibr" target="#b27">Wang &amp; Wang, 2014;</ref><ref type="bibr" target="#b18">Liang et al., 2015)</ref>, and in many other areas <ref type="bibr" target="#b0">(Acevedo et al., 2009)</ref>. Audio processing also raises basic research questions pertaining to time series and generative modeling <ref type="bibr" target="#b11">(Haykin &amp; Chen, 2005;</ref><ref type="bibr" target="#b3">Bilmes, 2004)</ref>.</p><p>One of the most significant recent advances in machine learning-based audio processing has been the ability to directly model raw signals in the time domain using neural networks <ref type="bibr" target="#b26">(van den Oord et al., 2016;</ref><ref type="bibr" target="#b20">Mehri et al., 2016)</ref>. Although this affords us the maximum modeling flexibility, it is also computationally expensive, requiring us to handle &gt; 10, 000 audio samples at every second.</p><p>In this paper, we explore new lightweight modeling algorithms for audio. In particular, we focus on a specific audio generation problem called bandwidth extension, in which the task is to reconstruct high-quality audio from a low-quality, down-sampled input containing only a small fraction (15-50%) of the original samples. We introduce a new neural network-based technique for this problem that is inspired image super-resolution algorithms <ref type="bibr" target="#b7">(Dong et al., 2016)</ref>, which use machine learning techniques to interpolate a low-resolution image into a higher-resolution one. Learning-based methods often perform better in this context than general-purpose interpolation schemes such as splines because they leverage sophisticated domain-specific models of the appearance of natural signals.</p><p>As in image super-resolution, our model is trained on pairs of low and high-quality samples; at testtime, it predicts the missing samples of a low-resolution input signal. Unlike recent neural networks for generating raw audio, our model is fully feedforward and can be run in real-time. In addition to having multiple practical applications, our method also suggests new ways to improve existing generative models of audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CONTRIBUTIONS</head><p>From a practical perspective, our technique has applications in telephony, compression, text-tospeech generation, forensic analysis, and in other domains. It outperforms baselines at 2×, 4×, and 6× upscaling ratios, while also being significantly simpler than previous methods. Whereas most existing audio enhancement methods make substantial use of signal processing theory, our 1 arXiv:1708.00853v1 [cs.SD] 2 Aug 2017</p><p>Workshop track -ICLR 2017 approach is conceptually very simple and requires no specialized knowledge to implement. Our neural networks are simply trained to map one audio time series into another. Our approach is also among the first to use convolutional architectures for bandwidth extension; as a result, it scales better with dataset size and computational resources relative to current alternatives.</p><p>From a generative modeling perspective, our work demonstrates that purely feedforward architectures operating in a non-discretized output space can achieve good performance on an important audio generation task. This hints at the possibility of designing improved generative models for audio that combine both feedforward and recurrent components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SETUP AND BACKGROUND</head><p>Audio processing. We represent an audio signal as a function s(t) : [0, T ] → R, where T is the duration of the signal (in seconds) and s(t) is the amplitude at t. Taking a digital measurement of s requires us to discretize the continuous function s(t) into a vector x(t) : { 1 R , 2 R , ..., RT R } → R. We refer to R as the sampling rate of x (in Hz). Sampling rates may range from 4 KHz (low-quality telephone speech) to 44 Khz (high-fidelity music).</p><p>In this work, we interpret R as the resolution of x; our goal is to increase the resolution of audio samples by predicting x from a fraction of its samples taken at { 1 R , 2 R , ..., RT R }. Note that by basic signal processing theory, this is equivalent to predicting the higher frequencies of x.</p><p>Bandwidth extension. Audio upsampling has been studied in the audio processing community under the name bandwidth extension <ref type="bibr" target="#b8">(Ekstrand, 2002;</ref><ref type="bibr" target="#b14">Larsen &amp; Aarts, 2005)</ref>. Several learningbased approaches have been proposed, including Gaussian mixture models <ref type="bibr" target="#b5">(Cheng et al., 1994;</ref><ref type="bibr" target="#b22">Park &amp; Kim, 2000)</ref> and neural networks <ref type="bibr" target="#b16">(Li et al., 2015)</ref>. These methods typically involve handcrafted features and use relatively simple models (e.g., neural networks with at most 2-3 densely connected layers) that are often part of a larger, more complex systems. In comparison, our method is conceptually simple (operating directly on the raw audio signal), scalable (our neural networks are fully convolutional and fully feed-forward), more accurate, and is also among the few to have been tested on non-speech audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SETUP</head><p>Given a low resolution signal x = {x 1/R1 , ...x R1T1/R1 } sampled at a rate R 1 , our goal is to reconstruct a high-resolution version y = {y 1/R2 , ...y R2T2/R2 } of x that has a sampling rate R 2 &gt; R 1 . For example, x may be a voice signal transmitted via a standard telephone connection at 4 KHz; y may be a high-resolution 16 KHz reconstruction of the orignal. We use r = R 2 /R 1 to denote Our reconstruction of y <ref type="figure">Figure 2</ref>: Audio super-resolution visualized using spectrograms. A high-quality speech signal (leftmost) is subsampled at r = 4, resulting in the loss of high frequencies (2nd from left). We recover the missing signal using a trained neural network (rightmost), greatly outperforming the cubic baseline (second from right).</p><p>the upsampling ratio of the two signals, which in our work equals r = 2, 4, 6. We thus expect that</p><formula xml:id="formula_0">y rt/R2 ≈ x t/R1 for t = 1, 2, ..., T 1 R 1 .</formula><p>To recover the under-defined signal, we learn a model p(y|x) of the higher-resolution y, conditioned on its low-resolution instantiation x. We assume that the relationship between the time series x, y follows the equation</p><formula xml:id="formula_1">y = f θ (x) + , where ∼ N (0, 1)</formula><p>is Gaussian noise and f θ is a model parametrized by θ. Our framework also extends to more complex noise models which the user may provide as a prior or that may be themselves parametrized by the model (similarly to how one parametrizes the normal distribution in a variational autoencoder).</p><p>The above formulation naturally leads to a mean squared error (MSE) objective</p><formula xml:id="formula_2">(D) = 1 n n i=1 ||y i − f θ (x i )|| 2 2<label>(1)</label></formula><p>for determining the parameters θ based on a dataset D = {x i , y i } n i=1 of source/target time series pairs. Since our model is fully convolutional, we may take the x i , y i to be small patches sampled from the full time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL ARCHITECTURE</head><p>We parametrize the function f with a deep convolutional neural network with residual connections; our neural network architecture is based on ideas from , <ref type="bibr" target="#b7">Dong et al. (2016)</ref>, and , and is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We highlight its main features below.</p><p>Bottleneck architecture. Our model contains B successive downsampling and upsampling blocks: each performs a convolution, batch normalization, and applies a ReLU non-linearity. Downsampling block b = 1, 2, ..., B contains max(2 6+b , 512) convolutional filters of length min(2 7−b + 1, 9) and a stride of 2. Upsampling block b has max(2 7+(B−b+1) , 512) filters of length min(2 7−(B−b+1) + 1, 9).</p><p>Thus, at a downsampling step, we halve the spatial dimension and double the filter size; during upsampling, this is reversed. This bottleneck architecture is inspired by auto-encoders, and is known to encourage the model to learn a hierarchy of features. For example, on an audio task, bottom layers may extract wavelet-style features, while higher ones may correspond to phonemes <ref type="bibr" target="#b1">Aytar et al. (2016)</ref>. Note that the model is fully convolutional, and may run on input sequences of arbitrary length.</p><p>Skip connections. When the source series x is similar to the target y, downsampling features will be also be useful for upsampling . We thus add additional skip connections which stack the tensor of b-th downsampling features with the (B −b+1)-th tensor of upsampling features. We also add an additive residual connection from the input to the final output: the model thus only needs to learn y − x, which in practice speeds up training.</p><p>Subpixel shuffling layer. In order to increase the time dimension during upscaling, we have implemented a one-dimensional version of the Subpixel layer of , which has been shown to be less prone to produce artifacts <ref type="bibr" target="#b21">(Odena et al., 2016)</ref>.</p><p>An upscaling block's convolution maps an input tensor of dimension F × d into one of size F/2 × d. The subpixel layer reshuffles this F/2×d tensor into another one of size F/4×2d (while preserving the tensor entries intact); these are concatenated with F/4 features from the downsampling stage, for a final output of size F/2 × 2d. Thus, we have halved the number of filters and doubled the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets. We use the VCTK dataset (Yamagishi) -which contains 44 hours of data from 108 different speakers -and the Piano dataset of <ref type="bibr" target="#b20">Mehri et al. (2016)</ref> (10 hours of Beethoven sonatas). We generate low-resolution audio signal from the 16 KHz originals by applying an order 8 Chebyshev type I low-pass filter before subsampling the signal by the desired scaling ratio.</p><p>We evaluate our method in three regimes. The SINGLESPEAKER task trains the model on the first 223 recordings of VCTK Speaker 1 (about 30 mins) and tests on the last 8 recordings. The MUL-TISPEAKER task assesses our ability to generalize to new speakers. We train on the first 99 VCTK speakers and test on the 8 remaining ones; our recordings feature different voices and accents (Scottish, Indian, etc.) Lastly, the PIANO task extends audio-super resolution to non-vocal data; we use the standard 88%-6%-6% data split.</p><p>Methods. We compare our method relative to two baselines: a cubic B-spline -which corresponds to the bicubic upsampling baseline used in image super-resolution -and the recent neural network-based technique of <ref type="bibr" target="#b16">Li et al. (2015)</ref>, The latter approach takes as input the short-time Fourier transform (STFT) of the input and predicts directly the phase and the magnitudes of the high frequency components using a dense neural network with three hidden layers of size 2048 and ReLU nonlinearities. <ref type="bibr" target="#b16">Li et al. (2015)</ref> have shown that this method is preferred over Gaussian Mixture Models in 84% of cases in a user study. This model requires that the scaling ratio be a power of 2, hence it is not applicable when r = 6. We instantiate our model with B = 4 blocks and train it for 400 epochs on patches of length 6000 (in the high-resolution space) using the ADAM optimizer with a learning rate of 10 −4 . To ensure source/target series are of the same length, the source input is pre-processed with cubic upscaling. We do not compare against previously-proposed matrix factorization techniques <ref type="bibr" target="#b2">(Bansal et al., 2005;</ref><ref type="bibr" target="#b17">Liang et al., 2013)</ref>, as they are typically trained on &lt; 10 input examples (Sun &amp; Mazumder, 2013) (due to the cost of jointly factorizing a large number of matrices), and do not scale to the size of our datasets.</p><p>Metrics Given a reference signal y and an approximation x, the Signal to Noise Ratio (SNR) is defined as SNR(x, y) = 10 log ||y|| 2 2 ||x − y|| 2 2 .</p><p>(2)</p><p>The SNR is a standard metric used in the signal processing literature. The Log-spectral distance (LSD) <ref type="bibr" target="#b10">(Gray &amp; Markel, 1976)</ref> measures the reconstruction quality of individual frequencies as follows:</p><formula xml:id="formula_3">LSD(x, y) = 1 L L =1 1 K K k=1 X( , k) −X( , k) 2 ,<label>(3)</label></formula><p>where X andX are the log-spectral power magnitudes of y and x, respectively. These are defined as X = log |S| 2 , where S is the short-time Fourier transform (STFT) of the signal. We use and k index frames and frequencies, respectively; in our experiments, we used frames of length 2048.   <ref type="table" target="#tab_1">Table 2</ref>. Our objective metrics show an improvement of 1-5 dB over the baselines, with the strongest improvements at higher upscaling factors. Although, the spline baseline achieves a high SNR, its signal often lacks higher frequencies; the LSD metric is better at identifying this problem. Our technique also improves over the DNN baseline; our convolutional architecture appears to use our modeling capacity more efficiently than a dense neural network, and we expect such architectures will soon be more widely used in audio generation tasks.</p><p>Next, we confirmed our objective experiments with a study in which human raters were asked to assess the quality of super-resolution using a MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor) test. For each trial an audio sample was upscaled using different techniques 1 . We collected four VCTK speaker recordings audio samples from the MULTISPEAKER testing set. For each recording, we collected the original utterance, a downsampled version at r = 4, as well as signals super-resolved using Splines, DNNs, and our model (six versions in total). We recruited 10 subjects and used an online survey to ask each of them to rate each sample on a scale of 0 (extremely bad) to 100 (excellent) reconstruction. The results from the experiment are summarized in <ref type="table">Table 1</ref>. Our method ranked as being the best out of the three upscaling techniques.</p><p>LPF <ref type="bibr">(Test)</ref> No LPF (Test) SNR LSD SNR LSD LPF <ref type="bibr">(Train)</ref> 30.1 3.4 0.42 4.5 No LPF <ref type="bibr">(Train)</ref> 0.43 4.4 33.2 3.3 <ref type="table">Table 3</ref>: Sensitivity of the model to whether low-resolution audio was subject to a lowpass filter (LPF) in dB.</p><p>Domain adaptation. We tested the sensitivity of our method to out-of-distribution input via an audio super-resolution experiment in which the training set did not use a low-pass filter, while the test set did, and vice-versa. We focused on the PIANO task and r = 2. The output from the model was noisier than expected, indicating that generalization is an important practical concern. We suspect this behavior may be common in super-resolution algorithms, but has not been widely documented. A potential solution would be to train on data that has been generated using multiple techniques.</p><p>In addition, we examined the ability of our model to generalize from speech to music and vice versa. We found that switching domains produced noisy output, again highlighting the specialization of the model.</p><p>Architectural analysis. We examined the importance of our various architectural design choices via an ablation analysis on the MULTISPEAKER audio super-resolution task using an upscaling ratio of r = 4. The adjacent figure displays the result: the green-ish line display the validation set 2 loss of the original model over time; the yellow curve removes the additive residual connection; the green curve further removes the additive skip connection (while preserving the same total number of filters). This shows that symmetric skip connections are crucial for attaining good performance; additive connections add an additional small, but perceptible, improvement.  Computational performance. Our model is computationally efficient and can be run in real time. On the PIANO task (where all input signals are 12s in length), our method processed a single second of audio in 0.11s on average on a Titan X GPU. Training our models, however, required about 2 days for the MULTISPEAKER task. Unlike sequence-to-sequence architectures our model does not require the complete input sequence in order to begin generating an output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LIMITATIONS</head><p>Finally, to explore the limits of our approach, we evaluated our method on the MagnaTagATune dataset, which consists of about 200 hours of music from 188 different genres. This dataset is larger and much more diverse that the ones we considered so far. We found that our model underfit the dataset, with very little reduction in the training error, and no improvement over the spline baseline. Other learning-based baselines fared similarly. However, we expect improved results with a larger model and more computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PREVIOUS WORK AND DISCUSSION</head><p>Time series modeling. In the machine learning literature, time series signals have most often been modeled with auto-regressive models, of which variants of recurrent networks are a special case <ref type="bibr" target="#b9">(Gers et al., 2001;</ref><ref type="bibr" target="#b19">Maas et al., 2012;</ref><ref type="bibr" target="#b20">Mehri et al., 2016)</ref>. Our approach instead generalizes conditional modeling ideas used in computer vision for tasks such as image super-resolution <ref type="bibr" target="#b7">(Dong et al., 2016;</ref><ref type="bibr" target="#b15">Ledig et al., 2016)</ref> or colorization <ref type="bibr" target="#b29">(Zhang et al., 2016)</ref>.</p><p>We identify a broad class of conditional time series modeling problems that arise in signal processing, biomedicine, and other fields and that are characterized by a natural alignment among source/target series pairs and differences that are well-represented by local transformations. We propose a general architecture for such problems and show that it works well in different domains.</p><p>Bandwidth extension. Existing learning-based approaches include Gaussian mixture models <ref type="bibr" target="#b5">(Cheng et al., 1994;</ref><ref type="bibr" target="#b22">Park &amp; Kim, 2000;</ref><ref type="bibr" target="#b23">Pulakka et al., 2011)</ref>, linear predictive coding <ref type="bibr" target="#b4">(Bradbury, 2000)</ref>, and neural networks <ref type="bibr" target="#b16">(Li et al., 2015)</ref>. Our work proposes the first convolutional architecture, which we find to scale better with dataset size and outperform recent, specialized methods. Moreover, while existing techniques involve many hand-crafted features (see e.g., <ref type="bibr" target="#b23">Pulakka et al. (2011)</ref>); our approach is fully domain-agnostic.</p><p>Audio applications. In telephony, commercial efforts are underway to transmit voice at higher rates (typically 16 Khz) in specific handsets; audio-super resolution is a step towards recreating this experience in software. Similar applications could be found in compression, text-to-speech generation, and forensic analysis. More generally, our work demonstrates the effectiveness of feedforward convolutional architectures on an audio generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Machine learning techniques based on deep neural networks have been successful at solving underdefined problems in signal processing such as image super-resolution, colorization, in-painting, and many others. Learning-based methods often perform better in this context than general-purpose algorithms because they leverage sophisticated domain-specific models of the appearance of natural signals.</p><p>In this work, we proposed new techniques that use this insight to upsample audio signals. Our technique extends previous work on image super-resolution to the audio domain; it outperforms previous bandwidth extension approaches on both speech and non-vocal music. Our approach is fast and simple to implement, and has applications in telephony, compression, and text-to-speech generation. It also demonstrates the effectiveness of feedforward architectures on an important audio generation task, suggesting new directions for generative audio modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Deep residual network used for audio super-resolution. We extract features via B residual blocks; upscaling is done via stacked SubPixel layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Model ablation analysis on the Multi-Speaker audio super-resolution task with r = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy evaluation of audio-super resolution methods (in dB) on each of the three superresolution tasks at upscaling ratios r = 2, 4, 6.</figDesc><table><row><cell>Evaluation The results of our experiments are summarized in</cell><cell cols="2">MultiSpeaker Sample 1 2 3 4 Ours 69 75 64 37</cell><cell>Average 61.3</cell></row><row><cell></cell><cell>DNN 51 55 66</cell><cell>53</cell><cell>56.3</cell></row><row><cell></cell><cell>Spline 31 25 38</cell><cell>47</cell><cell>35.3</cell></row><row><cell></cell><cell cols="3">Table 1: MUSHRA user study scores. We show</cell></row><row><cell></cell><cell cols="3">scores for each sample, averaged individual users.</cell></row><row><cell></cell><cell cols="3">Average across all samples is also displayed</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We have posted a our set of samples to: https://kuleshov.github.io/audio-super-res/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated classification of bird and amphibian calls using machine learning: A comparison of methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">J</forename><surname>Acevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor</forename><surname>Corrada-Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Corrada-Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Villanueva-Rivera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="206" to="214" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6146-soundnet-learning-sound-representations-from-unlabeled-video" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bandwidth expansion of narrowband speech using non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphical models and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical foundations of speech and language processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="191" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Linear predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bradbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Mc G. Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical recovery of wideband speech from narrowband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><forename type="middle">Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Shaughnessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multivariate autoregressive mixture models for music auto-tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Vaizman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert Rg</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2439281</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2015.2439281" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bandwidth extension of audio signals by spectral band replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Ekstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA02. Citeseer</title>
		<meeting>the 1st IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA02. Citeseer</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying lstm to time series predictable through time-window approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="669" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distance measures for speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustine</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="391" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1875" to="1902" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Audio bandwidth extension: application of psychoacoustics, signal processing and loudspeaker design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald M Aarts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<ptr target="http://arxiv.org/abs/1609.04802" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dnn-based speech bandwidth expansion and its application to adding high-frequency missing features for automatic speech recognition of narrowband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beta process sparse nonnegative matrix factorization for music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<idno>978-0-615-90065-0</idno>
		<ptr target="http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/229_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Society for Music Information Retrieval Conference</title>
		<editor>Alceu de Souza Britto Jr., Fabien Gouyon, and Simon Dixon</editor>
		<meeting>the 14th International Society for Music Information Retrieval Conference<address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11-04" />
			<biblScope unit="page" from="375" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content-aware collaborative music recommendation using pre-trained neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minshu</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pw</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for noise reduction in robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><forename type="middle">M</forename><surname>Oneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Samplernn: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arxiv:1612.07837</idno>
		<ptr target="http://arxiv.org/abs/1612.07837.cite" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Narrowband to wideband conversion of speech using gmm based transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youl</forename><surname>Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung Soon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1843" to="1846" />
		</imprint>
	</monogr>
	<note type="report_type">ICASSP&apos;00. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech bandwidth extension using gaussian mixture model-based estimation of the highband mel spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Pulakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulpu</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalle</forename><surname>Palomäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paavo</forename><surname>Alku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5100" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.207</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2016.207" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-negative matrix completion for bandwidth extension: A convex optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazumder</surname></persName>
		</author>
		<idno type="DOI">10.1109/MLSP.2013.6661924</idno>
		<ptr target="http://dx.doi.org/10.1109/MLSP.2013.6661924" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<meeting><address><addrLine>Southampton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-09-22" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<ptr target="http://arxiv.org/abs/1609.03499" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving content-based and hybrid music recommendation using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<ptr target="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colorful image colorization. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
