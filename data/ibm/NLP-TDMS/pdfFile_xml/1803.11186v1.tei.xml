<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unnat</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human conversation is a complex mechanism with the intent to exchange information between at least two people. It is often very subtle and nuances are particularly important. It is hence an ambitious goal to develop artificial intelligence based agents for human-computer conversation about visual observations, that goes far beyond development of a simple question-answer mechanism.</p><p>Nonetheless, to obtain a basic understanding about how to construct artificial intelligence powered agents for conversation about visual observations, it is important to develop early prototypes using dialogues containing questions and answers. In a recent effort to facilitate this task, Das et al. <ref type="bibr" target="#b5">[6]</ref> collected, curated and provided to the general public an impressive dataset which allows to design virtual assistants that can converse. Different from image captioning datasets, such as MSCOCO <ref type="bibr" target="#b20">[21]</ref>, or visual question answering datasets, such as VQA <ref type="bibr" target="#b1">[2]</ref>, the visual dialog dataset <ref type="bibr" target="#b5">[6]</ref> contains short dialogues about a scene between two people. To direct the dialogue, the dataset was constructed by showing a caption to the first person ('questioner') to inquire more about the hidden image. The second person ('answerer') could see both the image and it's caption to <ref type="figure">Figure 1</ref>: Visual dialog as a combination of two complementary tasks: <ref type="bibr" target="#b0">(1)</ref> predicting a contextual answer to a given question (VisDial <ref type="bibr" target="#b5">[6]</ref>); (2) predicting a contextual follow-up question to a given question-answer pair (VisDial-Q). provide answers to these questions.</p><p>Beyond providing the Visual Dialog dataset (VisDial), to facilitate a fair comparison, Das et al. <ref type="bibr" target="#b5">[6]</ref> suggest a concrete task that can be evaluated precisely. It requires the AI system to predict the next answer given the image, the question, and a history of question-answer pairs. A variety of discriminative and generative techniques were proposed, ranging from those based on Long-Short-Term-Memory (LSTM) units <ref type="bibr" target="#b11">[12]</ref> to reasonably complex ones, which make use of memory nets <ref type="bibr" target="#b3">[4]</ref> and hierarchical LSTM architectures.</p><p>In this paper we develop a deep net architecture that predicts an answer given a question, a caption, an image, and a question-answer history. The proposed approach outperforms existing baselines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> on the aforementioned answer prediction task. We present a careful assessment of its performance over five metrics. We also argue that the reverse setup, i.e., prediction of the next question given the image, caption, and a history of question-answer pairs is equally important. We therefore re-purpose the visual dialog dataset and demonstrate that our developed architecture is applicable to this new question prediction setup without significant changes. In <ref type="figure">Fig. 1</ref> we illustrate a combination of our models producing a visual dialog. To obtain this result, our discriminative questioning and answering modules communicate with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A conversation about an image or more generally an observation is hard to analyze, often very personal and even harder to predict. Despite or rather because of these difficulties, artificial intelligence based systems that master conversational capabilities are of great use, e.g., for aiding visually impaired or for improving human-computer interaction.</p><p>Related to artificial intelligence agents that master conversation are several areas that have received a considerable amount of attention: (i) image captioning, i.e., the task to describe the main content of an observed scene; (ii) visual question answering, i.e., the task to answer a question about the content of a provided image; and (iii) visual question generation, i.e., the task to generate a question about an observed scene. We briefly review each of those tasks in the following before discussing the visual dialog setup. Image Captioning: Classical methods formulate image captioning as a retrieval problem. The best fitting description from a pool of possible captions is found by evaluating the fitness between available textual descriptions and images. This metric is learned from a set of available image descriptions. While this permits end-to-end training, matching image descriptors to a sufficiently large pool of captions is computationally expensive. In addition, constructing a database of captions that is sufficient for describing a reasonably large fraction of images seems prohibitive.</p><p>To address this issue, recurrent neural nets (RNNs) decompose the space of a caption into a product space of individual words. They have found widespread use for image captioning because they have been shown to produce remarkable results. For instance, <ref type="bibr" target="#b27">[28]</ref> train an image CNN and a language RNN that shares a joint embedding layer. <ref type="bibr" target="#b40">[41]</ref> jointly trains a CNN with a language RNN to generate sentences, <ref type="bibr" target="#b41">[42]</ref> extends <ref type="bibr" target="#b40">[41]</ref> with additional attention parameters and learns to identify salient objects for caption generation. <ref type="bibr" target="#b17">[18]</ref> uses a bi-directional RNN along with a structured loss function in a shared vision-language space. Diversity was considered, e.g., by Wang et al. <ref type="bibr" target="#b37">[38]</ref>. Visual Question Answering: Beyond describing an image, a significant amount of research has been devoted to approaches which answer a question about a provided image. This task is often also used as a testbed for reasoning capabilities of deep nets. Using a variety of datasets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b16">17]</ref>, models based on multi-modal representation and attention <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34]</ref>, deep net architecture developments <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref> and dynamic memory nets <ref type="bibr" target="#b38">[39]</ref> have been discussed. Despite these efforts, it is hard to assess the reasoning capabilities of present day deep nets and differentiate them from memorization of training set statistics. Visual Question Generation: In spirit similar to question answering but often involving a slightly more complex language part is the task of visual question generation. It has been proposed very recently and is still very much an open-ended topic. For instance, Ren et al. <ref type="bibr" target="#b32">[33]</ref> discuss a rule-based algorithm which converts a given sentence into a corresponding question that has a single word answer. Mostafazadeh et al. <ref type="bibr" target="#b28">[29]</ref> were the first to learn a question generation model using human-authored questions instead of machine-generated captions. They focus on creating a 'natural and engaging' question. Recently, Vijayakumar et al. <ref type="bibr" target="#b36">[37]</ref> have shown preliminary results for this task as well. In contrast to the two aforementioned techniques, Jain et al. <ref type="bibr" target="#b14">[15]</ref> argued for more diverse predictions and employed a variational auto-encoder approach. Work by Li et al. <ref type="bibr" target="#b19">[20]</ref>, introduce VQA and VQG as dual tasks and suggest a joint training for the two tasks. They leverage the state-of-the art VQA model by Ben-younes et al. <ref type="bibr" target="#b2">[3]</ref> and achieve improvements for both VQA and VQG. Visual Dialog: A combination of the three aforementioned tasks is visual dialog. Strictly speaking it involves both generation of questions and corresponding answers. However, in its original form <ref type="bibr" target="#b5">[6]</ref>, visual dialog required to predict the answer for a given question, a given image and a provided history of question-answer pairs. While this largely resembles the visual question answering task, a variety of different approaches have been proposed recently.</p><p>For instance, in <ref type="bibr" target="#b5">[6]</ref>, three models are formulated based on -late fusion, attention based hierarchical LSTM, and memory networks. A baseline for simple models is set using the 'late fusion' architecture. While late fusion has a simple architecture, the other two complex models obtain better performance. Following up, <ref type="bibr" target="#b22">[23]</ref> proposed a generator-discriminator architecture where the outputs of the generator are improved using a perceptual loss from a pre-trained discriminator. The generator consists of an encoder (with two LSTM nets and attention mechanism) and a Gumbel-softmax <ref type="bibr" target="#b15">[16]</ref> based LSTM decoder. The discriminator employs a similar encoder and a deep metric learning based loss. Unlike the methods of <ref type="bibr" target="#b5">[6]</ref> which train in 4-8 epochs, <ref type="bibr" target="#b22">[23]</ref> report pretraining of the generator and discriminator networks for 20 and 30 epochs respectively. Afterwards the generator is finetuned for additional epochs to obtain the final model. <ref type="figure" target="#fig_0">Fig. 2</ref> summarizes the difference between our approach and the existing methods for Visual Dialog. A study of similar type was done by Jabri et al. <ref type="bibr" target="#b13">[14]</ref> for VQA. All aforementioned methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> first encode the question, image, caption and history into a fused representation. Later this encoded representation is used to obtain similarity with the 100 answer options. In contrast, our model uses the answer option under evaluation as an early input. We perform both fusion and similarity scoring together using a multilayer perceptron network. This joint optimization improves performance significantly compared to even memory networks <ref type="bibr" target="#b5">[6]</ref>. We obtain quantitative results slightly better than the methods in <ref type="bibr" target="#b22">[23]</ref>. Also, training of all our models converges within 5 epochs, which is significantly faster than the techniques proposed in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Despite strong dialog information, the suggested evaluation of the VisDial dataset is strongly one-sided as mentioned before. To tackle this issue, Das et al. <ref type="bibr" target="#b6">[7]</ref> introduced an image guessing game as a proxy to build visual question and answer bots. They adopt reinforcement learning based methods which they found to outperform maximum likelihood based supervised learning on respective metrics. Despite training both questioning and answering agents on the VisDial dataset, only answer metrics are reported. This is because at present there isn't an objective question generation protocol for the VisDial dataset. To bridge this gap, we provide a reconfiguration of the VisDial dataset, i.e., 'VisDial-Q. <ref type="bibr">'</ref> We introduce VisDial-Q to facilitate an evaluation of visual question generation agents. We also provide our baselines for VisDial-Q. We believe this reconfiguration to be useful for researchers that aim at evaluating the visual question generation side of the visual dialog task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>It is the purpose of this paper to maximally utilize the informativeness of options, i.e., to use early option input. Hence, we focus on discriminative visual dialog systems. In contrast, generative methods model a complex output space distribution. Since discriminative frameworks cannot provide such free-form answers, they are restricted to environments where a small number of answers or questions is sufficient. Beyond focusing on the visual question answering part like <ref type="bibr" target="#b5">[6]</ref>, in this paper, we also provide results for question generation. We argue that this part is at least as important for a successful visual dialog system as answering a question.</p><p>To this end we develop a unified deep net architecture for both visual question answering and question generation. We will demonstrate in Sec. 4 that the proposed approach performs well on both tasks. In the following we first provide an overview of the proposed approach before we discuss the developed architecture in greater detail and provide implementation details. We finally discuss how we repurpose the visual dialog dataset to obtain a training set for the question generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>An overview of our approach is provided in <ref type="figure" target="#fig_0">Fig. 2</ref>. The visual dialog dataset contains tuples (I, C, H t , Q t , A t ), consisting of an image I, a caption C, a question Q t asked at time t ∈ {1, . . . , T }, its corresponding answer A t , and a time dependent history H t . T is the maximally considered time horizon. The history itself is a set of past questionanswer pairs, i.e.,</p><formula xml:id="formula_0">H = {(Q k , A k )} for k ∈ {1, . . . , t − 1}.</formula><p>At a high level, any visual dialog system, just like ours, operates on image embeddings, embeddings of the history and caption, and an embedding of the question. Generative techniques use embeddings of those three elements, or a combination thereof to model a probability distribution over all possible answers. Note that generative techniques typically don't take a set of answer options or their embeddings into account. In contrast, discriminative techniques operate on a set of answers, particularly their embeddings, and assess the fitness of every set member w.r.t. the remaining data, i.e., the image I, the history H t , the caption C and the question Q t . One member of the answer set constitutes the groundtruth, while other possible answers are assembled to obtain a reasonably challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Deep Net Architecture</head><p>A detailed illustration of our architecture is provided in pair in the history via the corresponding LSTM nets, we compute a single embedding by combining both representations via a fully connected layer. Concatenation of embeddings for all pairs in the history set H constitutes the history embedding. We then concatenate the question embedding, the image embedding, the caption embedding, the history embedding, and the answer embedding for each of the possible answer options and employ a similarity network to predict a probability distribution over the possible answers. Since we score each option independently, our architecture works even if a different number of options are being evaluated at test time. We provide more details for each of the components in the following. Question and Answer Embedding: The VisDial dataset questions are truncated to contain a maximum of N Q words. A Stop token is introduced to mark the end of the question. Each word's V -dimensional one-hot encoding is transformed into a real valued word representation using a matrix W Q ∈ R E Q ×V . These E Q -dimensional word embeddings are used as input for an LSTM which transforms them to L Q -dimensional hidden state representations. The hidden state output corresponding to the last Stop token is used as the sentence embedding of the question.</p><p>The methodology to obtain the representation for the answer options is identical. Each answer option is truncated to contain a maximum of N O words. V -dimensional one-hot representations of the words of an answer are transformed using a word embedding matrix W O ∈ R E O ×V . These E O -dimensional word embeddings when transformed using an LSTM network give rise to an L O -dimensional sentence embedding of the particular answer option at the last LSTM unit. If the question has 100 answer options, we extract a sentence embedding for each of the 100 options. Caption Embedding: Similar to question and answer em-beddings, captions are truncated to contain a maximum of N C words. Then a Stop token is concatenated and these one-hot vectors are first transformed using an embedding matrix W C ∈ R E C ×V before transformation into an L Cdimensional caption embedding using an LSTM net f C (·). Image Representation: To obtain an image representation we make use of pretrained CNN features to represent images. For a fair comparison with baseline architectures proposed in <ref type="bibr" target="#b5">[6]</ref>, we use the activations of the second to last layer of the VGG-16 <ref type="bibr" target="#b35">[36]</ref> deep net. We normalize these L I -dimensional activations by dividing via their 2 norm, as also performed in <ref type="bibr" target="#b5">[6]</ref>. History Embedding: All question-answer pairs (Q k ,A k ) before the query time t, i.e., k ∈ {1, . . . , t − 1}, serve as history. An embedding matrix W qh ∈ R E qh ×V maps one-hot word vectors to real valued embeddings. These are transformed by a question-history LSTM f qh (·) to an L qhdimensional sentence embedding. Similarly, the answerhistory is encoded via W qh ∈ R E qh ×V and f ah (·) to obtain an L ah -dimensional sentence embedding. Both the question and answer embedding are combined using a fully connected layer to obtain an L H -dimensional representation of a pair {(Q k , A k )}. The number of question-answer pairs before the current query is variable (t − 1 ∈ [0, T − 1]). Existing models tackle this issue of variable length history in different ways. For instance, Das et al.'s <ref type="bibr" target="#b5">[6]</ref> 'Late Fusion' (LF) concatenates words of all previous questions and answers and transforms it using another LSTM network. They also implement a hierarchical LSTM to address this challenge. Their model performing best in terms of accuracy is based on a memory network which maintains every previous question and answer as a 'fact entry.' Lu et al. <ref type="bibr" target="#b22">[23]</ref> use an attention based mechanism to combine all previous rounds of history to get a single representation. On the con- <ref type="figure">Figure 4</ref>: Comparison of our method to state-of-the-art discriminative models-memory networks (MN) <ref type="bibr" target="#b5">[6]</ref> and HCIAE <ref type="bibr" target="#b22">[23]</ref>. We use the authors' implementations. HCIAE-D-NP-ATT is the best performing discriminative model proposed by <ref type="bibr" target="#b22">[23]</ref>, which we abbreviate as HCIAE.</p><p>trary, we found a very simple method to be effective. We introduce an Empty token to our vocabulary of words (which already includes the stop token Stop). For all the missing question-answer rounds, we pass the [Empty, Stop] sequence to the f qh (·) and f ah (·) LSTM nets. Using this we always have T − 1 embeddings of question-answer pairs. A concatenation results in the (T − 1) · L H -dimensional history representation. Similarity Scoring + Fusion Network: The individual representations of the question, image, caption, history as well as an answer option are concatenated to form an ensemble. This ensemble is represented by an L S = L Q + L I + L C + (T −1) * L H +L O dimensional vector. As mentioned before, unlike previous methods, we perform similarity scoring and feature fusion jointly. This is achieved using a multi-layer perceptron (MLP). To reduce the number of parameters, the MLP is structured to have a decreasing number of activations for the intermediate layers before arriving at a single scalar score for each L S -dimensional representation. During inference we choose the answer option having the highest score. During learning the answer option scores are transformed into probabilities using a softmax layer. We report results of architectures with MLP having one and two hidden layers. The single hidden layer MLP has L s /2 hidden nodes. The two hidden layered MLP has L s /2 and L s /4 nodes in its intermediate representation layers.</p><p>To simplify training, we employ Batch Normalization <ref type="bibr" target="#b12">[13]</ref> layers after every linear layer which we found to be more robust. We normalize before the ReLU non-linearity, as suggested in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training</head><p>To describe training more formally, let F w (O i ) denote the score for answer option i obtained from the 'similarity scoring + fusion network,' and let w denote all the parameters of the architecture illustrated in <ref type="figure">Fig. 3</ref>. For simplicity we avoid to explicitly mention other inputs such as the query, the image, etc. While inference chooses the highest scoring answer i * = arg max i F w (O i ) given learned parameters w, training optimizes for the parameters w via the multi-class cross-entropy loss:</p><formula xml:id="formula_1">min w D   ln 100 î =1 exp F w (Oî) − F w (O i * )   ,</formula><p>where D denotes the dataset containing ground truth information i * . All our models are trained using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 −3 . We experimented with both normal initialization by He et al. <ref type="bibr" target="#b10">[11]</ref> and Xavier normal initialization <ref type="bibr" target="#b9">[10]</ref> and found the former to work better in our case for both MLP and LSTM weights. We found that sharing the weights of the language embedding matrices greatly helps in learning better word representations. Two hidden layered MLP nets assessing similarity and fusing the representations consistently performed better than a one layered MLP. We use the data splits suggested in <ref type="bibr" target="#b5">[6]</ref> for VisDial v0.9: 80k images for training, 3k image for validation and 40k for test. We use the validation set to determine when training doesn't progress any further and report metrics on the test set. All our models converge in under 5 epochs of training on this dataset, which is illustrated in <ref type="figure">Fig. 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>The VisDial dataset has ten rounds of question-answer pairs, hence T = 10. N Q , N A and N C are set to 20, 20 and 40 respectively. Dimensions of all embeddings, i.e., E Q , E O , E C , E qh and E ah are set to 128. LSTM hidden state dimension of query and options, i.e., L Q and L O , are set to 512. LSTM hidden state dimension of caption, question-history and answer-history, i.e., L C , L qh and L ah , are set to 128. All the LSTMs are single layered. In accordance to the baselines of <ref type="bibr" target="#b5">[6]</ref>, we use pretrained VGG-16 relu7 features for the image embedding, hence, L I = 4096. Note that on the contrary, <ref type="bibr" target="#b22">[23]</ref> utilize 25k dimensional VGG-19 pool5 features. Also, <ref type="bibr" target="#b22">[23]</ref> report their result after making use of 82k training images which is more than the 80k images suggested in <ref type="bibr" target="#b5">[6]</ref> for VisDial v0.9. Finally, <ref type="bibr" target="#b22">[23]</ref> utilize deep metric learning and a self-attention mechanism to train a discriminator network which leverages the availability of answer options. We achieve this via a simple LSTM-MLP approach. However, it must be noted that <ref type="bibr" target="#b22">[23]</ref> also investigate generative models for the VisDial dataset which we don't explore here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">VisDial-Q Dataset and Evaluation</head><p>Das et al. <ref type="bibr" target="#b5">[6]</ref> highlight the challenge of evaluating dialog systems and they propose to evaluate individual responses at each round of the dialog. To this end they create a multiple choice retrieval setup as a 'VisDial evaluation protocol.' As explained earlier, the system is required to choose one out of 100 answer options for a given question. The image, caption and previous question-answer pairs can be leveraged by the system to help make this choice. However, no surrogate task for assessment of question generation is provided. To test the questioner side of visual dialog, we therefore create a similar 'VisDial-Q evaluation protocol.' A visual question generation system is required to choose one out of 100 next question candidates for a given question-answer pair. To do this it may utilize the image, caption and previous question-answer pairs. What is left to answer is how these 100 candidates for the next question are selected.</p><p>We closely follow the methodology adopted by Das et al. <ref type="bibr" target="#b5">[6]</ref> to select 100 answer candidates from the visual dialog dataset of the human question-answer rounds. We select 100 candidate follow-up questions to a given questionanswer (QA) pair as the union of the following four sets: Correct: The next question asked by the human is the ground truth question. Plausible: Plausible questions are follow-up questions to the 50 most similar QA pairs in the dataset. Similar QA pairs are found by comparing concatenated GloVe embeddings <ref type="bibr" target="#b29">[30]</ref> of the QA pair being considered with the representation of other QA pairs. Question GloVe embeddings are obtained following <ref type="bibr" target="#b5">[6]</ref>, i.e., (1) concatenate the GloVe embedding of the first three words of the question; (2) average the GloVe embeddings of the remaining words; and (3) concatenate both vectors. Answer GloVe embeddings are obtained by averaging the GloVe embeddings of all its words. 2 distance computed on the concatenated question and answer GloVe embeddings is used to find nearest neighbor QA pairs. We make sure that a nearest neighbor QA pair is not from the same image (same as <ref type="bibr" target="#b5">[6]</ref>). Additionally, for the VisDial-Q evaluation, we also ensure that the nearest neighbor QA pair isn't the last (10 th ) QA round of a dialog, as no human follow-up question is available. Popular: Question possibilities also contain the 30 most popular questions of the original dataset. Our intention for creating a set of question options using this methodology is analogous to <ref type="bibr" target="#b5">[6]</ref>. These candidates encourage an algorithm to distinguish between correct, plausible, and popular candidates.</p><p>At this point it is important to address a strong difference in the nature of evaluating a module for generating an answer from a technique producing a question. While answering a given question based on options (and some additional information) has fairly little randomness, the questioning analog is significantly more challenging. That is, for a given QA pair, there could be more than one 'correct' follow-up question in the options. Despite this inherent ambiguity, objective evaluation of the question generation procedure is equally important. It depicts the questioning system's ability to rank a human generated question. The system should be encourage to rank the human generated question in its top ranks, if not at the highest one. Therefore, the ensemble of metrics proposed in <ref type="bibr" target="#b5">[6]</ref> and described in Sec. 4.2 is even more important than a single Recall@1 based evaluation.</p><p>Our deep net architecture developed for the answering task in Sec. 3.2 can be deployed for the VisDial-Q task, with almost no adjustments. Since there exists no followup question to the last QA pair in a dialog of the VisDial dataset, the maximally considered time horizon T is 9 for the VisDial-Q dataset. The 'query' for the original visual  <ref type="table">Table 2</ref>: VisDial-Q evaluation metrics. '-1' and '-2' denote one and two hidden layers in MLP respectively. '-se' denotes shared embedding matrices for all LSTMs.</p><p>dialog task is a question whose answer we wish to choose. On the other hand, 'query' for the questioning side of visual dialog (VisDial-Q) is a QA pair for which we wish to choose the most relevant follow-up question. For VisDial-Q evaluation, words of the QA pair (concatenation of question and answer words) serve as input to the 'query' LSTM in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following we evaluate our proposed architecture on prediction of both answers and questions. To this end, we first provide details about the datasets and evaluation metrics used. We then discuss our quantitative assessment before providing qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We train our models on the VisDial v0.9 dataset <ref type="bibr" target="#b5">[6]</ref> which currently contains over 123k image-caption-dialog tuples. Each dialog has 10 question-answer pairs. The images are unique and are obtained from the MSCOCO <ref type="bibr" target="#b20">[21]</ref> train and validation split. The dataset was collected by recording a conversation between two people on Amazon Mechanical Turk. The first person is only provided the caption to start the conversation, and is tasked to ask questions about the hidden image to better understand the scene. The second person has access to both image and caption and is asked to answer the first person's questions. Both are encouraged to talk in a natural manner, which is markedly different from <ref type="bibr" target="#b1">[2]</ref>. Due to this setup, the obtained questionanswer pairs have inherent temporal continuity and are also visually grounded. The VisDial v0.9 train, validation and test sets consists of 80k, 3k and 40k images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Many popular metrics like BLEU, ROUGE and ME-TEOR are empirically shown to have low correlation with   human judgement of dialog systems <ref type="bibr" target="#b21">[22]</ref>. For an objective evaluation of visual dialog systems, <ref type="bibr" target="#b5">[6]</ref> suggests metrics for predicted rank of the correct answer option. These are Re-call@1, Recall@5, Recall@10, Mean Reciprocal Rank, and Mean Rank of the ground truth answer. Recall@k is the percentage of questions for which the correct answer option is ranked in the top k predictions of a model. Mean Rank is the empirical average of the rank allotted by a model to the ground truth answer option. Mean Reciprocal Rank is the empirical average of 1/rank allotted by a model to the ground truth answer option. Lower values for Mean Rank and higher values for all the other metrics are desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Assessment</head><p>In the following we provide a quantitative assessment of our approach. We first discuss results for the question answering task before focusing on question generation. Visual Question Answering: The performance of the proposed architecture for predicting a contextual answer to a given question (VisDial evaluation) is presented in Tab. 1. We gradually increase context from only question (Q), to question and image (QI), and finally all given context (QIH). Our 'similarity scoring + fusion' (SF) performs best in all three scenarios. Adding image and history cues improves results. We provide the metrics for baselines from existing work evaluating on the VisDial dataset. This includes models proposed in <ref type="bibr" target="#b5">[6]</ref>, based on late fusion (LF), hierarchical LSTM net (HRE), and memory networks (MN). Another important baseline is the best performing discriminative model (HCIAE-D-NP-ATT) <ref type="bibr" target="#b22">[23]</ref>. We use the abbreviation HCIAE for this model. <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref> compare the mean rank and recall@5 of different models. Our SF-QIH model achieves 78.96% recall@5 and 4.70 mean rank. Visual Question Generation: A similar evaluation of the proposed architecture for the task of predicting the next question based on a given QA pair and context (VisDial-Q evaluation) is presented in Tab. 2. By closely investigating our results, we obtain some intuitive insights. First, without any context, predicting the next question is a much more difficult task than answering a question without context. This can be observed from the average mean rank for VisDial-Q (∼ 20) in comparison to the average mean rank for VisDial (∼ 7). Second, large improvements when comparing Q vs QI and QI vs QIH suggest that image and history cues are much more important for the question prediction task than for answer prediction. <ref type="figure" target="#fig_5">Figs. 7, 8</ref> compare the mean rank and recall@5 of different models. Our SF-QIH model achieves a 55.17% recall@5 and 9.32 mean rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluation</head><p>In this section we discuss qualitative results. Instead of presenting two separate qualitative evaluations of our architecture on the answering and questioning side of visual dialog, we provide a joint analysis. After completing the answering task of choosing the best option for a given question, we provide this QA pair to our pretrained question generation module. The newly generated question is then again put up for discriminative answering by the answering module. Hence we 'generate' dialog using our discriminative models. <ref type="figure" target="#fig_6">Fig. 9</ref> summarizes a few of those unrolled examples. A few arrangements are necessary to jointly unroll our discriminative questioning and answering modules, since answer options and next question options are available for only dataset dialogs, while we are 'generating' (i.e., selecting) new sequences. Hence we need to create options on the fly, by choosing from a set of questions and answers of nearest neighbor images. We uniformly sample one of the top 10 ranking questions chosen by the question module to add some more diversity. We again emphasize that these dialogs are 'generated' by choosing from a set of options, which differs from truly generative approaches.</p><p>Based on the observed empirical results we conclude that our models capture cues from all three contexts, image, caption and history. There are questions pertaining to partially visible objects, which can be attributed to the caption cue. The same is true for objects visible in the images which aren't mentioned in the history/caption text. We experimented with different number of rounds of initial history -1, 2, 3 and 5. In all cases, our models choose relevant followup questions and fairly correct answers. Since there are no groundtruth options for these predicted dialog sequences, we can't report quantitative metrics for this dynamic setup where our models communicate with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We developed a discriminative method for the visual dialog task, i.e., predicting an answer given question and context. Our approach outperforms existing baselines which often use complex architectures. More importantly, our approach can be applied with almost no change to prediction of a question given context, which we think is equally important. We introduce the VisDial-Q evaluation protocol to quantitatively assess this task and also illustrate how to combine both discriminative methods to obtain a system for visual dialog. Going forward we plan to combine visual dialog and textual grounding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The supplementary document is organized as follows:</p><p>• Sec. <ref type="bibr" target="#b5">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">VisDial-Q and VisDial comparison</head><p>Sec. 3.5 explains the re-purposing of VisDial to VisDial-Q, using correct, plausible, popular and random question options. Here we include a comparison of the distribution of answers and questions. Sentence distribution of target questions are shown in <ref type="figure" target="#fig_7">Fig. 10</ref>. A steeper slope of answer distribution vs. question distribution shows the challenging nature of question generation. This is supported by entropy (higher 4.71bits for question and 4.52bits for answer). <ref type="figure" target="#fig_7">Fig. 10b</ref> has examples of popular question candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Quantitative Results</head><p>In the following we present additional quantitative results, some of which were already mentioned in the paper. We report Recall@1, Recall@5, Recall@10, Mean Reciprocal Rank (MRR) and Mean rank for the test sets of both answer prediction task (VisDial evaluation) and question prediction task (VisDial-Q evaluation). <ref type="figure">Fig. 11</ref> and <ref type="figure" target="#fig_0">Fig. 12</ref> summarize these metrics as training proceeds for the two tasks. Our models perform significantly better than the most complex architectures of <ref type="bibr" target="#b5">[6]</ref>. Our models are easy to train, with convergence in under 5 epochs in contrast to a 20-30 epoch pre-training required for the baseline set by generator-discriminator architecture in <ref type="bibr" target="#b22">[23]</ref>. Since we introduce a new evaluation protocol for question prediction in visual dialog, there aren't any existing baselines for this task in <ref type="figure" target="#fig_0">Fig. 12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Qualitative Results</head><p>As mentioned in the paper, we decide to unroll both our question prediction and answer prediction module together to show how these discriminative models can be used to 'generate' dialog. The answer module chooses the best answer option to a given question while the question module chooses the best next question to a given question-answer pair. As mentioned in the paper, a few arrangements are necessary to jointly unroll questioning and answering modules, since answer options and next question options are available for only dataset dialogs, while we are 'generating' (i.e., selecting) new sequences. We create options on the fly, by choosing from a set of questions and answers of nearest neighbor images. Since there are no ground-truth options for these predicted dialog sequences, we can't report quantitative metrics for this dynamic setup where our models communicate with each other.</p><p>We test our models in two different visual dialog setups. Firstly, we unroll our VQA and VQG modules when there is very little history. A visual dialog system needs to be more inquisitive in such a setup and 'explore' the image. <ref type="figure" target="#fig_9">Fig. 13</ref> shows both short and long dialogs predicted by our models in such a setup. Secondly, we also test our models when there is a long history available to build on. Here, the models need to be consistent with existing context -avoid repetitions, and handle co-reference resolution. In such a setup the models 'exploit' the available history to find finer details about the image. The generations do not repeat questions from the history and reference objects using correct pronouns. <ref type="figure" target="#fig_10">Fig. 14</ref> shows both short and long visual dialogs predicted by our discriminative VQA and VQG models.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed approach: Joint similarity scoring of answer option and fusion of all input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Figure 3 :</head><label>33</label><figDesc>Using LSTM nets we compute embeddings for the question at hand, the caption and the set of possible answer options. Similarly, to obtain an embedding for a questionanswer pair, we use a question and an answer LSTM to encode all question-answer pairs in the history set H. Upon encoding the question and the answer of a question-answer Architecture of our model for selecting the best answer option from a set of 100 candidates. LSTM nets transform all sequential inputs to a fixed size representation. The combined representations of T − 1 previous question-answer pairs are concatenated to obtain the final history representation. Multi-class cross-entropy loss is computed by comparing a one-hot ground truth vector (based on the correct option) to output probabilities of the answer options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Figure 5 :Figure 6 :</head><label>256</label><figDesc>VisDial evaluation: Mean rank values for our models and best models from<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> VisDial evaluation: Recall@5 values for our models and best models from<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> (same legend asFig. 5)Random: The remaining question options which are left to complete a set of 100 unique candidates are filled with random questions from the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Figure 7 :</head><label>27</label><figDesc>VisDial-Q evaluation: Mean rank values for our models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>VisDial-Q evaluation: Recall@5 values for our models. (same legend as Fig. 7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Joint unrolling of questioning and answering modules on test images. The VQG module chooses the most relevant next question based on previous QA pairs and context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>(a) Comparing target answer and target question distributions (b) Target question distribution (top 30) (a) compares target distribution of questions and answers (top 30 ranked targets). Steeper slope of answers indicates higher frequency biases in the answer targets. (b) displays the frequency distribution of questions, analogous to Fig. 15 in [6].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>VisDial evaluation protocol: Evaluation metrics for our models and best models from<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> -Late fusion (LF) and HCIAE-D-NP-ATT (abbreviated as HCIAE). '-1' and '-2' refer to one and two hidden layers in our 'similarity learning + fusion net' (SF) model. '-se' refers to shared word embeddings across all LSTM nets. (Legend is same as (e)) VisDial-Q evaluation protocol: Metrics for our models on the newly proposed VisDial-Q evaluation protocol. '-1' and '-2' refer to one and two hidden layers in our 'similarity learning + fusion net' (SF) model. '-se' refers to shared word embeddings across all LSTM nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Joint unrolling of VQA and VQG modules for short history (1 QA pair): Short and long dialogs 'generated' by our discriminative models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Joint unrolling of VQA and VQG modules for long history (5 QA pair): Short and long dialogs 'generated' by our discriminative models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>VisDial evaluation metrics. '-1' and '-2' denote one and two hidden MLP layers respectively. '-se' denotes shared embedding matrices for all LSTMs.</figDesc><table><row><cell>Model</cell><cell>MRR R@1 R@5 R@10 Mean Query only</cell></row><row><cell>LF-Q [6] SF-Q-1 SF-Q-se-1 SF-Q-se-2</cell><cell>0.5508 41.24 70.45 79.83 7.08 0.5619 42.11 72.12 81.39 6.55 0.5651 42.32 72.54 81.83 6.39 0.5664 42.45 72.75 81.98 6.32 Query + Image only</cell></row><row><cell cols="2">LF-QI [6] SF-QI-1 SF-QI-se-1 0.5964 45.72 76.25 85.64 5.29 0.5759 43.33 74.27 83.68 5.87 0.5940 45.49 75.95 85.19 5.40 SF-QI-se-2 0.6010 46.19 76.73 85.95 5.18 Query + Image + Caption + History</cell></row><row><cell cols="2">LF-QIH [6] 0.5807 43.82 74.68 84.07 5.78 HRE-QIH [6] 0.5868 44.82 74.81 84.36 5.66 MN-QIH [6] 0.5965 45.55 76.22 85.37 5.46 HCIAE [23] 0.6222 48.48 78.75 87.59 4.81 SF-QIH-1 0.6101 47.04 77.69 86.78 5.00 SF-QIH-se-1 0.6207 48.19 78.66 87.53 4.79 SF-QIH-se-2 0.6242 48.55 78.96 87.75 4.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.18 26.18 38.87 23.03 SF-Q-se-1 0.1936 9.57 26.20 38.66 22.99 SF-Q-se-2 0.1950 9.70 26.44 38.67 22.92 Query + Image only SF-QI-1 0.2953 16.82 41.58 56.27 14.57 SF-QI-se-1 0.2970 17.06 41.60 56.05 14.48 SF-QI-se-2 0.3021 17.38 42.32 57.16 14.03 Query + Image + Caption + History SF-QIH-1 0.3877 25.03 53.03 68.33 10.09 SF-QIH-se-1 0.4028 26.51 54.74 69.95 9.54 SF-QIH-se-2 0.4060 26.76 55.17 70.39 9.32</figDesc><table><row><cell>Model</cell><cell>MRR R@1 R@5 R@10 Mean Query only</cell></row><row><cell>SF-Q-1</cell><cell>0.1909 9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The options O 1 , . . . , O 100 are now candidate follow-up questions, instead of candidate response answers. All other parameters are identical to the ones mentioned in Sec. 3.3 and Sec. 3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>covers additional details of VisDial-Q. • Sec. 7 shows additional quantitative evaluations beyond Mean Rank and Recal@5 (included in the main paper). • Sec. 8 shows additional qualitative examples of unrolling question generation and answering modules.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We thank NVIDIA for providing the GPUs used for this research. This material is based upon work supported in part by the National Science Foundation under Grants No. 1563727 and 1718221, and Samsung.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Largescale Simple Question Answering with Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1506.02075" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? Dataset and Methods for Multilingual Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Creativity: Generating Diverse Questions using Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. * equal contribution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01144" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual Question Generation as Dual Task of Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1709.07192" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to answer questions from image using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.6632</idno>
		<title level="m">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-Order Attention Models for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised Textual Grounding: Linking Words to Image Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
