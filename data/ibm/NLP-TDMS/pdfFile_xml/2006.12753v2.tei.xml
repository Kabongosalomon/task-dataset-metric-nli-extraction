<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>She</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format: Zhiqiang Wang, Qingyun She, PengTao Zhang, Junlin Zhang. 2020. Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction. In Proceedings of 2020 Association for Computing Machinery (DLP-KDD&apos;20). ACM, New York, NY, USA, 8 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalization has become one of the most fundamental components in many deep neural networks for machine learning tasks while deep neural network has also been widely used in CTR estimation field. Among most of the proposed deep neural network models, few model utilize normalization approaches. Though some works such as Deep &amp; Cross Network (DCN) and Neural Factorization Machine (NFM) use Batch Normalization in MLP part of the structure, there isn't work to thoroughly explore the effect of the normalization on the DNN ranking systems. In this paper, we conduct a systematic study on the effect of widely used normalization schemas by applying the various normalization approaches to both feature embedding and MLP part in DNN model. Extensive experiments are conduct on three real-world datasets and the experiment results demonstrate that the correct normalization significantly enhances model's performance. We also propose a new and effective normalization approaches based on LayerNorm named variance only LayerNorm(VO-LN) in this work. A normalization enhanced DNN model named NormDNN is also proposed based on the abovementioned observation. As for the reason why normalization works for DNN models in CTR estimation, we find that the variance of normalization plays the main role and give an explanation in this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Normalization has become one of the most fundamental components in many deep neural networks for machine learning tasks, especially in Computer Vision (CV) and Natural Language Processing (NLP). However, very different kinds of normalization are used in CV and NLP. For example, Batch Normalization (BatchNorm Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DLP-KDD'20, August 24, 2020, San Diego, California USA © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-6317-4/19/07. . . $15.00 https://doi.org <ref type="bibr">/10.1145/1122445.1122456</ref> or BN) <ref type="bibr" target="#b8">[8]</ref> is widely adopted in CV, but it leads to significant performance degradation when naively used in NLP. Instead, Layer Normalization (LayerNorm or LN) <ref type="bibr" target="#b0">[1]</ref> is the standard normalization method utilized in NLP.</p><p>On the other side, deep neural network has also been widely used in CTR estimation field <ref type="bibr">[2-7, 9-11, 17, 19-21]</ref>. Some deep learning based models have been introduced and achieved success such as wide &amp; deep <ref type="bibr" target="#b2">[3]</ref>, DeepFM <ref type="bibr" target="#b4">[4]</ref> and xDeepFM <ref type="bibr" target="#b9">[9]</ref> .Most DNN ranking models use the feature embedding to represent information and shallow MLP layers to model high-order interactions in an implicit way. These two commonly used components play important roles in current state-of-the-art ranking systems.</p><p>Taking the both sides into consideration, we care about the following questions: What's the effect of various normalization methods on deep neural network models for CTR estimation? Is there one method outperforming other normalization approaches just like LayerNorm in NLP or BatchNorm in CV? What's the reason if some normalization works?</p><p>Among most of the proposed deep neural network models, few of them utilize normalization approaches. Though some works such as DCN <ref type="bibr" target="#b15">[15]</ref> and Neural Factorization Machine (NFM) <ref type="bibr" target="#b5">[5]</ref> use Batch-Norm in MLP part of the structure, there isn't work to thoroughly explore the effect of the normalization on the DNN ranking systems. In this paper, we conduct a systematic study on the effect of widely used normalization schemas by applying the various normalization approaches to both feature embedding and MLP part in DNN model. Experimental results show the correct normalization helps the training of DNN models and boosts the model performance with a large margin. We also simplify the LayerNorm and propose a new and effective normalization method in this work. A normalization enhanced DNN model named NormDNN is also proposed based on the above-mentioned observation. Further more, we find the variance of normalization mainly contributes to this positive effect. To the best of our knowledge, this is the first work to verify the importance of normalization on DNN ranking system through systematic study.</p><p>The contributions of our work are summarized as follows:</p><p>( NormDNN is more applicable in many industry applications because of its better performance and high computation efficiency compared with many state-of-the-art complex neural network models. (4) To prove the universal validity of normalization for neural network ranking model, we also apply several normalization approaches to DeepFM and xDeepFM model. The experiments results imply that the correct normalization also boosts these model's performances with a large margin. (5) As for the reason why normalization works for DNN models in CTR estimation, we find that the variance of normalization plays the main role and give an explanation in this paper.</p><p>The rest of this paper is organized as follows. Section 2 introduces some related works which are relevant with our work. We introduce our proposed models in detail in Section 3. The experimental results on three real world datasets are presented and discussed in Section 4. Section 5 concludes our work in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Normalization</head><p>Normalization techniques have been recognized as very effective components in deep learning. Many normalization approaches have been proposed with the three most popular ones being BatchNorm <ref type="bibr" target="#b8">[8]</ref>, LayerNorm <ref type="bibr" target="#b0">[1]</ref> and GroupNorm <ref type="bibr" target="#b16">[16]</ref>. Batch Normalization (Batch Norm or BN) <ref type="bibr" target="#b8">[8]</ref> normalizes the features by the mean and variance computed within a mini-batch. This has been shown by many practices to ease optimization and enable very deep networks to converge. Another example is layer normalization (Layer Norm or LN) <ref type="bibr" target="#b0">[1]</ref> which was proposed to ease optimization of recurrent neural networks. Statistics of layer normalization are not computed across the N samples in a mini-batch but are estimated in a layer-wise manner for each sample independently. It's an easy way to extend LayerNorm to GroupNorm (GN) <ref type="bibr" target="#b16">[16]</ref>, where the normalization is performed across a partition of the features/channels with different pre-defined groups. Normalization methods have shown success in accelerating the training of deep networks. In general, BatchNorm <ref type="bibr" target="#b8">[8]</ref> and GroupNorm <ref type="bibr" target="#b16">[16]</ref> are widely adopted in CV and LayerNorm <ref type="bibr" target="#b0">[1]</ref> is the standard normalization scheme used in NLP.</p><p>Another line of research on normalization is to understand why BatchNorm helps training in CV and LayerNorm helps training in NLP. For example, the original explanation was that BatchNorm reduces the so-called "Internal Covariance Shift" <ref type="bibr" target="#b8">[8]</ref>. However, this explanation was viewed as incorrect or incomplete and the study of <ref type="bibr" target="#b12">[12]</ref> argued that the underlying reason that BatchNorm helps training is that it results in a smoother loss landscape. Shen etc <ref type="bibr" target="#b13">[13]</ref> perform a systematic study of NLP transformer models to understand why BatchNorm has a poor performance and find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training which results in instability. Xu etc <ref type="bibr" target="#b18">[18]</ref> find that the derivatives of the mean and variance in Layer-Norm used in Transformer for NLP tasks are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, they find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Network based CTR Models</head><p>Many deep learning based CTR models have been proposed in recent years and it is the key factor for most of these neural network based models to effectively model the feature interactions or feature importance.</p><p>Factorization-Machine Supported Neural Networks (FNN) <ref type="bibr" target="#b20">[20]</ref> is a feed-forward neural network using FM to pre-train the embedding layer. Wide &amp; Deep Learning <ref type="bibr" target="#b2">[3]</ref> jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. However, expertise feature engineering is still needed on the input to the wide part of Wide &amp; Deep model. To alleviate manual efforts in feature engineering, DeepFM <ref type="bibr" target="#b4">[4]</ref> replaces the wide part of Wide &amp; Deep model with FM and shares the feature embedding between the FM and deep component. FiBiNet <ref type="bibr" target="#b7">[7]</ref> and FAT-DeepFFM <ref type="bibr" target="#b19">[19]</ref> dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism based on different backbone networks.</p><p>While most DNN ranking models process high-order feature interactions in implicit way, some works explicitly introduce highorder feature interactions by sub-network. Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b15">[15]</ref> efficiently captures feature interactions of bounded degrees in an explicit fashion. Similarly, eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b9">[9]</ref> also models the low-order and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part. AutoInt <ref type="bibr" target="#b14">[14]</ref> proposes a multi-head self-attentive neural network with residual connections to explicitly model the feature interactions in the low-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR WORK</head><p>In this section, we first describe the proposed variance-only Layer-Norm. We conduct extensive experiments to verify the effectiveness of normalization in section 4 and the details about how to apply the normalization on feature embedding and MLP will be introduced in this section. Finally the reason why normalization works is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variance-Only LayerNorm</head><p>First, we briefly review the formulation of LayerNorm. Let x = (x 1 , x 2 , ..., x H ) denotes the vector representation of an input of size H to normalization layers. LayerNorm re-centers and re-scales input x as</p><formula xml:id="formula_0">h = g ⊙ N (x) + b, N (x) = x − µ δ , µ = 1 H H i=1 x i , δ = 1 H H i=1 (x i − µ) 2 (1)</formula><p>where h is the output of a LayerNorm layer. ⊙ is a dot production operation. µ and δ are the mean and standard deviation of input. Bias b and gain g are parameters with the same dimension H . As we all know, LayerNorm has been widely used and proved to be effective in many NLP tasks. However, Xu etc. <ref type="bibr" target="#b18">[18]</ref> point out that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Their experiments in four NLP datasets show that a simple version of Lay-erNorm without the bias and gain outperforms LayerNorm. Though their conclusion is draw mainly from the NLP tasks and they primarily consider normalization on Transformer and Transformer-XL networks. We wonder whether the same conclusion can be draw in DNN ranking models and design a similar simper version Layer-Norm by removing bias and gain from LN as follows:</p><formula xml:id="formula_1">h = N (x), N (x) = x − µ δ , µ = 1 H H i=1 x i , δ = 1 H H i=1 (x i − µ) 2<label>(2)</label></formula><p>We call this version LayerNorm simple-LayerNorm (S-LN) just as the original paper <ref type="bibr" target="#b18">[18]</ref> named. Our experimental results show that simple-LayerNorm has comparable performance with LayerNorm, which implies the bias and gain in LayerNorm bring neither good nor bad effect to DNN models in CTR estimation field. Our conclusion is slightly different from that in NLP field because their experimental results <ref type="bibr" target="#b18">[18]</ref> show the advantages for simple-LayerNorm over the standard LayerNorm in several NLP tasks. We deem that may come from the difference of network structure and research field.</p><p>According to our empirical observations, we find re-centering the input x in simple-LayerNorm has little effect on the performance of DNN ranking model. So we propose variance-only LayerNorm(VO-LN) by further removing the mean from simple-LayerNorm as follows:</p><formula xml:id="formula_2">h = x δ , µ = 1 H H i=1 x i , δ = 1 H H i=1 (x i − µ) 2<label>(3)</label></formula><p>Though the variance-only LayerNorm seems rather simple, our experimental results demonstrate it has comparable or even better performance in several CTR datasets than standard LayerNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NormDNN</head><p>Most DNN ranking models use the feature embedding to represent information and shallow MLP layers to model high-order interactions in an implicit way. These two commonly used components play important roles in current state-of-the-art ranking systems. So we have three options to apply normalization: normalization only on feature embedding, normalization only on MLP part, normalization both on feature embedding and MLP part.</p><p>We also find different parts of DNN model need different normalization method and propose the following unified normalization combination strategy: variance-only LayerNorm or LayerNorm for numerical feature, BatchNorm for categorical feature and varianceonly LayerNorm for MLP. We call this normalization enhanced DNN model with this unified normalization strategy "NormDNN" in this paper. NormDNN achieves significantly better performance than complex model such as xDeepFM. We will discuss this in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Normalization on Feature Embedding. The input data of CTR tasks usually consists of sparse and dense features and the sparse features are mostly categorical type. Such features are encoded as one-hot vectors which often lead to excessively highdimensional feature spaces for large vocabularies. The common solution to this problem is to introduce the embedding layer.</p><p>An embedding layer is applied upon the raw feature input to compress it to a low dimensional, dense real-value vector. The result of embedding layer is a wide concatenated vector:</p><formula xml:id="formula_3">V emb = concat(e 1 , e 2 , ..., e i , ..., e f )<label>(4)</label></formula><p>where f denotes the number of fields, and e i ∈ R k denotes the embedding of one field. Although the feature lengths of instances can be various, their embedding are of the same length f ×k, where k is the dimension of field embedding. As we all know, features in CTR tasks usually can be segregated into categorical features and numerical features. There are two widely used approaches to convert the numerical feature into embedding. The first one is to quantize each numerical feature into discrete buckets, and the feature is then represented by the bucket ID. We can map bucket ID to an embedding vector. The second method maps the feature field into an embedding vector as follows:</p><formula xml:id="formula_4">v i = e i x i<label>(5)</label></formula><p>where e i is an embedding vector for field i, and x i is a scalar value.</p><p>In our experiments, we adopt the second approach to convert numerical features into embedding. We apply normalization on feature embedding based on the feature field as follows:</p><formula xml:id="formula_5">N (V emb ) = concat(N (e 1 ), N (e 2 ), ..., N (e i ), ..., N (e f ))<label>(6)</label></formula><p>where N can be BatchNorm, LayerNorm, GroupNorm, Simple-LayerNorm or variance-only LayerNorm. The bias and gain are shared for features in same feature field if the normalization contains these parameters.</p><p>For the LayerNorm based normalization approaches (LayerNorm, Simple-LayerNorm and variance-only LayerNorm), we regard each feature's embedding as a layer to compute the mean and variance of normalization. As for the GroupNorm, the feature embedding is divided into several groups to compute mean and variance. Batch-Norm computes the statistics within a mini-batch.</p><p>In real life applications, the CTR tasks usually contain both categorical features and numerical features. We find the different 3.2.2 Normalization on MLP Part. As for the feed-forward layer in DNN model, the normalization on MLP is just as usual method does. That is to say, BatchNorm's mean and variance are computed within a mini-batch and LayerNorm based normalizations's statistics are estimated in a layer-wise manner. As for the GroupNorm, we can divide the neural units contained in MLP into several groups and the statistics are estimated in group-wise manner.</p><p>Notice that we have two places to put normalization operation on the MLP: one place is before non-linear operation and another place is after non-linear operation. For clarity of the description, we use LayerNorm as an example. If we put normalization after non-linear operation, we have:</p><formula xml:id="formula_6">LN (V hidden ) = LN (ReLU (W i x))<label>(7)</label></formula><p>where x ∈ R t refers to the input of feed-forward layer, W i ∈ R m×t are parameters for the layer, t and m respectively denotes the size of input layer and neural number of feed-forward layer. If we put normalization before non-linear operation, we have:</p><formula xml:id="formula_7">LN (V hidden ) = ReLU (LN (W i x))<label>(8)</label></formula><p>We find the performance of the normalization before non-linear consistently outperforms that of the normalization after non-linear operation. So all the normalization used in MLP part is put before non-linear operation in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Understanding Why Normalization Works</head><p>In this section, we discuss the reason why normalization works in DNN model. The related experimental results are presented in Section 4.2.</p><p>As mentioned in Section 3.1, we find three LayerNorm based models( LayerNorm, Simple-LayerNorm and variance-only Layer-Norm) have comparable performance on three real-life datasets and all the three normalization approaches work. From these observations, we can draw the following conclusion: Because the simple LayerNorm just removes the bias and gain from LayerNorm, the similar model performance implies the bias and gain have no effect on the final model performance. Further more, the variance-only LayerNorm removes the mean from simple LayerNorm and it has comparable performance with LayerNorm and simple LayerNorm. That implies the mean in simple LayerNorm doesn't contribute to the final better performance. So we can draw the conclusion that it's To understand how the variance influences the model performance, we analyze the change of statistics both in embedding layer and MLP after applying variance-only LayerNorm on Cretio dataset ( <ref type="figure" target="#fig_0">Figure. 1</ref>). From <ref type="figure" target="#fig_0">Figure 1</ref>,we can see that the average mean and variance of feature embedding and MLP are very small positive number if we don't apply normalization on any part of DNN model. If we use variance-only LayerNorm only on feature embedding, the variance of feature embedding greatly increases and that change pushes bit value of many feature embedding to a much larger negative number <ref type="figure" target="#fig_0">(Figure 1 and Figure 2</ref>). Through the network connections, these statistics changes of feature embedding are transferred to the MLP layer and the corresponding statistics of MLP show the similar trend even though we didn't apply any normalization on it <ref type="figure" target="#fig_0">(Figure 1</ref>). If we just apply variance-only LayerNorm on MLP of DNN model, we see the similar changes that the average variance of MLP neurons greatly increases and that also pushes output of many neurons to negative number <ref type="figure" target="#fig_2">(Figure 3</ref>). If we utilize variance-only LayerNorm both on feature embedding and MLP, we observe the similar trend.</p><p>As for the influence of variance on MLP, we can see that large fraction of neuron outputs is pushed into negative number from small positive number after applying variance-only LayerNorm <ref type="figure" target="#fig_2">(Figure 3</ref>). That means these neuron responses were removed because the following non-linear function is ReLU. We deem this avoid many noises in MLP responses and accelerate the training of the model because of the introduction of the variance.</p><p>As for the influence of variance on feature embedding, we can analyze the effect of normalization from another viewpoint. As we all know, the features in CTR tasks are very sparse and there are a large amount of low frequency features. This will lead to the underfitting of these long tail feature's embedding because there is less training data for them. The under-fitting embedding may contain noise which brings difficulty for feed-forward layer to capture We can derive the derivative of L with respect to input x i after inserting variance-only LayerNorm as follows. Assume the derivative of L with respect to h is given, ie. ∂ L ∂h is known. Then the derivative with respect to input x i can be write as:</p><formula xml:id="formula_8">∂L ∂x i = ∂L ∂h * ∂h ∂x i (9) ∂h ∂x i = 1 δ − x i (x i − µ) δ 3 · H<label>(10)</label></formula><p>where µ and δ are the mean and standard deviation of input. H is the size of input.</p><p>We can see that the second sub-item of formula (10) is nearly zero and can be ignored during the beginning phrase of the model training because parameter initiation approach usually sets the initial value of parameters to be very little random number near zero. So it's the first sub-item that mainly influences the derivative of h respect to input x i . From figure 1, we know that the variance before normalization is rather small positive number. That is to say, the derivative will be made much larger because of the introduction of variance-only LayerNorm. This means the loss will become much more sensitive to the little change of input x i because of the existence of variance-only LayerNorm. The variance brings faster convergence for low frequency feature embedding and alleviates the under-fitting of these feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we empirically evaluate the effect of various normalization approaches on deep neural networks on three real-world datasets and conduct detailed studies to answer the following research questions:</p><p>•   <ref type="table" target="#tab_1">Table 1</ref> lists the statistics of the evaluation datasets. For these datasets, a small improvement in prediction accuracy is regarded as practically significant because it will bring a large increase in a company's revenue if the company has a very large user base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation</head><p>Metric. AUC (Area Under ROC) is used in our experiments as the evaluation metrics. This metric is very popular for binary classification tasks. AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. It is insensitive to the classification threshold and the positive ratio. AUC's upper bound is 1 and larger value indicates a better performance. We mainly use DNN model as the baseline to evaluation the effect of various normalization methods because it's a commonly used component in many current neural network models. DeepFM and xDeepFM are also regarded as another two baselines to further verify the effectiveness of these approaches. Among these baseline models, DNN and DeepFM implicitly capture high order interactions while xDeepFM models high order interactions in explicit way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement all the models with Tensorflow in our experiments. For optimization method, we use the Adam with a mini-batch size of 1000 and a learning rate is set to 0.0001. Focusing on normalization approaches in our paper, we make the dimension of field embedding for all models to be a fixed value of 10. For models with DNN part, the depth of hidden layers is set to 3, the number of neurons per layer is 400, all activation function are ReLU. We conduct our experiments with 2 Tesla K40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Normalization on Feature Embedding (RQ1)</head><p>To verify the effectiveness of various normalization approaches on DNN models, comparison experiments are conduced on three evaluation datasets. We add different kinds of normalization either on the embedding layer or hidden layer of a standard DNN model which has 3 MLP layers with 400 neurons per layer. As for GourpNorm, we find GN with 2 groups perform best compared with other group number setting. So we only report the experimental results with this setting (GN2). We find normalization on feature embedding helps boosting DNN model's performance. The <ref type="table" target="#tab_2">Table 2</ref> shows the experimental results. From <ref type="table" target="#tab_2">Table 2</ref>, we have the following observations:</p><p>(   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Normalization on MLP Part (RQ2)</head><p>We also conduct experiments to apply the different kinds of normalization only on MLP part of DNN model. The overall performances of DNN model with different normalizations on three evaluation datasets are show in the <ref type="table" target="#tab_4">Table 3</ref>. From the experimental results, we can see that:</p><p>(  <ref type="table" target="#tab_5">Table 4</ref>. From the results in <ref type="table" target="#tab_5">Table 4</ref>, we can see that:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Normalization for Numerical and Categorical Feature (RQ4)</head><p>From the experimental results shown in <ref type="table" target="#tab_1">Table 1</ref>, we observe that the performance degrades if we adopt BatchNorm instead of Layer-Norm based approaches on feature embedding on Criteo dataset. Considering only the Criteo dataset contains both categorical and numerical feature, we assume that the performance difference is related to the numerical or categorical feature. So we design some normalization combination experiments to testify this assumption. As discussed in Section 4.4, we fix the normalization used in MLP to be variance-only LayerNorm and apply different normalization for numerical and categorical feature. The experimental results can be seen in <ref type="table" target="#tab_9">Table 5</ref>. From the results in <ref type="table" target="#tab_9">Table 5</ref>, we can see that:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance of NormDNN (RQ5)</head><p>If we adopt the following unified normalization combination strategy in DNN ranking model: variance-only LayerNorm or Layer-Norm for numerical feature, BatchNorm for categorical feature and   variance-only LayerNorm for MLP, we can gain the best performance model on all three datasets, which achieves significantly better performance than complex model such as xDeepFM. We call this normalization enhanced model with this unified normalization strategy "NormDNN" in this paper. The experimental results in <ref type="table" target="#tab_10">Table 6</ref> prove this observation. It is easy to find that NormDNN is more applicable in many industry applications because of its better performance and high computation efficiency compared with many state-of-the-art complex neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Normalization on DeepFM and xDeepFM Models (RQ6)</head><p>In the following part of the paper, we study the impacts of normalization on two other popular deep neural network models, including DeepFM and xDeepFM. We design some normalization experiments to observe whether it also works for these two models. Notice that the input of FM component in DeepFM is the feature embedding before normalization. The performance of DeepFM degrades if FM component uses the same feature embedding after normalization as DNN component does. The results in <ref type="table" target="#tab_11">Table 7</ref> show the impact of the various normalizations on model performance. It can be observed that:</p><p>(1) The performances of both models apparently increase when we add normalization into different parts of model. The experimental results tell us the normalization works for many current state-of-the-art models. (2) If we select correct normalization combination for simple model such as DNN or DeepFM, the performances of the model with normalization outperform complex model without normalization such as xDeepFM. That means it's more practical to adding normalization on simple models in reallife applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we firstly apply various normalization approaches to the feature embedding part and the MLP part of DNN model. Extensive experiments are conduct on three real-world datasets and the experiment results demonstrate that the correct normalization significantly enhances model's performance. We also simplify the LayerNorm and propose two new and effective normalization methods in this work. Further more, we find the variance of normalization mainly contributes to this positive effect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Average Statistics of Different Modelskinds of feature need corresponding normalization method and we will discuss this in detail in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Statistics Changes after VO-LN on Feature Embedding variance in variance-only LayerNorm that helps boosting model's performance. The reason why LayerNorm and simple LayerNorm also work lies in that they contain variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Statistics Changes after VO-LN on MLP Partcomplex feature interactions. We think the correct normalization on feature embedding can alleviate this situation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>RQ1 What's the effect of various normalization approaches applied only on feature embedding part of DNN model? • RQ2 What's the effect of various normalizations approaches applied only on MLP part of DNN model? • RQ3 What's the effect of various normalization approaches applied both on feature embedding part and the MLP part of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We apply various normalization approaches to the feature embedding part and the MLP part of DNN model, including commonly used normalization and our proposed approach. Extensive experiments are conduct on three realworld datasets and the experiment results demonstrate that</figDesc><table><row><cell>the correct normalization or normalization combination sig-</cell></row><row><cell>nificantly enhances model's performance. As far as we know,</cell></row><row><cell>this is the first work to apply normalization to feature em-</cell></row><row><cell>bedding and prove its effectiveness.</cell></row><row><cell>(3) We propose NormDNN model in this paper which is a nor-</cell></row><row><cell>malization enhanced DNN adopting the following normal-</cell></row><row><cell>ization strategy: variance-only LayerNorm or LayerNorm</cell></row><row><cell>for numerical feature, BatchNorm for categorical feature</cell></row><row><cell>and variance-only LayerNorm for MLP. NormDNN achieves</cell></row><row><cell>significantly better performance than complex model such</cell></row><row><cell>as xDeepFM.</cell></row></table><note>1) In this work, we propose a new normalization approach based on LayerNorm: variance-only LayerNorm(VO-LN). The experimental results show that the proposed normaliza- tion method has comparable performance with layer normal- ization and significantly enhance DNN model's performance.arXiv:2006.12753v2 [cs.LG] 7 Jul 2020(2)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets</figDesc><table><row><cell cols="2">Datasets #Instances</cell><cell>#fields</cell><cell>#features</cell></row><row><cell>Criteo</cell><cell>45M</cell><cell>39(26 Cat;13 Num)</cell><cell>30M</cell></row><row><cell>Malware</cell><cell>8.92M</cell><cell>82(all Cat)</cell><cell>9.89M</cell></row><row><cell>Avazu</cell><cell>40.43M</cell><cell>24(all Cat)</cell><cell>0.64M</cell></row><row><cell cols="4">DNN model? Does our proposed variance-only LayerNorm</cell></row><row><cell>work?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">• RQ4 Does categorical feature or numerical feature need</cell></row><row><cell cols="3">specific normalization?</cell><cell></cell></row><row><cell cols="4">• RQ5 Is there a best normalization combination for DNN</cell></row><row><cell>model?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">• RQ6 Can we draw the similar conclusion about the effect</cell></row><row><cell cols="4">of normalization in other state-of-the-art models such as</cell></row><row><cell cols="2">DeepFM or xDeepFM?</cell><cell></cell><cell></cell></row><row><cell cols="4">In the following, we will first describe the experimental settings,</cell></row><row><cell cols="4">followed by answering the above research questions.</cell></row><row><cell cols="3">4.1 Experiment Setup</cell><cell></cell></row><row><cell cols="4">4.1.1 Datasets. The following three data sets are used in our ex-</cell></row><row><cell>periments:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(1) Criteo 1 Dataset: As a very famous public real world display</cell></row><row><cell cols="4">ad dataset with each ad display information and correspond-</cell></row><row><cell cols="4">ing user click feedback, Criteo data set is widely used in</cell></row><row><cell cols="4">many CTR model evaluation. There are 26 anonymous cate-</cell></row><row><cell cols="4">gorical fields and 13 continuous feature fields in Criteo data</cell></row><row><cell>set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(2) Malware 2 Dataset: Malware is a dataset from Kaggle com-</cell></row><row><cell cols="4">petitions published in the Microsoft Malware Classification</cell></row><row><cell cols="4">Challenge. It is almost half a terabyte when uncompressed</cell></row><row><cell cols="4">and consists of disassembly and bytecode malware files rep-</cell></row><row><cell cols="4">resenting a mix of 9 different families. All the 82 feature</cell></row><row><cell cols="2">fields are categorical.</cell><cell></cell><cell></cell></row><row><cell cols="4">(3) Avazu 3 Dataset: The Avazu dataset consists of several days</cell></row><row><cell cols="4">of ad click-through data which is ordered chronologically.</cell></row><row><cell cols="4">For each click data, there are 24 fields which indicate ele-</cell></row><row><cell cols="3">ments of a single ad impression.</cell><cell></cell></row><row><cell cols="4">We randomly split instances by 8 : 1 : 1 for training , validation</cell></row><row><cell>and test while</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall performance (AUC) of DNN models with various normalization approaches on feature embedding on three datasets</figDesc><table><row><cell></cell><cell cols="2">Criteo Avazu Malware</cell></row><row><cell>DNN</cell><cell>0.8054 0.7820</cell><cell>0.7263</cell></row><row><cell>+BN</cell><cell>0.8066 0.7847</cell><cell>0.7364</cell></row><row><cell>+GN2</cell><cell>0.8096 0.7835</cell><cell>0.7330</cell></row><row><cell>+LN</cell><cell>0.8093 0.7848</cell><cell>0.7341</cell></row><row><cell>+S-LN</cell><cell>0.8093 0.7843</cell><cell>0.7343</cell></row><row><cell cols="2">+VO-LN 0.8094 0.7839</cell><cell>0.7358</cell></row><row><cell cols="2">4.1.3 Models for Comparisons.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Overall performance (AUC) of DNN models with various normalization approaches on MLP part on three datasets</figDesc><table><row><cell></cell><cell cols="2">Criteo Avazu Malware</cell></row><row><cell>DNN</cell><cell>0.8054 0.7820</cell><cell>0.7263</cell></row><row><cell>+BN</cell><cell>0.8071 0.7836</cell><cell>0.7388</cell></row><row><cell>+GN2</cell><cell>0.8073 0.7836</cell><cell>0.7388</cell></row><row><cell>+LN</cell><cell>0.8071 0.7851</cell><cell>0.7378</cell></row><row><cell>+S-LN</cell><cell>0.8070 0.7847</cell><cell>0.7376</cell></row><row><cell cols="2">+VO-LN 0.8075 0.7849</cell><cell>0.7373</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Overall performance (AUC) of DNN models with</cell></row><row><cell cols="4">various normalization approaches on both embedding part</cell></row><row><cell cols="3">and MLP part on three datasets</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Criteo Avazu Malware</cell></row><row><cell>EMB</cell><cell>MLP</cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>w/o</cell><cell>0.8054 0.7820</cell><cell>0.7263</cell></row><row><cell>+BN</cell><cell>+BN</cell><cell>0.8068 0.7845</cell><cell>0.7393</cell></row><row><cell>+BN</cell><cell>+LN</cell><cell>0.8075 0.7863</cell><cell>0.7393</cell></row><row><cell>+BN</cell><cell cols="3">+VO-LN 0.8077 0.7869 0.7402</cell></row><row><cell>+LN</cell><cell>+BN</cell><cell>0.8094 0.7838</cell><cell>0.7387</cell></row><row><cell>+LN</cell><cell>+LN</cell><cell>0.8096 0.7852</cell><cell>0.7372</cell></row><row><cell>+LN</cell><cell cols="2">+VO-LN 0.8098 0.7857</cell><cell>0.7372</cell></row><row><cell>+VO-LN</cell><cell>+BN</cell><cell>0.8092 0.7823</cell><cell>0.7394</cell></row><row><cell>+VO-LN</cell><cell>+LN</cell><cell>0.8092 0.7841</cell><cell>0.7376</cell></row><row><cell cols="3">+VO-LN +VO-LN 0.8097 0.7850</cell><cell>0.7383</cell></row><row><cell cols="4">(3) As for LayerNorm based normalizations, LayerNorm,Simple-</cell></row><row><cell cols="4">LN and variance-only LN show comparable performance on</cell></row><row><cell cols="2">all three datasets.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>As discussed in Section 4.2, we can apply the normalization both on the feature embedding part and MLP part of DNN model. Extensive experiments have been conducted and we find the following three normalizations perform better when we combine various normalizations in different part of DNN model: BatchNorm, LayerNorm and Variance-only LN. So we present experimental results of 9 combinations in</figDesc><table><row><cell>4.4 Normalization Combination on Both</cell></row><row><cell>Feature Embedding and MLP (RQ3)</cell></row><row><cell>1) Various normalization approaches show comparable perfor-</cell></row><row><cell>mance on both Criteo and Malware datasets. BatchNorm</cell></row><row><cell>and GroupNorm slightly underperform LayerNorm based</cell></row><row><cell>approaches on Avazu dataset.</cell></row><row><cell>(2) Compared with normalization only on feature embedding,</cell></row><row><cell>normalization only on MLP part performs better on Malware</cell></row><row><cell>dataset and worse on Criteo dataset on the whole. That may</cell></row><row><cell>imply that selection between the normalization on embed-</cell></row><row><cell>ding and MLP part depends on specific task.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Choosing variance-only LayerNorm in MLP part, we usually have relatively higher performance models, no matter which normalization is used in feature embedding part. It tells us that we'd better use VO-LN as normalization in MLP part when we try to combine the normalizations.</figDesc><table><row><cell>1) If we choose the correct normalization combination, the</cell></row><row><cell>DNN model performs better than any model which only</cell></row><row><cell>uses normalization in one part of DNN model, either feature</cell></row><row><cell>embedding or MLP part. That means they are complementary</cell></row><row><cell>and it's better to use them both under real-life applications.</cell></row><row><cell>(2) Compared with a standard DNN model, the performances of</cell></row><row><cell>DNN model with normalizations outperform baseline with</cell></row><row><cell>a large margin when correct normalization combination are</cell></row><row><cell>selected.</cell></row><row><cell>(3) If we adopt BatchNorm in normalization combination, the</cell></row><row><cell>conclusion that its performance depends on dataset still</cell></row><row><cell>holds.</cell></row><row><cell>(4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Overall performance (AUC) of DNN models with various normalization approaches for numerical and categorical features on Criteo dataset</figDesc><table><row><cell cols="2">EMB</cell><cell>MLP</cell><cell></cell></row><row><cell>Num</cell><cell>Cat</cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>w/o</cell><cell>w/o</cell><cell>0.8054</cell></row><row><cell>+BN</cell><cell>+BN</cell><cell cols="2">+VO-LN 0.8077</cell></row><row><cell>+BN</cell><cell>+LN</cell><cell cols="2">+VO-LN 0.8068</cell></row><row><cell>+BN</cell><cell cols="3">+VO-LN +VO-LN 0.8066</cell></row><row><cell>+LN</cell><cell>+LN</cell><cell cols="2">+VO-LN 0.8097</cell></row><row><cell>+LN</cell><cell>+BN</cell><cell cols="2">+VO-LN 0.8105</cell></row><row><cell>+VO-LN</cell><cell>+BN</cell><cell cols="2">+VO-LN 0.8107</cell></row><row><cell cols="4">+VO-LN +VO-LN +VO-LN 0.8098</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Overall performance (AUC) of DNN model with unified normalization combination( NormDNN) on three datasets</figDesc><table><row><cell></cell><cell cols="2">Criteo Avazu Malware</cell></row><row><cell>DNN</cell><cell>0.8054 0.7820</cell><cell>0.7263</cell></row><row><cell>DeepFM</cell><cell>0.8056 0.7833</cell><cell>0.7295</cell></row><row><cell>xDeepFM</cell><cell>0.8063 0.7848</cell><cell>0.7322</cell></row><row><cell cols="3">NormDNN 0.8107 0.7869 0.7402</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Overall performance (AUC) of popular models with various normalization approaches on Criteo dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="3">DNN DeepFM xDeepFM</cell></row><row><cell>EMB</cell><cell>MLP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>w/o</cell><cell>0.8054</cell><cell>0.8056</cell><cell>0.8063</cell></row><row><cell>+LN</cell><cell>w/o</cell><cell>0.8093</cell><cell>0.8100</cell><cell>0.8100</cell></row><row><cell>+VO-LN</cell><cell>w/o</cell><cell>0.8094</cell><cell>0.8099</cell><cell>0.8100</cell></row><row><cell>w/o</cell><cell>+LN</cell><cell>0.8071</cell><cell>0.8073</cell><cell>0.8075</cell></row><row><cell>w/o</cell><cell cols="2">+VO-LN 0.8075</cell><cell>0.8076</cell><cell>0.8073</cell></row><row><cell>+LN</cell><cell>+LN</cell><cell>0.8096</cell><cell>0.8099</cell><cell>0.8100</cell></row><row><cell>+LN</cell><cell cols="2">+VO-LN 0.8098</cell><cell>0.8102</cell><cell>0.8103</cell></row><row><cell cols="3">+VO-LN +VO-LN 0.8097</cell><cell>0.8101</cell><cell>0.8101</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/ 2 Malware https://www.kaggle.com/c/malware-classification 3 Avazu http://www.kaggle.com/c/avazu-ctr-prediction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: A View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2488200</idno>
		<ptr target="https://doi.org/10.1145/2487575.2488200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD âĂŹ13)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD âĂŹ13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07845</idno>
		<title level="m">Rethinking Batch Normalization in Transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding and Improving Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4383" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06336</idno>
		<title level="m">FAT-DeepFFM: Field Attentive Deep Field-aware Factorization Machine</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
