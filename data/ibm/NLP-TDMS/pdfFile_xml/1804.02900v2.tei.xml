<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fully Progressive Approach to Single-Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Disney Research Input Input</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research Input Input</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research Input Input</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research Input Input</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Disney Research Input Input</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Fully Progressive Approach to Single-Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours w/o GAN</head><p>Ours w/o GAN Ours w/ GAN Ours w/ GAN Input Input Input Input Ours w/o GAN Ours w/o GAN Ours w/ GAN Ours w/ GAN 4× 8× Figure 1: Examples of our 4× and 8× upsampling results. Our model without GAN sets a new state-of-the-art benchmark in terms of PSNR/SSIM; our GAN-extended model yields high perceptual quality and is able to hallucinate plausible details up to 8× upsampling ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recent deep learning approaches to single image superresolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8×) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge <ref type="bibr" target="#b33">[34]</ref>. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The widespread availability of high resolution displays and rapid advancements in deep learning based image processing has recently sparked increased interest in superresolution. In particular, approaches to single image su-per resolution (SISR) have achieved impressive results by learning the mapping from low-resolution (LR) to highresolution (HR) images based on data. Typically, the upscaling function is a deep neural network (DNN) that is trained in a fully supervised manner with tuples of LR patches and corresponding HR targets. DNNs are able to learn abstract feature representations in the input image that allow some degree of disambiguation of the fine details in the HR output.</p><p>Most existing SISR networks adopt one of the two following direct approaches. The first upsamples the LR image with a simple interpolation method (e.g., bicubic) in the beginning and then essentially learns how to deblur <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. The second proposes upsampling only at the end of the processing pipeline, typically using a sub-pixel convolution layer <ref type="bibr" target="#b29">[30]</ref> or transposed convolution layer to recover the HR result <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>. While the first class of approaches has a large memory footprint and a high computational cost, as it operates on upsampled images, the second class is more prone to checkerboard artifacts <ref type="bibr" target="#b26">[27]</ref> due to simple concatenation of upsampling layers. Thus it remains challenging to achieve high quality results for large upsampling factors.</p><p>In this paper, we propose a method that is progressive both in architecture and training. We design the network to reconstruct a high resolution image in intermediate steps by progressively performing a 2× upsampling of the input from the previous level. As building blocks for each level of the pyramid, we propose dense compression units, which are adapted from dense blocks <ref type="bibr" target="#b15">[16]</ref> to suit super-resolution. Compared to existing progressive SISR models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, we improve the reconstruction accuracy by simplifying the information propagation within the network; furthermore we propose to use an asymmetric pyramidal structure with more layers in the lower levels to enable high upsampling ratios while remaining efficient. To obtain more photorealistic results, we adopt the GAN framework <ref type="bibr" target="#b13">[14]</ref> and design a discriminator that matches the progressive nature of our generator network by operating on the residual outputs of each scale. Such paired progressive design allows us to obtain a multi-scale generator with a unified discriminator in a single training.</p><p>In this framework, we can naturally utilize a form of curriculum learning, which is known to improve training <ref type="bibr" target="#b3">[4]</ref> by organizing the learning process from easy (small upsampling factors) to hard (large upsampling factors). Compared to common multi-scale training, the proposed training strategy not only improves results for all upsampling factors, but also significantly shortens the total training time and stabilizes the GAN training.</p><p>We evaluate our progressive multi-scale approach against the state-of-art on a variety of datasets, where we demonstrate improved performance in terms of traditional error measures (e.g., PSNR) as well as perceptual quality, particularly for larger upsampling ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single image super-resolution techniques (SISR) have been an active area of investigation for more than a decade <ref type="bibr" target="#b11">[12]</ref>. The ill-posed nature of this problem has typically been tackled using statistical techniques: most notably image priors such as heavy-tailed gradient distributions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>, gradient profiles <ref type="bibr" target="#b31">[32]</ref>, multi-scale recurrence <ref type="bibr" target="#b12">[13]</ref>, self-examples <ref type="bibr" target="#b10">[11]</ref>, and total variation <ref type="bibr" target="#b25">[26]</ref>. In contrast, exemplar-based approaches such as nearest-neighbor <ref type="bibr" target="#b11">[12]</ref> and sparse dictionary learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> have exploited the inherent redundancy of large-scale image datasets. Recently, Dong et al. <ref type="bibr" target="#b5">[6]</ref> showed the superiority of a simple three-layer convolutional network (CNN) over sparse coding techniques. Since then, deep convolutional architectures have consistently pushed the state-of-art forward.</p><p>Direct vs. Progressive Reconstruction. Direct reconstruction techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> upscale the image to the desired spatial resolution in a single step. Early approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref> upscale the LR image in a preprocessing step. Thus, the CNN learns to deblur the input image. However, this requires the network to learn a feature representation for a high-resolution image which is computationally expensive <ref type="bibr" target="#b29">[30]</ref>. To overcome this limitation, many approaches opt for operating on the low dimensional features and perform upsampling at the end of the network via sub-pixel convolution <ref type="bibr" target="#b29">[30]</ref> or transposed convolution.</p><p>A popular progressive reconstruction approach is described by LapSRN by Lai et al. <ref type="bibr" target="#b20">[21]</ref>. In their work, the upsampling follows the principle of Laplacian pyramids, i.e. each level learns to predict a residual that should explain the difference between a simple upscale of the previous level and the desired result. Since the loss functions are computed at each scale, this provides a form of intermediate supervision. Lai et al. improved their method with deep and wider recursive architecture and multi-scale training <ref type="bibr" target="#b21">[22]</ref>. While <ref type="bibr" target="#b21">[22]</ref> improved the accuracy, there remains a considerable gap between the top-performing approach in terms of PSNR <ref type="bibr" target="#b23">[24]</ref>. In particular, as we show in Section 4.2, the Laplacian pyramidal structure aggravates the optimization difficulty. Furthermore, the recursive pyramids result in quadratic growth of computation in the higher pyramid level, becoming the bottleneck for reducing runtime and expanding the network capability. Lastly, in addition to a progressive generator, we also propose a progressive discriminator along with a progressive training strategy.</p><p>Perceptual Loss Functions. The aforementioned techniques optimize the reconstruction error by minimizing the ℓ 1 -norm and descendants such as the Charbonnier penalty function <ref type="bibr" target="#b20">[21]</ref>. Although these approaches yield small reconstruction errors, they are unable to hallucinate perceptually plausible high-frequencies details. To this end, Ledig et al. <ref type="bibr" target="#b22">[23]</ref> proposed a perceptual loss function consisting of a content loss that captures perceptual similarities and an adversary to steer the reconstruction closer to the latent manifold of natural solutions. Based on this, Sajjadi et al. <ref type="bibr" target="#b27">[28]</ref> apply an additional texture loss to encourage similarity with the original image. In contrast to these works, we design a discriminator that operates on the residual outputs of each scale and train progressively with a strategy based on curriculum learning. With this, our GAN model is able to upsample perceptually pleasing SR images for multiple scales up to 8×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Progressive Multi-scale Super-resolution</head><p>Given a set of n LR input images with corresponding HR target images {(x 1 , y 1 ), . . . , (x n , y n )}, we consider the problem of estimating an upscaling function u : X → Y , where X and Y denote the space of LR and HR images, respectively. Finding a suitable parameterisation for the upscaling function u for large upsampling ratios, is challenging: the larger the ratio, the more complex the function class required.</p><p>To this end, we propose a progressive solution to learn the upscaling function u. In the following, we propose our pyramidal architecture, ProSR, for multi-scale superresolution in Section 3.1 and 3.2. In Section 3.3 we propose ProGanSR, a progressive multi-scale GAN for perceptual enhancement. Finally, we discuss a curriculum learning scheme in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pyramidal Decomposition</head><p>We propose a pyramidal decomposition of u into a series of simpler functions u 0 , . . . , u s . Each function-or levelis tasked with refining the feature representation and performing a 2× upsampling of its own input. Each level of the pyramid consists of a cascade of dense compression units (DCUs) followed by a sub-pixel convolution layer. We assign more DCUs in the lower pyramid levels, resulting in the asymmetric structure. Having more computation power in the lower pyramid not only reduces the memory consumption but also increases the receptive field with respect to the original image, hence it outperforms the symmetric variant in terms of reconstruction quality and runtime. While the decomposition of u is shared among the pyramid levels, we also use two scale-specific sub-networks, denoted by v s and r s , which allow for an individual transformation between scale-varying image space and a normalized feature space. A schematic illustration of our progressive upsampling architecture is detailed in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>To simplify learning, the network is designed to output the residual</p><formula xml:id="formula_0">R s (x) = (r s • u s • · · · • u 0 • v s )(x)<label>(1)</label></formula><p>w.r.t a fixed upsampling of the input ϕ s (x) through e.g. bicubic interpolation. Thus, for a given scaling factor s the estimated HR image can be computed aŝ</p><formula xml:id="formula_1">y = R s (x) + ϕ s (x).<label>(2)</label></formula><p>Notably, our network doesn't follow the Laplacian pyramid principle like in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, i.e. the intermediate sub-net outputs are neither supervised nor used as base image in the subsequent level. Such design performs favorably over the Laplacian alternative, as it simplifies the backward-pass and thus reduces the optimization difficulty. Additionally we do not downsample the groundtruth to create labels, which is done for the intermediate supervision in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. This avoids artefacts that may result from subsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Compression Units</head><p>We base the construction of each pyramid level on the recently proposed DenseNet architecture <ref type="bibr" target="#b15">[16]</ref>. Similarly to skip connections <ref type="bibr" target="#b14">[15]</ref>, dense connections improve gradient flow alleviating vanishing and shattered gradients <ref type="bibr" target="#b2">[3]</ref>.</p><p>The core component in each level of the pyramid is a dense compression unit (DCU), which consists of a modified densely connected block followed by 1 × 1 convolution CONV <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>.</p><p>The original dense layer is composed of BN-RELU-CONV(1,1)-BN-RELU-CONV <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>. Following recent practice in super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>, we remove all batch normalizations. However, since the features from previous layers may have varying scales, we also remove the first ReLU to rescale the features with CONV(1-1). This leads to a modified dense layer composition: CONV(1,1)-RELU-CONV <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>.</p><p>Contrary to DenseNet, we break the dense connection at the end of each DCU with a CONV(1,1) compression layer, which re-assembles information efficiently and leads to a slight performance gain in spite of the breakage of dense connection. For a very deep model we apply pyramid-wise as well as local residual links to improve the gradient propagation as shown in <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive GAN</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b13">[14]</ref> have emerged as a powerful method to enhance the perceptual quality of the upsampled images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref> in SISR.</p><p>However, training GANs is notoriously difficult and success at applying GANs to SISR has been limited to singlescale upsampling at relatively low target resolutions. In order to enable multi-scale GAN-enhanced SISR, we propose a modular and progressive discriminator network similar to the generator network proposed in the previous section. As <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the architecture has a reverse pyramid structure, where each level gradually reduces the spatial dimension of the input image with AVGPOOLING. To accommodate the multi-scale outputs from the generator, the network is fully convolutional and outputs a small patch of features similar to PatchGAN <ref type="bibr" target="#b17">[18]</ref>. The complete specs of the discriminator can be found in the supplemental material.</p><p>Similar to the generator network, the discriminator operates on the residual between the original and bicubic upsampled image. This allows both generator and discriminator to concentrate only on the important sources of variation which are not already well captured by the standard upsampling operation. Since these regions are challenging to upsample well, they correspond to the largest perceptual errors. This can also be viewed as subtracting a datadependent baseline from the discriminator which helps to reduce variance.</p><p>As the training objective, we use the more stable least square loss instead of the original cross-entropy loss <ref type="bibr" target="#b24">[25]</ref>. Denoting the predicted residual and real residual asr and r, the discriminator loss and generator loss for a training ex-ample of scale s can be expressed as</p><formula xml:id="formula_2">L i Ds =(D (r i s )) 2 + (D (r s i ) − 1) 2 (3) L i Rs =(D (r i s ) − 1) 2 + (4) k∈{2,4} Φ k (ŷ i ) − Φ k (y i ) 2 ,</formula><p>where Φ k denotes the k-th pooling layer input in VGG16 <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Curriculum Learning</head><p>Curriculum learning <ref type="bibr" target="#b3">[4]</ref> is a strategy to improve training by gradually increasing the difficulty of the learning task. It is often used in sequence prediction tasks and in sequential decision making problems where large speedups in training time and improvements in generalisation performance can be obtained.</p><p>The pyramidal decomposition of u allows us to apply curriculum learning in a natural way. The loss for a training example (x s i , y i ) of scale s can be defined as</p><formula xml:id="formula_3">L i Rs = R s (x s i ) + ϕ s (x s i ) − y i 1<label>(5)</label></formula><p>where x s i corresponds to s× downsampled version of y i . Then the goal at scale s is to find</p><formula xml:id="formula_4">θ s = argmin θs s ′ ≤s i L i R s ′ ,<label>(6)</label></formula><p>where θ s parameterises all functions in and below the current scale (u 0 , v 0 , r 0 , . . . , u s , v s , r s ) according to our pyramidal network shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Our training curriculum starts by training only the 2× portion of the network. When we proceed to a new phase in the curriculum (e.g. to 4×), a new level of the pyramid is gradually blended in to reduce the impact on the previously trained layers. As <ref type="figure" target="#fig_1">Figure 3</ref> shows, this is achieved by linearly combining the output of the newly added level with the upsampled output of the previous level. A similar idea was proposed in <ref type="bibr" target="#b18">[19]</ref>. As a result we incrementally add training pairs of the next scale. Finally, to assemble the batches, we randomly select one of the scales s to avoid mixing batch statistics as suggested in <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>Before we compare with popular state-of-the-art approaches, we first discuss the benefits of each of our proposed components using a small 24-layer model. All presented models are trained with the DIV2K <ref type="bibr" target="#b33">[34]</ref> training set, which contains 800 high-resolution images. The training details are listed in the supplemental material. For evaluation, the benchmark datasets Set5 <ref type="bibr" target="#b4">[5]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref>, BSD100 <ref type="bibr" target="#b0">[1]</ref>, Urban100 <ref type="bibr" target="#b16">[17]</ref>, and the DIV2K validation set <ref type="bibr" target="#b33">[34]</ref> are used. As it is commonly done in SISR, all evaluations are conducted on the luminance channel.  <ref type="table">Table 1</ref> summarizes the consistent increase in reconstruction quality stemming from each proposed component. As a baseline, we start from a single dense block with two sub-pixel upsampling layers in the end and a residual connection from the LR input to the final output. In the following, we describe the individual steps in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Compression Units.</head><p>To demonstrate the benefit of DCUs described in Section 3.2, we replace the single-block from the baseline model with multiple DCUs. As <ref type="table">Table 1</ref> shows, the number of network parameters can be drastically reduced without harming the reconstruction accuracy. We can even observe a slight performance gain as the network is able to reassemble features more efficiently due to the injection of compression layers.</p><p>Asymmetric Pyramid. In this section we show the advantage of the proposed asymmetric pyramidal architecture. We compare the following constellations while keeping the total number of DCUs constant:</p><formula xml:id="formula_5">Model Architecture Direct D − D − D − D − S − S Asymmetric Pyramid D − D − D − S − D − S</formula><p>Here, D denotes a dense compression unit with 6 dense layers and S denotes the sub-pixel upsampler. As <ref type="table">Table 1</ref> shows, the asymmetric pyramidal architecture considerably improves the reconstruction accuracy compared to direct upsampling. This demonstrates the advantage of utilizing high-dimensional features directly.</p><p>Curriculum Learning. We extend the 4-DCU asymmetric pyramid model to 8× upsampling to quantify the benefit of curriculum learning over simultaneous multi-scale training. As <ref type="table" target="#tab_2">Table 2</ref> shows, simultaneous training typically has small or even negative impact on the lowest scale (2×), which is also evident in VDSR <ref type="bibr" target="#b19">[20]</ref> (see <ref type="table" target="#tab_2">Table 2</ref>). On the other hand, curriculum learning always improves the reconstruction quality and outperforms simultaneous training by an average of 0.04dB.</p><p>Furthermore, curriculum learning considerably shortens the training time. As <ref type="figure" target="#fig_4">Figure 4</ref> shows, the network reaches the same number of epochs and quality faster than simultaneous training, since the 2× subnet requires less computation and hence less time for each update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with other progressive architectures.</head><p>In contrast to our approach, existing progressive methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> typically rely on deep supervision. They impose a loss on all scales which can be denoted as</p><formula xml:id="formula_6">L i s = s ′ &lt;s ℓ 1 ψ s ′ (y i ),ŷ s ′ i + ℓ 1 (y s i ,ŷ s i ) ,<label>(7)</label></formula><p>with ψ s ′ being a downsampling operation to scale s ′ . Futhermore, following the structure of a Laplacian pyramid, each level is encouraged to learn the difference between a bicubic upscale of the previous level instead of the upsampled LR image. Thus the residual connections are given bŷ</p><formula xml:id="formula_7">y s =r s + ϕ 2 y s−1 ,<label>(8)</label></formula><p>where ϕ 2 denotes an upscaling operator by a factor of 2.</p><p>We also evaluate such alternative progressive architecture but observed large decrease in PSNR as shown in <ref type="table" target="#tab_4">Table 3</ref>. Therefore, we conclude that it is less stable to use varying sub-scale upsampling results as base images compared to fixed interpolated results and that using a downsampling kernel to create the HR label images could introduce undesired artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Approaches</head><p>In this section, we provide an extensive quantitative and qualitative comparison with other state-of-the-art approaches.</p><p>Quantitative Comparison. For a quantitative comparison, we benchmark against VDSR <ref type="bibr" target="#b19">[20]</ref>, DRRN <ref type="bibr" target="#b32">[33]</ref>, Lap-SRN <ref type="bibr" target="#b20">[21]</ref>, MsLapSRN <ref type="bibr" target="#b21">[22]</ref>, EDSR. We obtained models from Lai et al. <ref type="bibr" target="#b21">[22]</ref> for 8× versions of VDSR and DRRN,     that have been retrained with 8× data. To produce 8× EDSR results, we extend their 4× model by adding another sub-pixel convolution layer. For training, we follow their practice which means we initialize the weights of the 8× model from the pretrained 4× model. Due to discrepancy in the model size within existing approaches, we divide them into two classes based on whether they have more or less than 5 million parameters. Accordingly, we provide two models with different sizes, denoted as ProSR s and ProSR ℓ , to compete in both classes. ProSR s has 56 dense layers in total with growth-rate k = 12 and a total of 3.1M parameters. ProSR ℓ has 104 dense layers with growth-rate k = 40 and 15.5M parameters which is roughly a third of the parameters of EDSR. <ref type="table" target="#tab_6">Table 4</ref> summarizes the quantitative comparison with other state-of-the-art approaches in terms of PSNR. An ex-tended list that includes SSIM scores can be found in the supplemental material. As <ref type="table" target="#tab_6">Table 4</ref> shows, ProSR s achieves the lowest error in most datasets. The very deep model, ProSR ℓ , shows consistent advantage in higher upsampling ratios and is comparable with EDSR in 2×. In general, our progressive design allows to raise the margin in PSNR between our results and the state-of-the art as the upsampling ratio increases.</p><p>Qualitative comparison. First, we qualitatively compare our method without GAN to other methods that also minimise the ℓ 1 loss or related norms. <ref type="figure">Figure 7</ref> show results of our method and the most recent state-of-the-art approaches in 4× and 8×.</p><p>Concerning our perceptually-driven model with GAN, we compare with SRGAN <ref type="bibr" target="#b22">[23]</ref> and EnhanceNet <ref type="bibr" target="#b27">[28]</ref>. As <ref type="figure">Figure 5</ref> shows, the hallucinated details align well with fine structures in the ground truth, even though we do not have an explicit texture matching loss as EnhanceNet <ref type="bibr" target="#b27">[28]</ref>. While SRGAN and EnhanceNet can only upscale 4×, our method is able to extend to 8×. Results are shown in <ref type="figure">Figure 6</ref>. We provide an extended qualitative comparison in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Runtime.</head><p>The asymmetric pyramid architecture contributes to faster runtime compared to other approaches that have sim-PSNR <ref type="bibr" target="#b22">[23]</ref> [28] Ours HR PSNR <ref type="bibr" target="#b22">[23]</ref> [28] Ours HR ilar reconstruction accuracy. In our test environment with NVIDIA TITAN XP and cudnn6.0, ProSR ℓ takes on average 0.8s, 2.1s and 4.4s to upsample a 520 × 520 image by 2×, 4× and 8×. In the NTIRE challenge, we reported the runtime including geometric ensemble, which requires 8 forward passes for each transformed version of the input image. Nonetheless, our runtime is still 5 times faster than the top-ranking team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">NTIRE Challenge</head><p>The "New Trends in Image Restoration and Enhancement" (NTIRE) 2018 super-resolution challenge aims at benchmarking SISR methods in challenging scenarios.</p><p>In particular, one of the challenge tracks targets 8× upscaling, where the low resolution images are generated with known downsampling kernels (bicubic). We participated in the challenge with the ProSR ℓ network. In addition to the method described above, we utilised the geometry ensemble used in <ref type="bibr" target="#b23">[24]</ref>, which yielded a 0.07dB PSNR gain in the validation set. Our model ranks 2nd in terms of SSIM and 4th in terms of PSNR. Compared to the top-ranking team, our model is marginally lower by 0.002 and 0.04dB in SSIM and PSNR respectively, but runs 5 times as fast in test time.</p><p>Other tracks in the challenge target 4× upscaling but consider unknown degradation. Given that this task is different to the bicubic 8× setting, the participating teams and the rankings differ. Without specific adaptation for this scenario, we also participated in these tracks for completeness and ranked in the mid-range (7th/9th/7th). We believe further improvement can be achieved with targeted preprocessing and extended training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work we propose a progressive approach to address SISR. Specifically, we design a novel architecture ProSR that incrementally upsamples the input image by a factor of 2, forming a pyramidal shape. In particular, we show the benefit of an asymmetric pyramid design with more computation resource in lower pyramid levels both in terms of reconstruction quality and the memory efficiency. To allow for a very deep architecture and thus more modeling power, we have proposed Dense Compression Units (DCU) as building blocks which leverage dense connectivity. Furthermore we leverage a form of curriculum learning which not only enables high-quality reconstruction in large upsampling ratios but also simultaneously increases the performance for all scales. Our ProSR sets a new state-ofthe-art benchmark in terms of the traditional error measures and in higher upsampling ratios (e.g. 4× and 8×) even outperforms existing methods by a large margin. Finally, we demonstrate that the same progressive principle can be applied to GAN-extended method for optimizing perceptual quality of the upsampled image. To this end we propose ProGanSR, which shows prominent improvement against the previous state-of-the-art both in terms of visual quality and stability; to our knowledge, it is also the first multiscale model that could yield perceptually pleasing images in SISR up to 8×.   <ref type="figure">Figure 7</ref>: Visual comparison with other state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Asymmetric pyramidal architecture. More DCUs are allocated in the lower pyramid level to improve the reconstruction accuracy and to reduce memory consumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Schematic illustration of the blending procedure in curriculum training for the generator (top) and the discriminator (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Compared to simple multi-scale training where training examples from different scales are simultaneously fed to the network, such progressive training strategy greatly shortens the total training time. Furthermore, it yields a further performance gain for all included scales compared to singlescale and simple multi-scale training and alleviates instabilities in GAN training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Training time comparison between curriculum learning and multiscale simultaneous learning. We train the multiscale model and plot the PSNR evaluation of the individual scales. The elapsed epoch is encoded as the line color. Because curriculum learning activates the smaller subnets first, it requires much less time to reach the same evaluation quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Comparison of 4× GAN results (best viewed when zoomed in). Our approach is less prone to artefacts and aligns well with the original image. Hallucinated details in 8× upsample result with adversarial loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>+0.03/+0.05 +0.02/+0.01/+0.03 +0.12/+0.06/+0.08 +0.06/-0.02/+0.05 +0.06/+0.02/+0.05 curriculum +0.05/+0.11/+0.08 +0.08/+0.04/+0.06 +0.07/+0.03/+0.05 +0.21/+0.09/+0.08 +0.13/+0.02/+0.05 +0.13/+0.05/+0.06</figDesc><table><row><cell>Improvement w.r.t single-scale 2×/4×/8× (dB)</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>U100</cell><cell>DIV2K</cell><cell>average</cell></row><row><cell>simultaneous</cell><cell>-0.05/+0.09/-0.01</cell><cell>+0.01/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Gain of simultaneous training and curriculum learning w.r.t. single-scale training on all datasets. The average is computed accounting the number of images in the datasets. Curriculum learning improves the training for all scales while simultaneous training hampers the training of the lowest scale.</figDesc><table><row><cell></cell><cell></cell><cell>2×</cell><cell></cell><cell></cell><cell>4×</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22.6</cell><cell></cell></row><row><cell></cell><cell>32 Curriculum</cell><cell></cell><cell></cell><cell>Curriculum</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>31</cell><cell cols="2">Simultaneous</cell><cell>25.5</cell><cell cols="2">Simultaneous</cell><cell>22.4</cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell>22.2</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>0</cell><cell>500</cell><cell>1,000</cell></row><row><cell></cell><cell cols="3">Time elapsed (min)</cell><cell cols="3">Time elapsed (min)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with other progressive approaches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art approaches. For clarity, we highlight the best approach in blue.</figDesc><table><row><cell>8× LR</cell><cell>DRRN [33] 24.31 dB/0.6627</cell><cell>MsLapSRN [22] 24.29 dB/0.667</cell><cell>EDSR [24] 24.96 dB/0.699</cell><cell>ProSR ℓ (Ours) 25.18 dB/0.708</cell><cell>HR</cell></row><row><cell>8× LR</cell><cell>DRRN [33] 27.55 dB/0.7663</cell><cell>MsLapSRN [22] 27.62 dB/0.769</cell><cell>EDSR [24] 27.93 dB/0.776</cell><cell>ProSR ℓ (Ours) 28.20 dB/0.781</cell><cell>HR</cell></row><row><cell>4× LR</cell><cell>DRRN [33] 21.50 dB/0.5218</cell><cell>MsLapSRN [22] 21.49 dB/0.524</cell><cell>EDSR [24] 21.79 dB/0.553</cell><cell>ProSR ℓ (Ours) 21.86 dB/0.557</cell><cell>HR</cell></row><row><cell>4× LR</cell><cell>DRRN [33] 22.32 dB/0.6926</cell><cell>MsLapSRN [22] 22.25 dB/0.698</cell><cell>EDSR [24] 22.91 dB/0.719</cell><cell>ProSR ℓ (Ours) 22.93 dB/0.715</cell><cell>HR</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1157" to="1164" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution via transform-invariant group-sparse regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3336" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09-27" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Densely connected convolutional networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01992</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image super-resolution by tv-regularization and bregman iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marquina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="382" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<idno>abs/1612.07919</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast image/video upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<idno>153:1-153:7</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image super-resolution with a parameter economic residual-like convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On single image scaleup using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces -7th International Conference</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
