<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
							<email>luanhuanbo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qiangyang@webank.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
							<email>evgeny.kharlamov@de.bosch.com</email>
							<affiliation key="aff3">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of semi-supervised learning on graphs, for which graph neural networks (GNNs) have been extensively explored. However, most existing GNNs inherently suffer from the limitations of over-smoothing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, non-robustness <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b50">51]</ref>, and weak-generalization when labeled nodes are scarce. In this paper, we propose a simple yet effective framework-GRAPH RANDOM NEURAL NETWORKS (GRAND)-to address these issues. In GRAND, we first design a random propagation strategy to perform graph data augmentation. Then we leverage consistency regularization to optimize the prediction consistency of unlabeled nodes across different data augmentations. Extensive experiments on graph benchmark datasets suggest that GRAND significantly outperforms state-ofthe-art GNN baselines on semi-supervised node classification. Finally, we show that GRAND mitigates the issues of over-smoothing and non-robustness, exhibiting better generalization behavior than existing GNNs. The source code of GRAND is Let G = (V, E) denote a graph, where V is a set of |V | = n nodes and E ⊆ V × V is a set of |E| edges between nodes. A ∈ {0, 1} n×n denotes the adjacency matrix of G, with each element A ij = 1 indicating there exists an edge between v i and v j , otherwise A ij = 0.</p><p>Semi-Supervised Learning on Graphs. This work focuses on semi-supervised graph learning, in which each node v i is associated with 1) a feature vector X i ∈ X ∈ R n×d and 2) a label vector Y i ∈ Y ∈ {0, 1} n×C with C representing the number of classes. For semi-supervised classification, m nodes (0 &lt; m n) have observed their labels Y L and the labels Y U of the remaining n − m nodes are missing. The objective is to learn a predictive function f : G, X, Y L → Y U to infer the missing labels Y U for unlabeled nodes. Traditional approaches to this problem are mostly based on graph Laplacian regularizations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2]</ref>. Recently, graph neural networks (GNNs) have emerged as a powerful approach for semi-supervised graph learning, which are reviewed below.</p><p>Graph Neural Networks. GNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24]</ref> generalize neural techniques into graph-structured data. The core operation in GNNs is graph propagation, in which information is propagated from each node to its neighborhoods with some deterministic propagation rules. For example, the graph convolutional network (GCN) [24] adopts the propagation rule H (l+1) = σ(ÂH (l) W (l) ), whereÂ is the symmetric normalized adjacency matrix, σ(.) denotes the ReLU function, and W (l) and H (l) are the weight matrix and the hidden node representation in the l th layer with H (0) = X.</p><p>The GCN propagation rule could be explained via the approximation of the spectral graph convolutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>, neural message passing <ref type="bibr" target="#b14">[15]</ref>, and convolutions on direct neighborhoods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19]</ref>. Recent attempts to advance this architecture include GAT [41], GMNN [36], MixHop [1], and GraphNAS [14], etc. In addition, sampling based techniques have also been developed for fast and scalable GNN training, such as GraphSAGE [19], FastGCN [8], AS-GCN [21], and LADIES [53].</p><p>The sampling based propagation used in those models may also be used as a graph augmentation</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs serve as a common language for modeling structured and relational data <ref type="bibr" target="#b25">[26]</ref>, such as social networks, knowledge graphs, and the World Wide Web. Mining and learning graphs can benefit various real-world problems and applications. The focus of this work is on the problem of semisupervised learning on graphs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>, which aims to predict the categories of unlabeled nodes of a given graph with only a small proportion of labeled nodes. Among its solutions, graph neural networks (GNNs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1]</ref> have recently emerged as powerful approaches. The main idea of GNNs lies in a deterministic feature propagation process to learn expressive node representations.</p><p>However, recent studies show that such propagation procedure brings some inherent issues: First, most GNNs suffer from over-smoothing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="bibr">Li et al.</ref> show that the graph convolution operation is a special form of Laplacian smoothing <ref type="bibr" target="#b26">[27]</ref>, and consequently, stacking many GNN layers tends to make nodes' features indistinguishable. In addition, a very recent work <ref type="bibr" target="#b33">[34]</ref> suggests that the coupled non-linear transformation in the propagation procedure can further aggravate this issue. Second, GNNs are often not robust to graph attacks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b50">51]</ref>, due to the deterministic propagation adopted in most of them. Naturally, the deterministic propagation makes each node highly dependent with its (multi-hop) neighborhoods, leaving the nodes to be easily misguided by potential data noise and susceptible to adversarial perturbations.</p><p>The third issue lies in the general setting of semi-supervised learning, wherein standard training methods (for GNNs) can easily overfit the scarce label information <ref type="bibr" target="#b5">[6]</ref>. Most efforts to addressing this broad issue are focused on how to fully leverage the large amount of unlabeled data. In computer vision, recent attempts, e.g. MixMatch <ref type="bibr" target="#b2">[3]</ref>, UDA <ref type="bibr" target="#b45">[46]</ref>, have been proposed to solve this problem by designing data augmentation methods for consistency regularized training, which have achieved great success in the semi-supervised image classification task. This inspires us to apply this idea into GNNs to facilitate semi-supervised learning on graphs.</p><p>In this work, we address these issues by designing graph data augmentation and consistency regularization strategies for semi-supervised learning. Specifically, we present the GRAPH RANDOM NEURAL NETWORKS (GRAND), a simple yet powerful graph-based semi-supervised learning framework.</p><p>To effectively augment graph data, we propose random propagation in GRAND, wherein each node's features can be randomly dropped either partially (dropout) or entirely, after which the perturbed feature matrix is propagated over the graph. As a result, each node is enabled to be insensitive to specific neighborhoods, increasing the robustness of GRAND. Further, the design of random propagation can naturally separate feature propagation and transformation, which are commonly coupled with each other in most GNNs. This empowers GRAND to safely perform higher-order feature propagation without increasing the complexity, reducing the risk of over-smoothing for GRAND. More importantly, random propagation enables each node to randomly pass messages to its neighborhoods. Under the assumption of homophily of graph data <ref type="bibr" target="#b29">[30]</ref>, we are able to stochastically generate different augmented representations for each node. We then utilize consistency regularization to enforce the prediction model, e.g., a simple Multilayer Perception (MLP), to output similar predictions on different augmentations of the same unlabeled data, improving GRAND's generalization behavior under the semi-supervised setting.</p><p>Finally, we theoretically illustrate that random propagation and consistency regularization can enforce the consistency of classification confidence between each node and its multi-hop neighborhoods. Empirically, we also show both strategies can improve the generalization of GRAND, and mitigate the issues of non-robustness and over-smoothing that are commonly faced by existing GNNs. Altogether, extensive experiments demonstrate that GRAND achieves state-of-the-art semi-supervised learning results on GNN benchmark datasets. </p><formula xml:id="formula_0">C Z 1 E x l H K T y E N t M D f D R l 4 Z C n r l m x q l Y O O E t q B a m A A k 3 X / L T 9 E M c B 4 Q o z J G W v Z k X K S Z B Q F D O S l u 1 Y k g j h E R q Q n q Y c B U Q 6 S X 5 V C v e 1 4 s N + K P T j C u b q 7 4 4 E B V K O A 0 9 X Z o v K v 1 4 m / u f 1 Y t U / d x L K o 1 g R j i c f 9 W M G V Q i z i K B P B c G K j T V B W F C 9 K 8 R D J B B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W O s h y H s J F h t O f k 2 d J + 7 h a q 1 f r t y e V x m U R R w n s g j 1 w A G r g D D T A N W i C F s D g A T y B F / B q P B r P x p v x P i m d M 4 q e H T A F 4 + M b y U W f / A = = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss:</head><p>Partial Labels (b) Augmented Features    method. However, its potential effects under semi-supervised setting have not been well-studied, which we try to explore in future work.</p><formula xml:id="formula_1">D g O S K g w Q 1 L 2 b C t S b o K E o p i R t O T E k k Q I j 9 G Q 9 D Q N U U C k m + R H p H B f K w P o c 6 F f q G C u / k w k K J B y E n h 6 M l t U / v U y 8 T + v F y v / 1 E 1 o G M W K h H j 6 k R 8 z q D j M G o E D K g h W b K I J w o L q X S E e I Y G w 0 r 2 V 8 h L O M h x / n z x P 2 o c 1 u 1 6 r X x 1 V G u e z O o q g D P Z A F d j g B D T A J W i C F s D g H j y C Z / B i P B h P x q v x N h 0 t G L P M L v g F 4 / 0 L e f C Y w A = = &lt; / l a t e x i t &gt; X (1) &lt; l a</formula><formula xml:id="formula_2">G u l v Y x T 6 V l I h 0 F H m o / V C p s J o = " &gt; A A A C B X i c b V D L S s N A F J 3 4 r P U V d a m L Y B H q p i R W f O y K b l x W t A 9 o Y p l M b t q h k w c z E 6 W E b N z 4 K 2 5 c K O L W f 3 D n 3 5 i k Q d R 6 4 M L h n H u 5 9 x 4 7 Z F R I X f 9 U Z m b n 5 h c W S 0 v l 5 Z X V t X V 1 Y 7 M t g o g T a J G A B b x r Y w G M + t C S V D L o h h y w Z z P o 2 K P z z O / c A h c 0 8 K / l O A T L w w O f</formula><formula xml:id="formula_3">4 = " &gt; A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q E t d B I t Q N y W 1 4 m N X d O O y o n 1 g E 8 t k c t M O n T y Y m S g l Z O P G X 3 H j Q h G 3 / o M 7 / 8 Y k D a L W A x c O 5 9 z L v f d Y A a N C 6 v q n U p i Z n Z t f K C 6 W l p Z X V t f U 9 Y 2 2 8 E N O o E V 8 5 v O u h Q U w 6 k F L U s m g G 3 D A r s W g Y 4 3 O U r 9 z C 1 x Q 3 7 u S 4 w B M F w 8 8 6 l C C Z S L 1 1 W 3 j j t o g K b M h M l w s h 5 Y T X c f x T V S 5 3 I v 7 a l m v 6 h m 0 a V L L S R n l a P b V D 8 P 2 S e i C J w n D Q v R q e i D N C H N J C Y O 4 Z I Q C A k x G e A C 9 h H r Y B W F G 2 R e x t p</formula><p>Regularization Methods for GNNs. Another line of work has aimed to design powerful regularization methods for GNNs, such as VBAT <ref type="bibr" target="#b9">[10]</ref>, GraphVAT <ref type="bibr" target="#b11">[12]</ref>, G 3 NN <ref type="bibr" target="#b28">[29]</ref>, GraphMix <ref type="bibr" target="#b41">[42]</ref>, and DropEdge <ref type="bibr" target="#b36">[37]</ref>. For example, VBAT <ref type="bibr" target="#b9">[10]</ref> and GraphVAT <ref type="bibr" target="#b11">[12]</ref> first apply consistency regularized training into GNNs via virtual adversarial training <ref type="bibr" target="#b31">[32]</ref>, which is highly time-consuming in practice. GraphMix <ref type="bibr" target="#b41">[42]</ref> introduces the MixUp strategy <ref type="bibr" target="#b48">[49]</ref> for training GNNs. Different from GRAND, GraphMix augments graph data by performing linear interpolation between two samples in the hidden space, and regularizes GNNs by encouraging the model to predict the same interpolation of corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH RANDOM NEURAL NETWORKS</head><p>We present the GRAPH RANDOM NEURAL NETWORKS (GRAND) for semi-supervised learning on graphs, as illustrated in <ref type="figure">Figure 1</ref>. The idea is to design a propagation strategy (a) to stochastically generate multiple graph data augmentations (b), based on which we present a consistency regularized training (c) for improving the generalization capacity under the semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Propagation for Graph Data Augmentation</head><p>Given an input graph G with its adjacency matrix A and feature matrix X, the random propagation module generates multiple data augmentations. For each augmentation X, it is then fed into the classification model, a two-layer MLP, for predicting node labels. The MLP model can also be replaced with more complex and advanced GNN models, such as GCN and GAT.</p><p>Random Propagation. There are two steps in random propagation. First, we generate a perturbed feature matrix X by randomly dropping out elements in X. Second, we leverage X to perform feature propagation for generating the augmented features X.</p><p>In doing so, each node's features are randomly mixed with signals from its neighbors. Note that the homophily assumption suggests that adjacent nodes tend to have similar features and labels <ref type="bibr" target="#b29">[30]</ref>. Thus, the dropped information of a node could be compensated by its neighbors, forming an approximate representation for it in the corresponding augmentation. In other words, random propagation allows us to stochastically generate multiple augmented representations for each node.</p><p>In the first step, there are different ways to perturb the input data X. Straightforwardly, we can use the dropout strategy <ref type="bibr" target="#b39">[40]</ref>, which has been widely used for regularizing neural networks. Specifically, dropout perturbs the feature matrix by randomly setting some elements of X to 0 during training, i.e., X ij = ij 1−δ X ij , where ij draws from Bernoulli(1 − δ). In doing so, dropout makes the input feature matrix X noisy by randomly dropping out its elements without considering graph structures.</p><p>To account for the structural effect, we can simply remove some nodes' entire feature vectorsreferred to as DropNode, instead of dropping out single feature elements. In other words, DropNode enables each node to only aggregate information from a subset of its (multi-hop) neighbors by completely ignoring some nodes' features, which reduces its dependency on particular neighbors and thus helps increase the model's robustness (Cf. Section 4.5). Empirically, it generates more stochastic data augmentations and achieves better performance than dropout (Cf. Section 4.2).</p><p>Formally, in DropNode, we first randomly sample a binary mask i ∼ Bernoulli(1 − δ) for each node v i . Second, we obtain the perturbed feature matrix X by multiplying each node's feature vector with its corresponding mask, i.e., X i = i · X i where X i denotes the i th row vector of X. Finally, we scale X with the factor of 1 1−δ to guarantee the perturbed feature matrix is in expectation equal to X. Note that the sampling procedure is only performed during training. During inference, we directly set X as the original feature matrix X.</p><p>In the second step of random propagation, we adopt the mixed-order propagation, i.e., X = A X,</p><formula xml:id="formula_4">where A = K k=0 1 K+1Â</formula><p>k is the average of the power series ofÂ from order 0 to order K. This propagation rule enables the model to incorporate more local information, reducing the risk of over-smoothing when compared with directly usingÂ K <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47]</ref>. Note that calculating the dense matrix A is computationally inefficient, thus we compute X by iteratively calculating and summing up the product of sparse matrixÂ andÂ k X (0 ≤ k ≤ K − 1) in implementation.</p><p>With this propagation rule, we could observe that DropNode (dropping the i th row of X) is equivalent to dropping the i th column of A. This is similar to DropEdge <ref type="bibr" target="#b36">[37]</ref>, which aims to address oversmoothing by randomly removing some edges. In practice, DropEdge could also be adopted as the perturbation method here. Specifically, we first generate a corrupted adjacency matrixÃ by dropping some elements fromÂ, and then useÃ to perform mix-order propagation as the substitute ofÂ at each epoch. We empirically compare the effects of different perturbation methods in Section 4.2. By default, we use DropNode as the perturbation method.</p><p>Prediction. After performing random propagation for S times, we generate S augmented feature matrices {X (s) |1 ≤ s ≤ S}. Each of these augmented data is fed into a two-layer MLP to get the corresponding outputs:</p><formula xml:id="formula_5">Z (s) = f mlp (X (s) , Θ),</formula><p>where Z (s) ∈ [0, 1] n×C denotes the prediction probabilities on X (s) and Θ are the model parameters.</p><p>Observing the data flow from random propagation to the prediction module, it can be realized that GRAND actually separates the feature propagation step, i.e., X = A X, and transformation step, i.e., f mlp (XW, Θ). Note that these two steps are commonly coupled with each other in standard GNNs, that is, σ(AXW). This separation allows us to perform the high-order feature propagation without conducting non-linear transformations, reducing the risk of over-smoothing (Cf. Section 4.6). A similar idea has been adopted by Klicpera et al. <ref type="bibr" target="#b24">[25]</ref>, with the difference that they first perform the prediction for each node and then propagate the prediction probabilities over the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consistency Regularized Training</head><p>In graph based semi-supervised learning, the objective is usually to smooth the label information over the graph with regularizations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b23">24]</ref>, i.e., its loss function is a combination of the supervised loss on the labeled nodes and the graph regularization loss. Given the S data augmentations generated in random propagation, we can naturally design a consistency regularized loss for GRAND's semisupervised learning.</p><p>Supervised Loss. With m labeled nodes among n nodes, the supervised objective of the graph node classification task in each epoch is defined as the average cross-entropy loss over S augmentations:</p><formula xml:id="formula_6">Lsup = − 1 S S s=1 m−1 i=0 Y i log Z (s) i .<label>(1)</label></formula><p>Consistency Regularization Loss. In the semi-supervised setting, we propose to optimize the prediction consistency among S augmentations for unlabeled data. Considering a simple case of S = Algorithm 1 GRAND</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Adjacency matrixÂ, feature matrix X ∈ R n×d , times of augmentations in each epoch S, DropNode/dropout probability δ, learning rate η, an MLP model: f mlp (X, Θ). Output:</p><p>Prediction Z. 1: while not convergence do 2:</p><p>for s = 1 : S do 3:</p><p>Pertube the input: X (s) ∼ DropNode(X, δ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Perform propagation:</p><formula xml:id="formula_7">X (s) = 1 K+1 K k=0Â</formula><p>k X (s) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Predict class distribution using MLP:</p><formula xml:id="formula_8">Z (s) = f mlp (X (s) , Θ)</formula><p>6: end for 7:</p><p>Compute supervised classification loss Lsup via Eq. 1 and consistency regularization loss via Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Update the parameters Θ by gradients descending: Θ = Θ − η∇Θ(Lsup + λLcon) 9: end while 10: Output prediction Z via:</p><formula xml:id="formula_9">Z = f mlp ( 1 K+1 K k=0Â k X, Θ).</formula><p>2, we can minimize the squared L 2 distance between the two outputs, i.e., min</p><formula xml:id="formula_10">n−1 i=0 Z (1) i − Z (2) i 2 2 .</formula><p>To extend this idea into the multiple-augmentation situation, we first calculate the label distribution center by taking the average of all distributions, i.e.,</p><formula xml:id="formula_11">Z i = 1 S S s=1 Z (s)</formula><p>i . Then we utilize the sharpening [3] trick to "guess" the labels based on the average distributions. Specifically, the i th node's guessed probability on the j th class is calculated by:</p><formula xml:id="formula_12">Z ij = Z 1 T ij C−1 c=0 Z 1 T ic , (0 ≤ j ≤ C − 1),<label>(2)</label></formula><p>where 0 &lt; T ≤ 1 acts as the "temperature" that controls the sharpness of the categorical distribution.</p><p>As T → 0, the sharpened label distribution will approach a one-hot distribution. We minimize the distance between Z i and Z i in GRAND:</p><formula xml:id="formula_13">Lcon = 1 S S s=1 n−1 i=0 Z i − Z (s) i 2 2 .<label>(3)</label></formula><p>Therefore, by setting T as a small value, we can enforce the model to output low-entropy predictions. This can be viewed as adding an extra entropy minimization regularization into the model, which assumes that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution <ref type="bibr" target="#b17">[18]</ref>.</p><p>Training and Inference. In each epoch, we employ both the supervised classification loss in Eq. 1 and the consistency regularization loss in Eq. 3 on S augmentations. The final loss of GRAND is:</p><formula xml:id="formula_14">L = Lsup + λLcon,<label>(4)</label></formula><p>where λ is a hyper-parameter that controls the balance between the two losses. Algorithm 1 outlines GRAND's training process. During inference, as mentioned in Section 3.1, we directly use the original feature X for propagation. This is justified because we scale the perturbed feature matrix X during training to guarantee its expectation to match X. Hence the inference formula is Z = f mlp (AX, Θ).</p><p>Complexity. The complexity of random propagation is O(Kd(n + |E|)), where K denotes propagation step, d is the dimension of node feature, n is the number of nodes and |E| denotes edge count.</p><p>The complexity of its prediction module (two-layer MLP) is O(nd h (d + C)), where d h denotes its hidden size and C is the number of classes. By applying consistency regularized training, the total computational complexity of GRAND is O(S(Kd(n + |E|) + nd h (d + C))), which is linear with the sum of node and edge counts.</p><p>Limitations. GRAND is based on the homophily assumption <ref type="bibr" target="#b29">[30]</ref>, i.e., "birds of a feather flock together", a basic assumption in the literature of graph-based semi-supervised learning <ref type="bibr" target="#b51">[52]</ref>. Due to that, however, GRAND may not succeed on graphs with less homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Analysis</head><p>We theoretically discuss the regularization effects brought by random propagation and consistency regularization in GRAND. For analytical simplicity, we assume that the MLP used in GRAND has one single output layer, and the task is binary classification. Thus we have Z = sigmoid(A X · W), where W ∈ R d is the learnable parameter vector. For the i th node, the corresponding conditional distribution isz yi i (1 −z i ) 1−yi , in whichz i ∈ Z and y i ∈ {0, 1} denotes the corresponding label. As for the consistency regularization loss, we consider the simple case of generating S = 2 augmentations. Then the loss L con = 1 2 n−1 i=0</p><formula xml:id="formula_15">z (1) i −z (2) i 2 , wherez (1) i andz (2) i</formula><p>represent the model's two outputs on node i corresponding to the two augmentations, respectively.</p><p>With these assumptions, we have the following theorem with proofs in Appendix B.1. Theorem 1. In expectation, optimizing the unsupervised consistency loss L con is approximate to optimize a regularization term:</p><formula xml:id="formula_16">E (L con ) ≈ R c (W) = n−1 i=0 z 2 i (1 − z i ) 2 Var A i X · W .</formula><p>DropNode Regularization. With DropNode as the perturbation method, we can easily check that</p><formula xml:id="formula_17">Var (A i X · W) = δ 1−δ n−1 j=0 (X j · W) 2 (A ij ) 2 ,</formula><p>where δ is drop rate. Then the corresponding regularization term R c DN can be expressed as:</p><formula xml:id="formula_18">R c DN (W) = δ 1 − δ n−1 j=0 (Xj · W) 2 n−1 i=0 (Aij) 2 z 2 i (1 − zi) 2 .<label>(5)</label></formula><p>Note that z i (1 − z i ) (or its square) is an indicator of the classification uncertainty for the i th node, as z i (1 − z i ) (or its square) reaches its maximum at z i = 0.5 and minimum at z i = 0 or 1. Thus</p><formula xml:id="formula_19">m−1 i=0 (A ij ) 2 z 2 i (1 − z i ) 2</formula><p>can be viewed as the weighted average classification uncertainty over the j th node's multi-hop neighborhoods with the weights as the square values of A's elements, which is related to graph structure. On the other hand, (X j · W) 2 -as the square of the input of sigmoid-indicates the classification confidence for the j th node. In optimization, in order for a node to earn a higher classification confidence (X j · W) 2 , it is required that the node's neighborhoods have lower classification uncertainty scores. Hence, the random propagation with the consistency regularization loss can enforce the consistency of the classification confidence between each node and its multi-hop neighborhoods.</p><p>Dropout Regularization. With X perturbed by dropout, the variance term <ref type="bibr">Var</ref> </p><formula xml:id="formula_20">(A i X · W) = δ 1−δ n−1 j=0 d−1 k=0 X 2 jk W 2 k (A ij ) 2 . The corresponding regularization term R c Do is R c Do (W) = δ 1 − δ d−1 h=0 W 2 h n−1 j=0 X 2 jh n−1 i=0 z 2 i (1 − zi) 2 (Aij) 2 .<label>(6)</label></formula><p>Similar to DropNode, this extra regularization term also includes the classification uncertainty z i (1−z i ) of neighborhoods. However, we can observe that different from the DropNode regularization, dropout is actually an adaptive L 2 regularization for W, where the regularization coefficient is associated with unlabeled data, classification uncertainty, and the graph structure.</p><p>Previous work <ref type="bibr" target="#b42">[43]</ref> has also drawn similar conclusions for the case of applying dropout in generalized linear models.</p><p>By applying the Cauchy-Schwarz Inequality, we have R c Do ≥ R c DN . That is to say, dropout's regularization term is the upper bound of DropNode's. By minimizing this term, dropout can be regarded as an approximation of DropNode.</p><p>Random propagation w.r.t supervised classification loss. We also discuss the regularization effect of random propagation with respect to the supervised classification loss.</p><p>With the previous assumptions, the supervised classification loss is:</p><formula xml:id="formula_21">L sup = m−1 i=0 −y i log(z i ) − (1 − y i ) log(1 −z i ).</formula><p>Note that L sup refers to the perturbed classification loss with DropNode on the node features. By contrast, the original (non-perturbed) classification loss is defined as:</p><formula xml:id="formula_22">L org = m−1 i=0 −y i log(z i ) − (1 − y i ) log(1 − z i ), where z i = sigmoid(A i X · W)</formula><p>is the output with the original feature matrix X. Then we have the following theorem with proof in Appendix B.2. Theorem 2. In expectation, optimizing the perturbed classification loss L sup is equivalent to optimize the original loss L org with an extra regularization term R(W), which has a quadratic approximation</p><formula xml:id="formula_23">form R(W) ≈ R q (W) = 1 2 m−1 i=0 z i (1 − z i )Var A i X · W .</formula><p>This theorem suggests that DropNode brings an extra regularization loss to the optimization objective. Expanding the variance term, this extra quadratic regularization loss can be expressed as:</p><formula xml:id="formula_24">R q DN (W) = 1 2 δ 1 − δ n−1 j=0 (Xj · W) 2 m−1 i=0 (Aij) 2 zi(1 − zi) .<label>(7)</label></formula><p>Different from R c DN in Eq. 5, the inside summation term in Eq. 7 only incorporates the first m nodes, i.e, the labeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We follow exactly the same experimental procedure-such as features and data splits-as the standard GNN settings on semi-supervised graph learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. The setup and reproducibility details are covered in Appendix A.</p><p>Datasets. We conduct experiments on three benchmark graphs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41</ref>]-Cora, Citeseer, and Pubmed-and also report results on six publicly available and large datasets in Appendix C.1.</p><p>Baselines. By default, we use DropNode as the perturbation method in GRAND and compare it with 14 GNN baselines representative of three different categories, as well as its variants:</p><p>• Eight graph convolutions: GCN <ref type="bibr" target="#b23">[24]</ref>, GAT <ref type="bibr" target="#b40">[41]</ref>, APPNP <ref type="bibr" target="#b24">[25]</ref>, Graph U-Net <ref type="bibr" target="#b12">[13]</ref>, SGC <ref type="bibr" target="#b44">[45]</ref>,</p><p>MixHop <ref type="bibr" target="#b0">[1]</ref>, GMNN <ref type="bibr" target="#b35">[36]</ref> and GrpahNAS <ref type="bibr" target="#b13">[14]</ref>.</p><p>• Two sampling based GNNs: GraphSAGE <ref type="bibr" target="#b18">[19]</ref> and FastGCN <ref type="bibr" target="#b7">[8]</ref>.</p><p>• Four regularization based GNNs: VBAT <ref type="bibr" target="#b9">[10]</ref>, G 3 NN <ref type="bibr" target="#b28">[29]</ref>, GraphMix <ref type="bibr" target="#b41">[42]</ref> and Dropedge <ref type="bibr" target="#b36">[37]</ref>. We report the results of these methods with GCN as the backbone model.</p><p>• Four GRAND variants: GRAND_dropout, GRAND_DropEdge, GRAND_GCN and GRAND_GAT. In GRAND_dropout and GRAND_DropEdge, we use dropout and DropEdge as the perturbation method respectively, instead of DropNode. In GRAND_GCN and GRAND_GAT, we replace MLP with more complex models, i.e., GCN and GAT, respectively. <ref type="table" target="#tab_0">Table 1</ref> summarizes the prediction accuracies of node classification. Following the community convention <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36]</ref>, the results of baselines are taken from the original works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25]</ref>. The results of GRAND are averaged over 100 runs with random weight initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Results</head><p>From the top part of <ref type="table" target="#tab_0">Table 1</ref>, we can observe that GRAND consistently achieves large-margin outperformance over all baselines across all datasets. Note that the improvements of GRAND over other baselines are all statistically significant (p-value 0.01 by a t-test). Specifically, GRAND improves upon GCN by a margin of 3.9%, 5.1%, and 3.7% (absolute differences) on Cora, Citeseer, and Pubmed, while the margins improved by GAT upon GCN were 1.5%, 2.2%, and 0%, respectively. When compared to the very recent regularization based model-DropEdge, the proposed model achieves 2.6%, 3.1%, and 3.1% improvements, while DropEdge's improvements over GCN were only 1.3%, 2.0%, and 0.6%, respectively. To better examine the effectiveness of GRAND in semi-supervised setting, we further evaluate GRAND under different label rates in Appendix C.6.</p><p>We observe GRAND_dropout and GRAND_DropEdge also outperform most of baselines, though still lower than GRAND. This indicates DropNode is the best way to generate graph data augmentations in random propagation. Detailed experiments to compare DropNode and dropout under different propagation steps K are shown in Appendix C.4.</p><p>We interpret the performance of GRAND_GAT, GRAND_GCN from two perspectives. First, both GRAND_GAT and GRAND_GCN outperform the original GCN and GAT models, demonstrating the positive effects of the proposed random propagation and consistency regularized training methods. Second, both of them are inferior to GRAND with the simple MLP model, suggesting GCN and GAT are relatively easier to over-smooth than MLP. More analyses can be found in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cora  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct an ablation study to examine the contributions of different components in GRAND.</p><p>• Without consistency regularization (CR): We only use the supervised classification loss, i.e., λ = 0.</p><p>• Without multiple DropNode (mDN): Do DropNode once at each epoch, i.e., S = 1, meaning that CR only enforces the model to give low-entropy predictions for unlabeled nodes.</p><p>• Without sharpening: The sharpening trick in Eq. 2 is not used in getting the distribution center, i.e., T = 1.</p><p>• Without CR and DropNode (CR &amp; DN): Remove DropNode (as a result, the CR loss is also removed), i.e., δ = 0, λ = 0. In this way, GRAND becomes the combination of deterministic propagation and MLP.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, the bottom part summarizes the results of the ablation study, from which we have two observations. First, all GRAND variants with some components removed witness clear performance drops when comparing to the full model, suggesting that each of the designed components contributes to the success of GRAND. Second, GRAND without consistency regularization outperforms almost all eight non-regularization based GCNs and DropEdge in all three datasets, demonstrating the significance of the proposed random propagation technique for semi-supervised graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization Analysis</head><p>We examine how the proposed techniques-random propagation and consistency regularizationimprove the model's generalization capacity. To achieve this, we analyze the model's cross-entropy losses on both training and validation sets on Cora. A small gap between the two losses indicates a model with good generalization. <ref type="figure" target="#fig_5">Figure 2</ref> reports the results for GRAND and its two variants. We can observe the significant gap between the validation and training losses when without both consistency regularization (CR) and random propagation (RP), indicating an obvious overfitting issue. When applying only the random propagation (without CR), the gap becomes much smaller. Finally, when further adding the CR loss to make it the full GRAND model, the validation loss becomes much closer to the training loss and both of them are also more stable. This observation demonstrates both the random propagation and consistency regularization can significantly improve GRAND's generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Robustness Analysis</head><p>We study the robustness of GRAND by generating perturbed graphs with two adversarial attack methods: Random Attack perturbs the graph structure by randomly adding fake edges, and Metattack <ref type="bibr" target="#b54">[55]</ref> attacks the graph by removing or adding edges based on meta learning. <ref type="figure" target="#fig_6">Figure 3</ref> presents the classification accuracies of different methods with respect to different perturbation rates on the Cora dataset. We observe that GRAND consistently outperforms GCN and GAT across all perturbation rates on both attacks. When adding 10% new random edges into Cora, we observe only a 7% drop in classification accuracy for GRAND, while 12% for GCN and 37% for GAT. Under Metattack, the gap between GRAND and GCN/GAT also enlarges with the increase of the perturbation rate. This study suggests the robustness advantage of the GRAND model (with or without) consistency regularization over GCN and GAT.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Over-Smoothing Analysis</head><p>Many GNNs face the over-smoothing issue-nodes with different labels become indistinguishablewhen enlarging the feature propagation steps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>. We study how vulnerable GRAND is to this issue by using MADGap <ref type="bibr" target="#b6">[7]</ref>, a measure of the over-smoothness of node representations. A smaller MADGap value indicates the more indistinguishable node representations and thus a more severe over-smoothing issue. <ref type="figure" target="#fig_7">Figure 4</ref> shows both the MADGap values of the last layer's representations and classification results w.r.t. different propagation steps. In GRAND, the propagation step is controlled by the hyperparameter K, while for GCN and GAT, it is adjusted by stacking different hidden layers. The plots suggest that as the propagation step increases, both metrics of GCN and GAT decrease dramatically-MADGap drops from ∼0.5 to 0 and accuracy drops from 0.75 to 0.2-due to the over-smoothing issue. However, GRAND behaves completely different, i.e., both the performance and MADGap benefit from more propagation steps. This indicates that GRAND is much more powerful to relieve over-smoothing, when existing representative GNNs are very vulnerable to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we study the problem of semi-supervised learning on graphs and present the GRAPH RANDOM NEURAL NETWORKS (GRAND). In GRAND, we propose the random propagation strategy to stochastically generate multiple graph data augmentations, based on which we utilize consistency regularization to improve the model's generalization on unlabeled data. We demonstrate its consistent performance superiority over fourteen state-of-the-art GNN baselines on benchmark datasets. In addition, we theoretically illustrate its properties and empirically demonstrate its advantages over conventional GNNs in terms of robustness and resistance to over-smoothing. To conclude, the simple and effective ideas presented in GRAND may generate a different perspective in GNN design, in particular for semi-supervised graph learning. In future work, we aim to further improve the scalability of GRAND with some sampling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Over the past years, GNNs have been extensively studied and widely used for semi-supervised graph learning, with the majority of efforts devoted to designing advanced and complex GNN architectures. Instead of heading towards that direction, our work focuses on an alternative perspective by examining whether and how simple and traditional machine learning (ML) techniques can help overcome the common issues that most GNNs faced, including over-smoothing, non-robustness, and weak generalization.</p><p>Instead of the nonlinear feature transformations and advanced neural techniques (e.g., attention), the presented GRAND model is built upon dropout (and its simple variant), linear feature propagation, and consistency regularization-the common ML techniques. Its consistent and significant outperformance over 14 state-of-the-art GNN baselines demonstrates the effectiveness of our alternative direction. In addition, our results also echo the recent discovery in SGC <ref type="bibr" target="#b44">[45]</ref> to better understand the source of GCNs' expressive power. More importantly, these simple ML techniques in GRAND empower it to be more robust, better avoid over-smoothing, and offer stronger generalization than GNNs.</p><p>In light of these advantages, we argue that the ideas in GRAND offer a different perspective in understanding and advancing GNN based semi-supervised learning. For future research in GNNs, in addition to designing complex architectures, we could also invest in simple and traditional graph techniques under the regularization framework which has been widely used in (traditional) semisupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix-Graph Random Neural Network for Semi-Supervised Learning on Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility</head><p>A.1 Datasets Details <ref type="table" target="#tab_1">Table 2</ref> summarizes the statistics of the three benchmark datasets -Cora, Citeseer and Pubmed. Our preprocessing scripts for Cora, Citeseer and Pubmed is implemented with reference to the codes of Planetoid <ref type="bibr" target="#b47">[48]</ref>. We use exactly the same experimental settings-such as features and data splits-on the three benchmark datasets as literature on semi-supervised graph mining <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref> and run 100 trials with 100 random seeds for all results on Cora, Citeseer and Pubmed reported in Section 4. We also evaluate our method on six publicly available and large datasets, the statistics and results are summarized in Appendix C.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>We make use of PyTorch to implement GRAND and its variants. The random propagation procedure is efficiently implemented with sparse-dense matrix multiplication. The codes of GCN and GRAND_GCN are implemented referring to the PyTorch version of GCN 1 . As for GRAND_GAT and GAT, we adopt the implementation of GAT layer from the PyTorch-Geometric library 2 in our experiments. The weight matrices of classifier are initialized with Glorot normal initializer <ref type="bibr" target="#b15">[16]</ref>. We employ Adam <ref type="bibr" target="#b22">[23]</ref> to optimize parameters of the proposed methods and adopt early stopping to control the training epochs based on validation loss. Apart from DropNode (or dropout <ref type="bibr" target="#b39">[40]</ref>) used in random propagation, we also apply dropout on the input layer and hidden layer of the prediction module used in GRAND as a common practice of preventing overfitting in optimizing neural network.</p><p>For the experiments on Pubmed, we also use batch normalization <ref type="bibr" target="#b21">[22]</ref> to stabilize the training procedure. All the experiments in this paper are conducted on a single NVIDIA GeForce RTX 2080 Ti with 11 GB memory size. Server operating system is Unbuntu 18.04. As for software versions, we use Python 3.7.3, PyTorch 1.2.0, NumPy 1.16.4, SciPy 1.3.0, CUDA 10.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameter Details</head><p>Overall Results in Section 4.2. GRAND introduces five additional hyperparameters, that is the DropNode probability δ in random propagation, propagation step K, data augmentation times S at each training epoch, sharpening temperature T when calculating consistency regularization loss and the coefficient of consistency regularization loss λ trading-off the balance between L sup and L con . In practice, δ is always set to 0.5 across all experiments. As for other hyperparameters, we perform hyperparameter search for each dataset. Specifically, we first search K from { 2,4,5,6,8}.</p><p>With the best selection of K, we then search S from {2,3,4}. Finally, we fix K and S to the best values and take a grid search for T and λ from {0.1, 0.2, 0.3,0.5} and {0.5, 0.7, 1.0} respectively. For each search of hyperparameter configuration, we run the experiments with 20 random seeds and select the best configuration of hyperparameters based on average accuracy on validation set. Other hyperparameters used in our experiments includes learning rate of Adam, early stopping patience, L2 weight decay rate, hidden layer size, dropout rates of input layer and hidden layer. We didn't spend much effort to tune these hyperparameters in practice, as we observe that GRAND is not very 1 https://github.com/tkipf/pygcn 2 https://pytorch-geometric.readthedocs.io sensitive with those. <ref type="table" target="#tab_2">Table 3</ref> reports the best hyperparameters of GRAND we used for the results reported in <ref type="table" target="#tab_0">Table 1</ref>. Robustness Analysis in Section 4.5. For random attack, we implement the attack method with Python and NumPy library. The propagation step K of GRAND (with or without CR) is set to 5.</p><p>And the other hyperparameters are set to the values in <ref type="table" target="#tab_2">Table 3</ref>. As for Metattack <ref type="bibr" target="#b54">[55]</ref>, we use the publicly available implementation 3 published by the authors with the same hyperparameters used in the original paper. We observe GRAND (with or without CR) is sensitive to the propagation step K under different perturbation rates. Thus we search K from {5,6,7,8} for each perturbation rate. The other hyperparameters are fixed to the values reported in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Other Experiments. For the other results reported in Section 4.2 -4.6, the hyperparameters used in GRAND are set to the values reported in <ref type="table" target="#tab_2">Table 3</ref> with one or two changed for the corresponding analysis.</p><p>Baseline Methods. For the results of GCN or GAT reported in Section 4.5 -4.6, the learning rate is set to 0.01, early stopping patience is 100, L2 weight decay rate is 5e-4, dropout rate is 0.5. The hidden layer size of GCN is 32. For GAT, the hidden layer consists 8 attention heads and each head consists 8 hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theorem Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof for Theorem 1</head><p>Proof. The expectation of L con is:</p><formula xml:id="formula_25">1 2 n−1 i=0 E (z (1) i −z (2) i ) 2 = 1 2 n−1 i=0 E (z (1) i − zi) − (z (2) i − zi) 2 .<label>(8)</label></formula><p>Here z i = sigmoid(A i X·W),z i = sigmoid(A i X·W). For the term ofz i −z i , we can approximate it with its first-order Taylor expansion around</p><formula xml:id="formula_26">A i X · W, i.e.,z i − z i ≈ z i (1 − z i )(A i ( X − X) · W).</formula><p>Applying this rule to the above equation, we have:</p><formula xml:id="formula_27">1 2 n−1 i=0 E (z (1) i −z (2) i ) 2 ≈ 1 2 n−1 i=0 z 2 i (1 − zi) 2 E (Ai( X (1) − X (2) ) · W) 2 = n−1 i=0 z 2 i (1 − zi) 2 Var Ai X · W .<label>(9)</label></formula><p>B.2 Proof for Theorem 2</p><p>Proof. Expanding the logistic function, L org is rewritten as:</p><formula xml:id="formula_28">Lorg = m−1 i=0 −yiAiX · W + A(Ai, X) ,<label>(10)</label></formula><p>where A(A i , X) = − log exp(−AiX·W) 1+exp(−AiX·W) . Then the expectation of perturbed classification loss can be rewritten as:</p><formula xml:id="formula_29">E (Lsup) = Lorg + R(W),<label>(11)</label></formula><p>where</p><formula xml:id="formula_30">R(W) = m−1 i=0 E A(A i , X) − A(A i , X) .</formula><p>Here R(W) acts as a regularization term for W. To demonstrate that, we can take a second-order Taylor expansion of A(A i , X) around A i X · W:</p><formula xml:id="formula_31">E A(Ai, X) − A(Ai, X) ≈ 1 2 A (Ai, X)Var Ai X · W .<label>(12)</label></formula><p>Note that the first-order term E A (A i , X)( X − X) vanishes since E ( X) = X. We can easily</p><formula xml:id="formula_32">check that A (A i , X) = z i (1 − z i ).</formula><p>Applying this quadratic approximation to R(W) , we get the quadratic approximation form of R(W):</p><formula xml:id="formula_33">R(W) ≈ R q (W) = 1 2 m−1 i=0 z i (1 − z i )Var (A i X · W).<label>(13)</label></formula><p>C Additional Experiments C.1 Results on Large Datasets We also evaluate our methods on six relatively large datasets, i.e., Cora-Full, Coauthor CS, Coauthor Physics, Amazon Computers, Amazon Photo and Aminer CS. The statistics of these datasets are given in <ref type="table" target="#tab_3">Table 4</ref>. Cora-Full is proposed in <ref type="bibr" target="#b3">[4]</ref>. Coauthor CS, Coauthor Physics, Amazon Computers and Amazon Photo are proposed in <ref type="bibr" target="#b38">[39]</ref>. We download the processed versions of the five datasets here <ref type="bibr" target="#b3">4</ref> . Aminer CS is extracted from the DBLP data downloaded from https://www.aminer.cn/ citation. In Aminer CS, each node corresponds to a paper in computer science, and edges represent citation relations between papers. These papers are manually categorized into 18 topics based on their publication venues. We use averaged GLOVE-100 <ref type="bibr" target="#b34">[35]</ref> word vector of paper abstract as the node feature vector. Our goal is to predict the corresponding topic of each paper based on feature matrix and citation graph structure.</p><p>Following the evaluation protocol used in <ref type="bibr" target="#b38">[39]</ref>, we run each model on 100 random train/validation/test splits and 20 random initializations for each split (with 2000 runs on each dataset in total). For each trial, we choose 20 samples for training, 30 samples for validation and the remaining samples for test. We ignore 3 classes with less than 50 nodes in Cora-Full dataset as done in <ref type="bibr" target="#b38">[39]</ref>. The results are presented in <ref type="table" target="#tab_4">Table 5</ref>. The results of GCN and GAT on the first five datasets are taken from <ref type="bibr" target="#b38">[39]</ref>. We can observe that GRAND significantly outperforms GCN and GAT on all these datasets. (b) Classification Accuracy <ref type="figure">Figure 5</ref>: Efficiency Analysis for GRAND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Efficiency Analysis</head><p>The efficiency of GRAND is mainly influenced by two hyperparameters: the propagation step K and augmentation times S. <ref type="figure">Figure 5</ref> reports the average per-epoch training time and classification accuracy of GRAND on Cora under different values of K and S with #training epochs fixed to 1000. It also includes the results of the two-layer GCN and two-layer GAT with the same learning rate, #training epochs and hidden layer size as GRAND.</p><p>From <ref type="figure">Figure 5</ref>, we can see that when K = 2, S = 1, GRAND outperforms GCN and GAT in terms of both efficiency and effectiveness. In addition, we observe that increasing K or S can significantly improve the model's classification accuracy at the cost of its training efficiency. In practice, we can adjust the values of K and S to balance the trade-off between performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Parameter Sensitivity</head><p>We investigate the sensitivity of consistency regularization (CR) loss coefficient λ and DropNode probability δ in GRAND and its variants on Cora. The results are shown in <ref type="figure" target="#fig_8">Figure 6</ref>. We observe that their performance increase when enlarging the value of λ. As for DropNode probability, GRAND, GRAND_GCN and GRAND_GAT reach their peak performance at δ = 0.5. This is because the augmentations produced by random propagation in that case are more stochastic and thus make GRAND generalize better with the help of consistency regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 DropNode vs Dropout</head><p>We compare GRAND and GRAND_dropout under different values of propagation step K. The results on Cora, Citeseer and Pubmed are illustrated in <ref type="figure">Figure 7</ref>. We observe GRAND always achieve better performance than GRAND_dropout, suggesting DropNode is much more suitable for graph data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 GRAND vs. GRAND_GCN &amp; GRAND_GAT</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, GRAND_GCN and GRAND_GAT get worse performances than GRAND, indicating GCN and GAT perform worse than MLP under the framework of GRAND. Here we  conduct a series of experiments to analyze the underlying reasons. Specifically, we compare the MADGap values and accuracies GRAND, GRAND_GCN and GRAND_GAT under different values of propagation step K with other parameters fixed. The results are shown in <ref type="figure">Figure 8</ref>. We find that the MADGap and classification accuracy of GRAND increase significantly when enlarging the value of K. However, both the metrics of GRAND_GCN and GRAND_GAT have little improvements or even decrease. This indicates that GCN and GAT have higher over-smoothing risk than MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Performance of GRAND under different label rates</head><p>We have conducted experiments to evaluate GRAND under different label rates. For each label rate setting, we randomly create 10 data splits, and run 10 trials with random initialization for each split.</p><p>We compare GRAND with GCN and GAT. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. We observe that GRAND consistently outperforms GCN and GAT across all label rates on three benchmarks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " s 7 z J V I T O O x 8 C Y y 5 K m l t c + f d j x E U = " &gt; A A A B + n i c b V D L S g M x F M 3 U V 6 2 v q S 7 d B I v g q s y o + N g V 3 b h w U c E + o B 2 G T J p p Q z P J k G S U M s 6 n u H G h i F u / x J 1 / Y 6 Y d R K 0 H A o d z 7 u W e n C B m V G n H + b R K C 4 t L y y v l 1 c r a + s b m l l 3 d b i u R S E x a W D A h u w F S h F F O W p p q R r q x J C g K G O k E 4 8 v c 7 9 w R q a j g t 3 o S E y 9 C Q 0 5 D i p E 2 k m 9 X + x H S I 4 x Y e p 3 5 K R Y 8 8 + 2 a U 3 e m g P P E L U g N F G j 6 9 k d / I H A S E a 4 x Q 0 r 1 X C f W X o q k p p i R r N J P F I k R H q M h 6 R n K U U S U l 0 6j Z 3 D f K A M Y C m k e 1 3 C q / t x I U a T U J A r M Z B 5 U / f V y 8 T + v l + j w z E s p j x N N O J 4 d C h M G t Y B 5 D 3 B A J c G a T Q x B W F K T F e I R k g h r 0 1 Z l W s J 5 j p P v L 8 + T 9 m H d P a o f 3 R z X G h d F H W W w C / b A A X D B K W i A K 9 A E L Y D B P X g E z + D F e r C er F f r b T Z a s o q d H f A L 1 v s X / e 2 U n Q = = &lt; / l a t e x i t &gt; Lsup &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e M V n T 7 u v x k g s E / 9 F V P 2 4 0 K T y u T o = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d D B b B V U l U f O y K b l y 4 q G A f 0 I Y w m U 7 a o Z M H M x O l x H y K G x e K u P V L 3 P k 3 T t o g a j 0 w c D j n X u 6 Z 4 8 W c S W V Z n 0 Z p Y X F p e a W 8 W l l b 3 9 j c M q v b b R k l g t A W i X g k u h 6 W l L O Q t h R T n H Z j Q X H g c d r x x p e 5 3 7 m j Q r I o v F W T m D o B H o b M Z w Q r L b l m t R 9 g N S K Y p 9 e Z m 8 o k z l y z Z t W t K d A 8 s Q t S g w J N 1 / z o D y K S B D R U h G M p e 7 Y V K y f F Q j H C a V b p J 5 L G m I z x k P Y 0 D X F A p Z N O o 2 d o X y s D 5 E d C v 1 C h q f p z I 8 W B l J P A 0 5 N 5 U P n X y 8 X / v F 6 i / D M n Z W G c K B q S 2 S E / 4 U h F K O 8 B D Z i g R P G J J p g I p r M i M s I C E 6 X b q k x L O M 9 x 8 v 3 l e d I + r N t H 9 a O b 4 1 r j o q i j D L u w B w d g w y k 0 4 A q a 0 A I C 9 / A I z / B i P B h P x q v x N h s t G c X O D v y C 8 f 4 F I p q U t Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " A n h C x O e v V h w 2 c d l f Y + N T H p k C c w o = " &gt; A A A C F X i c b V D L S s N A F J 3 4 r P U V d e l m s A i C U l I r P n Z F N y 5 c V L A P a E K Y T K b t 0 M k k z E y E E v I T b v w V N y 4 U c S u 4 8 2 + c p E G s 9 c D A 4 Z x 7 7 9 x 7 v I h R q S z r y 5 i b X 1 h c W i 6 t l F f X 1 j c 2 z a 3 t t g x j g U k L h y w U X Q 9 J w i g n L U U V I 9 1 I E B R 4 j H S 8 0 V X m d + 6 J k D T k d 2 o c E S d A A 0 7 7 F C O l J d c 8 s g O k h h i x 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>⌦⌦</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u Z 3 U b T u W S M D f r U n z 6 e E g j 1 Q 8 w d 4 = " &gt; A A A C y 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V V I r P n Z F N 2 6 E C v Y B V S R J p 3 V o X s x M h F p d + g N u 9 b / E P 9 C / 8 M 6 Y i l J E b 0 h y 5 t x z 7 s y d 6 y U B l 8 p x X n P W 1 P T M 7 F x + v r C w u L S 8 U l x d a 8 o 4 F T 5 r + H E Q i 7 b n S h b w i D U U V w F r J 4 K 5 o R e w l j c 4 1 v n W D R O S x 9 G 5 G i b s M n T 7 E e 9 x 3 1 V E t S 9 i x U M m C 1 f F k l N 2 T N i T o J K B E r K o x 8 U X X K C L G D 5 S h G C I o A g H c C H p 6 a A C B w l x l x g R J w h x k 2 e 4 R 4 G 8 K a k Y K V x i B / T t 0 6 q T s R G t d U 1 p 3 D 7 t E t A r y G l j i z w x 6 Q R h v Z t t 8 q m p r N n f a o 9 M T X 2 2 I f 2 9 r F Z I r M I 1 s X / 5 x s r / + n Q v C j 0 c m B 4 4 9 Z Q Y R n f n Z 1 V S c y v 6 5 P a 3 r h R V S I j T u E t 5 Q d g 3 z v E 9 2 8 Y j T e / 6 b l 2 T f z N K z e q 1 n 2 l T v O t T m g E f 6 t j 7 G u c k a O 6 U K 9 V y 9 W y 3 V D v K R p 3 H B j a x T f P c R w 0 n q K N h 5 v i I J z x b p 5 a 0 b q 2 7 T 6 m V y z z r + B H W w w e F 4 Z J 5 &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " D q 5 B S s Y W H i N o x T x 9 K C j e e 5 a N f g U = " &gt; A A A B 7 H i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q q P n Z F N y 4 r 2 A e 0 o U y m k 3 b o z C T M T I Q S + g 1 u X C j i 1 g 9 y 5 9 8 4 S Y O o 9 c C F w z n 3 c u 8 9 Q c y Z N q 7 7 6 Z S W l l d W 1 8 r r l Y 3 N r e 2 d 6 u 5 e W 0 e J I r R F I h 6 p b o A 1 5 U z S l m G G 0 2 6 s K B Y B p 5 1 g c p P 5 n Q e q N I v k v Z n G 1 B d 4 J F n I C D Z W a g m s J 3 p Q r b l 1 N w d a J F 5 B a l C g O a h + 9 I c R S Q S V h n C s d c 9 z Y + O n W B l G O J 1 V + o m m M S Y T P K I 9 S y U W V P t p f u w M H V l l i M J I 2 Z I G 5 e r P i R Q L r a c i s J 0 C m 7 H + 6 2 X i f 1 4 v M e G l n z I Z J 4 Z K M l 8 U J h y Z C G W f o y F T l B g + t Q Q T x e y t i I y x w s T Y f C p 5 C F c Z z r 9 f X i T t k 7 p 3 W j + 9 O 6 s 1 r o s 4 y n A A h 3 A M H l x A A 2 6 h C S 0 g w O A R n u H F k c 6 T 8 + q 8 z V t L T j G z D 7 / g v H 8 B G B 2 P A g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u Z 3 U b T u W S M D f r U n z 6 e E g j 1 Q 8 w d 4= " &gt; A A A C y 3 i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V V I r P n Z F N 2 6 E C v Y B V S R J p 3 V o X s x M h F p d + g N u 9 b / E P 9 C / 8 M 6 Y i l J E b 0 h y 5 t x z 7 s y d 6 y U B l 8 p x X n P W 1 P T M 7 F x + v r C w u L S 8 U l x d a 8 o 4 F T 5 r + H E Q i 7 b n S h b w i D U U V w F r J 4 K 5 o R e w l j c 4 1 v n W D R O S x 9 G 5 G i b s M n T 7 E e 9 x 3 1 V E t S 9 i x U M m C 1 f F k l N 2 T N i T o J K B E r K o x 8 U X X K C L G D 5 S h G C I o A g H c C H p 6 a A C B w l x l x g R J w h x k 2 e 4 R 4 G 8 K a k Y K V x i B / T t 0 6 q T s R G t d U 1 p 3 D 7 t E t A r y G l j i z w x 6 Q R h v Z tt 8 q m p r N n f a o 9 M T X 2 2 I f 2 9 r F Z I r M I 1 s X / 5 x s r / + n Q v C j 0 c m B 4 4 9 Z Q Y R n f n Z 1 V S c y v 6 5 P a 3 r h R V S I j T u E t 5 Q d g 3 z v E 9 2 8 Y j T e / 6 b l 2 T f z N K z e q 1 n 2 l T v O t T m g E f 6 t j 7 G u c k a O 6 U K 9 V y 9 W y 3 V D v K R p 3 H B j a x T f P c R w 0 n q K N h 5 v i I J z x b p 5 a 0 b q 2 7 T 6 m V y z z r + B H W w w e F 4 Z J 5 &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 3 w 2 Z 8 a Z F k 7 W D 7 m c o W b h g C 9 I e R g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R W f O y K b l x W s A 9 s Q 5 l M J + 3 Q y S T M T I Q S 8 h d u X C j i 1 r 9 x 5 9 8 4 S Y O o 9 c D A 4 Z x 7 m X O P F 3 G m t G 1 / W q W l 5 Z X V t f J 6 Z W N z a 3 u n u r v X U W E s C W 2 T k I e y 5 2 F F O R O 0 r Z n m t B d J i g O P 0 6 4 3 v c 7 8 7 g O V i o X i T s 8 i 6 g Z 4 L J j P C N Z G u h 8 E W E 8 8 P + m l w 2 r N r t s 5 0 C J x C l K D A q 1 h 9 W M w C k k c U K E J x 0 r 1 H T v S b o K l Z o T T t D K I F Y 0 w m e I x 7 R s q c E C V m + S J U 3 R k l B H y Q 2 m e 0 C h X f 2 4 k O F B q F n h m M k u o / n q Z + J / X j 7 V / 4 S Z M R L G m g s w / 8 m O O d I i y 8 9 G I S U o 0 n x m C i W Q m K y I T L D H R p q R K X s J l h r P v k x d J 5 6 T u N O q N 2 9 N a 8 6 q o o w w H c A j H 4 M A 5 N O E G W t A G A g I e 4 R l e L G U 9 W a / W 2 3 y 0 Z B U 7 + / A L 1 v s X 5 W G R M w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " D q 5 B S s Y W H i N o x T x 9 K C j e e 5 a N f g U = " &gt; A A A B 7 H i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q q P n Z F N y 4 r 2 A e 0 o U y m k 3 b o z C T M T I Q S + g 1 u X C j i 1 g 9 y 5 9 8 4 S Y O o 9 c C F w z n 3 c u 8 9 Q c y Z N q 7 7 6 Z S W l l d W 1 8 r r l Y 3 N r e 2 d 6 u 5 e W 0 e J I r R F I h 6 p b o A 1 5 U z S l m G G 0 2 6 s K B Y B p 5 1 g c p P 5 n Q e q N I v k v Z n G 1 B d 4 J F n I C D Z W a g m s J 3 p Q r b l 1 N w d a J F 5 B a l C g O a h + 9 I c R S Q S V h n C s d c 9 z Y + O n W B l G O J 1 V + o m m M S Y T P K I 9 S y U W V P t p f u w M H V l l i M J I 2 Z I G 5 e r P i R Q L r a c i s J 0 C m 7 H + 6 2 X i f 1 4 v M e G l n z I Z J 4 Z K M l 8 U J h y Z C G W f o y F T l B g + t Q Q T x e y t i I y x w s T Y f C p 5 C F c Z z r 9 f X i T t k 7 p 3 W j + 9 O 6 s 1 r o s 4 y n A A h 3 A M H l x A A 2 6 h C S 0 g w O A R n u H F k c 6 T 8 + q 8 z V t L T j G z D 7 / g v H 8 B G B 2 P A g = = &lt; / l a t e x i t &gt; (c) Consistency Regularized Training X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 3 w 2 Z 8 a Z F k 7 W D 7 m c o W b h g C 9 I e R g = " &gt; A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R W f O y K b l x W s A 9 s Q 5 l M J + 3 Q y S T M T I Q S 8 h d u X C j i 1 r 9 x 5 9 8 4 S Y O o 9 c D A 4 Z x 7 m X O P F 3 G m t G 1 / W q W l 5 Z X V t f J 6 Z W N z a 3 u n u r v X U W E s C W 2 T k I e y 5 2 F F O R O 0 r Z n m t B d J i g O P 0 6 4 3 v c 7 8 7 g O V i o X i T s 8 i 6 g Z 4 L J j P C N Z G u h 8 E W E 8 8 P + m l w 2 r N r t s 5 0 C J x C l K D A q 1 h 9 W M w C k k c U K E J x 0 r 1 H T v S b o K l Z o T T t D K I F Y 0 w m e I x 7 R s q c E C V m + S J U 3 R k l B H y Q 2 m e 0 C h X f 2 4 k O F B q F n h m M k u o / n q Z + J / X j 7 V / 4 S Z M R L G m g s w / 8 m O O d I i y 8 9 G I S U o 0 n x m C i W Q m K y I T L D H R p q R K X s J l h r P v k x d J 5 6 T u N O q N 2 9 N a 8 6 q o o w w H c A j H 4 M A 5 N O E G W t A G A g I e 4 R l e L G U 9 W a / W 2 3 y 0 Z B U 7 + / A L 1 v s X 5 W G R M w = = &lt; / l a t e x i t &gt; e X (1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C o E 5 M K 4 n 1 Z i S 1 h b x 8 1 Y B C j U 0 q U Y = " &gt; A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q E t d B I t Q N y W x 4 m N X d O O y g n 1 A E 8 t k c t M O n T y Y m S g l Z O P G X 3 H j Q h G 3 / o M 7 / 8 Y k D a L W A x c O 5 9 z L v f f Y I a N C 6 v q n U p q b X 1 h c K i 9 X V l b X 1 j f U z a 2 O C C J O o E 0 C F v C e j Q U w 6 k N b U s m g F 3 L A n s 2 g a 4 8 v M r 9 7 C 1 z Q w L + W k x A s D w 9 9 6 l K C Z S o N 1 F 3 z j j o g K X M g N j 0 s R 7 Y b 9 5 L k J q 4 Z B 8 l A r e p 1 P Y c 2 S 4 y C V F G B 1 k D 9 M J 2 A R B 7 4 k j A s R N / Q Q 2 n F m E t K G C Q V M x I Q Y j L G Q + i n 1 M c e C C v O v 0 i 0 / V R x N D f g a f l S y 9 W f E z H 2 h J h 4 d t q Z H S r + e p n 4 n 9 e P p H t q x d Q P I w k + m S 5 y I 6 b J Q M s i 0 R z K g U g 2 S Q k m n K a 3 a m S E O S Y y D a 6 S h 3 C W 4 f j 7 5 V n S O a w b j X r j 6 q j a P C / i K K M d t I d q y E A n q I k u U Q u 1 E U H 3 6 B E 9 o x f l Q X l S X p W 3 a W t J K W a 2 0 S 8 o 7 1 / 4 z p j / &lt; / l a t e x i t &gt; X (S) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g Y Q / / M N Q U A P / W D b V t X b 8 7 N a 2 V U I = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u B o t Q N y W x 4 m N X d O O y o n 1 A E 8 t k O m m H T j J h Z i K U k I U b f 8 W N C 0 X c + h H u / B s n a R G 1 H h g 4 n H M P c + / x I k a l s q x P o 7 C w u L S 8 U l w t r a 1 v b G 6 Z 2 z t t y W O B S Q t z x k X X Q 5 I w G p K W o o q R b i Q I C j x G O t 7 4 I v M 7 d 0 R I y s M b N Y m I G 6 B h S H 2 K k d J S 3 y w 7 X N t Z O n E C p E a e n 3 T T 9 D a p X h + k f b N i 1 a w c c J 7 Y M 1 I B M z T 7 5 o c z 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " / 1 P S Z 9 d A l J U k r V s y / c T g u f C C 0 + 4 = " &gt; A A A C B H i c b V D L S s N A F J 3 4 r P U V d d n N Y B H q p i R W f O y K b l x W s A 9 o Y p l M J + 3 Q S S b M T I Q S s n D j r 7 h x o Y h b P 8 K d f + M k D a L W A w O H c + 5 h 7 j 1 e x K h U l v V p L C w u L a + s l t b K 6 x u b W 9 v m z m 5 H 8 l h g 0 s a c c d H z k C S M h q S t q G K k F w m C A o + R r j e 5 z P z u H R G S 8 v B G T S P i B m g U U p 9 i p L Q 0 M C s O 1 3 a W T p w A q b H n J 7 0 0 v U 1 q 9 m E 6 M K t W 3 c o B 5 4 l d k C o o 0 B q Y H 8 6 Q 4 z g g o c I M S d m 3 r U i 5 C R K K Y k b S s h N L E i E 8 Q S P S 1 z R E A Z F u k h + R w g O t D K H P h X 6 h g r n 6 M 5 G g Q M p p 4 O n J b F H 5 1 8 v E / 7 x + r P w z N 6 F h F C s S 4 t l H f s y g 4 j B r B A 6 p I F i x q S Y I C 6 p 3 h X i M B M J K 9 1 b O S z j P c P J 9 8 j z p H N X t R r 1 x f V x t X h R 1 l E A F 7 I M a s M E p a I I r 0 A J t g M E 9 e A T P 4 M V 4 M J 6 M V + N t N r p g F J k 9 8 A v G + x d G J J i e &lt; / l a t e x i t &gt; e X (S) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b N 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>u p R g m U p 9 d c e 8 o w 5 I y h y I T Q / L o e 3 G 3 S S 5 i a t X + 0 l f r e g 1 P Y c 2 T Y y C V F C B Z l / 9 M J 2 A R B 7 4 k j A s R M / Q Q 2 n F m E t K G C R l M x I Q Y j L C A + i l 1 M c e C C v O v 0 i 0 v V R x N D f g a f l S y 9 W f E z H 2 h B h 7 d t q Z H S r + e p n 4 n 9 e L p H t i x d Q P I w k + m S x y I 6 b J Q M s i 0 R z K g U g 2 T g k m n K a 3 a m S I O S Y y D a 6 c h 3 C a 4 e j 7 5 W n S P q g Z 9 V r 9 8 r D S O C v i K K F t t I u q y E D H q I E u U B O 1 E E H 3 6 B E 9 o x f l Q X l S X p W 3 S e u M U s x s o V 9 Q 3 r 8 A L K m Z I Q = = &lt; / l a t e x i t &gt; e Z (S) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R V C O r Z 0 z R m d x e P c z q p + / N 9 s T 5 Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>s o t u b 4 PFigure 1 :</head><label>41</label><figDesc>C l P a p n 6 c y L C r h B j 1 0 o 6 0 0 P F X y 8 V / / N 6 o X S O z Y h 6 Q S j B I 5 N F T s g 0 6 W t p J J p N O R D J x g n B h N P k V o 0 M M c d E J s G V s h B O U h x + v z x N 2 v v V W r 1 a v z g o N 0 7 z O I p o C + 2 g C q q h I 9 R A 5 6 i J W o i g e / S I n t G L 8 q A 8 K a / K 2 6 S 1 o O Q z m + g X l P c v L 8 G Z I w = = &lt; / l a t e x i t &gt; e Z (1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g J b N T j C X 7 w n 2 L 9 p f t 4 W h 3 T U n L e c = " &gt; A A A C B X i c b V D L S s N A F J 3 4 r P U V d a m L Y B H q p i R W f O y K b l x W s A 9 s Y p l M b t q h k w c z E 6 W E b N z 4 K 2 5 c K O L W f 3 D n 3 5 i k Q d R 6 4 M L h n H u 5 9 x 4 7 Z F R I X f 9 U Z m b n 5 h c W S 0 v l 5 Z X V t X V 1 Y 7 M t g o g T a J G A B b x r Y w G M + t C S V D L o h h y w Z z P o 2 K P z z O / c A h c 0 8 K / k O A T L w w O f u p R g m U p 9 d c e 8 o w 5 I y h y I T Q / L o e 3 G 1 0 l y E 1 e N / a S v V v S a n k O b J k Z B K q h A s 6 9 + m E 5 A I g 9 8 S R g W o m f o o b R i z C U l D J K y G Q k I M R n h A f R S 6 m M P h B X n X y T a X q o 4 m h v w t H y p 5 e r P i R h 7 Q o w 9 O + 3 M D h V / v U z 8 z + t F 0 j 2 x Y u q H k Q S f T B a 5 E d N k o G W R a A 7 l Q C Q b p w Q T T t N b N T L E H B O Z B l f O Q z j N c P T 9 8 j R p H 9 S M e q 1 + e V h p n B V x l N A 2 2 k V V Z K B j 1 E A X q I l a i K B 7 9 I i e 0 Y v y o D w p r 8 r b p H V G K W a 2 0 C 8 o 7 1 / 7 5 p k B &lt; / l a t e x i t &gt; Illustration of GRAND with DropNode as the perturbation method. GRAND designs random propagation (a) to generate multiple graph data augmentations (b), which are further used as consistency regularization (c) for semi-supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>LossFigure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " t Y Y 2 D Q D t d 7 Y A k v b i A D 5 Z u 1 H L o o o = " &gt; A A A B 9 X i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V V I V H 7 u i C 1 1 W s A 9 o Y 5 l M J u 3 Q y S T M 3 K g l 9 D / c u F D E r f / i z r 9 x k g Z R 6 4 E L h 3 P u 5 d 5 7 3 E h w D b b 9 a R X m 5 h c W l 4 r L p Z X V t f W N 8 u Z W S 4 e x o q x J Q x G q j k s 0 E 1 y y J n A Q r B M p R g J X s L Y 7 u k j 9 9 h 1 T m o f y B s Y R c w I y k N z n l I C R b n v A H k D T 5 F I R 6 U 3 6 5 Y p d t T P g W V L L S Q X l a P T L H z 0 v p H H A J F B B t O 7 W 7 A i c h C j g V L B J q R d r F h E 6 I g P W N V S S g G k n y a 6 e 4 D 2 j e N g P l S k J O F N / T i Q k 0 H o c u K Y z I D D U f 7 1 U / M / r x u C f O g m X U Q x M 0 u k i P x Y Y Q p x G g D 2 u G A U x N o R Q x c 2 t m A 6 J I h R M U K U s h L M U x 9 8 v z 5 L W Q b V 2 W D 2 8 P q r U z / M 4 i m g H 7 a J 9 V E M n q I 6 u U A M 1 E U U K P a J n 9 G L d W 0 / W q / U 2 b S 1 Y + c w 2 + g X r / Q s v 1 p M Y &lt; / l a t e x i t &gt; Loss Loss Generalization on Cora (x: epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Robustness Analysis on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Over-Smoothing on Cora</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Parameter sensitivity of λ and δ on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>GRAND vs. GRAND_dropout. Over-smoothing: GRAND vs. GRAND_GCN &amp; GRAND_GAT on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN [24]</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell></row><row><cell>GAT [41]</cell><cell>83.0±0.7</cell><cell>72.5±0.7</cell><cell>79.0±0.3</cell></row><row><cell>APPNP [25]</cell><cell>83.8±0.3</cell><cell>71.6± 0.5</cell><cell>79.7 ± 0.3</cell></row><row><cell>Graph U-Net [13]</cell><cell>84.4±0.6</cell><cell>73.2±0.5</cell><cell>79.6±0.2</cell></row><row><cell>SGC [45]</cell><cell>81.0 ±0.0</cell><cell>71.9 ± 0.1</cell><cell>78.9 ± 0.0</cell></row><row><cell>MixHop [1]</cell><cell>81.9± 0.4</cell><cell>71.4±0.8</cell><cell>80.8±0.6</cell></row><row><cell>GMNN [36]</cell><cell>83.7</cell><cell>72.9</cell><cell>81.8</cell></row><row><cell>GraphNAS [14]</cell><cell>84.2±1.0</cell><cell>73.1±0.9</cell><cell>79.6±0.4</cell></row><row><cell>GraphSAGE [19]</cell><cell>78.9±0.8</cell><cell>67.4±0.7</cell><cell>77.8±0.6</cell></row><row><cell>FastGCN [8]</cell><cell>81.4±0.5</cell><cell>68.8±0.9</cell><cell>77.6±0.5</cell></row><row><cell>VBAT [10]</cell><cell>83.6±0.5</cell><cell>74.0±0.6</cell><cell>79.9±0.4</cell></row><row><cell>G 3 NN [29]</cell><cell>82.5±0.2</cell><cell>74.4±0.3</cell><cell>77.9 ±0.4</cell></row><row><cell>GraphMix [42]</cell><cell>83.9±0.6</cell><cell>74.5±0.6</cell><cell>81.0±0.6</cell></row><row><cell>DropEdge [37]</cell><cell>82.8</cell><cell>72.3</cell><cell>79.6</cell></row><row><cell>GRAND_dropout</cell><cell>84.9±0.4</cell><cell>75.0±0.3</cell><cell>81.7±1.0</cell></row><row><cell>GRAND_DropEdge</cell><cell>84.5±0.3</cell><cell>74.4±0.4</cell><cell>80.9±0.9</cell></row><row><cell>GRAND_GCN</cell><cell>84.5±0.3</cell><cell>74.2±0.3</cell><cell>80.0±0.3</cell></row><row><cell>GRAND_GAT</cell><cell>84.3±0.4</cell><cell>73.2± 0.4</cell><cell>79.2±0.6</cell></row><row><cell>GRAND</cell><cell>85.4±0.4</cell><cell>75.4±0.4</cell><cell>82.7±0.6</cell></row><row><cell>w/o CR</cell><cell>84.4±0.5</cell><cell>73.1±0.6</cell><cell>80.9±0.8</cell></row><row><cell>w/o mDN</cell><cell>84.7±0.4</cell><cell>74.8±0.4</cell><cell>81.0±1.1</cell></row><row><cell>w/o sharpening</cell><cell>84.6±0.4</cell><cell>72.2±0.6</cell><cell>81.6±0.8</cell></row><row><cell>w/o CR &amp; DN</cell><cell>83.2±0.5</cell><cell>70.3±0.6</cell><cell>78.5±1.4</cell></row></table><note>Overall classification accuracy (%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Benchmark Dataset statistics.</figDesc><table><row><cell cols="5">Dataset Nodes Edges Train/Valid/Test Nodes Classes Features</cell></row><row><cell>Cora</cell><cell>2,708 5,429</cell><cell>140/500/1,000</cell><cell>7</cell><cell>1,433</cell></row><row><cell cols="2">Citeseer 3,327 4,732</cell><cell>120/500/1,000</cell><cell>6</cell><cell>3,703</cell></row><row><cell cols="2">Pubmed 19,717 44,338</cell><cell>60/500/1,000</cell><cell>3</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters of GRAND for results inTable 1</figDesc><table><row><cell>Hyperparameter</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>DropNode probability δ</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Propagation step K</cell><cell>8</cell><cell>2</cell><cell>5</cell></row><row><cell>Data augmentation times S</cell><cell>4</cell><cell>2</cell><cell>4</cell></row><row><cell>CR loss coefficient λ</cell><cell>1.0</cell><cell>0.7</cell><cell>1.0</cell></row><row><cell>Sharpening temperature T</cell><cell>0.5</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell>Learning rate</cell><cell>0.01</cell><cell>0.01</cell><cell>0.2</cell></row><row><cell>Early stopping patience</cell><cell>200</cell><cell>200</cell><cell>100</cell></row><row><cell>Hidden layer size</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>L2 weight decay rate</cell><cell>5e-4</cell><cell>5e-4</cell><cell>5e-4</cell></row><row><cell>Dropout rate in input layer</cell><cell>0.5</cell><cell>0.0</cell><cell>0.6</cell></row><row><cell>Dropout rate in hidden layer</cell><cell>0.5</cell><cell>0.2</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Statistics of Large Datasets.</figDesc><table><row><cell></cell><cell cols="2">Classes Features</cell><cell>Nodes</cell><cell>Edges</cell></row><row><cell>Cora-Full</cell><cell>67</cell><cell>8,710</cell><cell>18,703</cell><cell>62,421</cell></row><row><cell>Coauthor CS</cell><cell>15</cell><cell>6,805</cell><cell>18,333</cell><cell>81,894</cell></row><row><cell>Coauthor Physics</cell><cell>5</cell><cell>8,415</cell><cell>34,493</cell><cell>247,962</cell></row><row><cell>Aminer CS</cell><cell>18</cell><cell cols="3">100 593,486 6,217,004</cell></row><row><cell>Amazon Computers</cell><cell>10</cell><cell>767</cell><cell>13,381</cell><cell>245,778</cell></row><row><cell>Amazon Photo</cell><cell>8</cell><cell>745</cell><cell>7,487</cell><cell>119,043</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on large datasets. ± 0.6 91.1 ± 0.5 92.8 ± 1.0 82.6 ± 2.4 91.2 ± 1.2 49.9 ± 2.0 GAT 51.9 ± 1.5 90.5 ± 0.6 92.5 ± 0.9 78.0 ± 19.0 85.7 ± 20.3 49.6 ± 1.7 GRAND 63.5 ±0.6 92.9 ± 0.5 94.6 ± 0.5 85.7 ± 1.8 92.5 ± 1.7 52.8 ± 1.2</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell>Cora Full</cell><cell>Coauthor CS</cell><cell>Coauthor Physics</cell><cell cols="2">Amazon Computer</cell><cell>Amazon Photo</cell><cell>Aminer CS</cell></row><row><cell>Time (ms)</cell><cell>10 15 20 25 30 35 40 45 5</cell><cell cols="3">2-layer GCN 2-layer GAT Grand, S=1 Grand, S=2 Grand, S=3 Grand, S=4 GCN 62.2 1 2 3 4 5 6 7 8 9 10</cell><cell>Accuracy</cell><cell>0.76 0.78 0.80 0.82 0.84 0.86 0.74</cell><cell>1 2 3 4 5 6 7 8 9 10 2-layer GCN 2-layer GAT Grand, S=1 Grand, S=2 Grand, S=3 Grand, S=4</cell></row><row><cell></cell><cell></cell><cell cols="3">Propagation Step K</cell><cell></cell><cell></cell><cell>Propagation Step K</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) Per-epoch Training Time</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Classification Accuracy under different label rates (%). 8±5.3 76.1±1.9 79.6±2.1 63.4±2.9 70.6±1.7 72.2±1.1 71.5±2.1 77.5±1.8 80.8±1.5 GAT 64.3±5.8 77.2±2.4 80.8±2.1 64.4±2.9 70.4±1.9 72.0±1.3 72.0±2.1 77.6±1.6 80.6±1.2 GRAND 69.1±4.0 79.5±2.2 83.0±1.6 65.3±3.3 72.3±1.8 73.8±0.9 74.7±3.4 81.4±2.1 83.8±1.3</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Pubmed</cell><cell></cell></row><row><cell>Label Rate</cell><cell>1%</cell><cell>3%</cell><cell>5%</cell><cell>1%</cell><cell>3%</cell><cell>5%</cell><cell>0.1%</cell><cell>0.3%</cell><cell>0.5%</cell></row><row><cell>GCN</cell><cell>62.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/danielzuegner/gnn-meta-attack</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/shchur/gnn-benchmark</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Greg Ver Steeg, and Aram Galstyan</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;20</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Graph u-nets. ICML&apos;19</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<title level="m">Graph neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR&apos;14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Snap: A general-purpose network analysis and graph-mining library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rok</forename><surname>Sosič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman Akoglu Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns. ICLR&apos;20</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A flexible generative framework for graphbased semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmnn</surname></persName>
		</author>
		<title level="m">Graph markov neural networks. ICML&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. ICLR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning. ICLR&apos;19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
