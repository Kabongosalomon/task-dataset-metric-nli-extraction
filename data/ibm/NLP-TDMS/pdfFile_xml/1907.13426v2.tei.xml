<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Expectation-Maximization Attention Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
							<email>zszhong@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Academy for Advanced Interdisciplinary Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<email>hongliu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Expectation-Maximization Attention Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PAS-CAL VOC, PASCAL Context and COCO Stuff, on which we set new records 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental and challenging problem of computer vision, whose goal is to assign a semantic category to each pixel of the image. It is critical for various tasks such as autonomous driving, image editing and robot sensing. In order to accomplish the semantic segmentation task effectively, we need to distinguish some confusing categories and take the appearance of different objects into account. For example, 'grass' and 'ground' have similar color in some cases and 'person' may have various scales, figures and clothes in different locations of the image. Meanwhile, the label space of the output is quite com- <ref type="bibr" target="#b0">1</ref>   pact and the amount of the categories for a specific dataset is limited. Therefore, this task can be treated as projecting data points in a high-dimensional noisy space into a compact sub-space. The essence lies in de-noising these variation and capturing the most important semantic concepts.</p><p>Recently, many state-of-the-art methods based on fully convolutional networks (FCNs) <ref type="bibr" target="#b21">[22]</ref> have been proposed to address the above issues. Due to the fixed geometric structures, they are inherently limited by local receptive fields and short-range contextual information. To capture longrange dependencies, several works employ the multi-scale context fusion <ref type="bibr" target="#b16">[17]</ref>, such as astrous convolution <ref type="bibr" target="#b3">[4]</ref>, spatial pyramid <ref type="bibr" target="#b36">[37]</ref>, large kernel convolution <ref type="bibr" target="#b24">[25]</ref> and so on. Moreover, to keep more detailed information, the encoderdecoder structures <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref> are proposed to fuse mid-level and high-level semantic features. To aggregate information from all spatial locations, attention mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31]</ref> is used, which enables the feature of a single pixel to fuse information from all other positions. However, the original attention-based methods need to generate a large attention map, which has high computation complexity and occupies a huge number of GPU memory. The bottleneck lies in that both the generation of attention map and its usage are computed w.r.t all positions.</p><p>Towards the above issues, in this paper, we rethink the attention mechanism from the view of expectationmaximization (EM) algorithm <ref type="bibr" target="#b6">[7]</ref> and propose a novel attention-based method, namely Expectation-Maximization Attention (EMA). Instead of treating all pixels themselves as the reconstruction bases <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref>, we use the EM algorithm to find a more compact basis set, which can largely reduce the computational complexity. In detail, we regard the bases for construction as the parameters to learn in the EM algorithm and attention maps as latent variables. In this setting, the EM algorithm aims to find a maximum likelihood estimate of parameters (bases). Given the current parameters, the expectation (E) step works as estimating the expectation of attention map and maximization (M) step functions as updating the parameters (bases) by maximizing the complete data likelihood. The E step and the M step execute alternately. After convergence, the output can be computed as the weighted sum of bases, where the weights are the normalized final attention maps. The pipeline of EMA is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We further embed the proposed EMA method into a module for neural network, which is named EMA Unit. EMA Unit can be simply implemented by common operators. It is also light-weighted and can be easily embedded into existing neural networks. Moreover, to make full use of its capacity, we also propose two more methods to stabilize the training process of EMA Unit. We also evaluate its performance on three challenging datasets.</p><p>The main contributions of this paper are listed as follows:</p><p>• We reformulate the self-attention mechanism into an expectation-maximization iteration manner, which can learn a more compact basis set and largely reduce the computational complexity. To the best of our knowledge, this paper is the first to introduce EM iterations into attention mechanism.</p><p>• We build the proposed expectation-maximization attention as a light-weighted module for neural network and set up specific manners for bases' maintenance and normalization.</p><p>• Extensive experiments on three challenging semantic segmentation datasets, including PASCAL VOC, PAS-CAL Context and COCO Stuff, demonstrate the superiority of our approach over other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Semantic segmentation. Fully convolutional network (FCN) <ref type="bibr" target="#b21">[22]</ref> based methods have made great progress in image semantic segmentation by leveraging the powerful convolutional features of classification networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> pre-trained on large-scale data <ref type="bibr" target="#b27">[28]</ref>. Several model variants are proposed to enhance the multi-scale contextual aggregation. For example, DeeplabV2 <ref type="bibr" target="#b3">[4]</ref> makes use of the astrous spatial pyramid pooling (ASPP) to embed contextual information, which consists of parallel dilated convolutions with different dilated rates. DeeplabV3 <ref type="bibr" target="#b3">[4]</ref> extends ASPP with image-level feature to further capture global contexts.</p><p>Meanwhile, PSPNet <ref type="bibr" target="#b36">[37]</ref> proposes a pyramid pooling module to collect contextual information of different scales. GCN <ref type="bibr" target="#b24">[25]</ref> adopts decoupling of large kernel convolution to gain a large receptive field for the feature map and capture long-range information.</p><p>For the other type of variants, they mainly focus on predicting more detailed output. These methods are based on U-Net <ref type="bibr" target="#b26">[27]</ref>, which combines the advantages of high-level features with mid-level features. RefineNet <ref type="bibr" target="#b20">[21]</ref> makes use of the Laplacian image pyramid to explicitly capture the information available along the down-sampling process and output predictions from coarse to fine. DeeplabV3+ <ref type="bibr" target="#b4">[5]</ref> adds a decoder upon DeeplabV3 to refine the segmentation results especially along object boundaries. Exfuse <ref type="bibr" target="#b35">[36]</ref> proposes a new framework to bridge the gap between low-level and high-level features and thus improves the segmentation quality. Attention model. Attention is widely used for various tasks such as machine translation, visual question answering and video classification. The self-attention methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref> calculate the context coding at one position by a weighted summation of embeddings at all positions in sentences. Nonlocal <ref type="bibr" target="#b30">[31]</ref> first adopts self-attention mechanism as a module for computer vision tasks, such as video classification, object detection and instance segmentation. PSANet <ref type="bibr" target="#b37">[38]</ref> learns to aggregate contextual information for each position via a predicted attention map. A 2 Net <ref type="bibr" target="#b5">[6]</ref> proposes the double attention block to distribute and gather informative global features from the entire spatio-temporal space of the images. DANet <ref type="bibr" target="#b10">[11]</ref> applies both spatial and channel attention to gather information around the feature maps, which costs even more computation and memory than the Nonlocal method.</p><p>Our approach is motivated by the success of attention in the above works. We rethink the attention mechanism from the view of the EM algorithm and compute the attention map in an iterative manner as the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Before introducing our proposed method, we first review three highly correlated methods, that is the EM algorithm, the Gaussian mixture model and the Non-local module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Expectation-Maximization Algorithm</head><p>The expectation-maximization (EM) <ref type="bibr" target="#b6">[7]</ref> algorithm aims to find the maximum likelihood solution for latent variables models. Denote X = {x 1 , x 2 , · · · , x N } as the data set which consists of N observed samples and each data point x i has its corresponding latent variable z i . We call {X, Z} the complete data and its likelihood function takes the form ln p (X, Z|θ), where θ is the set of all parameters of the model. In practice, the only knowledge of latent variables in Z is given by the posterior distribution p (Z|X, θ).</p><p>The EM algorithm is designed to maximize the likelihood ln p (X, Z|θ) by two steps, i.e., the E step and the M step.</p><p>In the E step, we use the current parameters θ old to find the posterior distribution of Z given by p (X, Z|θ). Then we use the posterior distribution to find the expectation of the complete data likelihood Q θ, θ old , which is given by:</p><formula xml:id="formula_0">Q θ, θ old = z p Z|X, θ old ln p (X, Z|θ) .<label>(1)</label></formula><p>Then in the M step, the revised parameter θ new is determined by maximizing the function:</p><formula xml:id="formula_1">θ new = arg max θ Q θ, θ old .<label>(2)</label></formula><p>The EM algorithm executes the E step and the M step alternately until the convergence criterion is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gaussian Mixture Model</head><p>Gaussian mixture model (GMM) <ref type="bibr" target="#b25">[26]</ref> is a special case of the EM algorithm. It takes the distribution of data x n as a linear superposition of Gaussians:</p><formula xml:id="formula_2">p (x n ) = K k=1 z nk N (x n |µ k , Σ k ) ,<label>(3)</label></formula><p>where the mean µ k and the covariance Σ k are parameters for the k-th Gaussian basis. Here we leave out the prior π k . The likelihood of the complete data is formulated as:</p><formula xml:id="formula_3">ln p (X, Z|µ, Σ) = N n=1 ln K k=1 z nk N (x n |µ k , Σ k ) ,<label>(4)</label></formula><p>where k z nk = 1. z nk can be viewed as the responsibility that the k-th basis takes for the observation x n . For GMM, in the E step, the expected value of z nk is given by:</p><formula xml:id="formula_4">z new nk = N (x n |µ new k , Σ k ) K j=1 N x n |µ new j , Σ j .<label>(5)</label></formula><p>In the M step, the parameters are re-estimated as follows:</p><formula xml:id="formula_5">µ new k = 1 N k N n=1 z new nk x n , Σ new k = 1 N k N n=1 z new nk x n − µ old k x n − µ old k ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">N k = N n=1 z new nk .<label>(7)</label></formula><p>After the convergence of the GMM parameters, the reestimated x new n can be formulated as:</p><formula xml:id="formula_7">x new n = K k=1 z new nk µ new k .<label>(8)</label></formula><p>In real applications, we can simply replace Σ k as the identity matrix I and leave out the Σ k in the above equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Non-local</head><p>The Non-local module <ref type="bibr" target="#b30">[31]</ref> functions the same as the self-attention mechanism. It can be formulated as:</p><formula xml:id="formula_8">y i = 1 C (x i ) j f (x i , x j ) g (x j ) ,<label>(9)</label></formula><p>where f (·, ·) represents a general kernel function, C (x) is a normalization factor and x i denotes the feature vector for the location i. As this module is applied upon the feature map of convolutional neural networks (CNN).</p><p>Considering that N (x n |µ k , Σ k ) in Eq. <ref type="formula" target="#formula_4">(5)</ref> is a specific kernel function between x n and µ k , Eq. <ref type="formula" target="#formula_7">(8)</ref> is just a specific design of Eq. (9). Then, from the viewpoint of GMM, the Non-local module is just a re-estimation of X, without E steps and M steps. Specifically, µ is just selected as the X in Non-local.</p><p>In GMM, the number of Gaussian bases is selected manually and usually satisfies K N . But in the Non-local module, the bases are selected as the data themselves, so it has K = N . There are two obvious disadvantages of the Non-local module. First, the data are lying in a low dimensional manifold, so the bases are over-complete. Second, the computation overhead is heavy and the memory cost is also large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expectation-Maximization Attention</head><p>In view of the high computational complexity of the attention mechanism and limitations of the Non-local module, we first propose the expectation-maximization attention (EMA) method, which is an augmented version of selfattention. Unlike the Non-local module that selects all data points as bases, we use the EM iterations to find a compact basis set.</p><p>For simplicity, we consider an input feature map X of size C ×H ×W from a single sample. X is the intermediate activations of a CNN. To simplify the symbols, we reshape X into N × C, where N = H × W , and x i ∈ R C indexes the C dimensional feature vector at pixel i. Our proposed EMA consists of three operations, including responsibility estimation (A E ), likelihood maximization (A M ) and data re-estimation (A R ). Briefly, given the input X ∈ R N ×C and the initial bases µ ∈ R K×C , A E estimates the latent variables (or the 'responsibility') Z ∈ R N ×K , so it functions as the E step in the EM algorithm. A M uses the estimation to update the bases µ, which works as the M step.</p><p>The A E and A M steps execute alternately for a pre-specified number of iterations. Then, with the converged µ and Z, A R reconstructs the original X as Y and outputs it.  It has been proved that, with the iteration of EM steps, the complete data likelihood ln p (X, Z) will increase monotonically. As ln p (X) can be estimated by marginalizing ln p (X, Z) with Z, maximizing ln p (X, Z) is a proxy of maximizing ln p (X). Therefore, with the iterations of A E and A M , the updated Z and µ have better ability to reconstruct the original data X. The reconstructedX can capture important semantics from X as much as possible.</p><p>Moreover, compared with the Non-local module, EMA finds a compact set of bases for pixels of an input image. The compactness is non-trivial. Since K N ,X lies in a subspace of X. This mechanism removes much unnecessary noise and makes the final classification of each pixel more tractable. Moreover, this operation reduces the complexity (both in space and time) from O N 2 to O (N KT ), where T is the number of iterations for A E and A M . The convergence of EM algorithm is also guaranteed. Notably, EMA takes only three iterations to get promising results in our experiments. So T can be treated as a small constant, which means that the complexity is only O (N K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Responsibility Estimation</head><p>Responsibility estimation (A E ) functions as the E step in the EM algorithm. This step computes the expected value of z nk , which corresponds to the responsibility of the kth basis µ to x n , where 1 ≤ k ≤ K and 1 ≤ n ≤ N . We formulate the posterior probability of x n given µ k as follows:</p><p>p (x n |µ k ) = K (x n , µ k ) ,</p><p>where K represents the general kernel function. And now, Eq. (5) can be reformulated into a more general form:</p><formula xml:id="formula_10">z nk = K (x n , µ k ) K j=1 K (x n , µ j ) .<label>(11)</label></formula><p>There are several choices for K (a, b), such as inner dot a b, exponential inner dot exp a b , Euclidean distance</p><formula xml:id="formula_11">a − b 2 2 , RBF kernel exp − a − b 2 2</formula><p>/σ 2 and so on. As compared in the Non-local module, the choice of these functions makes trivial differences in the final results. So we simply take the exponential inner dot exp a b in our paper. In experiments, Eq. (11) can be implemented as a matrix multiplication plus one softmax layer. In conclusion, the operation of A E in the t-th iteration is formulated as:</p><formula xml:id="formula_12">Z (t) = softmax λX µ (t−1) ,<label>(12)</label></formula><p>where λ is a hyper-parameter to control the distribution of Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Likelihood Maximization</head><p>Likelihood maximization (A M ) works as the EM algorithm's M step. With the estimated Z, A M updates µ by maximizing the complete data likelihood. To keep the bases lying in the same embedding space as X, we update the bases µ using the weighted summation of X. So µ k is updated as</p><formula xml:id="formula_13">µ (t) k = z (t) nk x n N m=1 z (t) mk (13) in the t-th iteration of A M .</formula><p>It is noteworthy that if we set λ → ∞ in Eq. (12), then {z n1 , z n2 , · · · , z nK } will become a one-hot embedding. In this situation, each pixel is assigned to only one basis. And the basis is updated by the average of those pixels assigned to it. This is what the K-means clustering algorithm <ref type="bibr" target="#b9">[10]</ref> does. So the iterations of A E and A M can also be viewed as a soft version of K-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Re-estimation</head><p>EMA runs A E and A M alternately for T times. After that, the final µ (T ) and Z (T ) are used to re-estimate the X. We adopt Eq. (8) to construct the new X, namelyX, which is formulated as:X</p><formula xml:id="formula_14">= Z (T ) µ (T ) .<label>(14)</label></formula><p>AsX is constructed from a compact basis set, it has the low-rank property compared with the input X. We depict an example ofX in <ref type="figure" target="#fig_2">Fig. 2</ref>. It's obvious thatX outputed from A R is very compact in the feature space and the feature variance inside the object is smaller than that of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EMA Unit</head><p>In order to better incorporate the proposed EMA with deep neural networks, we further propose the Expectationmaximization Attention Unit (EMAU) and apply it to semantic segmentation task. In this section, we will describe EMAU in detail. We first introduce the overall structure of EMAU and then discuss bases' maintenance and normalization mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Structure of EMA Unit</head><p>The overall structure of EMAU is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. EMAU looks like the bottleneck of ResNet at the first glance, except it replaces the heavy 3 × 3 convolution with the EMA operations. The first convolution without the ReLU activation is prepended to transform the value range of input from (0, +∞) to (−∞, +∞). This transformation is very important, or the estimated µ (T ) will also lie in [0, +∞), which halves the capacity compared with general convolution parameters. The last 1 × 1 convolution is inserted to transform the re-estimatedX into the residual space of X.</p><p>For each of A E , A M and A R steps, the computation complexity is O (N KC). As we set K C, several iterations of A E and A M plus one A R is just the same magnitude as a 1 × 1 convolution with input and output channel numbers all being C. Adding the extra computation from two 1 × 1 convolutions, the whole FLOPs of EMAU is around 1/3 of a module running 3 × 3 convolutions with the same number of input and output channels. Moreover, the parameters maintained by EMA just counts to KC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Bases Maintenance</head><p>Another issue for the EM algorithm is the initialization of the bases. The EM algorithm is guaranteed to converge, because the likelihood of complete data is limited, and at each iteration both E and M steps lift its current lower bound. However, converging to global maximum is not guaranteed. Thus, the initial values of bases before iterations are of great importance.</p><p>We only describe how EMA is used to process one image above. However, for a computer vision task, there are thousand of images in a dataset. As each image X has different pixel feature distributions from others, it is not suitable to use the µ computed upon an image to reconstruct feature maps of other images. So we run EMA on each image.</p><p>For the first mini-batch, we initialize µ (0) using Kaiming's initialization <ref type="bibr" target="#b12">[13]</ref>, where we treat matrix multiplication as a 1 × 1 convolution. For the following mini-batches, one simple choice is to update µ (0) using standard back propagation. However, as iterations of A E and A M can be unrolled as a recurrent neural network (RNN), the gradients propagating though them will encounter the vanishing or explosion problem. Therefore, the updating of µ (0) is unstable, and the training procedure of EMA Unit may collapse.</p><p>In this paper, we use the moving averaging to update µ (0) in the training process. After iterating over an image, the generated µ (T ) can be regarded as a biased update of µ (0) , where the bias comes from the image sampling process. To make it less biased, we first average µ (T ) over a mini-batch and get theμ (T ) . Then we update µ (0) as:</p><formula xml:id="formula_15">µ (0) ← αµ (0) + (1 − α)μ (T ) ,<label>(15)</label></formula><p>where α ∈ [0, 1] is the momentum. For inference, the µ (0) keeps fixed. This moving averaging mechanism is also adopted in batch normalization (BN) <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Bases Normalization</head><p>In the above subsection, we accomplish the maintenance of µ (0) for each mini-batch. However, the stable update of µ (t) inside A E and A M iterations is still not guaranteed, due to the defect of RNN. The moving averaging mechanism described above requiresμ (T ) not to differ significantly from µ (0) , otherwise it will also collapse like backpropagation. This requirement also constrains the value range of µ (t) , 1 ≤ t ≤ T .</p><p>To this end, we need to apply normalization upon µ <ref type="bibr">(t)</ref> . At the first glance, BN or layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> sound to be good choices. However, these aforementioned normalization methods will change the direction of each basis µ       where sfm represents the softmax function. φ, θ and ρ represent three 1×1 convolutions with convolution kernels W φ , W θ and W ρ , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Compare with the Double Attention Block</head><formula xml:id="formula_16">Y = φ(X, W φ ) sfm(θ (X, W θ )) sfm(ρ (X, W ρ )),<label>(</label></formula><p>If we share parameters between θ and ρ, then we can mark both W θ and W ρ as µ. We can see that sfm (θ (X, W θ )) just computes Z the same as Eq. <ref type="formula" target="#formula_4">(5)</ref> and those variables lying inside [·] update µ. The whole process of A 2 block equals to EMA with only one iteration. The W θ in A 2 block is updated by the back-propagation, while our EMAU is updated by moving averaging. Above all, double attention block can be treated as a special form of EMAU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To evaluate the proposed EMAU, we conduct extensive experiments on the PASCAL VOC dataset <ref type="bibr" target="#b8">[9]</ref>, the PASCAL Context dataset <ref type="bibr" target="#b23">[24]</ref>, and the COCO Stuff dataset <ref type="bibr" target="#b2">[3]</ref>. In this section, we first introduce implementation details. Then we perform ablation study to verify the superiority of proposed method on the PASCAL VOC dataset. Finally, we report our results on the PASCAL Context dataset and the COCO Stuff dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>We use ResNet <ref type="bibr" target="#b13">[14]</ref> (pretrained on ImageNet <ref type="bibr" target="#b27">[28]</ref>) as our backbone. Following prior works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, we employ a poly learning rate policy where the initial learning rate is multiplied by (1 − iter/total iter) 0.9 after each iteration. The initial learning rate is set to be 0.009 for all datasets. Momentum and weight decay coefficients are set to 0.9 and 0.0001, respectively. For data augmentation, we apply the common scale (0.5 to 2.0), cropping and flipping of the image to augment the training data. Input size for all datasets is set to 513 × 513. The synchronized batch normalization is adopted in all experiments, together with the multi-grid <ref type="bibr" target="#b3">[4]</ref>. For evaluation, we adopt the commonly used Mean IoU metric. The output stride of the backbone is set to 16 for training on PASCAL VOC and PASCAL Context, and 8 for training on COCO Stuff and evaluating on all datasets. To speed up the training procedure, we carry out all ablation studies on ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, with batch size 12. For all models to be  compared with state-of-the-art, we train them on ResNet-101, with batch size 16. We train 30K iterations on PAS-CAL VOC and COCO Stuff, and 15K on PASCAL Context. We use a 3 × 3 convolution to reduce the channel number from 2, 048 to 512, and then stack EMAU upon it. We call the whole network as EMANet. We set the basis number K = 64, λ = 1 and the number of iterations T = 3 for training as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results on the PASCAL VOC Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Bases Maintenance and Normalization</head><p>In this part, we first compare different strategies of maintaining µ (0) . We set T = 3 in training, and 1 ≤ T ≤ 8 in evaluation. As shown in the left part of <ref type="figure" target="#fig_8">Fig. 3</ref>, performance of all strategies increases with more iterations of A E and A M . When T ≥ 4, the gain from more iterations becomes marginal. Moving average performs the best among them. It achieves the highest performances in all iterations and surpasses others by at least 0.9 in mIoU. Surprisingly, updating by the back propagation shows no merit compared with no updating and even performs worse when T ≥ 3.</p><p>We then compare the performances with no normalization, LN and L2Norm as described above. From the right part of <ref type="figure" target="#fig_8">Fig. 3</ref>, it is clear to see that LN is even worse than no normalization. Since it can partially relieve the gradient chores of RNN-like structure. The performance of LN and no normalization has little correlation with the number of iteration T . By contrast, L2Norm's performance increases as the iterations become larger and it outperforms LN and no normalization when T ≥ 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Ablation Study for Iteration Number</head><p>From <ref type="figure" target="#fig_8">Fig. 3</ref>, it is obvious that the performance of EMAU gain from more iterations during evaluation, and the gain becomes marginal when T &gt; 4. In this subsection, we also study the influence of T in training. We plot the performance matrix upon T train and T eval as <ref type="figure" target="#fig_9">Fig. 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU (%) Wide ResNet <ref type="bibr" target="#b31">[32]</ref> WideResNet-38 84.9 PSPNet <ref type="bibr" target="#b36">[37]</ref> ResNet-101 85.4 DeeplabV3 <ref type="bibr" target="#b3">[4]</ref> ResNet-101 85.7 PSANet <ref type="bibr" target="#b37">[38]</ref> ResNet-101 85.7 EncNet <ref type="bibr" target="#b34">[35]</ref> ResNet-101 85.9 DFN <ref type="bibr" target="#b33">[34]</ref> ResNet-101 86.2 Exfuse <ref type="bibr" target="#b35">[36]</ref> ResNet-101 86.2 IDW-CNN <ref type="bibr" target="#b29">[30]</ref> ResNet-101 86.3 SDN <ref type="bibr" target="#b11">[12]</ref> DenseNet-161 86.6 DIS <ref type="bibr" target="#b22">[23]</ref> ResNet-101 86.8 EMANet</p><p>ResNet-101 87.7 GCN <ref type="bibr" target="#b24">[25]</ref> ResNet-152 83.6 RefineNet <ref type="bibr" target="#b20">[21]</ref> ResNet-152 84.2 DeeplabV3+ <ref type="bibr" target="#b4">[5]</ref> Xception-71 87.8 Exfuse <ref type="bibr" target="#b35">[36]</ref> ResNeXt-131 87.9 MSCI <ref type="bibr" target="#b19">[20]</ref> ResNet-152 88.0 EMANet</p><p>ResNet-152 88.2</p><p>From <ref type="figure" target="#fig_9">Fig. 4</ref>, it is clear that mIoU increases monotonically with more iterations in evaluation, no matter what T train is. They finally converge to a fixed value. However, this rule does not work in training. The mIoUs peak when T train = 3 and decrease with more iterations. This phenomenon may be caused by the RNN-like behavior of EMAU. Though Moving Average and L2Norm can relieve to a certain degree, the problem persists.</p><p>We also carry out experiments on A 2 block <ref type="bibr" target="#b5">[6]</ref>, which can be regarded as a special form of EMAU as mentioned in Sec. 5.4. Similarly, the non-local module can also be viewed as a special form of EMAU without A M step, which includes more bases and T train = 1. With the same backbone and training scheduler, A 2 block achieves 77.41% and the non-local module achieves 77.78% in mIoU, respectively. As a comparison, EMANet achieves 77.34% when </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU (%) PSPNet <ref type="bibr" target="#b36">[37]</ref> ResNet-101 47.8 DANet <ref type="bibr" target="#b10">[11]</ref> ResNet-50 50.1 MSCI <ref type="bibr" target="#b19">[20]</ref> ResNet-152 50.3 EMANet</p><p>ResNet-50 50.5 SGR <ref type="bibr" target="#b17">[18]</ref> ResNet-101 50.8 CCL <ref type="bibr" target="#b7">[8]</ref> ResNet-101 51.6 EncNet <ref type="bibr" target="#b34">[35]</ref> ResNet-101 51.7 SGR+ <ref type="bibr" target="#b17">[18]</ref> ResNet-101 52.5 DANet <ref type="bibr" target="#b10">[11]</ref> ResNet-101 52.6 EMANet</p><p>ResNet-101 53.1 T train = 1 and T eval = 1. These three results have small differences, which is coincident with our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Comparisons with State-of-the-arts</head><p>We first thoroughly compare EMANet with three baselines, namely DeeplabV3, DeeplabV3+ and PSANet on the validation set. We report mIoU, FLOPs, memory cost and parameter numbers in Tab. 1. We can see that EMANet outperforms these three baselines by a large margin. Moreover, EMANet is much lighter in computation and memory. We further compare our method with existing methods on the PASCAL VOC test set. Following previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, we train EMANet successively over COCO, the VOC trainaug and the VOC trainval set. We set the base learning rate as 0.009, 0.001 and 0.0001, respectively. We train 150K iterations on COCO, and 30K for the last two rounds. When inferring over the test set, we make use of multi-scale testing and left-right flipping.</p><p>As shown in Tab. 2, our EMANet sets the new record on PASCAL VOC, and improves DeeplabV3 <ref type="bibr" target="#b3">[4]</ref> with the same backbone by 2.0% in mIoU. Our EMANet achieves the best performance among networks with backbone ResNet-101, and outperforms the previous best one by 0.9%, which is significant due to the fact that this benchmark is very competitive. Moreover, it achieves the performance that is comparable with methods based on some larger backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Label z·i z·j z ·k z ·l <ref type="figure">Figure 5</ref>: Visualization of responsibilities Z at the last iteration. The first two rows illustrate two examples from the PASCAL VOC validation set. The last two rows illustrate two examples from the PASCAL Context validation set. z ·i represents the responsibilities of the i-th basis to all pixels in the last iteration, i, j, k and l are four randomly selected indexes, where 1 ≤ i, j, k, l ≤ K. Best viewed on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results on the PASCAL Context Dataset</head><p>To verify the generalization of our proposed EMANet, we conduct experiments on the PASCAL Context dataset. Quantitative results of PASCAL Context are shown in Tab. 3. To the best of our knowledge, EMANet based on ResNet-101 achieves the highest performance on the PASCAL Context dataset. Even pretrained on additional data (COCO Stuff), SGR+ is still inferior to EMANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results on the COCO Stuff Dataset</head><p>To further evaluate the effectiveness of our method, we also carry out experiments on the COCO Stuff dataset. Comparisons with previous state-of-the-art methods are shown in Tab. 4. Remarkably, EMANet achieves 39.9% in mIoU and outperforms previous methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Visualization of Bases Responsibilities</head><p>To get a deeper understanding of our proposed EMAU, we visualize the iterated responsibility map Z in <ref type="figure">Fig. 5</ref>. For each image, we randomly select four bases (i, j, k and l) and show their corresponding responsibilities of all pixels in the last iteration. Obviously, each basis corresponds to an abstract concept of the image. With the progress of iterations A E and A M , the abstract concept becomes more compact and clear. As we can see, these bases converge to some specific semantics and do not just focus on foreground and background. Concretely, the bases of the first two rows focus on specific semantics such as human, wine glass, cutlery and profile. The bases of the last two rows focus on sailboat, mountain, airplane and lane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a new type of attention mechanism, namely the expectation-maximization attention (EMA), which computes a more compact basis set by iteratively executing as the EM algorithm. The reconstructed output of EMA is low-rank and robust to the variance of input. We well formulate the proposed method as a light-weighted module that can be easily inserted to existing CNNs with little overhead. Extensive experiments on a number of benchmark datasets demonstrate the effectiveness and efficiency of the proposed EMAU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pipeline of the proposed expectationmaximization attention method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overall structure of the proposed EMAU. The key component is the EMA operator, in which A E and A M execute alternately. In addition to the EMA operator, we add two 1 × 1 convolutions at the beginning and the end of EMA and sum the output with original input, to form a residual-like block. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>k</head><label></label><figDesc>, which changes their properties and semantic meanings. To keep the direction of each basis untouched, we choose Euclidean normalization (L2Norm), which divides each µ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>k</head><label></label><figDesc>by its length. By applying it, µ (t) then lies in a K-dimensional united hyper-sphere, and sequence of µ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A 2</head><label>2</label><figDesc>Net<ref type="bibr" target="#b5">[6]</ref> proposes the double attention block (A 2 block), in which the output Y is computed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study on strategy of bases maintenance (left) and normalization (right) of EMAU. Experiments are carried out upon ResNet-50 with batch size 12 and training output stride 16 on the PASCAL VOC dataset. The iteration number T for training is set as 3. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Ablation study on the iteration number T . Experiments are conducted upon ResNet-50 with training output stride 16 and batch size 12 on the PASCAL VOC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Project address: https://xialipku.github.io/EMANet</figDesc><table><row><cell>Input feature maps</cell><cell></cell><cell></cell><cell>Output feature maps</cell></row><row><cell>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Input Feature maps . . . . .</cell><cell>EM</cell><cell>Attention Maps Bases E M</cell><cell>Output Feature maps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>77.34 77.52 77.60 77.59 77.59 77.59 77.59 77.59 2 77.75 78.04 78.15 78.15 78.12 78.12 78.17 3 78.52 78.80 78.86 78.88 78.89 78.88 4 78.14 78.25 78.27 78.28 78.27</figDesc><table><row><cell></cell><cell></cell><cell>66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Evaluation Iterations (mIoU %)</cell></row><row><cell>Training Iterations</cell><cell>1 5 6 7 8</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell cols="2">5 77.70 77.76 77.82 77.86 6 7 8 77.85 77.91 77.92 77.11 77.14 77.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Detailed comparisons on PASCAL VOC with DeeplabV3/V3+ and PSANet in mIoU (%). All results are achieved with the backbone ResNet-101 and output stride 8. The FLOPs and memory are computed with the input size 513 × 513. SS: Single scale input during test. MS: Multi-scale input. Flip: Adding left-right flipped input. EMANet (256) and EMANet (512) represent EMANet with the number of input channels as 256 and 512, respectively.</figDesc><table><row><cell>Method</cell><cell cols="3">SS MS+Flip FLOPs Memory Params</cell></row><row><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>190.6G 2.603G 42.6M</cell></row><row><cell cols="4">DeeplabV3 [4] 78.51 79.77 +63.4G +66.0M +15.5M</cell></row><row><cell cols="4">DeeplabV3+ [5] 79.35 80.57 +84.1G +99.3M +16.3M</cell></row><row><cell>PSANet [38]</cell><cell cols="3">78.51 79.77 +56.3G +59.4M +18.5M</cell></row><row><cell cols="4">EMANet (256) 79.73 80.94 +21.1G +12.3M +4.87M</cell></row><row><cell cols="4">EMANet (512) 80.05 81.32 +43.1G +22.1M +10.0M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on the PASCAL VOC test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with state-of-the-art on the PASCAL Context test set. '+' means pretrained on COCO Stuff.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on the COCO Stuff test set.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU (%)</cell></row><row><cell>RefineNet [21]</cell><cell>ResNet-101</cell><cell>33.6</cell></row><row><cell>CCL [8]</cell><cell>ResNet-101</cell><cell>35.7</cell></row><row><cell>DANet [11]</cell><cell>ResNet-50</cell><cell>37.2</cell></row><row><cell>DSSPN [19]</cell><cell>ResNet-101</cell><cell>37.3</cell></row><row><cell>EMANet</cell><cell>ResNet-50</cell><cell>37.6</cell></row><row><cell>SGR [18]</cell><cell>ResNet-101</cell><cell>39.1</cell></row><row><cell>DANet [11]</cell><cell>ResNet-101</cell><cell>39.7</cell></row><row><cell>EMANet</cell><cell>ResNet-101</cell><cell>39.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency versus interpretability of classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1858" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamicstructured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On bayesian analysis of mixtures with an unknown number of components (with discussion)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: series B (statistical methodology)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="731" to="792" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning object interactions and descriptions for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5859" to="5867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with alternately updated clique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2413" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
