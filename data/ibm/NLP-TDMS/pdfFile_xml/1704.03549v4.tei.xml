<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Extraction of Structured Information from Street View Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gorban</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar-Shyang</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Julian Ibarz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London † Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Extraction of Structured Information from Street View Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a neural network model -based on Convolutional Neural Networks, Recurrent Neural Networks and a novel attention mechanism -which achieves 84.2% accuracy on the challenging French Street Name Signs (FSNS) dataset, significantly outperforming the previous state of the art (Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler and more general than the previous approach. To demonstrate the generality of our model, we show that it also performs well on an even more challenging dataset derived from Google Street View, in which the goal is to extract business names from store fronts. Finally, we study the speed/accuracy tradeoff that results from using CNN feature extractors of different depths. Surprisingly, we find that deeper is not always better (in terms of accuracy, as well as speed). Our resulting model is simple, accurate and fast, allowing it to be used at scale on a variety of challenging real-world text extraction problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Text recognition in an unconstrained natural environment is a challenging computer vision and machine learning problem. Traditional Optical Character Recognition (OCR) systems mainly focus on extracting text from scanned documents. Text acquired from natural scenes is more challenging due to visual artifacts, such as distortion, occlusions, directional blur, cluttered background or different viewpoints. Despite these difficulties, recent advances in deep learning have made significant progress on this problem <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this paper, we concentrate not just on transcribing all the text in a given image, but instead on the harder problem of extracting a subset of useful pieces of information. The model has to focus on the important parts of the scene and to ignore visual clutter. We propose a model which leverages convolutional neural networks (CNNs), recurrent neural networks (RNNs), and a new form of spatial attention.</p><p>We benchmark our model on the French Street Name Signs dataset (FSNS) <ref type="bibr" target="#b6">[7]</ref>, derived from Google Street View. The dataset contains over 1M labeled images of visual text "in the wild"; this is significantly more than COCO Text <ref type="bibr" target="#b4">[5]</ref>, which only includes 63k labeled images. We achieve 84.2% accuracy on FSNS, significantly outperforming the previous state-ofthe-art <ref type="bibr" target="#b6">[7]</ref>, which achieved 72.46%.</p><p>The previous state of the art method on FSNS <ref type="bibr" target="#b6">[7]</ref> shifts different views of the same sign into the batch dimension, has multiple multilayer LSTMs designed to treat every line of text separately (up to 3 lines) and uses CTC loss. Our model is simpler, more accurate and makes fewer assumptions about the data. To demonstrate its broad applicability, we also evaluate our new model on the Street View Business Names dataset <ref type="bibr" target="#b7">[8]</ref>, showing strong results.</p><p>Finally, we study the accuracy and speed of using 3 different CNN-based feature extractors (namely inception-v2 <ref type="bibr" target="#b8">[9]</ref>, inception-v3 <ref type="bibr" target="#b9">[10]</ref> and inception-resnet-v2 <ref type="bibr" target="#b9">[10]</ref>) as input to our attention model. We find that inception-v3 and inceptionresnet-v2 perform comparably, and both significantly outperform inception-v2. Motivated by the need for speed, we also study the effect of using "ablated" versions of these models, which use fewer layers. Interestingly, we find that for all three networks, the accuracy initially increases with depth, but then starts to decrease. This is in contrast to models trained on the ILSVRC Imagenet dataset <ref type="bibr" target="#b10">[11]</ref>, which is comparable in size to FSNS. For image classification, accuracy tends to increase with depth monotonically. We believe the difference is that image classification needs very complicated features, which are spatially invariant, whereas, for text extraction, it hurts to use to use such features.</p><p>In summary, our contributions are as follows: (1) We present a novel attention-based text reading architecture, trained in an end-to-end way, that beats the previous state of the art on the FSNS dataset by a significant margin while being considerably simpler and more general. <ref type="bibr" target="#b1">(2)</ref> We show how our new model also gives excellent results on a newer, even more challenging, Street View dataset. <ref type="bibr" target="#b2">(3)</ref> We study the speed/accuracy tradeoff that results from using CNNs of different depths and recommend a configuration that is accurate and efficient.</p><p>The source code and a trained model are available at: https://github.com/tensorflow/models/tree/master/attention ocr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>In this section, we describe our model, which processes the image through a CNN, and then passes the (attentionally weighted) features into an RNN. See <ref type="figure">Figure 1</ref> for a high level summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CNN-based feature extraction</head><p>We consider 3 kinds of CNN: inception-v2 <ref type="bibr" target="#b8">[9]</ref>, inception-v3 <ref type="bibr" target="#b9">[10]</ref> and inception-resnet-v2 <ref type="bibr" target="#b9">[10]</ref>, which combines inception <ref type="figure">Fig. 1</ref>: Architecture of our model. We pass each of the four views through the same CNN feature extractor, and then concatenate the results into a single large feature map, shown by the cube labeled "f". We take a spatially weighted combination to create a fixed-sized feature vector u t , which is fed into the RNN.</p><p>with resnets <ref type="bibr" target="#b11">[12]</ref>. These models achieve state-of-the-art performance on the Imagenet classification challenge <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>It has been shown in <ref type="bibr" target="#b13">[14]</ref> that features from lower layers of CNNs trained on Imagenet transfer well to other tasks. However, this still leaves open the question of which layer to use as our feature representation. We study this in Section IV-B. (We also compared the effects of pre-training on Imagenet vs. training from random initialization, and found no notable difference, so we only report results using the latter.)</p><p>We will use f = {f i,j,c } to denote the feature map derived by passing the image x through a CNN (here i, j index locations in the feature map, and c indexes channels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RNN</head><p>The main challenge is how to convert the feature maps into a single text string. Following previous work, we use an RNN (specifically, an LSTM <ref type="bibr" target="#b14">[15]</ref>) for this; this acts as a characterlevel language model, which takes inputs from the image, as we explain below.</p><p>Let s t be the hidden state of the RNN at time t. The input to the RNN is determined by a spatially weighted combination of image features. This spatial attention mask is denoted by α t = {α t,i,j }; we explain how to compute this in Section II-C. Once we have computed the spatial mask, we compute a weighted combination of the features (the context) as follows:</p><formula xml:id="formula_0">u t,c = i,j α t,i,j f i,j,c<label>(1)</label></formula><p>The total input to the RNN at time t is defined aŝ</p><formula xml:id="formula_1">x t = W c c OneHot t−1 + W u1 u t−1<label>(2)</label></formula><p>where c t−1 is the index of the previous letter (ground truth during training, predicted during test time). We then compute the output and next state of the RNN as follows:</p><formula xml:id="formula_2">(o t , s t ) = RNNstep(x t , s t−1 )<label>(3)</label></formula><p>The final predicted distribution over letters at time t is given byô</p><formula xml:id="formula_3">t = softmax(W o o t + W u2 u t )<label>(4)</label></formula><p>This combines information from the RNN, o t , with information from the attentional feature vector, u t . Finally, we compute the most likely letter:</p><formula xml:id="formula_4">c t = arg max cô t (c)<label>(5)</label></formula><p>This is called greedy decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial attention</head><p>Most prior works that use spatial attention for OCR (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref>) predict the mask based on the current RNN state, as follows:</p><formula xml:id="formula_5">a t,i,j = V T a tanh(W s s t + W f f i,j,: ) (6) α t = softmax i,j (a t )<label>(7)</label></formula><p>where V a is a vector and tanh is applied elementwise to its vector argument. This combines content from the image, via W f f , with a time-varying offset, via W s s t , to determine where to look. We will use this as our baseline attention method. The above attention mechanism is permutation invariant, meaning we could shuffle the order of the pixels and the mapping from f to α t would remain the same (since it is applied elementwise to each location). To make the model "location aware", we concatenate f i,j,: with a one-hot encoding of the spatial coordinates (i, j), as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. More precisely, we replace the argument to the tanh function with the following:</p><formula xml:id="formula_6">W s s t + W f1 f i,j,: + W f2 e i + W f3 e j<label>(8)</label></formula><p>where e i is a one-hot encoding of coordinate i, and similarly for e j . This is equivalent to adding a spatially varying matrix of bias terms. Our proposal is different than the location-aware attention mechanism proposed in <ref type="bibr" target="#b20">[21]</ref>. They suggested adding W a2 * α t−1 as input to the tanh function in <ref type="bibr">Equation 6</ref>, where W a2 is a convolution kernel. However, in multiline text recognition problems, we sometimes have to make a big jump to the left side of the line below (see <ref type="figure">Figure 6</ref> for example), which cannot be captured by this approach. (We tried their approach of location-aware attention, and it did not give good results for our problem, details omitted for brevity.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Handling multiple views</head><p>In the FSNS dataset, we have four views for each input sign, each of size 150x150. We process each of these independently, through the same CNN-based feature extractor (parameters are shared), to compute four feature maps. We then concatenate these horizontally to create a single input feature map. For example, suppose the feature map for each of the four views is 16 × 16 × 320; then after concatenation, the feature map f i,j,c will be 64 × 16 × 320. (The actual spatial resolution of the feature maps varies, as we discuss in Section IV-B.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training</head><p>We train the model using (penalized) maximum likelihood estimation, that is, we maximize T t=1 log p(y t |y 1:t−1 , x), where x is the input image, y t is the predicted label for location t, and T = 37 for the FSNS dataset. (If the output string is less than 37 characters, the model is required to predict a null character.) Since the model is autoregressive, we pass in the ground truth labels as history during training, as is standard <ref type="bibr" target="#b21">[22]</ref>. Note that we do not need ground truth bounding boxes around any of the text, which makes collecting training data much easier.</p><p>Our training method allows us to use autoregressive connections easily, which is not possible when using CTC loss <ref type="bibr" target="#b22">[23]</ref>, which was used by the previous state of the art model on FSNS <ref type="bibr" target="#b6">[7]</ref>. We find the use of such autoregressive dependencies improves accuracy by 6%, and speeds up training 2x (details omitted for brevity).</p><p>We use stochastic gradient descent optimization with initial learning rate 0.002, decay factor 0.1 after 1, 200, 000 steps and momentum 0.75. We finish training after 2, 000, 000 steps. We use the following augmentation procedure per view: We randomly crop with the requirement to cover at least 0.8 area of the original input image and new aspect ratio to be between 0.8 and 1.2. After cropping we resize it to the initial size with one randomly chosen interpolation procedure: bilinear, bicubic, area, or nearest-neighbor. Then we apply random image distortions: change of contrast, hue, brightness, and saturation. To regularize the model, we use weight decay 0.00004, label smoothing 0.9 <ref type="bibr" target="#b8">[9]</ref> and LSTM values clipping to 10. LSTM unit size is 256. We use a batch size of 32 with asynchronous training on 40 machines. For inception-resnet-v2 we have used a batch size of 12 due to GPU memory limitations. It takes less than 10 hours to train a single model with the inception-v3 network. We apply Polyak averaging <ref type="bibr" target="#b23">[24]</ref> with decay 0.9999 to derive the weights for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASETS</head><p>In this section, we describe the datasets that we use in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FSNS dataset</head><p>The FSNS dataset <ref type="bibr" target="#b6">[7]</ref> contains 965, 917 training images, 38, 633 validation images and 42, 839 test images. Each image has up to 4 tiles, intended to be a different view of the same physical street sign from Street View imagery from France. The size of every tile is 150x150 pixels. The first view represents the ground truth physical sign (as views do not always correspond to the same sign). See <ref type="figure" target="#fig_3">Figure 4</ref> for some examples.</p><p>All the transcriptions of the street name are up to 37 characters long. (Our model takes advantage of this fact and always runs 37 steps, with an optional out-of-alphabet padded symbol.) There are 134 possible characters to choose from at each location, but most of the street names consist only of Latin letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Street View Business Names Dataset</head><p>This is an internal dataset which contains ∼ 1M single view images of business storefronts extracted from Google Street View imagery. See <ref type="figure" target="#fig_4">Figure 5</ref> for some examples. The size of every image is 352x352. All transcriptions contain up to 33 symbols, with 128 characters in the vocabulary.</p><p>This dataset is significantly more challenging than FSNS, as storefronts have a lot more variation, and richer contextual information, compared to the street name signs. Moreover, the business name can be a small fraction of the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we report our experimental results on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accuracy on FSNS</head><p>We use the full sequence accuracy metric to benchmark our model. It is a challenging metric, as it requires every character to agree with the ground truth. We compare the previous state-of-the-art <ref type="bibr" target="#b6">[7]</ref> with five different versions of our model. In particular, we use three feature extractors (inception-v2, inception-v3 and inception-resnet-v2), and combine this with two attention models (standard and our novel position-dependent attention). As we see from <ref type="table" target="#tab_0">Table I</ref>, all our methods significantly outperform the previous state-of-the-art; inception-v3 and inception-resnet-v2 give similar performance, and both significantly beat inception-v2; finally, we see that our novel location-aware attention helps by 0.9% over standard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The effect of depth on FSNS</head><p>The main computational bottleneck in our model is the CNN-based feature extractor. To see how accuracy and speed vary as a function of the depth of the CNN, we considered "cutting" standard models at different layers, and then using these as feature extractors. We evaluate the accuracy of the resulting trained model (measured as the percentage of full sequences predicted correctly), as well as the speed (measured in milliseconds per single image inference on Tesla K40 GPU). <ref type="table" target="#tab_0">Table II</ref> shows the results for inception-v2, <ref type="table" target="#tab_0">Table III</ref> shows the results for inception-v3, and <ref type="table" target="#tab_0">Table IV</ref> shows the results for inception-resnet-v2. Note that these results are using standard attention, not location-aware attention.</p><p>We see that the accuracy improves for a while, and then starts to drop as the depth increases. This trend holds for all three models. We believe the reason for this is that character recognition does not benefit from the high-level features that are needed for image classification. Also, the spatial resolution of the image features used as input for attention decreases after every max pooling operation, which limits the precision of the attention mask on a particular character. We don't see any dependency between accuracy and the theoretical receptive field of the neurons in the last convolutional layer, but the effective field of view can be much smaller.</p><p>When comparing the different architectures, we see that inception-resnet-v2 is the most accurate (0.833), then inception-v3 (0.831), and finally inception-v2 (0.807). We chose to use inception-v3 features from the mixed-5d layer for all the other experiments in this paper, since this is almost twice as fast as the best inception-resnet-v2 cut, and has very similar accuracy, it is optimal choice of the architecture for the given computational budget. For the fixed spatial resolution, processing time grows with the depth of the network. After the max pooling layer which decreases the spatial resolution of image features used in attention, we usually observe speed up in the processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of attention on FSNS</head><p>To understand the behavior of our model, we use a visualization procedure similar to one proposed by <ref type="bibr" target="#b24">[25]</ref>. For  every predicted character k, we compute the partial derivative of its logit a k with respect to the input image x. We can then compute the saliency of pixel (i, j) using the formula v i,j = ∂a k ∂xi,j,: 2 , where we compute the L2 norm of the derivative across all the color channels. To get a less noisy saliency map, we create 16 slightly perturbed versions of the image (by adding a small amount of Gaussian noise) and then average the results. Additionally, we visualize the attention map α t by upsampling with nearest-neighbor interpolation to the size of the input.</p><p>A visualization example is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The red color represents the saliency map, and the green color represents the attention map. On the first time step, the attention map highlights the letter 'R,' which is the start of the name. Since 'Rue' is the most common beginning of a street name in this dataset, the model can predict the string 'Rue' without paying attention to the image for the next three iterations. This phenomenon is represented by the vertical green bars, which show that the model is just paying attention to the edge of the image, which has no informative content. We also see that the saliency map is spatially diffused in the first step since any of the letters 'R', 'u' or 'e' give evidence in support of 'Rue'; for the remaining three iterations, the saliency map is  On the 9th step, the model correctly emits the letter 'F,' which is the start of the word 'Fonds'. Note how this word is blurred out in the first view (top left image): this kind of blurring occurs on some words in FSNS, due to mistakes in the license plate or face detection systems. Consequently, the attention and saliency maps are zero for the first view (left column) when processing 'Fonds'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Error Analysis on FSNS</head><p>We analyze 100 randomly sampled wrong predictions to understand the weaknesses of our model better. 48% of the "errors" is due to the incorrect ground truth. <ref type="table" target="#tab_4">Table V</ref> gives a more detailed breakdown. The most common error is due to the wrong accent over the letter e (it should be eitheré or e); interestingly, this is also the most common mistake in the ground truth transcription.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we show some of the test cases where our model has a different prediction than the ground truth. (In the last example, this is due to an error in the ground truth.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on internal datasets</head><p>We now report results on a new internal dataset. Since this dataset is not public, we just show qualitative results; the purpose is to show how the very same model can be used for extracting information from many different kinds of street signs.    The ground truth string is "Lambert's Tire Service", which is correctly predicted by the model. This name is written on the top of the store in a "wavy" font, but also on the right-hand side (green sign) in a more standard horizontal font. Right: Visualization of the time-averaged saliency maps (in red) and attention masks (in green). Notice how they focus on the location that contains the store name. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of our system applied to an image from the Business Names dataset. Notice how the name of the business, "Lambert's Tire Service", appears in two locations: at the top, and at the bottom right. The model chooses to attend to the latter, perhaps because the font is more standard, and the writing is horizontal and not "wavy". <ref type="figure">Figure 6</ref> shows another example. This time, we see that the attention maps scans left to right, to read "Autopartes", but has to jump down a line and all the way to the left (like the carriage return action of a mechanical typewriter) to read the second line of text, "Lubricantes Tauro".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>We have presented an end-to-end approach for scene text recognition which gives state-of-the-art results on the challenging FSNS dataset, and an internal dataset. Our novel attention mechanism allows us to extract structured text information by reading only the interesting parts of the whole image. <ref type="figure">Fig. 6</ref>: Visualization of saliency maps (in red) and attention masks (in green) on a Business Names image. The model correctly predicts the string "Autopartes Lubricantes Tauro"; we just show the first few steps.</p><p>In the future, we would like to investigate more sophisticated ways of training the RNN, such as scheduled sampling <ref type="bibr" target="#b25">[26]</ref> or hybrid ML/RL methods <ref type="bibr" target="#b26">[27]</ref>. We would also like to extend the system to full structured extraction of business information from storefronts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Adding pixel coordinates to image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of saliency maps (in red) and attention masks (in green) on an FSNS image. The model correctly predicts the string "Rue des Fonds Gadons"; we just show the first 12 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Confused by font. Pred = 'Avenue Georges Frere', GT = 'Avenue General Frere'. (b) Read text from the wrong view. Pred='Boulevard des Talus', GT='Boulevard Charles'.(c) Confusion due to scratched letter, which looks like 'J', but model uses its prior to produce 'O'. Pred='Impasse des Jorfèvres', GT='Impasse des Orfévres'.(d) The model has better language prior than the human annotator. Pred='Avenue des Erables', Wrong GT='Avenue des Enadles'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of FSNS signs where our prediction (Pred.) differs from ground truth (GT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Left: An example image from the internal Business Names dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Accuracy on FSNS test set.</figDesc><table><row><cell>CNN</cell><cell>Attention</cell><cell>Accuracy</cell></row><row><cell>Smith et al. [7]</cell><cell>NA</cell><cell>72.46%</cell></row><row><cell>Inception-v2</cell><cell>Standard</cell><cell>80.7%</cell></row><row><cell>Inception-v2</cell><cell>Location</cell><cell>81.8%</cell></row><row><cell>Inception-v3</cell><cell>Standard</cell><cell>83.1%</cell></row><row><cell>Inception-v3</cell><cell>Location</cell><cell>84.0%</cell></row><row><cell>Inception-resnet-v2</cell><cell>Standard</cell><cell>83.3%</cell></row><row><cell>Inception-resnet-v2</cell><cell>Location</cell><cell>84.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance of different cuts of inception-v2. Mixed layers are cuts after the inception block of concatenated convolutions.</figDesc><table><row><cell>Inception-v2</cell><cell>Size</cell><cell>Acc</cell><cell>ms/</cell><cell>Depth</cell><cell>Rec.</cell></row><row><cell>layer</cell><cell>per view</cell><cell></cell><cell>Image</cell><cell></cell><cell>Field</cell></row><row><cell>MaxPool 3a 3x3</cell><cell>19x19x192</cell><cell>0.539</cell><cell>26</cell><cell>3</cell><cell>27</cell></row><row><cell>Mixed 3b</cell><cell>19x19x256</cell><cell>0.777</cell><cell>31</cell><cell>6</cell><cell>59</cell></row><row><cell>Mixed 3c</cell><cell>19x19x320</cell><cell>0.803</cell><cell>37</cell><cell>9</cell><cell>91</cell></row><row><cell>Mixed 4a</cell><cell>10x10x576</cell><cell>0.765</cell><cell>34</cell><cell>12</cell><cell>155</cell></row><row><cell>Mixed 4b</cell><cell>10x10x576</cell><cell>0.789</cell><cell>37</cell><cell>15</cell><cell>219</cell></row><row><cell>Mixed 4c</cell><cell>10x10x576</cell><cell>0.805</cell><cell>38</cell><cell>18</cell><cell>283</cell></row><row><cell>Mixed 4d</cell><cell>10x10x576</cell><cell>0.804</cell><cell>41</cell><cell>21</cell><cell>347</cell></row><row><cell>Mixed 4e</cell><cell>10x10x576</cell><cell>0.807</cell><cell>44</cell><cell>24</cell><cell>411</cell></row><row><cell>Mixed 5a</cell><cell>5x5x1024</cell><cell>0.791</cell><cell>43</cell><cell>27</cell><cell>539</cell></row><row><cell>Mixed 5b</cell><cell>5x5x1024</cell><cell>0.760</cell><cell>45</cell><cell>30</cell><cell>667</cell></row><row><cell>Mixed 5c</cell><cell>5x5x1024</cell><cell>0.792</cell><cell>47</cell><cell>33</cell><cell>795</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance of different cuts of inception-v3.</figDesc><table><row><cell>Inception-v3</cell><cell>Size</cell><cell>Acc</cell><cell>ms/</cell><cell>Depth</cell><cell>Rec.</cell></row><row><cell>layer</cell><cell>per view</cell><cell></cell><cell>Image</cell><cell></cell><cell>Field</cell></row><row><cell>MaxPool 3a 3x3</cell><cell>35x35x64</cell><cell>0.157</cell><cell>26</cell><cell>3</cell><cell>15</cell></row><row><cell>Conv2d 3b 1x1</cell><cell>35x35x80</cell><cell>0.541</cell><cell>31</cell><cell>4</cell><cell>15</cell></row><row><cell>Conv2d 4a 3x3</cell><cell>33x33x192</cell><cell>0.674</cell><cell>37</cell><cell>5</cell><cell>23</cell></row><row><cell>MaxPool 5a 3x3</cell><cell>16x16x192</cell><cell>0.712</cell><cell>35</cell><cell>5</cell><cell>31</cell></row><row><cell>Mixed 5b</cell><cell>16x16x256</cell><cell>0.818</cell><cell>29</cell><cell>8</cell><cell>63</cell></row><row><cell>Mixed 5c</cell><cell>16x16x288</cell><cell>0.816</cell><cell>33</cell><cell>11</cell><cell>95</cell></row><row><cell>Mixed 5d</cell><cell>16x16x288</cell><cell>0.831</cell><cell>36</cell><cell>14</cell><cell>127</cell></row><row><cell>Mixed 6a</cell><cell>7x7x768</cell><cell>0.822</cell><cell>34</cell><cell>17</cell><cell>159</cell></row><row><cell>Mixed 6b</cell><cell>7x7x768</cell><cell>0.804</cell><cell>37</cell><cell>22</cell><cell>351</cell></row><row><cell>Mixed 6c</cell><cell>7x7x768</cell><cell>0.822</cell><cell>40</cell><cell>27</cell><cell>543</cell></row><row><cell>Mixed 6d</cell><cell>7x7x768</cell><cell>0.820</cell><cell>42</cell><cell>32</cell><cell>735</cell></row><row><cell>Mixed 6e</cell><cell>7x7x768</cell><cell>0.826</cell><cell>45</cell><cell>37</cell><cell>927</cell></row><row><cell>Mixed 7a</cell><cell>3x3x1280</cell><cell>0.674</cell><cell>45</cell><cell>41</cell><cell>1033</cell></row><row><cell>Mixed 7b</cell><cell>3x3x2048</cell><cell>0.802</cell><cell>48</cell><cell>44</cell><cell>1161</cell></row><row><cell>Mixed 7c</cell><cell>3x3x2048</cell><cell>0.810</cell><cell>51</cell><cell>47</cell><cell>1289</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of different cuts of inception-resnet-v2. We only report results from layer Mixed 5b and above as cuts for lower layers are identical to the inception-v3 network.</figDesc><table><row><cell>Inception-resnet-v2</cell><cell>Size</cell><cell>Acc</cell><cell>ms/</cell><cell>Depth</cell><cell>Rec.</cell></row><row><cell>layer</cell><cell>per view</cell><cell></cell><cell>Image</cell><cell></cell><cell>Field</cell></row><row><cell>Mixed 5b</cell><cell>16x16x320</cell><cell>0.818</cell><cell>43</cell><cell>8</cell><cell>63</cell></row><row><cell>Mixed 5b + 5</cell><cell>16x16x320</cell><cell>0.822</cell><cell>57</cell><cell>28</cell><cell>223</cell></row><row><cell>Mixed 5b + 10</cell><cell>16x16x320</cell><cell>0.824</cell><cell>66</cell><cell>48</cell><cell>383</cell></row><row><cell>Mixed 6a</cell><cell>7x7x1088</cell><cell>0.833</cell><cell>70</cell><cell>51</cell><cell>415</cell></row><row><cell>Mixed 6a + 5</cell><cell>7x7x1088</cell><cell>0.827</cell><cell>90</cell><cell>71</cell><cell>895</cell></row><row><cell>Mixed 6a + 10</cell><cell>7x7x1088</cell><cell>0.817</cell><cell>125</cell><cell>91</cell><cell>1375</cell></row><row><cell>Mixed 6a + 15</cell><cell>7x7x1088</cell><cell>0.829</cell><cell>136</cell><cell>111</cell><cell>1855</cell></row><row><cell>Mixed 6a + 20</cell><cell>7x7x1088</cell><cell>0.824</cell><cell>158</cell><cell>131</cell><cell>2335</cell></row><row><cell>Mixed 7a</cell><cell>3x3x2080</cell><cell>0.819</cell><cell>307</cell><cell>134</cell><cell>2399</cell></row><row><cell>Mixed 7a + 5</cell><cell>3x3x2080</cell><cell>0.823</cell><cell>406</cell><cell>154</cell><cell>2719</cell></row><row><cell>Mixed 7a + 10</cell><cell>3x3x2080</cell><cell>0.825</cell><cell>512</cell><cell>174</cell><cell>3039</cell></row><row><cell>Conv2d 7b 1x1</cell><cell>3x3x1536</cell><cell>0.832</cell><cell>546</cell><cell>175</cell><cell>3039</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Breakdown of error types on FSNS.</figDesc><table><row><cell>Error type</cell><cell>Percent</cell></row><row><cell>Wrong ground truth</cell><cell>48</cell></row><row><cell>Wrong / Added / Missing accent over e</cell><cell>17</cell></row><row><cell>Wrong single letter inside the word</cell><cell>9</cell></row><row><cell>Wrong single letter at the</cell><cell>8</cell></row><row><cell>beginning / end of word</cell><cell></cell></row><row><cell>Added / Missing hyphen (-)</cell><cell>7</cell></row><row><cell>Wrong full word</cell><cell>6</cell></row><row><cell>Read from the wrong view</cell><cell>3</cell></row><row><cell>Wrong / Added / Missing accent</cell><cell>2</cell></row><row><cell>over different letter than e</cell><cell></cell></row><row><cell>zero everywhere.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Sergio Guadarrama, Alex Lacoste, Ray Smith, Quoc Le, Christian Szegedy, Oriol Vinyals, Ilya Sutskever, Sergey Ioffe, Vincent Vanhoucke and Sacha Arnoud for valuable discussions and TensorFlow support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Downtown osaka scene text dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="440" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end interpretation of the french street name signs dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="411" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale business discovery from street level imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05430</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scan, attend and read: Endto-end handwritten paragraph recognition with mdlstm attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Messina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03286</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint line segmentation and transcription for end-to-end handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08352</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
