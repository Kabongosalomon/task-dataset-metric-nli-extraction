<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Broaden Your Views for Self-Supervised Video Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Recasens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Pȃtrȃucean</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Broaden Your Views for Self-Supervised Video Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves stateof-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past few years, self-supervised methods have revolutionized the field of representation learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b68">69]</ref>. These methods directly learn from data without the need for manually defined labels that are hard to get at scale. Doing so, one can successfully leverage large amounts of uncurated data to improve representations. Even more importantly, self-supervised learning enables richer training tasks to be defined, compared to the standard approach of trying to categorize diverse visual inputs into a fixed set of categories. This has led to self-supervised representations outperforming supervised ones on downstream tasks <ref type="bibr" target="#b33">[34]</ref>. Video is a natural domain for self-supervised learning since data is rich and abundant but hard to annotate at scale due to the additional temporal complexity. However, most methods † Correspondence to: Adrià Recasens (arecasens@google.com) Time Broad View Narrow View Prediction <ref type="figure" target="#fig_4">Figure 1</ref>. Given a narrow view corresponding to a video clip of a few seconds, BraVe is tasked with predicting a broad view that spans a longer temporal context of the video in different modalities (here visual and audio). Solving that task requires the representation to extrapolate what happened before, during and after the narrow view, and results in state-of-the-art video representations.</p><p>in the video domain take direct inspiration from methods developed for images without fully taking advantage of its distinctly different dimension: time.</p><p>In particular, one common aspect of self-supervised methods for images is to extract two views from a given instance using the same general augmentation procedure, feed them into a shared backbone, and extract a supervisory signal from the fact that these two views originate from the same source. This is true for most recent approaches irrespective of their underlying learning principle: contrastive approaches <ref type="bibr" target="#b17">[18]</ref>, clustering-based method <ref type="bibr" target="#b13">[14]</ref>, or regression algorithms <ref type="bibr" target="#b68">[69]</ref>. The same principle has been followed in the video domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b67">68]</ref>. Specifically, most video methods extract the different views from a source video clip in a symmetric fashion with respect to time: all extracted views have the same temporal extent in the video <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b67">68]</ref>. However, doing so does not benefit from learning from information contained at different time scales.</p><p>In this paper, we introduce an algorithm dubbed "Broaden your Views" (BraVe), that breaks this symmetry in order to improve representation learning from videos. In detail, given a narrow view corresponding to a video clip of a few seconds, BraVe learns a representation by predicting a broad view that spans the longer temporal context of the full video clip as illustrated in <ref type="figure" target="#fig_4">Figure 1</ref>. Solving such a task requires extrapolating to the general context in which a given event occurs. In the example of <ref type="figure" target="#fig_4">Figure 1</ref>, one has to predict what happened before the person is in the sky (they probably jumped with the help of some device, given the height), as well as what is going to happen next (they will probably fall down somewhere soft) in order to solve the task. This task arguably requires a good understanding of the structure of events and is therefore a promising task for learning representations. While related local-to-global proxy tasks have been studied in the image domain via network architectural designs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> or multi-size cropping <ref type="bibr" target="#b17">[18]</ref>, applying these techniques to videos is not straightforward, because of the increased computational complexity incurred by the time dimension and the artifacts introduced when doing similar resize operations in spatio-temporal volumes. To address this challenge, we propose to process broad views with a dedicated model. We demonstrate that under a fixed computational budget, learning from the supervision provided by our broad views performs better than alternatives relying on symmetric augmentation procedures. Our algorithm is simple and does not require a cumbersome creation of explicit negatives as in contrastive methods. Instead we use a direct regression-based approach inspired by BYOL <ref type="bibr" target="#b28">[29]</ref>, where the views are processed by dedicated backbones and regress each other. Breaking the symmetry enables the use of stronger augmentations and different modalities for the broad view, which improves the quality of the final representations.</p><p>Contributions. We make the following contributions: (i) We propose a novel framework for representation learning, called BraVe, which generates views at different time scales and learns representations via simple regression across views, (ii) We explore using different augmentations and modalities in the broad view such as audio, flow or randomly convolved RGB frames. (iii) We evaluate this framework in the video domain, both with and without audio as an auxiliary supervisory signal, where we obtain state-of-the-art results on video and audio classification benchmarks UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image-based self-supervised learning. Most successful self-supervised methods learn a representation by defining a pretext task, whose resolution typically entails learning useful representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b89">90]</ref>. In particular, contrastive methods have provided spectacular performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>. Contrastive methods learn by pulling representations of different transformations of the same image (positive instances) closer, and pushing representations of different images (negatives) apart <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b59">60]</ref>. The main drawbacks of contrastive approaches are that they require a careful choice of positive and negative pairs <ref type="bibr" target="#b79">[80]</ref> and that they often rely on large number of such negatives, inducing a high computational cost <ref type="bibr" target="#b17">[18]</ref>. Alternatives to the contrastive approach, such as clustering and regression, avoid the need and cost of multiple negatives.</p><p>Clustering-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b85">86]</ref> alternate between learning representations using clusters as targets, and clustering using the current representations (either online or offline). Most related to our work are regressionbased methods that instead try to directly regress a representation extracted from a different view of the image <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>BraVe is directly inspired from <ref type="bibr" target="#b28">[29]</ref> but the views come from different modalities and augmentations, are processed by dedicated backbones and regress each other.</p><p>Video-based self-supervised learning. In the video domain, the pretext tasks for self-supervision have included predicting the future in pixel space by minimising an MSE loss <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83]</ref> or adversarial losses <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b81">82]</ref>. However, the predictions of these models are usually blurred and cannot go beyond predicting short clips into the future. To avoid these difficulties, other works focus on learning representations in a more abstract space, by using pretext tasks that predict the temporal order of video frames <ref type="bibr" target="#b56">[57]</ref> or the arrow of time <ref type="bibr" target="#b84">[85]</ref>. In this direction also, video contrastive methods have been very successful <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">68]</ref>. In addition to data augmentations used for images, these works use temporal cues to build positive pairs. Yet the costs of training such systems are significant and complex hard-negative mining strategies are needed to improve the training efficiency <ref type="bibr" target="#b22">[23]</ref>. Our method circumvents the use of negatives, considerably alleviating the training complexity while obtaining state-of-the-art performance on popular video benchmarks. Furthermore, our approach may leverage predictive tasks, such as predicting other crops in the video or optical flow, reminiscent of earlier predictive work <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b83">84]</ref>; but predicting in a learned feature space by building on a more recent self-supervised approach <ref type="bibr" target="#b28">[29]</ref>.</p><p>Audio-video self-supervised learning. Video and audio have been used as a rich source of self-supervision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b70">71]</ref>. A simple but effective approach to train representations consists in classifying whether a video clip and an audio sample correspond to each other <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b70">71]</ref>. Some works propose to use language obtained from speech recognition as an additional supervisory signal <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b76">77]</ref>. Related to ours, recent work finds that distilling flow and audio into a RGB encoder leads to strong representations <ref type="bibr" target="#b66">[67]</ref>, using an evolutionary search algorithm on the loss function. In contrast with this approach,       <ref type="figure" target="#fig_2">Figure 2</ref>. BraVe. Given a narrow view xn spanning a few seconds at high resolution and broad views x 1 b and x 2 b covering a larger temporal extent in the video for different modalities, we train independent networks running on the narrow and the broad views to mutually regress each other. This is done by defining two regression losses: L n→b to predict a broad view from the narrow view, and L b→n enforcing the other way around. To avoid collapse of the learned representations, we introduce three stages of processing as previously done in BYOL <ref type="bibr" target="#b28">[29]</ref>: backbone networks (fn for the narrow view and f 1 b , f 2 b for the broad views), projector networks (gn and g 1 b , g 2 b ) and predictor networks (hn and h 1 b , h 2 b ). For the broad views, we consider both visual modalities (RGB frames or optical flow) and audio modality.</p><formula xml:id="formula_0">i S j W V J D 0 o 2 H M k Q 7 R f H c 0 Y J I S z a f G Y C K Z m R W R M Z a Y a H O h v D n C c v f / T e v U d i u 2 c + W W 6 m V I l Y N D O I I T c O E c 6 n A J D W g C g Q n c w y M 8 W Z H 1 Y D 1 b L 2 l p x v r q 2 Y c f s l 4 / A b g + k g Y = &lt; / l a t e x i t &gt; h 1 b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 u P F L f P M k S 9 C 7 F H B 8 K j j C a J m t V Y = " &gt; A A A B 7 n i c d Z C 7 S g N B F I b P x k t i v E U t L G w G g 2 C 1 7 M Y Q k y 5 g Y x n B X C B Z w + x k k g y Z n V 1 m Z o W w p P Q B b C w U s b X 1 V e x 8 G</formula><formula xml:id="formula_1">L h b E k t E l C H s q O j x X l T N C m Z p r T T i Q p D n x O 2 / 7 k Y p 6 3 b 6 l U L B T X e h p R L 8 A j w Y a M Y G 1 Q e 3 x T 6 i f + r F 8 o O n a t V q t W K 8 i 1 n Y V Q S s r O k h T r h z t 3 Z 2 9 Z 3 e g X 3 n u D k M Q B F Z p w r F T X d S L t J V h q R j i d 5 X u x o h E m E z y i X W M F D q j y k s W 4 M 3 R i y A A N Q 2 m e 0 G h B v 3 c k O F B q G v i m M s B 6 r H 5 n c / h X 1 o 3 1 s O o l T E S x p o K k H w 1 j j n S I 5 r u j A Z O U a D 4 1 B h P J z K y I j L H E R J s L 5 c 0 R l r v / b 1 o l 2 6 3 Y z p V b r J c h V Q 6 O 4 B h O w Y V z q M M l N K A J B C Z w D 4 / w Z E X W g / V s v</formula><formula xml:id="formula_2">4 = " &gt; A A A B 7 n i c d Z C 7 S g N B F I b P x k t i v E U t L G w G g 2 A V d m O I S R e w s Y x g L p C s Y X Y y m w y Z n V 1 m Z o W w p P Q B b C w U s b X 1 V e x 8 G y d Z C S r 6 w 8 D P 9 5 / D n H O 8 i D O l b f v D y q y s r q 1 n c x v 5 z a 3 t n d 3 C 3 n 5 b h b E k t E V C H s q u h x X l T N C W Z p r T b i Q p D j x O O 9 7 k Y p 5 3 b q l U L B T X e h p R N 8 A j w X x G s D a o 4 9 + U B 4 k 3 G x S K d q l e r 9 d q V e S U 7 I V Q S i r 2 k h Q b h z t 3 Z 2 9 Z 3 R w U 3 v v D k M Q B F Z p w r F T P s S P t J l h q R j i d 5 f u x o h E m E z y i P W M F D q h y k 8 W 4 M 3 R i y B D 5 o T R P a L S g 3 z s S H C g 1 D T x T G W A 9 V r + z O f w r 6 8 X a r 7 k J E 1 G s q S D p R 3 7 M k Q 7 R f H c 0 Z J I S z a f G Y C K Z m R W R M Z a Y a H O h v D n C c v f / T b t c c q o l + 8 o p N i q Q K g d H c A y n 4 M A 5 N O A S m t A C A h O 4 h 0 d 4 s i L r w X q 2 X t L S j P X V c w A / Z L 1 + A r a y k g U = &lt; / l a t e x i t &gt; f 2 b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P p Y R 9 5 G Y t j E z K x O j Y A 6 0 L t B x 7 3 M = " &gt; A A A B 7 n i c d Z C 9 S g N B F I X v x p / E q D F q Y W E z G A S r s K s h J l 3 A x j K C + Y F k D b O T 2 W T I 7 O w y M y u E J a U P Y G O h i K 2 t r 2 L n 2 z j J S l D R A w O</formula><formula xml:id="formula_3">b + U a Q D Q 8 w = " &gt; A A A B 7 n i c b Z D L T g I x F I b P 4 A X E G 1 5 2 b h q J i R v J j D H q k s S N S 0 z k E m E k n X K A h k 5 n 0 n Y 0 O O E h 3 L j Q q F u f x 5 1 v Y w E X C v 5 J k y / / f 0 5 6 z g l i w b V x 3 S 8 n s 7 C 4 t J z N r e R X 1 9 Y 3 N g t b 2 z U d J Y p h l U U i U o 2 A a h R c Y t V w I 7 A R K 6 R h I L A e D C 7 G e f 0 O l e a R v D b D G P 2 Q 9 i T v c k a N</formula><formula xml:id="formula_4">F p O V V s x L r W 8 q E F V U M 9 J U B 8 = " &gt; A A A C B 3 i c b V A 9 S w N B E N 2 L H 4 n x 6 9 T C Q p D F I F i F O x W 1 D N h Y W E Q w H 5 C c Y W + z S Z b s 7 R 2 7 c 0 o 4 0 m n h X 7 E R U c T W v 2 D n v 3 E v S a G J D w Y e 7 8 0 w M 8 + P B N f g O N 9 W Z m 5 + Y T G b W 8 o v r 6 y u r d s b m 1 U d x o q y C g 1 F q O o + 0 U x w y S r A Q b B 6 p B g J f M F q f v 8 8 9 W u 3 T G k e y m s Y R M w L S F f y D q c E j N S y d 5 s B g R 4 l I r k c 3 r i t x G 8 q 3 u 0 B U S q 8 w 3 L Y s g t O 0 R k B z x J 3 Q g q l 7 b W H o 5 c s l F v 2 V 7 M d 0 j h g E q g g W j d c J w I v I Q o 4 F W y Y b 8 a a R Y T 2 S Z c 1 D J U k Y N p L R n 8 M 8 b 5 R 2 r g T K l M S 8 E j 9 P Z G Q Q O t B 4 J v O 9 G o 9 7 a X i f 1 4 j h s 6 Z l 3 A Z x c A k H S / q x A J D i N N Q c J s r R k E M D C F U c X M r p j 2 i C A U T X d 6 E 4 E 6 / P E u q h 0 X 3 p O h c u Y X S M R o j h 3 b Q H j p A L j p F J X S B y q i C K L p H T + g V v V m P 1 r P 1 b n 2 M W z P W Z G Y L / Y H 1 + Q O 2 w 5 w J &lt; / l a t e x i t &gt; L 1 b!n &lt; l a</formula><p>our framework does not require to define modality-specific losses, is simpler to train (no need to balance the losses), and obtains better performance across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Broaden Your Views for Self-Supervised Video Learning</head><p>In this section, we detail our approach dubbed BraVe for learning self-supervised state-of-the-art representations from a large set of videos, as measured by performance when transferring to downstream tasks. BraVe, illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, learns by direct regression from a high resolution narrow view that only spans a short clip to a lower resolution broader view which covers a larger temporal context of the video. Multiple options can be considered for the broad view: it can either come from the same modality as the narrow view (RGB in our case) or a different one such as flow or audio. Multiple views can also be combined to further improve performance. Next, we formally describe the learning framework in Section 3.1 and provide intuition why this may be a good self-supervised objective. Then, in Section 3.2, we describe the components and views we use in practice in two standard settings: learning from (i) visual signals alone, and from (ii) visual and audio modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The BraVe learning framework</head><p>General overview. Given a video x that can be composed of multiple modalities, we randomly extract two complementary views: a narrow view x n that spans a short timeframe in the video (around 1-3 seconds) and a broad view x b that covers a larger extent of the video (around 5-10 seconds). Details on how these views are obtained are given in Section 3.2. By introducing this temporal asymmetry in the creation of the views, the proposed task consists in extrapolating the full context of the video (the broad view) from only a small portion of the video (the narrow view) as illustrated in <ref type="figure" target="#fig_4">Figure 1</ref>. We hypothesize that to solve this task, good representations must be learned, which can then be useful for semantic downstream tasks. More formally, we train networks to minimize the training loss L defined for a given video x as follows:</p><formula xml:id="formula_5">L(x) = L n→b (x) Narrow→Broad + L b→n (x) Broad→Narrow .<label>(1)</label></formula><p>This loss is composed of two terms: (i) a prediction loss from the narrow to the broad view, and (ii) a complementary loss to regress the narrow view from the broad view. BraVe: losses and architectures. For simplicity and computational purposes, we opt for simple regression losses for L n→b and L b→n . This is indeed simpler than standard contrastive losses that require large batches and therefore high compute to work well <ref type="bibr" target="#b17">[18]</ref>. One challenge however, is the risk of collapse, since a trivial solution could be to always predict a constant which would lead to perfect regression losses across views. To avoid this, we draw inspiration from recent work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> in the way we design our networks and losses, as detailed next. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, we first define a backbone network f n whose role is to extract a representation from the narrow view x n . Similarly, we define a backbone network f b acting on the broad view x b . Note that in our framework, the parameters and even the underlying architectures of f n and f b can differ since they act on views of a different nature. These representations are then respectively transformed by projectors g n and g b , projecting f n (x n ) and f b (x b ) to yield the narrow embedding z n = g n (f n (x n )) and the broad one</p><formula xml:id="formula_6">z b = g b (f b (x b )</formula><p>). Inspired by <ref type="bibr" target="#b28">[29]</ref>, we then define a third stage of processing called the narrow view predictor h n that takes the projected embedding from the narrow view z n and produces a prediction h n (z n ) that is used to regress the broad view z b using the following loss:</p><formula xml:id="formula_7">L n→b (x) = h n (z n ) h n (z n ) 2 − sg z b z b 2 2 2 ,<label>(2)</label></formula><p>where sg[·] denotes the "stop gradient" operator, which operates on its input as the identity, but has zero partial derivatives. Since the loss L n→b only depends on the networks associated with the narrow view, we also define a loss to provide training signal for the broad view network. To that end, we introduce a broad view predictor h b that takes the projected embedding from the broad view z b and produces a prediction h b (z b ) that is used to regress the narrow view embedding z n using the following loss:</p><formula xml:id="formula_8">L b→n (x) = h b (z b ) h b (z b ) 2 − sg z n z n 2 2 2 .<label>(3)</label></formula><p>The role of these predictors is crucial to avoid collapse as found in <ref type="bibr" target="#b28">[29]</ref>, which we confirm experimentally. The same is true for the stop gradient operator. Differently from <ref type="bibr" target="#b28">[29]</ref>, we do not use exponential moving averages (EMA) on the weights of the network that process the view being regressed. Unlike <ref type="bibr" target="#b28">[29]</ref>, who required the moving average for improved performance, we find that this is not necessary in our case. Intuitions about what needs to be learned by BraVe. While the proposed approach avoids plain collapse of the representations, it is also important to question what needs to be learned in order for the loss (1) to be optimized. In particular, we want the narrow backbone to learn to predict the full context represented by the broad view. However, one challenge is to prevent the broad backbone from instead simply learning to throw the broad information away and only keeping the signal contained in the narrow view. To avoid this, we sample the narrow and broad views independently in time when they come from the same visual modality so that it is difficult for the broad backbone to predict what the narrow view is going to be. By doing so, we argue that the best solution to solve the task is for the narrow backbone to extrapolate what is happening in the broad view. We empirically verify the importance of this independent sampling in our experiments in section 4. Dealing with multiple views and modalities. BraVe can be extended to handle K broad views (with K &gt; 1) coming from different modalities. To do so and as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, we keep a single narrow backbone network f n but introduce specific narrow projectors and predictors for each broad views: {(g 1 n ,h 1 n ), · · · , (g K n , h K n )}. Each additional broad view x k b has its own set of backbone, projector and predictor : f k b , g k b and h k b , respectively. Given this, all regression losses are simply aggregated over all pairs composed by the narrow view x n and the different broad views {x k b } k :</p><formula xml:id="formula_9">L(x) = K k=1 L k n→b (x) + L k b→n (x).<label>(4)</label></formula><p>When using different modalities, the risk for the broad network to only focus on the narrow view is reduced due to the modality gap between the two views. Furthermore, when using audio, syncing helps slightly as previously observed in visual-audio work <ref type="bibr" target="#b44">[45]</ref>. We verify this experimentally and report the results in Appendix D.</p><p>Final loss. Given a large set of videos {x i } N i=1 , we train our model to minimize:</p><formula xml:id="formula_10">min fn,gn,hn f b ,g b ,h b N i=1 L(x i ).<label>(5)</label></formula><p>Next, we provide more details on the specific components that are used when BraVe is applied in the unimodal setting and the multimodal setting; as well as how the narrow and broad views are constructed in each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Broad views from visual and audio modalities</head><p>In our framework, we regress the representation of a broad backbone which sees a larger context of the video. The broad view is meant to provide information about the full video clip including more temporal context, in order to supervise the narrow backbone f n . As the different views are processed by different backbones, we can apply a different set of preprocessing and augmentation functions to any of the views. In this section, we first describe the set of transformations that we use when training with visual inputs alone, and then when training with both visual and audio inputs. Visual modalities. When sampling the broad view from the visual modalities, we aim to cover a large temporal context, the full clip. Accessing more temporal context typically means increasing the number of frames, and thus introducing extra computational complexity. To avoid this overhead, we decrease the spatial resolution of the broad view in order to keep the number of pixels constant. In Section 4 we show the effectiveness of trading temporal context for spatial resolution in the broad view. By keeping the computational cost fixed, we ensure that our method is computationally competitive with alternative self-supervised approaches.</p><p>Additionally to the temporal sampling, the set of transformations we consider for use on the narrow and broad views are motivated from two complementary perspectives. First, we can design the transformations T b used for the broad view to extract specific features from the input modality, sought to enrich the learned representations f n (x n ) with a certain type of information. Second, similarly to the use of augmentations in a wide number of machine learning approaches, and in particular in contrastive and regressionbased self-supervised learning approaches, we also employ such stochastic transformations to enforce invariance or equivariance constraints on the learned representations. In contrast to the use of augmentations in these self-supervised frameworks however, we emphasize that we do not impose that the set of transformations T n used on the narrow view be the same as the set of transformations T b used on the broad views. To explore this, we employ a recently introduced augmentation procedure relying on random convolutions <ref type="bibr" target="#b87">[88]</ref>, by which we augment only the broad view.</p><p>Alternatively, we can use optical flow as substitute of RGB in the broad view, which is reminiscent of <ref type="bibr" target="#b75">[76]</ref>, where the flow network is used to teach the RGB network. Optical flow from sequential images can provide supervision to emphasize motion in the learned representations extracted from the source, which has shown to be important for predicting actions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b83">84]</ref>. Optical flow can be extracted using an off-the-shelf unsupervised flow extraction algorithm. As flow is computed once for the full dataset, its computational overhead is negligible compared to training time. Audio modalities. Our framework can leverage audio as supervisory signal in the broad view. We can either use a single audio broad view or combine a visual broad view and an audio broad view for stronger self-supervision. Audio is a strong supervisory signal, and has been extensively used for self-supervision in videos as it strongly correlates with the visual content, while being easier to process computationally. As pre-processing, we extract spectrograms from consecutive short-time windows on the waveform using Fourier transforms. This approach has been shown to be very effective in obtaining state-of-the-art performance on supervised <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref> and unsupervised <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> approaches. For this reason, we encode the audio using a log-mel spectrogram representation as x b ∈ R Ts×D where T s is the number of spectrogram frames and D denotes the number of features. Similar to the unimodal setting, we experiment with enlarging the temporal window for the extraction of the audio view, compared with the temporal window of the narrow video view, seeking to increase the amount of context information present in the supervisory signal. Finally, as explained in the previous section, we make sure that the visual narrow view and the audio broad view are in sync at their starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate BraVe and compare its performance against relevant state-of-the-art methods trained on similar data and modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setting</head><p>Video-only experiments. In the video-only setting, we conduct our experiments on the Kinetics-600 dataset <ref type="bibr" target="#b15">[16]</ref>. The dataset has 600 action classes and contains 447k videos at the time of submission, 362k in the train set. Audio-video experiments. In the crossmodal training setting, we use the AudioSet <ref type="bibr" target="#b24">[25]</ref> as pre-training dataset. The dataset has 527 action classes and contains 1.9M videos in the training set at the time of submission. Architectures. For spatiotemporal volumes such as the sequences of RGB or flow frames, unless specified otherwise, we use the TSM-ResNet50 (TSM-50) <ref type="bibr" target="#b48">[49]</ref> architecture for the narrow backbone. For the broad visual backbone we always use a TSM-50 backbone. Video inputs are sampled at 12.5 frames per second (FPS). Unless stated otherwise, we train the narrow backbone on inputs of 16 frames (1.3 seconds) at resolution 224 × 224, and the broad backbone on inputs of 64 frames at 6.25 FPS (10s) at resolution 112×112. To see how our method scales to different and bigger architectures, we also experiment with different backbones for the narrow network with the R(2+1)D architecture <ref type="bibr" target="#b80">[81]</ref>, R3D architecture <ref type="bibr" target="#b32">[33]</ref> and TSM with twice the number of channels in each layer (TSM-50x2). We also introduce a video variant of the recent NF-Net-F0 architecture <ref type="bibr" target="#b12">[13]</ref>, by applying the TSM on it (details in Appendix C), which we call TSM-NF-F0. We use these networks only for the narrow view and always use TSM-50 in the broad view. For the broad backbone processing log-mel spectrograms, we use ResNet-50 <ref type="bibr" target="#b35">[36]</ref>. All models are trained using a two-layer MLP for the projector heads (g n and g b ) with a hidden layer of dimension 512, and a three-layer MLP for the predictor heads (h n and h b ) with hidden layers of dimensions 4096. We use batch normalization after each hidden layer. We use 128 as the output dimension of projectors and predictors. Feature extraction. For flow extraction, we use the TV-L1 <ref type="bibr" target="#b88">[89]</ref> algorithm. We use 80 bins for extracting log-mel spectrograms. Augmentations. We sample and augment all the visual views independently. For any narrow view, we uniformly sample a temporal offset between 0 and T − τ n , where T is the duration of the video clip and τ n denotes the length of the narrow view. We extract the view starting at this offset. For the broad view, we randomly sample the offset between 0 and T . We pad any broad view of insufficient length with a clip extracted from the start of the video sample (i.e. looping over the sequence). For all visual modalities (including the flow), we use random cropping and horizontal flipping. For the RGB views, we additionally employ gaussian blurring as well as scale and color jittering. We also explore the use of random convolutions as an augmentation procedure. Following <ref type="bibr" target="#b47">[48]</ref>, we use He initialization <ref type="bibr" target="#b34">[35]</ref> for the weights and fixed zero bias, sampling the size of the kernel uniformly across odd values ranging from 1 to 11. For audio, we use the same starting point as the narrow view, but extend it for a longer time window. If necessary, similarly to the RGB case, we pad the broad audio view with audio extracted from the start of the audio clip. See Appendix A.2 for further details. Self-supervised training details. We discard labels at training time, and only use them for downstream evaluation. Unless stated otherwise, we employ a batch size of 512 and train for 300k steps, setting the initial learning rate to 0.002. We train all models using AdamW <ref type="bibr" target="#b49">[50]</ref>, with 5000 warm up steps and cosine learning rate schedule <ref type="bibr" target="#b50">[51]</ref>. Following BYOL <ref type="bibr" target="#b28">[29]</ref>, we multiply the learning rate for all predictors (h n and h b ) by 10. For batch norm layers, we use a decay rate of 0.9 and epsilon of 1e-5. We use weight-decay of 0.01. More details are given in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream tasks</head><p>We use two standard settings to evaluate the quality of the learned visual representations from the narrow backbone f n : in the linear setting, we train a linear layer over frozen features extracted by f n ; in the fine-tuning setting, we train f n and the classifier head end-to-end. During evaluation, we always use 32 frames as inputs at 12.5 FPS, irrespective of the pre-training regime, to be comparable to previous work. We evaluate video representations using the HMDB51 dataset <ref type="bibr" target="#b45">[46]</ref>, the UCF101 dataset <ref type="bibr" target="#b73">[74]</ref> and the Kinetics-600 <ref type="bibr" target="#b16">[17]</ref> validation set. The HMBD51 dataset contains 5K videos, corresponding to 51 classes. The UCF101 dataset contains 13K videos, corresponding to 101 classes. The Kinetics-600 validation set contains 28k videos. We also evaluate the learned audio representations from the corresponding broad backbone, f b , on both the test set of the AudioSet dataset (20K samples, 527 classes) as well as the smaller ESC-50 dataset <ref type="bibr" target="#b65">[66]</ref> (2K samples, 50 classes). Following standard procedure, we report top-1 accuracy for all datasets except for Audioset where we report the mean average precision <ref type="bibr" target="#b40">[41]</ref>. For the datasets that have official splits (3 for UCF101/HMDB51 and 5 for ESC-50), we follow the standard procedure where split#1 serves as the validation set and the average accuracy over all splits is then reported. Linear setting. For HMDB51, UCF101 and ESC-50, we extract representations from 10 epochs worth of augmented samples using the learned narrow backbone, and we train a linear SVM using scikit-learn <ref type="bibr" target="#b64">[65]</ref> on these frozen features. For Kinetics-600 and AudioSet which are larger, we instead train the linear classifier using the Adam optimizer <ref type="bibr" target="#b42">[43]</ref>. In all cases, we use the same augmentations as during unsupervised pre-training except for gaussian blur. Full details are provided in Appendix B. At test time, we average the prediction over 30 clips (10 temporal clips each with 3 spatial crops) as done in <ref type="bibr" target="#b67">[68]</ref>. For AudioSet, we follow <ref type="bibr" target="#b40">[41]</ref> and use a fully-connected classifier, with one hidden layer of 512 units, in place of the linear classifier. Fine-tuning setting. In this setting, we add a single, ran- <ref type="table">Table 1</ref>. Importance of the broad view. We evaluate the impact of the temporal extent of the narrow (τn) and broad (τ b ) views. M b is the modality used in the broad view. RC stands for random convolutions. K600 stands for Kinetics-600 and AS for AudioSet. domly initialized, linear layer at the output of the narrow backbone. We initialize the narrow backbone's weights with those learned using BraVe, and we fine-tune this architecture end-to-end. Following previous work, we perform this evaluation on the HMDB51 and UCF101 datasets. We use the same test time procedure as for the linear setting. Details are given in Appendix B.</p><formula xml:id="formula_11">Dataset M b τ n τ b<label>HMDB51</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In this section, we study the effect of the different components of BraVe on the performance of the narrow backbone f n . Specifically, we study four main elements: (i) the effect of the temporal extents of the narrow and broad views, (ii) the improvements brought by different choices of transformations for the visual modality, (iii) the importance of having separate weights for the narrow view and the broad view network components and (iv) the effect of temporally syncing the narrow view and the broad view. By default, we conduct this analysis using the HMDB51 and UCF101 benchmarks in the linear setting. Importance of the broad view. We study the effect of the temporal extent of the narrow and broad views in the RGB-only setting (using random convolutions RGB+RC for the broad view) and the multimodal setting (using audio spectrogram for the broad view). We report results in <ref type="table">Table 1</ref>. First, in the unimodal setting, we find that for a narrow view extent τ n of 1.3s, performance improves significantly across the two downstream tasks as we increase the duration of the broad view τ b from 1.3s to 10s, (e.g. from 57.4 to 63.3 on HMDB51). This empirically supports our intuition that broader views can provide better supervision. Second, we find that using temporally large views of 10s for both the narrow view and the broad view degrades performance, as the task becomes significantly easier and we are unlikely to get rich embeddings. In the multimodal setting, we find that increasing the context from 1.3s to 5s also brings an improvement, although it is smaller than in the visual setting. We do not see further improvements when extending the broad view to 10s, and hence choose 5s for the temporal extent of the audio broad view. Visual transformation for the broad view. In <ref type="table" target="#tab_0">Table 2</ref>, we investigate the effect of using different visual inputs in the broad view. First, we see that using Random Convolutions (RC) <ref type="bibr" target="#b87">[88]</ref> on the RGB frames significantly improves performance, compared to using standard RGB frames. BraVe enables the use of such an aggressive augmentation since it has a dedicated backbone for that view. Moreover, only using this augmentation on the broad view ensures that the backbone trained on the narrow view does not suffer from shift in distribution of intensities <ref type="bibr" target="#b87">[88]</ref>. Furthermore, using optical flow for the broad view leads to further improvement when compared to using RC augmentation. This demonstrates a surprisingly high effectiveness of leveraging hand-designed feature extraction process, probably because this allows important factors -here motion and segmentation information -to be included in the desired representation. Weight sharing. In <ref type="table">Table 3</ref>, we study the effect of sharing weights across the different components of our model. First, we observe a significant decrease in performance when sharing the backbone networks. In this case, to solve the task we propose, the single backbone may need to split its capacity to extract features useful for both prediction tasks, from the narrow to the broad, and vice versa; which visibly hurts performance on the downstream task. While we could increase the capacity of the shared backbone, this would not provide the flexibility of separate backbones for processing different broad modalities and views. Next, we see an even larger drop, when additionally sharing the projector. Finally, when sharing all components, the important performance gap overall compared with our approach confirms our intuition that integrating information from local and global temporal context by only doing data augmentation as in the image case <ref type="bibr" target="#b17">[18]</ref> is not a good strategy for videos, and further highlights the benefit of our proposed approach. Syncing views. In <ref type="table">Table 4</ref>, we study the effect of having the same temporal starting point for the narrow and the broad view. As expected, when using a broad visual modality, syncing significantly decreases performance in UCF101 (−2.7%) and Kinetics-600 (−6.9%) but slightly benefits HMDB51 (+1.7%). We hypothesise that when both views are in sync, the broad network can simply focus its prediction only on the narrow view since the relative position of the views is deterministic hence making the self-supervised task easier as explained in the intuition paragraph of Section 3.1. As such, the network specialises in predicting short clips which would explain the slight improvement on the short clips of HMDB51 and the important decrease in performance for Kinetics and UCF101 that have longer clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the state-of-the-art</head><p>We compare BraVe against the state-of-the-art for selfsupervised video representation learning in <ref type="table">Table 5</ref>. Note that when evaluating in visual tasks, we only use the RGB modality to be comparable to previous work. Visual only on Kinetics600. In the setting where we use only the video modality combined with random convolutions in the broad view, we find that our TSM-50 model outperforms the current state-of-the-art CVRL approach [68] on UCF101 finetuning, and on HMDB51 linear and finetuning, despite having less parameters in our network (23.5M versus 33.3M). When integrating the flow modality we further increase the performance on UCF101 and HMDB51 to set a new state-of-the-art when training only from Kinetics-600 from the visual modality alone. On the Kinetics-600 linear evaluation, we obtain lower performance (66.9 versus 70.4) that we hypothesize is due to the advantage of contrastivebased approaches for such in-domain tasks. We also compare to using the same backbone (R3D50) as CVRL but observe slightly worse performance that we hypothesize to be due to our setting being more adapted to TSM-50. Multimodal on AudioSet. We also compare our approach in the multimodal (visual and audio modalities) setting by training BraVe on AudioSet. In that setting, we train for 620k steps instead of 300k, as AudioSet is significantly larger than Kinetics-600. We also increase the number of input frames of the narrow network from 16 to 32 frames (at 12.5FPS) and the number of input frames of the broad network from 64 (at 6.25FPS) to 128 (at 12.5FPS) during A. Pre-training details A.1. Architecture and model hyperparameters Each predictor is a three-layer MLP with hidden dimensions of 4096; each projector is a two-layer MLP with hidden dimension of 512. To train our models, we use the AdamW optimizer <ref type="bibr" target="#b49">[50]</ref> with cosine decay on the learning rate, with 5000 steps of linear warm up (starting from 0.0 to the initial learning rate value). All the models are trained with initial learning rate 2 · 10 −3 and batch size 512. We use weight decay with value 0.01. Following <ref type="bibr" target="#b28">[29]</ref>, we multiply the learning rate of the predictor MLPs by 10. We train all models for 300k steps except for the models with audio reported in <ref type="table">Table 5</ref>, which are trained for 620k steps. We use 16 Cloud TPUs to train all models except for the TSM-50x2 and the R(2+1)D-50 which we train with 32 Cloud TPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Data augmentation and feature extraction</head><p>RGB: Unless stated othwerwise, we subsample training videos to 12.5 FPS. For the broad views of the visual only models and the narrow view of the ablation using a 10s narrow view (first row, <ref type="table">Table 1</ref>), we subsample training videos to 6.25 FPS. In terms of spatial augmentations, we use random cropping, random flipping, color jittering, scale jittering and gaussian blurring; sampling their parameters independently for each view. Given the original frame, cropping is performed by sampling a bounding box with aspect ratio ranging between 1 2 and 2.0 and area between 30% and 100% of the full image. This bounding box is used to crop all frames of the video consistently in time. We horizontally flip all the frames with probability 0.5. With probability of 0.8 we apply color randomization in brightness, saturation, contrast and hue. This is done by adjusting brightness and hue by an additive offset, each uniformly sampled in respectively [−32/255, 32/255] and [−0.2, 0.2] on a persample basis; and similarly, adjusting contrast and saturation by a multiplicative factor, each sampled in [0.6, 1.4]. After this preprocessing, we clip the pixel values in the range [0, 1.0]. Furthermore, with probability 0.2, we convert the RGB sequence to a grayscale sequence. Finally, we apply gaussian blur with standard deviation σ uniformly sampled in [0.1, 2.0] and with kernel size equal to <ref type="bibr">1 10</ref> th of the crop side. Flow: Temporal sampling of the flow is performed similarly to the RGB case for the broad view. In terms of spatial augmentations, we use random cropping, sampling the crop independently from the narrow view. We also horizontally flip all the frames with probability 0.5. We resize the shortest size of the original frame to 128 and uniformly sample a 112 × 112 crop. We find that scale jittering in flow does not improve performance; as a result, we do not employ this augmentation. Random Convolutions: Following <ref type="bibr" target="#b87">[88]</ref>, we use He initialization <ref type="bibr" target="#b34">[35]</ref> for the weights, fixed zero bias and dimensionpreserving padding. We sample the size of the kernel uniformly across odd values ranging from 1 to 11. All sampling of kernel size and weights is performed on a per-sample basis. We refer the reader to the original paper for further details and illustration of the augmentation procedure. Spectrograms: The audio is sampled at 48k Hz. We take 80 bins of log-mel spectrograms extracted with Hanning windows of size 320 (6.67 ms) at a stride of 160 (3.33 ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Downstream task evaluation</head><p>Linear evaluation on HMDB51, UCF101 and ESC-50. For the linear evaluation on HMDB51, UCF101, and ESC-50 we use the SVM implementation of SciKit-Learn <ref type="bibr" target="#b64">[65]</ref>. For all three datasets, we use the same augmentations as during the pre-training stage except for gaussian blurring, and process 10 epochs worth of augmented samples. For each sample, we extract features using the pretrained backbone. When evaluating the TSM-50, R3D and R(2+1)D visual backbones and the RN-50 audio backbone, we find it helpful to rescale the features using a batch norm layer with fixed scaling and offset parameters (respectively of 1 and 0), collecting training statistics over the extracted features. We sweep the value for the regularization parameter of the SVM in the following set of values: {10 −5 , 3 · 10 −5 , 10 −4 , 3 · 10 −4 , 10 −3 , 3 · 10 −3 , 10 −2 }. When evaluating TSM-NF-F0 and TSM-50x2, we find it more effective to remove this normalization procedure. In these cases, we sweep the value for the regularization parameter of the SVM in the following set of values: {1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01}. For all models and downstream tasks, we use the first split to pick the optimal value and report the average of all the splits in that regime. At test time, we do not apply any augmentation. We subsample test videos to 12.5 FPS. For HMDB51 and UCF101, given a test video, we resize the minimum side to 256 and then average the predictions over 30 clips of size 224 × 224 (10 temporal clips regularly spaced within the video each providing 3 random spatial crops). For HMDB51 and UCF, we use clips of 32 frames. For ESC-50 we use a single window of 5s at test time. Finally, one special case is the ablation with a 10s narrow view (first row, <ref type="table">Table 1</ref>), which is trained with 64 frames at 6.25 FPS and 112 × 112 crops. For fairness, we evaluate it with clips of size 112 × 112 (minimum side 128) of 64 frames subsampled at 6.25 FPS (same frame rate than in training).</p><p>Finetuning evaluation on HMDB51 and UCF101. For fine-tuning, we use the SGD optimizer with momentum set to 0.9. We use a batch size of 256 for all methods except for R(2+1)D-50 and TSM-50x2 where we use a smaller batch size of 128 due to their high memory requirements. The batch is distributed over 32 workers. Although we use cross replica batch norm during pre-training (i.e. the statistics are accumulated over the 32 workers), during finetuning, we find it better to only compute statistics of batch norm within each worker. We hypothesize that this has a regularization effect on these small datasets. We use a linear warm up for the learning rate for 50 epochs (starting from 0.0 to the initial learning rate value). Learning rate is then decreased using a cosine decay for 550 epochs. Weight decay is employed on the weights of the network (except bias and batch norm parameters). We also apply dropout before the last linear layer mapping the representation to the logits of the classes. We cross validate the value of the initial learning rate (taking values in {0.03, 0.1, 0.3}), the weight decay (taking values in {0., 10 −7 }) and dropout rate (taking values in {0.1, 0.5}). Similarly to the linear setting, we select hyperparameters on split 1 of each downstream task and report averaged performance values across splits. We noticed that TSM-NF-Net needed slightly different parameters (probably due to the fact that this model does not use any form of normalization) so we adapted the range of the following hyperparameters: higher value of dropout in {0.5, 0.8} and smaller learning rate on HMDB51 in {0.01, 0.03}. The values of hyperparameters found for all networks are given in <ref type="table">Table 6</ref>. For training, we apply the following augmentation procedure in this order: temporal sampling, scale jittering, resizing the minimum side to 256, extracting a random crop of 224 × 224 and random horizontal flipping. For temporal sampling, we randomly sample in time a subclip of 32 frames from the original video clip. For scale jittering, we independently scale width and height by a value uniformly sampled from [0.8, 1.2]. At test time, we resize the minimum side to 256 and then average the predictions over 30 clips of size 224 × 224 (10 temporal clips regularly spaced within the video each providing 3 random spatial crops). We use the same FPS as during pre-training, i.e. 12.5 FPS.</p><p>Linear evaluation on Kinetics600. Since Kinetics600 is too large to fit in memory, we cannot use Scikit-Learn directly. Instead we train the linear layer for 80 epochs using the SGD optimizer with momentum set to 0.99 with a batch size of 256. We found it beneficial to apply batch norm and L2 normalization before the linear layer. We use a linear warm up for the learning rate for 5 epochs (starting from 0.0 to the initial learning rate value). Weight decay is employed on the linear layer's weights (excluding bias parameters). We also apply dropout just before the linear layer (after batch norm and L2 normalization). We cross validate the value of the initial learning rate (taking values in np.logspace(-0.5, 0, 4)), the weight decay (taking values in {0., 10 −8 }) and dropout rate (taking values in {0.0, 0.05}) on a small held out set from the training set (4K videos). For training, we apply the following augmentations in this order: temporal sampling, resizing the minimum side to 256, extracting a random crop of 224 × 224 and random horizontal flipping. For temporal sampling, we randomly sample in time a subclip of 32 frames from the original video clip. At test time, we resize the minimum side to 256 and then average the prediction over 30 clips of size 256 × 256 (10 temporal clips linearly spaced within the video each with 3 spatial crops). We do not apply scale jittering or horizontal flipping during test time. We use the same FPS as during pre-training, i.e. 12.5 FPS. We report the top 1 accuracy on the validation set of Kinetics600.</p><p>Shallow classifier evaluation on AudioSet. Following the protocols in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, we evaluate our audio representations by training a shallow MLP on AudioSet. The MLP has 1 hidden layer with 512 units, and is trained with the Adam optimizer using a batch size of 512 for 20 epochs. We use batch normalization layers on the frozen audio features and after the hidden layer. A ReLU activation function is applied after the second batch normalization. We use a linear warm up of 5000 steps starting from 0.0 to the initial learning rate of 2 × 10 −4 , which then decays following a cosine function. At test time, we use 10 overlapping crops of length 5s regularly spaced throughout the audio clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture details about TSM-NF-F0</head><p>Normalizer Free Networks (NF-Nets in short) are a recently introduced family of networks <ref type="bibr" target="#b11">[12]</ref> that do not use any form of normalization and are the current state-of-the-art on the ImageNet benchmark <ref type="bibr" target="#b12">[13]</ref>. We adapt this architecture to video by applying the Temporal Shift Module <ref type="bibr" target="#b48">[49]</ref> algorithm. In details, we insert the temporal shift module in all Normalizer Free blocks at the beginning of the residual branch, following same approach as for ResNets <ref type="bibr" target="#b48">[49]</ref>. In our work, we use the smallest network out of the NF-Net family (NF-Net-F0). As shown in <ref type="table">Table 5</ref> of the main paper, we obtain remarkable performance in the linear setting when using these networks even though their latent dimension is not much larger than our TSM-RN50 (3072 vs 2048). This may be due to the fact that these networks do not employ <ref type="table">Table 6</ref>. Hyperparameters for finetuning on HMDB51 and UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone  any form of normalizer which might make them more suited for linear evaluation. <ref type="table" target="#tab_2">Table 7</ref> shows the performance of a model trained with broad audio view when the narrow view and the broad view start at the same temporal instant (sync) or are independently randomly sampled in time (async). As discussed in Section 3.1 in the paper, this experiment supports already established evidence <ref type="bibr" target="#b44">[45]</ref> that syncing audio and video is beneficial for the resulting model in self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Syncing audio and video</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 - 1 n&lt; l a t e x i t s h a 1 _ b a s e 6 4 = 1 n&lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 n 2 n&lt; l a t e x i t s h a 1 _ b a s e 6 4 = 2 b&lt; l a t e x i t s h a 1 _ b a s e 6 4 = 1 b</head><label>511411422142141</label><figDesc>10 seconds, Low Res &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n d O d c J O s v l 5 T N T z e B G 4 o O H x X P u s = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 1 V n W p S L A I r s p M 0 T r d F d 2 4 b N F e o C 0 l k 2 b a 0 E x m S D J C G b p 0 6 c a F I m 5 9 i D 6 H O 5 / B l z B t R d T 6 Q + D j / 8 8 h 5 x w v 4 k x p 2 3 6 3 F h a X l l d W U 2 v p 9 Y 3 N r e 3 M z m 5 N h b E k t E p C H s q G h x X l T N C q Z p r T R i Q p D j x O 6 9 7 g c p L X b 6 l U L B Q 3 e h j R d o B 7 g v m M Y G 2 s a 7 8 j O p m s n S s W T 9 0 z F x l w C 6 6 T R 0 7 O n u o b s q W D c e X j 7 n B c 7 m T e W t 2 Q x A E V m n C s V N O x I 9 1 O s N S M c D p K t 2 J F I 0 w G u E e b B g U O q G o n 0 1 F H 6 N g 4 X e S H 0 j y h 0 d T 9 2 Z H g Q K l h 4 J n K A O u + + p t N z P + y Z q x 9 t 5 0 w E c W a C j L 7 y I 8 5 0 i G a 7 I 2 6 T F K i + d A A J p K Z W R H p Y 4 m J N t d J m y P M r T w P t X z O K e T s i p M t X c B M K d i H I z g B B 8 6 h B F d Q h i o Q 6 M E 9 P M K T x a 0 H 6 9 l 6 m Z U u W F 8 9 e / B L 1 u s n r j C R x g = = &lt; / l a t e x i t &gt; f n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 8 g C 0 Z 1 S k a j 2 N X 0 d Y 4 z / l G W B 5 A E = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 1 V n W p S L A I r o a Z o n W 6 K 7 p x 2 a K 9 Q D u U T J p p Q z M X k o x Q h i 5 d u n G h i F s f o s / h z m f w J U x b E b X + E P j 4 / 3 P I O c e L O Z P K s t 6 N h c W l 5 Z X V z F p 2 f W N z a z u 3 s 1 u X U S I I r Z G I R 6 L p Y U k 5 C 2 l N M c V p M x Y U B x 6 n D W 9 w O c k b t 1 R I F o U 3 a h h T N 8 C 9 k P m M Y K W t a 7 / j d X J 5 y y y V T p 0 z B 2 l w i o 5 d Q L Z p T f U N + f L B u P p x d z i u d H J v 7 W 5 E k o C G i n A s Z c u 2 Y u W m W C h G O B 1 l 2 4 m k M S Y D 3 K M t j S E O q H T T 6 a g j d K y d L v I j o V + o 0 N T 9 2 Z H i Q M p h 4 O n K A K u + / J t N z P + y V q J 8 x 0 1 Z G C e K h m T 2 k Z 9 w p C I 0 2 R t 1 m a B E 8 a E G T A T T s y L S x w I T p a + T 1 U e Y W 3 k e 6 g X T L p p W 1 c 6 X L 2 C m D O z D E Z y A D e d Q h i u o Q A 0 I 9 O A e H u H J 4 M a D 8 W y 8 z E o X j K + e P f g l 4 / U T n A C R u g = = &lt; / l a t e x i t &gt; f b Narrow view &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W O 1 5 P r 2 b A P E V l v p / r Z D 3 e q / X L d g = " &gt; A A A B 6 n i c b Z C 7 S g N B F I b P e o 3 x F h V s b A a D Y B V 2 L d Q y x M Y y Q X O B Z A m z k 9 l k y O z s M n N W D C G P Y G O h i K 2 t b + E T 2 N n 4 L E 4 u h S b + M P D x / + c w 5 5 w g k c K g 6 3 4 5 S 8 s r q 2 v r m Y 3 s 5 t b 2 z m 5 u b 7 9 m 4 l Q z X m W x j H U j o I Z L o X g V B U r e S D S n U S B 5 P e h f j f P 6 H d d G x O o W B w n 3 I 9 p V I h S M o r V u 7 t u q n c u 7 B X c i s g j e D P L F w 8 q 3 e C 9 9 l N u 5 z 1 Y n Z m n E F T J J j W l 6 b o L + k G o U T P J R t p U a n l D W p 1 3 e t K h o x I 0 / n I w 6 I i f W 6 Z A w 1 v Y p J B P 3 d 8 e Q R s Y M o s B W R h R 7 Z j 4 b m / 9 l z R T D S 3 8 o V J I i V 2 z 6 U Z h K g j E Z 7 0 0 6 Q n O G c m C B M i 3 s r I T 1 q K Y M 7 X W y 9 g j e / M q L U D s r e O c F t + L l i y W Y K g N H c A y n 4 M E F F O E a y l A F B l 1 4 g C d 4 d q T z 6 L w 4 r 9 P S J W f W c w B / 5 L z 9 A E j C k Y A = &lt; / l a t e x i t &gt; x n : visual modality : audio modality &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "I K E 2 O T i C G 6 B i t j x b B X / M 0 2 4 a Z 2 0 = " &gt; A A A B 7 n i c d Z D L S g M x F I b P 1 E t r v d X L z k 2 w C G 4 s M 1 J q u y u 4 c V n B X r A d S y Z N p 6 G Z z J B k l D L 0 I d y 4 U N S t z + P O t z H t S F H R H w I / 3 3 8 O O e d 4 E W d K 2 / a H l V l a X l n N 5 t b y 6 x u b W 9 u F n d 2 W C m N J a J O E P J Q d D y v K m a B N z T S n n U h S H H i c t r 3 x + S x v 3 1 K p W C i u 9 C S i b o B 9 w Y a M Y G 1 Q 2 7 9 x + o m Y 9 g t F u 1 S r 1 a r V C n J K 9 l w o J W V 7 Q Y r 1 / R P / L v t y 3 e g X 3 n u D k M Q B F Z p w r F T X s S P t J l h q R j i d 5 n u x o h E m Y + z T r r E C B 1 S 5 y X z c K T o y Z I C G o T R P a D S n 3 z s S H C g 1 C T x T G W A 9 U r + z G f w r 6 8 Z 6 W H U T J q J Y U 0 H S j 4 Y x R z p E s 9 3 R g E l K N J 8 Y g 4 l k Z l Z E R l h i o s 2 F 8 u Y I i 9 3 / N 6 3 T k l M p 2 Z d O s V 6 G V D k 4 g E M 4 B g f O o A 4 X 0 I A m E B j D P T z C k x V Z D9 a z 9 Z q W Z q y v n j 3 4 I e v t E + U J k i Y = &lt; / l a t e x i t &gt; g " i Z / L Y j 4 z k y n D A F v a 6 v n b J U Y 0 i 8 c = " &gt; A A A B 7 n i c d Z D L S g M x F I b P 1 E t r v d X L z k 2 w C G 4 c Z q T U d l d w 4 7 K C v W A 7 l k y a a U M z m S H J K G X o Q 7 h x o a h b n 8 e d b 2 P a S l H R H w I / 3 3 8 O O e f 4 M W d K O 8 6 H l V l a X l n N 5 t b y 6 x u b W 9 u F n d 2 m i h J J a I N E P J J t H y v K m a A N z T S n 7 V h S H P q c t v z R + T R v 3 V K p W C S u 9 D i m X o g H g g W M Y G 1 Q a 3 j j 9 l I x 6 R W K j l 2 t V i u V M n J t Z y Y 0 J y V n Q Y q 1 / Z P B X f b l u t 4 r v H f 7 E U l C K j T h W K m O 6 8 T a S 7 H U j H A 6 y X c T R W N M R n h A O 8 Y K H F L l p b N x J + j I k D 4 K I m m e 0 G h G v 3 e k O F R q H P q m M s R 6 q H 5 n U / h X 1 k l 0 U P F S J u J E U 0 H m H w U J R z p C 0 9 1 R n 0 l K N B 8 b g 4 l k Z l Z E h l h i o s 2 F 8 u Y I i 9 3 / N 8 1 T 2 y 3 b z q V b r J V g r h w c w C E c g w t n U I M L q E M D C I z g H h 7 h y Y q t B + v Z e p 2 X Z q y v n j 3 4 I e v t E + a T k i c = &lt; / l a t e x i t &gt; h " g t 3 I 2 l M G 4 z q f f H A 5 o l V 2 m x m s a f w = " &gt; A A A B 7 n i c d Z D L S g M x F I b P e G u t t 3 r Z u Q k W w Y 3 D T C m 1 3 R X c u K x g L 9 i O J Z N m 2 t B M Z k g y S h n 6 E G 5 c K O r W 5 3 H n 2 5 i 2 U l T 0 h 8 D P 9 5 9 D z j l + z J n S j v N h L S 2 v r K 5 l s u u 5 j c 2 t 7 Z 3 8 7 l 5 T R Y k k t E E i H s m 2 j x X l T N C G Z p r T d i w p D n 1 O W / 7 o f J q 3 b q l U L B J X e h x T L 8 Q D w Q J G s D a o N b w p 9 l I x 6 e U L j l 2 t V i u V M n J t Z y Y 0 J y V n Q Q q 1 g 9 P B X e b l u t 7 L v 3 f 7 E U l C K j T h W K m O 6 8 T a S 7 H U j H A 6 y X U T R W N M R n h A O 8 Y K H F L l p b N x J + j Y k D 4 K I m m e 0 G h G v 3 e k O F R q H P q m M s R 6 q H 5 n U / h X 1 k l 0 U P F S J u J E U 0 H m H w U J R z p C 0 9 1 R n 0 l K N B 8 b g 4 l k Z l Z E h l h i o s 2 F c u Y I i 9 3 / N 8 2 i 7 Z Z t 5 9 I t 1 E o w V x Y O 4 Q h O w I U z q M E F 1 K E B B E Z w D 4 / w Z M X W g / V s v c 5 L l 6 y v n n 3 4 I e v t E + g b k i g = &lt; / l a t e x i t &gt; h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E a R Y v e N j K j Q + D F E r k e t t t N R r X F Y = " &gt; A A A B 7 n i c d Z D L S g M x F I b P e G u t t 3 r Z u Q k W w Y 3 D T C m 1 3 R X c u K x g L 9 i O J Z N m 2 t B M Z k g y S h n 6 E G 5 c K O r W 5 3 H n 2 5 i 2 U l T 0 h 8 D P 9 5 9 D z j l + z J n S j v N h L S 2 v r K 5 l s u u 5 j c 2 t 7 Z 3 8 7 l 5 T R Y k k t E E i H s m 2 j x X l T N C G Z p r T d i w p D n 1 O W / 7 o f J q 3 b q l U L B J X e h x T L 8 Q D w Q J G s D a o N b g p 9 l I x 6 e U L j l 2 t V i u V M n J t Z y Y 0 J y V n Q Q q 1 g 9 P B X e b l u t 7 L v 3 f 7 E U l C K j T h W K m O 6 8 T a S 7 H U j H A 6 y X U T R W N M R n h A O 8 Y K H F L l p b N x J + j Y k D 4 K I m m e 0 G h G v 3 e k O F R q H P q m M s R 6 q H 5 n U / h X 1 k l 0 U P F S J u J E U 0 H m H w U J R z p C 0 9 1 R n 0 l K N B 8 b g 4 l k Z l Z E h l h i o s 2 F c u Y I i 9 3 / N 8 2 i 7 Z Z t 5 9 I t 1 E o w V x Y O 4 Q h O w I U z q M E F 1 K E B B E Z w D 4 / w Z M X W g / V s v c 5 L l 6 y v n n 3 4 I e v t E + a R k i c = &lt; / l a t e x i t &gt; g " f i Q t R o / l o P c Z a s v Q P 4 w C S c 7 a g y Y = " &gt; A A A B 7 n i c d Z C 7 S g N B F I b P x k t i v E U t L G w G g 2 C 1 7 M Y Q k y 5 g Y x n B X C B Z w + x k k g y Z n V 1 m Z o W w p P Q B b C w U s b X 1 V e x 8 G y d Z C S r 6 w 8 D P 9 5 / D n H P 8 i D O l H e f D y q y s r q 1 n c x v 5 z a 3 t n d 3 C 3 n 5 L h b E k t E l C H s q O j x X l T N C m Z p r T T i Q p D n x O 2 / 7 k Y p 6 3 b 6 l U L B T X e h p R L 8 A j w Y a M Y G 1 Q e 3 R T 6 i f + r F 8 o O n a t V q t W K 8 i 1 n Y V Q S s r O k h T r h z t 3 Z 2 9 Z 3 e g X 3 n u D k M Q B F Z p w r F T X d S L t J V h q R j i d 5 X u x o h E m E z y i X W M F D q j y k s W 4 M 3 R i y A A N Q 2 m e 0 G h B v 3 c k O F B q G v i m M s B 6 r H 5 n c / h X 1 o 3 1 s O o l T E S x p o K k H w 1 j j n S I 5 r u j A Z O U a D 4 1 B h P J z K y I j L H E R J s L 5 c 0 R l r v / b 1 o l 2 6 3 Y z p V b r J c h V Q 6 O 4 B h O w Y V z q M M l N K A J B C Z w D 4 / w Z E X W g / V s v a S l G e u r 5 w B + y H r 9 B L g 8 k g Y = &lt; / l a t e x i t &gt; g " t Y t i 4 Z S F n Q K 2 R e V a h Y / D 9 2 d l V 0 o = " &gt; A A A B 7 n i c d Z C 9 S g N B F I X v x p / E q D F q Y W E z G A S r Z V d D T L q A j W U E 8 w P J G m Y n k 2 T I 7 O w y M y u E J a U P Y G O h i K 2 t r 2 L n 2 z j J S l D R A w O H 7 9 z L 3 H v 9 i D O l H e f D y q y s r q 1 n c x v 5 z a 3 t w k 5 x d 6 + l w l g S 2 i Q h D 2 X H x 4 p y J m h T M 8 1 p J 5 I U B z 6 n b X 9 y M c / b t 1 Q q F o p r P Y 2 o F + C R Y E N G s D a o P b p x + 4 k / 6 x d L j l 2 r 1 a r V C n J t Z y G U k r K z J K X 6 Q e H u 7 C 2 r G / 3 i e 2 8 Q k j i g Q h O O l e q 6 T q S 9 B E v N C K e z f C 9 W N M J k g k e 0 a 6 z A A V V e s h h 3 h o 4 N G a B h K M 0 T G i 3 o 9 4 4 E B 0 p N A 9 9 U B l i P 1 e 9 s D v / K u r E e V r 2 E i S j W V J D 0 o 2 H M k Q 7 R f H c 0 Y J I S z a f G Y C K Z m R W R M Z a Y a H O h v D n C c v f / T e v U d i u 2 c + W W 6 m V I l Y N D O I I T c O E c 6 n A J D W g C g Q n c w y M 8 W Z H 1 Y D 1 b L 2 l p x v r q 2 Y c f s l 4 / A b a 0 k g U = &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a m z e 3 R 6 F W N M G e i d R n J l m X R 5 9 U k A = " &gt; A A A B 7 n i c d Z C 9 S g N B F I X v x p / E q D F q Y W E z G A S r Z V d D T L q A j W U E 8 w P J G m Y n k 2 T I 7 O w y M y u E J a U P Y G O h i K 2 t r 2 L n 2 z j J S l D R A w O H 7 9 z L 3 H v 9 i D O l H e f D y q y s r q 1 n c x v 5 z a 3 t w k 5 x d 6 + l w l g S 2 i Q h D 2 X H x 4 p y J m h T M 8 1 p J 5 I U B z 6 n b X 9 y M c / b t 1 Q q F o p r P Y 2 o F + C R Y E N G s D a o P b 5 x + 4 k / 6 x d L j l 2 r 1 a r V C n J t Z y G U k r K z J K X 6 Q e H u 7 C 2 r G / 3 i e 2 8 Q k j i g Q h O O l e q 6 T q S 9 B E v N C K e z f C 9 W N M J k g k e 0 a 6 z A A V V e s h h 3 h o 4 N G a B h K M 0 T G i 3 o 9 4 4 E B 0 p N A 9 9 U B l i P 1 e 9 s D v / K u r E e V r 2 E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y d Z C S r 6 w 8 D P 9 5 / D n H P 8 i D O l H e f D y q y s r q 1 n c x v 5 z a 3 t n d 3 C 3 n 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 b</head><label>2</label><figDesc>a S l G e u r 5 w B + y H r 9 B L n G k g c = &lt; / l a t e x i t &gt; h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e 8 p C m e H X J I p j f n h w H D a 6 c 8 B g 1 H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 b 1 b 2 b 1 b 2 b 2 n</head><label>112122</label><figDesc>H 7 9 z L 3 H u 9 i D O l b f v D y q y s r q 1 n c x v 5 z a 3 t w k 5 x d 6 + t w l g S 2 i I h D 2 X X w 4 p y J m h L M 8 1 p N 5 I U B x 6 n H W 9 y M c 8 7 t 1 Q q F o p r P Y 2 o G + C R Y D 4 j W B v U 8 W + c Q e L N B s W S X a 7 X 6 7 V a F T l l e y G U k o q 9 J K X G Q e H u 7 C 2 r m 4 P i e 3 8 Y k j i g Q h O O l e o 5 d q T d B E v N C K e z f D 9 W N M J k g k e 0 Z 6 z A A V V u s h h 3 h o 4 N G S I / l O Y J j R b 0 e 0 e C A 6 W m g W c q A 6 z H 6 n c 2 h 3 9 l v V j 7 N T d h I o o 1 F S T 9 y I 8 5 0 i G a 7 4 6 G T F K i + d Q Y T C Q z s y I y x h I T b S 6 U N 0 d Y 7 v 6 / a Z + W n W r Z v n J K j Q q k y s E h H M E J O H A O D b i E J r S A w A T u 4 R G e r M h 6 s J 6 t l 7 Q 0 Y 3 3 1 7 M M P W a + f t S q S B A = = &lt; / l a t e x i t &gt; f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g G M k 9 s 7 f p B e Z L Z v N A N s 6 S Q x I j W w = " &gt; A A A B 7 n i c b Z A 9 S w N B E I b n 4 k d i 1 B i 1 s L B Z D I J V u F N R y 4 C N Z Q T z A f E M e 5 t N s m R v 7 9 i d E + K R 0 h 9 g Y 6 G I r a 1 / x c 5 / 4 + a j 0 M Q X F h 7 e d 4 a d m S C W w q D r f j u Z p e W V 1 W x u L b + + s V n Y K m 7 v 1 E 2 U a M Z r L J K R b g b U c C k U r 6 F A y Z u x 5 j Q M J G 8 E g 8 t x 3 r j n 2 o h I 3 e A w 5 n 5 I e 0 p 0 B a N o r c b D n d d O g 1 G 7 W H L L 7 k R k E b w Z l C p 7 h c e T z y x W 2 8 W v 2 0 7 E k p A r Z J I a 0 / L c G P 2 U a h R M 8 l H + N j E 8 p m x A e 7 x l U d G Q G z + d j D s i h 9 b p k G 6 k 7 V N I J u 7 v j p S G x g z D w F a G F P t m P h u b / 2 W t B L s X f i p U n C B X b P p R N 5 E E I z L e n X S E 5 g z l 0 A J l W t h Z C e t T T R n a C + X t E b z 5 l R e h f l z 2 z s r u t V e q n M J U O d i H A z g C D 8 6 h A l d Q h R o w G M A T v M C r E z v P z p v z P i 3 N O L O e X f g j 5 + M H U p 2 R v w = = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 e w D N n K 3 P 3 O s H D 5 B 9 v z x J / 7 + 3 0 Y = " &gt; A A A B 7 n i c b Z A 9 S w N B E I b n 4 k d i 1 B i 1 s L A 5 D I J V u I u i l g E b y w j m A 5 I z 7 G 3 2 k i V 7 e 8 f u n B C P l P 4 A G w t F b G 3 9 K 3 b + G z c f h S a + s P D w v j P s z P i x 4 B o d 5 9 v K r K y u r W d z G / n N r e 3 C T n F 3 r 6 G j R F F W p 5 G I V M s n m g k u W R 0 5 C t a K F S O h L 1 j T H 1 5 N 8 u Y 9 U 5 p H 8 h Z H M f N C 0 p c 8 4 J S g s Z o P d 5 V u 6 o + 7 x Z J T d q a y l 8 G d Q 6 l 6 U H g 8 / c x i r V v 8 6 v Q i m o R M I h V E 6 7 b r x O i l R C G n g o 3 z n U S z m N A h 6 b O 2 Q U l C p r 1 0 O u 7 Y P j Z O z w 4 i Z Z 5 E e + r + 7 k h J q P U o 9 E 1 l S H C g F 7 O J + V / W T j C 4 9 F I u 4 w S Z p L O P g k T Y G N m T 3 e 0 e V 4 y i G B k g V H E z q 0 0 H R B G K 5 k J 5 c w R 3 c e V l a F T K 7 n n Z u X F L 1 T O Y K Q e H c A Q n 4 M I F V O E a a l A H C k N 4 g h d 4 t W L r 2 X q z 3 m e l G W v e s w 9 / Z H 3 8 A F Q l k c A = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g Z 0 L U s M C P u M j K 6 5 5 y 6 m N l R e T P V 8 = " &gt; A A A B 7 n i c b Z A 9 S w N B E I b n 4 k d i 1 B i 1 s L B Z D I J V u F N R y 4 C N Z Q T z A f E M e 5 t N s m R v 7 9 i d E 8 O R 0 h 9 g Y 6 G I r a 1 / x c 5 / 4 + a j 0 M Q X F h 7 e d 4 a d m S C W w q D r f j u Z p e W V 1 W x u L b + + s V n Y K m 7 v 1 E 2 U a M Z r L J K R b g b U c C k U r 6 F A y Z u x 5 j Q M J G 8 E g 8 t x 3 r j n 2 o h I 3 e A w 5 n 5 I e 0 p 0 B a N o r c b D n d d O g 1 G 7 W H L L 7 k R k E b w Z l C p 7 h c e T z y x W 2 8 W v 2 0 7 E k p A r Z J I a 0 / L c G P 2 U a h R M 8 l H + N j E 8 p m x A e 7 x l U d G Q G z + d j D s i h 9 b p k G 6 k 7 V N I J u 7 v j p S G x g z D w F a G F P t m P h u b / 2 W t B L s X f i p U n C B X b P p R N 5 E E I z L e n X S E 5 g z l 0 A J l W t h Z C e t T T R n a C + X t E b z 5 l R e h f l z 2 z s r u t V e q n M J U O d i H A z g C D 8 6 h A l d Q h R o w G M A T v M C r E z v P z p v z P i 3 N O L O e X f g j 5 + M H T 4 m R v Q = = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J q d E / H v / R c Z e E H A N b D W 1 h e A T G Z g = " &gt; A A A B 7 n i c b Z A 9 S w N B E I b n 4 k d i 1 B i 1 s L A 5 D I J V u I u i l g E b y w j m A 5 I z 7 G 3 2 k i V 7 e 8 f u n B i O l P 4 A G w t F b G 3 9 K 3 b + G z c f h S a + s P D w v j P s z P i x 4 B o d 5 9 v K r K y u r W d z G / n N r e 3 C T n F 3 r 6 G j R F F W p 5 G I V M s n m g k u W R 0 5 C t a K F S O h L 1 j T H 1 5 N 8 u Y 9 U 5 p H 8 h Z H M f N C 0 p c 8 4 J S g s Z o P d 5 V u 6 o + 7 x Z J T d q a y l 8 G d Q 6 l 6 U H g 8 / c x i r V v 8 6 v Q i m o R M I h V E 6 7 b r x O i l R C G n g o 3 z n U S z m N A h 6 b O 2 Q U l C p r 1 0 O u 7 Y P j Z O z w 4 i Z Z 5 E e + r + 7 k h J q P U o 9 E 1 l S H C g F 7 O J + V / W T j C 4 9 F I u 4 w S Z p L O P g k T Y G N m T 3 e 0 e V 4 y i G B k g V H E z q 0 0 H R B G K 5 k J 5 c w R 3 c e V l a F T K 7 n n Z u X F L 1 T O Y K Q e H c A Q n 4 M I F V O E a a l A H C k N 4 g h d 4 t W L r 2 X q z 3 m e l G W v e s w 9 / Z H 3 8 A F E R k b 4 = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z g y M j T Z f I / 8 s R c y l 7 d S m q 7 w e g e k = " &gt; A A A B 7 n i c b Z D L S g M x F I b P 1 E t r v d X L z k 2 w C G 4 s M 0 X U Z c G N y w r 2 g u 1 Y M m m m D c 1 k Q p J R 6 t C H c O N C U b c + j z v f x v S y 0 N Y f A h / / f w 4 5 5 w S S M 2 1 c 9 9 v J L C 2 v r G Z z a / n 1 j c 2 t 7 c L O b l 3 H i S K 0 R m I e q 2 a A N e V M 0 J p h h t O m V B R H A a e N Y H A 5 z h v 3 V G k W i x s z l N S P c E + w k B F s r N V 4 v C t 3 U j H q F I p u y Z 0 I L Y I 3 g 2 J l / 6 T 3 k H 2 / r X Y K X + 1 u T J K I C k M 4 1 r r l u d L 4 K V a G E U 5 H + X a i q c R k g H u 0 Z V H g i G o / n Y w 7 Q k f W 6 a I w V v Y J g y b u 7 4 4 U R 1 o P o 8 B W R t j 0 9 X w 2 N v / L W o k J L / y U C Z k Y K s j 0 o z D h y M R o v D v q M k W J 4 U M L m C h m Z 0 W k j x U m x l 4 o b 4 / g z a + 8 C P V y y T s r u d d e s X I K U + X g A A 7 h G D w 4 h w p c Q R V q Q G A A T / A C r 4 5 0 n p 0 3 5 2 N a m n F m P X v w R 8 7 n D 4 J 6 k e E = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L l Z W p z z V S r h 0 s 8 3 m i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 n</head><label>1</label><figDesc>t e o P t 1 4 7 l a N 2 o e i W 3 I n I P H g / U C z v H v X u s 2 8 3 l X b h s 9 W J W B K i N E x Q r Z u e G x s / p c p w J n C U b y U a Y 8 o G t I d N i 5 K G q P 1 0 M u 6 I H F i n Q 7 q R s k 8 a M n F / d 6 Q 0 1 H o Y B r Y y p K a v Z 7 O x + V / W T E z 3 3 E + 5 j B O D k k 0 / 6 i a C m I i M d y c d r p A Z M b R A m e J 2 V s L 6 V F F m 7 I X y 9 g j e 7 M r z U D s u e a c l 9 8 o r l k 9 g q h z s w T 4 c g g d n U I Z L q E A V G A z g E Z 7 h x Y m d J + f V e Z + W Z p y f n h 3 4 I + f j G 4 D y k e A = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C S C h 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 3 / D N u J h a / p j M p 3 l V k f / p p i L 5 z 3 M = " &gt; A A A C B 3 i c b V A 9 S w N B E N 2 L H 4 l R Y 9 T C Q p D F I F i F u y h q G b C x s I h g P i A 5 j 7 3 N J l m y t 3 f s z i n h S K e F f 8 V G R B F b / 4 K d / 8 b N R 6 G J D w Y e 7 8 0 w M 8 + P B N d g 2 9 9 W a m F x a T m d W c m u r q 3 n N v K b W z U d x o q y K g 1 F q B o + 0 U x w y a r A Q b B G p B g J f M H q f v 9 8 5 N d v m d I 8 l N c w i J g b k K 7 k H U 4 J G M n L 7 7 U C A j 1 K R H I 5 v C l 5 i d 9 S v N s D o l R 4 h + X Q y x f s o j 0 G n i f O l B T K O 7 m H o 5 c 0 V L z 8 V 6 s d 0 j h g E q g g W j c d O w I 3 I Q o 4 F W y Y b c W a R Y T 2 S Z c 1 D Z U k Y N p N x n 8 M 8 Y F R 2 r g T K l M S 8 F j 9 P Z G Q Q O t B 4 J v O 0 d V 6 1 h u J / 3 n N G D p n b s J l F A O T d L K o E w s M I R 6 F g t t c M Q p i Y A i h i p t b M e 0 R R S i Y 6 L I m B G f 2 5 X l S K x W d k 6 J 9 5 R T K x 2 i C D N p F + + g Q O e g U l d E F q q A q o u g e P a F X 9 G Y 9 W s / W u / U x a U 1 Z 0 5 l t 9 A f W 5 w + 4 W J w K &lt; / l a t e x i t &gt; L 2 b!n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N k v N 5 5 R l 9 A B B D m y m l s I Z e w B Y 5 u 8 = " &gt; A A A C B 3 i c b V A 9 S w N B E N 2 L H 4 l R Y 9 T C Q p D F I F i F u y h q G b C x s I h g P i A 5 j 7 3 N J l m y t 3 f s z i n h S K e F f 8 V G R B F b / 4 K d / 8 b N R 6 G J D w Y e 7 8 0 w M 8 + P B N d g 2 9 9 W a m F x a T m d W c m u r q 3 n N v K b W z U d x o q y K g 1 F q B o + 0 U x w y a r A Q b B G p B g J f M H q f v 9 8 5 N d v m d I 8 l N c w i J g b k K 7 k H U 4 J G M n L 7 7 U C A j 1 K R H I 5 v C l 5 i W w p 3 u 0 B U S q 8 w / 7 Q y x f s o j 0 G n i f O l B T K O 7 m H o 5 c 0 V L z 8 V 6 s d 0 j h g E q g g W j c d O w I 3 I Q o 4 F W y Y b c W a R Y T 2 S Z c 1 D Z U k Y N p N x n 8 M 8 Y F R 2 r g T K l M S 8 F j 9 P Z G Q Q O t B 4 J v O 0 d V 6 1 h u J / 3 n N G D p n b s J l F A O T d L K o E w s M I R 6 F g t t c M Q p i Y A i h i p t b M e 0 R R S i Y 6 L I m B G f 2 5 X l S K x W d k 6 J 9 5 R T K x 2 i C D N p F + + g Q O e g U l d E F q q A q o u g e P a F X 9 G Y 9 W s / W u / U x a U 1 Z 0 5 l t 9 A f W 5 w + 4 9 J w K &lt; / l a t e x i t &gt; L 2 n!b &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h e l 0 C Z 1 Z W E 7 s D K R 8 G e i C l f s l 7 R U = " &gt; A A A C B 3 i c b V A 9 S w N B E N 2 L H 4 n x 6 9 T C Q p D F I F i F O x W 1 D N h Y W E Q w H 5 C c Y W + z S Z b s 7 R 2 7 c 0 o 4 0 m n h X 7 E R U c T W v 2 D n v 3 E v S a G J D w Y e 7 8 0 w M 8 + P B N f g O N 9 W Z m 5 + Y T G b W 8 o v r 6 y u r d s b m 1 U d x o q y C g 1 F q O o + 0 U x w y S r A Q b B 6 p B g J f M F q f v 8 8 9 W u 3 T G k e y m s Y R M w L S F f y D q c E j N S y d 5 s B g R 4 l I r k c 3 r i t R D Y V 7 / a A K B X e Y X / Y s g t O 0 R k B z x J 3 Q g q l 7 b W H o 5 c s l F v 2 V 7 M d 0 j h g E q g g W j d c J w I v I Q o 4 F W y Y b 8 a a R Y T 2 S Z c 1 D J U k Y N p L R n 8 M 8 b 5 R 2 r g T K l M S 8 E j 9 P Z G Q Q O t B 4 J v O 9 G o 9 7 a X i f 1 4 j h s 6 Z l 3 A Z x c A k H S / q x A J D i N N Q c J s r R k E M D C F U c X M r p j 2 i C A U T X d 6 E 4 E 6 / P E u q h 0 X 3 p O h c u Y X S M R o j h 3 b Q H j p A L j p F J X S B y q i C K L p H T + g V v V m P 1 r P 1 b n 2 M W z P W Z G Y L / Y H 1 + Q O 3 X 5 w J &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Visual transformation for the broad view. We compare various augmentations for the visual input of the broad view, when pre-training on Kinetics-600. We use τn = 1.3s (narrow extent) and τ b = 10s (broad extent). RC stands for random convolutions.</figDesc><table><row><cell>UCF101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Weight sharing. We explore the effect of sharing weights across different components of the models. Models are trained on the Kinetics-600 dataset using RGB visual input in the broad view. Sync study. Effect of syncing the narrow and broad views.</figDesc><table><row><cell cols="5">Separate Backbone Projector Predictor Separate Separate HMDB51 UCF101</cell></row><row><cell></cell><cell></cell><cell>59.6</cell><cell></cell><cell>87.8</cell></row><row><cell></cell><cell></cell><cell>56.4</cell><cell></cell><cell>86.5</cell></row><row><cell></cell><cell></cell><cell>51.4</cell><cell></cell><cell>82.5</cell></row><row><cell></cell><cell></cell><cell>51.8</cell><cell></cell><cell>83.0</cell></row><row><cell>Dataset Sync</cell><cell>M b</cell><cell cols="3">HMDB51 UCF101 K600</cell></row><row><cell>K600</cell><cell>RGB+RC</cell><cell>63.3</cell><cell>89.5</cell><cell>66.9</cell></row><row><cell>K600</cell><cell>RGB+RC</cell><cell>65.0</cell><cell>86.8</cell><cell>60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7 .</head><label>7</label><figDesc>Sync study. Effect of syncing the narrow and broad views.</figDesc><table><row><cell>Dataset Sync</cell><cell>M b</cell><cell cols="3">HMDB51 UCF101 K600</cell></row><row><cell>AS</cell><cell>Audio</cell><cell>67.2</cell><cell>92.1</cell><cell>69.0</cell></row><row><cell>AS</cell><cell>Audio</cell><cell>68.1</cell><cell>92.4</cell><cell>69.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Antoine Miech, Bilal Piot, Evan Shelhamer and Sander Dieleman for fruitful discussions as well as Andy Brock for his help with the NFNet implementation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 5</ref><p>. Comparison of learnt representations against the state-of-the-art. We report the performance in the linear and fine-tuning (FT) settings, on three vision benchmarks: UCF101, HMDB51, Kinetics-600 (K600); as well as on two audio benchmarks: ESC-50 and AudioSet (AS). K400 is Kinetics-400, YT8M is Youtube-8M <ref type="bibr" target="#b0">[1]</ref>, IG65M is Instagram-65M <ref type="bibr" target="#b25">[26]</ref>. We specify dataset sizes in years. We denote the modalities M used for training by: V for RGB, F for flow and A for audio. All models use only RGB for the visual downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101</head><p>HMDB51 is on par with the best previous finetuned results. This is an important practical achievement as it enables the use of our models off-the-shelf, without the need for fine-tuning. Our TSM-50x2 model (93.9M parameters) is the best overall, setting a new state-of-the-art on HMDB51 finetuning with 77.8 even outperforming the best supervised results published to date (75.9 from <ref type="bibr" target="#b86">[87]</ref>). (iv) When evaluating the performance of the broad audio network we also significantly outperform previous state-of-the-art on two challenging benchmarks, ESC-50 and Audioset. Notably, we significantly improve the performance in AudioSet, the hardest of the audio tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced BraVe, a self-supervised learning framework for video. Our method efficiently learns its representation by supervising a temporally narrow view with a general broad view, which can be either computed from RGB, flow or audio. Our model achieves state-of-theart performance when trained on datasets such as Kinetics or AudioSet. Notably, when trained with larger backbones, BraVe outperforms the previous best supervised transfer result on the challenging HMDB51 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we provide additional details useful for reproduction of the results. In Section A we present the details of our training pipeline, including architecture and hyperparameter details (A.1), data augmentation and feature extraction (A.2). In Section B we detail the linear and fine-tuning evaluation procedures. Section C describes in more detail the TSM-NF-F0 architecture used in the paper. Section D evaluates the importance of syncing video and audio.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<idno>ICLR, 2021. 13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13278</idno>
		<title level="m">Representation learning with video deep infomax</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A deep residual network for large-scale acoustic scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Glass</surname></persName>
		</author>
		<editor>InterSpeech</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In NeurIPS, 2020. 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bootstrap latent-predictive representations for multitask reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="3875" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of semantic audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheet</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayang</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Contrastive representation learning: A framework and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Le-Khac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeaton</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Network randomization: A simple technique for generalization in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>ICLR, 2020. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Howto100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Audiovisual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-modal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03800</idno>
		<title level="m">Spatiotemporal contrastive video representation learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Byol works even without batch statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (SSL Workshop)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arda</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visual grounding in video for unsupervised word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno>WACV, 2020. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deepcluster: A general clustering framework based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD, 2017</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Robust and generalizable visual representation learning via random convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13003</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
