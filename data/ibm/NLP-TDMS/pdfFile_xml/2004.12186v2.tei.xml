<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientPose: Scalable single-person pose estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Groos</surname></persName>
							<email>daniel.groos@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Neuromedicine and Movement Science</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Heri</forename><surname>Ramampiaro</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Norwegian Univer-sity of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Espen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ihlen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Neuromedicine and Movement Science</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Groos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri</forename><surname>Ramampiaro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri@ntnu</forename><surname>No Espen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ihlen</surname></persName>
						</author>
						<title level="a" type="main">EfficientPose: Scalable single-person pose estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human pose estimation · Model scalabil- ity · High precision · Computational efficiency · Openly available</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-person human pose estimation facilitates markerless movement analysis in sports, as well as in clinical applications. Still, state-of-the-art models for human pose estimation generally do not meet the requirements of real-life applications. The proliferation of deep learning techniques has resulted in the development of many advanced approaches. However, with the progresses in the field, more complex and inefficient models have also been introduced, which have caused tremendous increases in computational demands. To cope with these complexity and inefficiency challenges, we propose a novel convolutional neural network architecture, called EfficientPose, which exploits recently proposed EfficientNets in order to deliver efficient and scalable single-person pose estimation. EfficientPose is a family of models harnessing an effective multi-scale feature extractor and computationally efficient detection blocks using mobile inverted bottleneck convolutions, while at the same time ensuring that the precision of the pose configurations is still improved. Due to its low complexity and efficiency, EfficientPose enables real-world applications on edge devices by limiting the memory footprint and computational cost. The results from our experiments, using the challenging MPII single-person benchmark, show that the proposed EfficientPose models substantially outperform the widely-used OpenPose model both in terms of accuracy and computational efficiency. In particular, our top-performing model achieves state-of-the-art accuracy on single-person MPII, with low-complexity ConvNets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single-person human pose estimation (HPE) refers to the computer vision task of localizing human skeletal keypoints of a person from an image or video frames. Single-person HPE has many real-world applications, ranging from outdoor activity recognition and computer animation to clinical assessments of motor repertoire and skill practice among professional athletes. The proliferation of deep convolutional neural networks (Con-vNets) has advanced HPE and further widen its application areas. ConvNet-based HPE with its increasingly complex network structures, combined with transfer learning, is a very challenging task. However, the availability of high-performing ImageNet <ref type="bibr" target="#b8">[9]</ref> backbones, together with large tailor-made datasets, such as MPII for 2D pose estimation <ref type="bibr" target="#b0">[1]</ref>, has facilitated the development of new improved methods to address the challenges.</p><p>An increasing trend in computer vision has driven towards more efficient models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46]</ref>. Recently, Effi-cientNet <ref type="bibr" target="#b47">[47]</ref> was released as a scalable ConvNet archi-tecture, setting benchmark record on ImageNet with a more computationally efficient architecture. However, within human pose estimation, there is still a lack of architectures that are both accurate and computationally efficient at the same time. In general, current stateof-the-art architectures are computationally expensive and highly complex, thus making them hard to replicate, cumbersome to optimize, and impractical to embed into real-world applications.</p><p>The OpenPose network <ref type="bibr" target="#b5">[6]</ref> (OpenPose for short) has been one of the most applied HPE methods in realworld applications. It is also the first open-source realtime system for HPE. OpenPose was originally developed for multi-person HPE, but has in recent years been frequently applied to various single-person applications within clinical research and sport sciences <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref>. The main drawback with OpenPose is that the level of detail in keypoint estimates is limited due to its low-resolution outputs. This makes OpenPose less suitable for precision-demanding applications, such as elite sports and medical assessments, which all depend on high degree of precision in the assessment of movement kinematics. Moreover, by spending 160 billion floatingpoint operations (GFLOPs) per inference, OpenPose is considered highly inefficient. Despite these issues, OpenPose seems to remain a commonly applied network for single-person HPE performing markerless motion capture from which critical decisions are based upon <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>In this paper, we stress the lack of publicly available methods for single-person HPE that are both computationally efficient and effective in terms of estimation precision. To this end, we exploit recent advances in ConvNets and propose an improved approach called Ef-ficientPose. Our main idea is to modify OpenPose into a family of scalable ConvNets for high-precision and computationally efficient single-person pose estimation from 2D images. To assess the performance of our approach, we perform two separate comparative studies. First, we evaluate the EfficientPose model by comparing it against the original OpenPose model on singleperson HPE. Second, we compare it against the current state-of-the-art single-person HPE methods on the official MPII challenge, focusing on accuracy as a function of the number of parameters. The proposed Efficient-Pose models aim to elicit high computational efficiency, while bridging the gap in availability of high-precision HPE networks.</p><p>In summary, the main contributions of this paper are the following:</p><p>-We propose an improvement of OpenPose, called EfficientPose, that can overcome the shortcomings of the popular OpenPose network on single-person HPE with improved level of precision, rapid convergence during optimization, low number of parameters, and low computational cost. -With EfficientPose, we suggest an approach providing scalable models that can suit various demands, enabling a trade-off between accuracy and efficiency across diverse application constraints and limited computational budgets. -We propose a new way to incorporate mobile Con-vNet components, which can address the need for computationally efficient architectures for HPE, thus facilitating real-time HPE on the edge. -We perform an extensive comparative study to evaluate our approach. Our experimental results show that the proposed method achieves significantly higher efficiency and accuracy in comparison to the baseline method, OpenPose. In addition, compared to existing state-of-the-art methods, it achieves competitive results, with a much smaller number of parameters.</p><p>The remainder of this paper is organized as follows: Section 2 describes the architecture of OpenPose and highlights research which it can be improved from. Based on this, Section 3 presents our proposed ConvNetbased approach, EfficientPose. Section 4 describes our experiments and presents the results from comparing EfficientPose with OpenPose and other existing approaches. Section 5 discusses our findings and suggests potential future studies. Finally, Section 6 summarizes and concludes the paper.</p><p>For the sake of reproducibility, we will make the Ef-ficientPose models available at https://github.com/ daniegr/EfficientPose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The proliferation of ConvNets for HPE following the success of DeepPose <ref type="bibr" target="#b54">[54]</ref> has set the path for accurate HPE. With OpenPose, Cao et al. <ref type="bibr" target="#b5">[6]</ref> made HPE available to the public. As depicted by <ref type="figure" target="#fig_0">Figure 1</ref>, OpenPose comprises a multi-stage architecture performing a series of detection passes. Provided an input image of 368 × 368 pixels, OpenPose utilizes an ImageNet pretrained VGG-19 backbone <ref type="bibr" target="#b41">[41]</ref> to extract basic features (step 1 in <ref type="figure" target="#fig_0">Figure 1</ref>). The features are supplied to a DenseNet-inspired detection block (step 2) arranged as five dense blocks <ref type="bibr" target="#b22">[23]</ref>, each containing three 3 × 3 convolutions with PReLU activations <ref type="bibr" target="#b19">[20]</ref>. The detection blocks are stacked in a sequence. First, four passes (step 3a-d in <ref type="figure" target="#fig_0">Figure 1</ref>) of part affinity fields <ref type="bibr" target="#b6">[7]</ref> map the associations between body keypoints. Subsequently, two detection passes (step 3e and 3f) predict keypoint heatmaps <ref type="bibr" target="#b53">[53]</ref> to obtain refined keypoint coordinate estimates. In terms of level of detail in the keypoint coordinates, OpenPose is restricted by its output resolution of 46 × 46 pixels.</p><p>The OpenPose architecture can be improved by recent advancements in ConvNets, as follows: First, automated network architecture search has found backbones <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr">62]</ref> that are more precise and efficient in image classification than VGG and ResNets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">41]</ref>. In particular, Tan and Le <ref type="bibr" target="#b47">[47]</ref> proposed compound model scaling to balance the image resolution, width (number of network channels), and depth (number of network layers). This resulted in scalable convolutional neural networks, called EfficientNets <ref type="bibr" target="#b47">[47]</ref>, with which the main goal was to provide lightweight models with a sensible trade-off between model complexity and accuracy across various computational budgets. For each model variant EfficientNet-Bφ, from the least computationally expensive one being EfficientNet-B0 to the most accurate model, EfficientNet-B7 (φ ∈ [0, 7] ∈ Z ≥ ), the total number of FLOPs increases by a factor of 2, given by</p><formula xml:id="formula_0">(α · β 2 · γ 2 ) φ ≈ 2 φ .<label>(1)</label></formula><p>Here, α, β and γ denote the coefficients for depth, width, and resolution, respectively, and are set as</p><formula xml:id="formula_1">α = 1.2, β = 1.1, γ = 1.15.<label>(2)</label></formula><p>Second, parallel multi-scale feature extraction has improved the precision levels in HPE <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b57">57]</ref>, emphasizing both high spatial resolution and low-scale semantics. However, existing multi-scale approaches in HPE are computationally expensive, both due to their large size and high computational requirements. For example, a typical multi-scale HPE approach has often a size of 16 − 58 million parameters and requires 10 − 128 GFLOPS <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr">61]</ref>. To cope with this, we propose cross-resolution features, operating on high-and low-resolution input images, to integrate features from multiple abstraction levels with low overhead in network complexity and with high computational efficiency. Existing works on Siamese Con-vNets have been promising in utilizing parallel network backbones <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Third, mobile inverted bottleneck convolution (MBConv) <ref type="bibr" target="#b38">[38]</ref> with built-in squeezeand-excitation (SE) <ref type="bibr" target="#b21">[22]</ref> and Swish activation <ref type="bibr" target="#b37">[37]</ref> integrated in EfficientNets has proven more accurate in image classification tasks <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref> than regular convolutions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">45]</ref>, while substantially reducing the computational costs <ref type="bibr" target="#b47">[47]</ref>. The efficiency of MBConv modules stem from the depthwise convolutions operating in a channel-wise manner <ref type="bibr" target="#b40">[40]</ref>. With this approach, it is possible to reduce the computational cost by a factor proportional to the number of channels <ref type="bibr" target="#b48">[48]</ref>. Hence, by replacing the regular 3 × 3 convolutions with up to 384 input channels in the detection blocks of OpenPose with MBConvs, we can obtain more computationally efficient detection blocks. Further, SE selectively emphasizes discriminative image features <ref type="bibr" target="#b21">[22]</ref>, which may reduce the required number of convolutions and detection passes by providing a global perspective on the estimation task at all times. Using MBConv with SE may have the potential to decrease the number of dense blocks in OpenPose. Fourth, transposed convolutions with bilinear kernel <ref type="bibr" target="#b30">[30]</ref> scale up the low-resolution feature maps, thus enabling a higher level of detail in the output confidence maps. By building upon the work of Tan and Le <ref type="bibr" target="#b47">[47]</ref>, we present a pool of scalable models for single-person HPE that is able to overcome the shortcomings of the commonly adopted OpenPose architecture. This enables trading off between accuracy and efficiency across different computational budgets in real-world applications. The main advantage of this is that we can use ConvNets that are small and computationally efficient enough to run on edge devices with little memory and low processing power, which is impossible with OpenPose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The EfficientPose approach</head><p>In this section, we explain in details the EfficientPose approach. This includes a detailed description of the Ef-ficientPose architecture in light of the OpenPose architecture, and a brief introduction to the proposed variants of EfficientPose. The input of the network consists of high and lowresolution images (1a and 1b in <ref type="figure">Figure 2</ref>). To get the low-resolution image, the high-resolution image is downsampled into half of its pixel height and width, through an initial average pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The feature extractor of EfficientPose is composed of the initial blocks of EfficientNets <ref type="bibr" target="#b47">[47]</ref> pretrained on  <ref type="figure">Figure 2</ref>). High-level semantic information is obtained from the high-resolution image using the initial three blocks of a high-scale Effi-cientNet with φ ∈ [2, 7] (see Equation 1), outputting C feature maps (2a in <ref type="figure">Figure 2</ref>). Low-level local information is extracted from the low-resolution image by the first two blocks of a lower-scale EfficientNet-backbone (2b in <ref type="figure">Figure 2</ref>) in the range φ ∈ [0, 3]. <ref type="table">Table 1</ref> provides an overview of the composition of EfficientNet back-bones, from low-scale B0 to high-scale B7. The first block of EfficientNets utilizes the MBConvs shown in <ref type="figure" target="#fig_2">Figure 3a</ref> and 3b, whereas the second and third blocks comprise the MBConv layers in <ref type="figure" target="#fig_2">Figure 3c</ref> and 3d.</p><p>The features generated by the low-level and highlevel EfficientNet backbones are concatenated to yield cross-resolution features (step 3 in <ref type="figure">Figure 2</ref>). This enables the EfficientPose architecture to selectively emphasize important local factors from the image of inter- <ref type="table">Table 1</ref> The architecture of the initial three blocks of relevant EfficientNet backbones. For Conv(K × K, N, S), K × K denotes filter size, N is number of output feature maps, and S is stride. BN denotes batch normalization. I defines input size, corresponding with image resolution on ImageNet, whereas α φ refers to the depth factor as determined by Equation 1</p><p>Block est and the overall structures that guide high-quality pose estimation. In this way, we enable an alternative simultaneous handling of different features at multiple abstraction levels. From the extracted features, the desired keypoints are localized through an iterative detection process, where each detection pass performs supervised prediction of output maps. Each detection pass comprises a detection block and a single 1 × 1 convolution for output prediction. The detection blocks across all detection passes elicit the same basic architecture, comprising Mobile DenseNets (see step 4 in <ref type="figure">Figure 2</ref>). Data from Mobile DenseNets are forwarded to subsequent layers of the detection block using residual connections. The Mobile DenseNet is inspired by DenseNets <ref type="bibr" target="#b22">[23]</ref> supporting reuse of features, avoiding redundant layers, and MB-Conv with SE, thus enabling low memory footprint. In our adaptation of the MBConv operation (E-M BConv6 (K × K, B, S) in <ref type="figure" target="#fig_2">Figure 3e</ref>), we consistently utilize the highest performing combination from <ref type="bibr" target="#b46">[46]</ref>, i.e., a kernel size (K × K) of 5 × 5 and an expansion ratio of 6. We also avoid downsampling (i.e., S = 1) and scale the width of Mobile DenseNets by outputting number of channels relative to the high-level backbone (B = C). We modify the original M BConv6 operation by incorporating E-swish as activation function with β value of 1.25 <ref type="bibr" target="#b15">[16]</ref>. This has a tendency to accelerate progression during training compared to the regular Swish activation <ref type="bibr" target="#b37">[37]</ref>. We also adjust the first 1 × 1 convolution to generate a number of feature maps relative to the output feature maps B rather than the input channels M . This reduces the memory consumption and computational latency since B ≤ M , with C ≤ M ≤ 3C. With each Mobile DenseNet consisting of three consecutive E-M BConv6 operations, the module outputs 3C feature maps.</p><formula xml:id="formula_2">B0 B1 B2 B3 B4 B5 B7 1 Conv(3 × 3, 32, 2) BN Swish Conv(3 × 3, 40, 2) BN Swish Conv(3 × 3, 48, 2) BN Swish M BConv6 * (5 × 5, 64, 1) × 4 M BConv6 * (5 × 5, 80, 1) × 6 I 224 × 224 240 × 240 260 × 260 300 × 300 380 × 380 456 × 456 600 × 600 C 40 48 56 64 80 α φ 1.2 0 = 1.0 1.2 1 = 1.2 1.2 2 = 1.4 1.2 3 = 1.7 1.2 4 = 2.1 1.2 5 = 2.5 1.2 7 = 3.6</formula><p>EfficientPose performs detection in two rounds (step 5a-c in <ref type="figure">Figure 2</ref>). First, the overall pose of the person is anticipated through a single pass of skeleton estimation (5a). This aims to facilitate the detection of feasible poses and to avoid confusion in case of several persons being present in an image. Skeleton estimation is performed utilizing part affinity fields as proposed in <ref type="bibr" target="#b6">[7]</ref>. Following skeleton estimation, two detection passes are performed to estimate heatmaps for keypoints of interest. The former of these acts as a coarse detector (5b in <ref type="figure">Figure 2</ref>), whereas the latter (5c in <ref type="figure">Figure 2</ref>) refines localization to yield more accurate outputs.</p><p>Note that in OpenPose, the heatmaps of the final detection pass are constrained to a low spatial resolution, which are incapable of achieving the amount of details that are normally inherent in the high-resolution input <ref type="bibr" target="#b5">[6]</ref>. To improve this limitation of OpenPose, a series of three transposed convolutions performing bilinear upsampling are added for 8× upscaling of the low-resolution heatmaps (step 6 in <ref type="figure" target="#fig_0">Figure 1</ref>). Thus, we project the low-resolution output onto a space of higher resolution in order to allow an increased level of detail. To achieve the proper level of interpolation while operating efficiently, each transposed convolution increases the map size by a factor of 2, using a stride of 2 with a 4 × 4 kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variants</head><p>Following the same principle as suggested in the original EfficientNet <ref type="bibr" target="#b47">[47]</ref>, we scale the EfficientPose network architecture by adjusting the three main dimensions, i.e., input resolution, network width, and network depth, using the coefficients of Equation 2. The results from this scaling are five different architecture variants that are given in <ref type="table" target="#tab_0">Table 2</ref>, referred to as EfficientPose I to IV and RT). As can be observed in this table, the input resolution, defined by the spatial dimensions of the image (H × W ), is scaled utilizing the high and low-level EfficientNet backbones that best match the resolution of high and low-resolution inputs (see <ref type="table">Table 1</ref>). Here, the network width refers to the number of feature maps that are generated by each E-M BConv6. As described in Section 3.1, width scaling is achieved using the same width as the high-level backbone (i.e., C). The scaling of network depth is achieved in the number of Mobile DenseNets (i.e., M D(C) in <ref type="table" target="#tab_0">Table 2</ref>) in the detection blocks. Also, this ensures that receptive fields across different models and spatial resolutions have similar relative sizes. For each model variant, we select the number (D) of Mobile DenseNets that best approximates the original depth factor α φ in the high-level Efficient-Net backbone ( <ref type="table">Table 1)</ref>. More specifically, the number of Mobile DenseNets are determined by Equation 3, rounding to the closest integer. In addition to Efficient-Pose I to IV, the single-resolution model EfficientPose RT is formed to match the scale of the smallest Ef-ficientNet model, providing HPE in extremely low latency applications.</p><formula xml:id="formula_3">D = α φ + 0.5<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary of proposed framework</head><p>As can be inferred from the discussion above, the Ef-ficientPose framework comprises a family of five Con-vNets (i.e., EfficientPose I-IV and RT) that are constructed by compound scaling <ref type="bibr" target="#b47">[47]</ref>. With this, Efficient- </p><formula xml:id="formula_4">-3) B2 (Block 1-3) B4 (Block 1-3) B5 (Block 1-3) B7 (Block 1-3) Low-resolution input − 128 × 128 184 × 184 240 × 240 300 × 300 Low-level backbone − B0 (Block 1-2) B0 (Block 1-2) B1 (Block 1-2) B3 (Block 1-2) Detection block M D(40) M D(48) [M D(56)] × 2 [M D(64)] × 3 [M D(80)] × 4 Prediction pass 1 Conv(1 × 1, 2P, 1) Prediction pass 2-3 Conv(1 × 1, Q, 1) Upscaling [Conv T (4 × 4, Q, 2)] × 3</formula><p>Pose exploits the advances in computationally efficient ConvNets for image recognition to construct a scalable network architecture that is capable of performing single-person HPE across different computational constraints. More specifically, EfficientPose utilizes both high and low-resolution images to provide two separate viewpoints that are processed independently through high and low-level backbones, respectively. The resulting features are concatenated to produce cross-resolution features, enabling selective emphasis on global and local image information. The detection stage employs a scalable mobile detection block to perform detection in three passes. The first pass estimates person skeletons through part affinity fields <ref type="bibr" target="#b6">[7]</ref> to yield feasible pose configurations. The second and third passes estimate keypoint locations with progressive improvement in precision. Finally, the low-resolution prediction of the third pass is scaled up through bilinear interpolation to further improve the precision level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We evaluate EfficientPose and compare it with Open-Pose on the single-person MPII dataset <ref type="bibr" target="#b0">[1]</ref>, containing images of mainly healthy adults in a wide range of different outdoor and indoor everyday activities and situations, such as sports, fitness exercises, housekeeping activities, and public events <ref type="figure">(Figure 4a</ref>). All models are optimized on MPII using stochastic gradient descent (SGD) on the mean squared error (MSE) of the model predictions relative to the target coordinates. More specifically, we applied SGD with momentum and cyclical learning rates (see Appendix B for more information and further details on the optimization procedure). The learning rate is bounded according to the model-specific value of which it does not diverge during the first cycle (λ max ) and λ min = λmax 3000 . The model backbones (i.e., VGG-19 for OpenPose, and Efficient-Nets for EfficientPose) are initialized with pretrained ImageNet weights, whereas the remaining layers employ random weight initialization. Supported by our experiments on training efficiency (see Appendix A), we train the models for 200 epochs, except for OpenPose, which requires a higher number of epochs to converge (see <ref type="figure" target="#fig_3">Figure 5</ref> and <ref type="table" target="#tab_2">Table 5</ref>).</p><p>The training and validation portion of the dataset comprises 29K images, and by adopting a standard random split, we obtain 26K and 3K instances for training and validation, respectively. We augment the images during training using random horizontal flipping, scaling (0.75−1.25), and rotation (+/− 45 degrees). We utilize a batch size of 20, except for the high-resolutional EfficientPose III and IV, which both require a smaller batch size to fit into the GPU memory, 10 and 5, respectively. The experiments are carried out on an NVIDIA Tesla V100 GPU.</p><p>The evaluation of model accuracy is performed using the P CK h @τ metric. P CK h @τ is defined as the fraction of predictions residing within a distance τ l from the ground truth location (see <ref type="figure">Figure 4b</ref>). l is 60% of the diagonal d of the head bounding box, and τ the accepted percentage of misjudgment relative to l. P CK h @50 is the standard performance metric for MPII but we also include the stricter P CK h @10 metric for assessing models' ability to yield highly precise keypoint estimates. As commonly done in the field, the final model predictions are obtained by applying multiscale testing procedure <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b57">57]</ref>. Due to the restriction in the number of attempts for official evaluation on MPII, we only used the test metrics on the OpenPose baseline, and the most efficient and most accurate models, EfficientPose RT and EfficientPose IV, respectively. To measure model efficiency, both FLOPs and number of parameters are supplied. <ref type="table">Table 3</ref> shows the results of our experiments with Open-Pose and EfficientPose on the MPII validation dataset. <ref type="figure">Fig. 4</ref> The MPII single-person pose estimation challenge. From left: a) 10 images from the MPII test set displaying some of the variation and difficulties inherent in this challenge. b) The evaluation metrics P CK h @50 and P CK h @10 define the average of predictions within τ l distance (l = 0.6d) from the ground-truth location (e.g., left elbow), with τ being 50% and 10%, respectively</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>As can be observed in this table, EfficientPose consistently outperformed OpenPose with regards to efficiency, with 2.2−184× reduction in FLOPs and 4−56× fewer number of parameters. In addition to this, all the model variants of EfficientPose achieved better highprecision localization, with a 0.8 − 12.9% gain in P CK h @10 as compared to OpenPose. In terms of P CK h @50, the high-end models, i.e., EfficientPose II-IV, managed to gain 0.6 − 2.2% improvements against OpenPose. As <ref type="table" target="#tab_3">Table 4</ref> depicts, EfficientPose IV achieved state-of-the-art results (a mean P CK h @50 of 91.2) on the official MPII test dataset for models with number of parameters of a size less than 10 million.</p><p>Compared to OpenPose, EffcientPose also exhibited rapid convergence during training. We optimized both approaches on similar input resolution, which defaults to 368 × 368 for OpenPose, corresponding to Efficient-Pose II. The training graph shown in <ref type="figure" target="#fig_3">Figure 5</ref> demonstrates that EfficientPose converges early, whereas Open-Pose requires up to 400 epochs before achieving proper convergence. Nevertheless, OpenPose benefited from this prolonged training in terms of precision, with a 2.6% improvement in P CK h @50 during the final 200 epochs, whereas EfficientPose II had a minor gain of 0.4% (see <ref type="table" target="#tab_2">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we discuss several aspects of our findings and possible avenues for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Improvements over OpenPose</head><p>The precision of HPE methods is a key success factor for analyses of movement kinematics, like segment positions and joint angles, for assessment of sport performance in athletes, or motor disabilities in patients. Facilitated by cross-resolution features and upscaling of output (see Appendix A), EfficientPose achieved a higher precision than OpenPose <ref type="bibr" target="#b5">[6]</ref>, with a 57% relative improvement in P CK h @10 on single-person MPII ( <ref type="table">Table 3)</ref>. What this means is that the EfficientPose architecture is generally more suitable in performing precision-demanding single-person HPE applications, like medical assessments and elite sports, than OpenPose.</p><p>Another aspect to have in mind is that, for some applications (e.g., exercise games and baby monitors), we might be more interested in the latency of the system and its ability to respond quickly. Hence, the degree of correctness in keypoint predictions might be less crucial. In such scenarios, with applications that demand high-speed predictions, the 460K parameter model, Ef-ficientPose RT, consuming less than one GFLOP, would be suitable. Nevertheless, it still manages to provide higher precision level than current approaches in the high-speed regime, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">50]</ref>. Further, the scalability of EfficientPose enables flexibility in various situations and across different types of hardware, whereas Open-Pose suffers from its large number of parameters and computational costs (FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Strengths of the EfficientPose approach</head><p>The use of MBConv in HPE is to the best of our knowledge an unexplored research area. This has also been partly our main motivation for exploring the use of   MBConv in our EfficientPose approach, recognizing its success in image classification <ref type="bibr" target="#b47">[47]</ref>. Our experimental results showed that EfficientPose approached state-ofthe-art performance on the single-person MPII benchmark despite a large reduction in the number of parameters (  <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>Further, the use of EfficientNet as a backbone, and the proposed cross-resolution feature extractor combining several EfficientNets for improved handling of basic features, are also interesting avenues to explore further. From the present study, it is reasonable to assume that EfficientNets could replace commonly used backbones for HPE, such as VGG and ResNets, which would reduce the computational overheads associated with these approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">41]</ref>. Also, a cross-resolution feature extractor could be useful for precision-demanding applications by providing an improved performance on P CK h @10 ( <ref type="table">Table 6</ref>).</p><p>We also observed that EfficientPose benefited from compound model scaling across resolution, width and depth. This benefit was reflected by the increasing improvements in P CK h @50 and P CK h @10 from Effi-cientPose RT through EfficientPose I to EfficientPose IV <ref type="table">(Table 3)</ref>. To conclude, we can exploit this to further examine scalable ConvNets for HPE, and thus obtain insights into appropriate sizes of HPE models (i.e., number of parameters), required number of FLOPs, and obtainable precision levels.</p><p>In this study, OpenPose and EfficientPose were optimized on the general-purpose MPII Human Pose Dataset. For many applications (e.g., action recognition and video surveillance) the variability in MPII may be sufficient for directly applying the models on real-world problems. Nonetheless, there are other particular scenarios that deviate from the setting addressed in this paper. The MPII dataset comprises mostly healthy adults in a variety of every day indoor and outdoor activities <ref type="bibr" target="#b0">[1]</ref>. In less natural environments (e.g., movement science laboratories or hospital settings) and with humans of different anatomical proportions such as children and infants <ref type="bibr" target="#b39">[39]</ref>, careful consideration must be taken. This could include a need for fine-tuning of the MPII models on more specific datasets related to the problem at hand. As mentioned earlier, our experiments showed that EfficientPose was more easily trainable than Open-Pose ( <ref type="figure" target="#fig_3">Figure 5</ref> and <ref type="table" target="#tab_2">Table 5</ref>). This trait of rapid convergence suggests that exploring the use of transfer learning on the EfficientPose models on other HPE data could provide interesting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Avenues for further research</head><p>The precision level of pose configurations provided by EfficientPose in the context of target applications is a topic considered beyond the scope of this paper and has for this reason been left for further studies. We can establish the validity of EfficientPose for robust singleperson pose estimation already by examining whether the movement information supplied by the proposed framework is of sufficiently good quality for tackling challenging problems, such as complex human behavior recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">29]</ref>. To assess this, we could, for example, compare the precision level of the keypoint estimates supplied by EfficientPose with the movement information provided by body-worn movement sensors. Moreover, we could combine the proposed image-based EfficientPose models with body-worn sensors, such as inertial measurement unit (IMU) <ref type="bibr" target="#b27">[27]</ref>, or physiological signals, like electrical cardiac activity and electrical brain activity <ref type="bibr" target="#b13">[14]</ref>, to potentially achieve improved precision levels and an increased robustness. Our hypothesis is that using body-worn sensors or physiological instruments could be useful in situations where body parts are extensively occluded, such that camera-based recognition alone may not be sufficient for accurate pose estimation.</p><p>Another path for further study and validation is the capability of EfficientPose to perform multi-person HPE. The improved computational efficiency of Effi-cientPose compared to OpenPose has the potential to also benefit multi-person HPE. State-of-the-art methods for multi-person HPE are dominated by top-down approaches, which require computation that is normally proportional to the number of individuals in the image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b59">59]</ref>. In crowded scenes, top-down approaches are highly resource demanding. Similar to the original OpenPose <ref type="bibr" target="#b5">[6]</ref>, and few other recent works on multiperson HPE <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>, EfficientPose incorporates part affinity fields, which would enable the grouping of keypoints into persons, and thus allowing to perform multi-person HPE in a bottom-up manner. This would reduce the computational overhead into a single network inference per image, and hence yield more computationally efficient multi-person HPE.</p><p>Further, it would be interesting to explore the extension of the proposed framework to perform 3D pose estimation as part of our future research. In accordance with recent studies, 3D pose projection from 2D images can be achieved, either by employing geometric relationships between 2D keypoint positions and 3D human pose models <ref type="bibr" target="#b58">[58]</ref>, or by leveraging occlusion-robust pose-maps (ORPM) in combination with annotated 3D poses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>The architecture of EfficientPose and the training process can be improved in several ways. First, the optimization procedure (see Appendix B) was developed for maximum P CK h @50 accuracy on OpenPose, and simply reapplied to EfficientPose. Other optimization procedures might be more appropriate, including alternative optimizers (e.g., Adam <ref type="bibr" target="#b26">[26]</ref> and RMSProp <ref type="bibr" target="#b52">[52]</ref>), and other learning rate and sigma schedules.</p><p>Second, only the backbone of EfficientPose was pretrained on ImageNet. This could restrict the level of accuracy on HPE because large-scale pretraining not only supplies robust basic features but also higher-level semantics. Thus, it would be valuable to assess the effect of pretraining on model precision in HPE. We could, for example, pretrain the majority of ConvNet layers on ImageNet, and retrain these on HPE data.</p><p>Third, the proposed compound scaling of Efficient-Pose assumes that the scaling relationship between resolution, width, and depth, as defined by Equation 2, is identical in HPE and image classification. However, the optimal compound scaling coefficients might be different for HPE, where the precision level is more dependent on image resolution, than for image classification. Based on this, a topic for further studies could be to conduct neural architecture search across different combinations of resolution, width, and depth in order to determine the optimal combination of scaling coefficients for HPE. Regardless of the scaling coefficients, the scaling of detection blocks in EfficientPose could be improved. The block depth (i.e., number of Mobile DenseNets) slightly deviates from the original depth coefficient in EfficientNets based on the rigid nature of the Mobile DenseNets. A carefully designed detection block could address this challenge by providing more flexibility with regards to the number of layers and the receptive field size.</p><p>Fourth, the computational efficiency of EfficientPose could be further improved by the use of teacher-student network training (i.e., knowledge distillation) <ref type="bibr" target="#b3">[4]</ref> to transfer knowledge from a high-scale EfficientPose teacher network to a low-scale EfficientPose student network. This technique has already shown promising results in HPE when paired with the stacked hourglass architecture <ref type="bibr" target="#b33">[33,</ref><ref type="bibr">60]</ref>. Sparse networks, network pruning, and weight quantization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b55">55]</ref> could also be included in the study to facilitate the development of more accurate and responsive real-life systems for HPE. Finally, for high performance inference and deployment on edge devices, further speed-up could be achieved by the use of specialized libraries such as NVIDIA TensorRT and TensorFlow Lite <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>In summary, EfficientPose tackles single-person HPE with an improved degree of precision compared to the commonly adopted OpenPose network <ref type="bibr" target="#b5">[6]</ref>. In addition to this, the EfficientPose models have the ability to yield high performance with a large reduction in number of parameters and FLOPs. This has been achieved by exploiting the findings from contemporary research within image recognition on computationally efficient ConvNet components, most notably MBConvs and Effi-cientNets <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b47">47]</ref>. Again, for the sake of reproducibility, we have made the EfficientPose models publicly available for other researchers to test and possibly further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have stressed the need for a publicly accessible method for single-person HPE that suits the demands for both precision and efficiency across various applications and computational budgets. To this end, we have presented a novel method called EfficientPose, which is a scalable ConvNet architecture leveraging a computationally efficient multi-scale feature extractor, novel mobile detection blocks, skeleton estimation, and bilinear upscaling. In order to have model variants that are able to flexibly find a sensible trade-off between accuracy and efficiency, we have exploited model scalability in three dimensions: input resolution, network width, and network depth. Our experimental results have demonstrated that the proposed approach has the capability to offer computationally efficient models, allowing real-time inference on edge devices. At the same time, our framework offers flexibility to be scaled up to deliver more precise keypoint estimates than commonly used counterparts, at an order of magnitude less parameters and computational costs (FLOPs). Taking into account the efficiency and high precision level of our proposed framework, there is a reason to believe that EfficientPose will provide an important foundation for the next-generation markerless movement analysis.</p><p>In our future work, we plan to develop new techniques to further improve the model effectiveness, especially in terms of precision, by investigating optimal compound model scaling for HPE. Moreover, we will deploy EfficientPose on a range of applications to validate its applicability, as well as feasibility, in real-world scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablation study</head><p>To determine the effect of different design choices in the Ef-ficientPose architecture, we carried out component analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training efficiency</head><p>We assessed the number of training epochs to determine the appropriate duration of training, avoiding demanding optimization processes. <ref type="figure" target="#fig_3">Figure 5</ref> suggests that the largest improvement in model accuracy occurs until around 200 epochs, after which training saturates. <ref type="table" target="#tab_2">Table 5</ref> supports this observation with less than 0.4% increase in P CK h @50 with 400 epochs of training. From this, it was decided to perform the final optimization of the different variants of EfficientPose over 200 epochs. <ref type="table" target="#tab_2">Table 5</ref> also suggests that most of the learning progress occurs during the first 100 epochs. Hence, for the remainder of the ablation study 100 epochs were used to determine the effect of different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-resolution features</head><p>The value of combining low-level local information with highlevel semantic information through a cross-resolution feature extractor was evaluated by optimizing the model with and without the low-level backbone. Experiments were conducted on two different variants of the EfficientPose model. On coarse prediction (P CK h @50) there is little to no gain in accuracy <ref type="table">(Table 6)</ref>, whereas for fine estimation (P CK h @10) some improvement (0.6 − 0.7%) is displayed taking into account the negligible cost of 1.02 − 1.06× more parameters and 1.03 − 1.06× increase in FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton estimation</head><p>The effect of skeleton estimation through the approximation of part affinity fields was assessed by comparing the architecture with and without the single pass of skeleton estimation. Skeleton estimation yields improved accuracy with 1.3 − 2.4% gain in P CK h @50 and 0.2 − 1.4% in P CK h @10 <ref type="table">(Table 7)</ref>, while only introducing an overhead in number of parameters and computational cost of 1.3 − 1.4× and 1.2 − 1.3×, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of detection passes</head><p>We also determined the appropriate comprehensiveness of detection, represented by number of detection passes. Efficient-Pose I and II were both optimized on three different variants <ref type="table">(Table 8</ref>). Seemingly, the models benefit from intermediate supervision with a general trend of increased performance level in accordance with number of detection passes. The major benefit in performance is obtained by expanding from one to two passes of keypoint estimation, reflected by 1.6 − 1.7% increase in P CK h @50 and 1.8 − 1.9% in P CK h @10. In comparison, a third detection pass yields only 0.5 − 0.8% relative improvement in P CK h @50 compared to two passes, and no gain in P CK h @10 while increasing number of parameters and computation by 1.3× and 1.2×, respectively. From these findings, we decided a beneficial trade-off in accuracy and efficiency would be the use of two detection passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upscaling</head><p>To assess the impact of upscaling, implemented as bilinear transposed convolutions, we compared the results of the two respective models. <ref type="table" target="#tab_6">Table 9</ref> reflects that upscaling yields improved precision on keypoint estimates by large gains of 9.2 − 12.3% in P CK h @10 and smaller improvements of 0.5 − 1.1% on coarse detection (P CK h @50). As a consequence of increased output resolution upscaling slightly increases number of FLOPs (1.04 − 1.1×) with neglectable increase in number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimization procedure</head><p>Most state-of-the-art approaches for single-person pose estimation are extensively pretrained on ImageNet <ref type="bibr" target="#b44">[44,</ref><ref type="bibr">61]</ref>, enabling rapid convergence for models when adapted to other tasks, such as HPE. In contrast to these approaches, few models, including OpenPose <ref type="bibr" target="#b5">[6]</ref> and EfficientPose, only utilize the most basic pretrained features. This facilitates construction of more efficient network architectures but at the same time requires careful design of optimization procedures for convergence towards reasonable parameter values. Training of pose estimation models is complicated due to the intricate nature of output responses. Overall, optimization is performed in a conventional fashion by minimizing the MSE of the predicted output maps Y with respect to ground truth valuesŶ across all output responses N .</p><p>The predicted output maps should ideally have higher values at the spatial locations corresponding to body part positions, while punishing predictions farther away from the correct location. As a result, the ground truth output maps must be carefully designed to enable proper convergence during training. We achieve this by progressively reducing the circumference from the true location that should be rewarded, defined by the σ parameter. Higher probabilities T ∈ [0, 1] are assigned for positions P closer to the ground truth position G (Equation 4).</p><formula xml:id="formula_5">T i = exp (− P i − G 2 2 σ 2 )<label>(4)</label></formula><p>The proposed optimization scheme ( <ref type="figure">Figure 6</ref>) incorporates a stepwise σ scheme, and utilizes SGD with momentum of 0.9 and a decaying triangular cyclical learning rate (CLR) policy <ref type="bibr" target="#b42">[42]</ref>. The σ parameter is normalized according to the output resolution. As suggested by Smith and Topin <ref type="bibr" target="#b43">[43]</ref>, the large learning rates in CLR provides regularization in network optimization. This makes training more stable and may even increase training efficiency. This is valuable for network architectures, such as OpenPose and EfficientPose, less heavily concerned with pretraining (i.e., having larger portions of randomized weights). In our adoption of CLR, we utilize a cycle length of 3 epochs. The learning rate (λ) converges towards λ ∞ (Equation 5), where λ max is the highest learning rate for which the model does not diverge during the first cycle and λ min = λ max 3000 , whereas σ 0 and σ ∞ are the initial and final sigma values, respectively. λ ∞ = 10 log (λmax )+log (λ min ) 2 · 2 σ 0 −σ ∞ (5) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 and</head><label>1</label><figDesc>Figure 2depict the architectures of Open-Pose and EfficientPose, respectively. As can be observed in these two figures, although being based on Open-Pose, the EfficientPose architecture is different from the OpenPose architecture in several aspects, including 1) both high and low-resolution input images, 2) scalable EfficientNet backbones, 3) cross-resolution features, 4) and 5) scalable Mobile DenseNet detection blocks in fewer detection passes, and 6) bilinear upscaling. For a more thorough component analysis of EfficientPose, see Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 Fig. 2</head><label>12</label><figDesc>OpenPose architecture utilizing 1) VGG-19 feature extractor, and 2) 4+2 passes of detection blocks performing 4+2 passes of estimating part affinity fields (3a-d) and confidence maps (3e and 3f) Proposed architecture comprising 1a) high-resolution and 1b) low-resolution inputs, 2a) high-level and 2b) low-level Ef-ficientNet backbones combined into 3) cross-resolution features, 4) Mobile DenseNet detection blocks, 1+2 passes for estimation of part affinity fields (5a) and confidence maps (5b and 5c), and 6) bilinear upscaling ImageNet (step 2a and 2b in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>The composition of MBConvs. From left: a-d) M BConv(K × K, B, S) in EfficientNets performs depthwise convolution with filter size K × K and stride S, and outputs B feature maps. M BConv * (b and d) extends regular MBConvs by including dropout layer and skip connection. e) E-M BConv6(K × K, B, S) in Mobile DenseNets adjusts M BConv6 with E-swish activation and number of feature maps in expansion phase as 6B. All MBConvs take as input M feature maps with spatial height and width of h and w, respectively. R is the reduction ratio of SE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>The progression of the mean error of EfficientPose II and OpenPose on the MPII validation set during the course of training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Variants of EfficientPose obtained by scaling resolution, width, and depth. Mobile DenseNets M D(C) computes 3C feature maps. P and Q denotes the number of 2D part affinity fields and confidence maps, respectively. Conv T (K × K, O, S) defines transposed convolutions with kernel size K × K, output maps O, and stride S</figDesc><table><row><cell>Stage</cell><cell cols="5">EfficientPose RT EfficientPose I EfficientPose II EfficientPose III EfficientPose IV</cell></row><row><cell>High-resolution input</cell><cell>224 × 224</cell><cell>256 × 256</cell><cell>368 × 368</cell><cell>480 × 480</cell><cell>600 × 600</cell></row><row><cell>High-level backbone</cell><cell>B0 (Block 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 Table 4</head><label>34</label><figDesc>Performance of EfficientPose compared to OpenPose on the MPII validation dataset, as evaluated by efficiency (number of parameters and FLOPs, and relative reduction in parameters and FLOPs compared to OpenPose) and accuracy (mean P CK h @50 and mean P CK h @10)ModelParameters Parameter reduction FLOPs FLOP reduction PCK h @50 PCK h @10 State-of-the-art results in P CK h @50 (both for individual body parts and overall mean value) on the official MPII test dataset<ref type="bibr" target="#b0">[1]</ref> compared to the number of parameters</figDesc><table><row><cell>OpenPose [6]</cell><cell>25.94M</cell><cell>1×</cell><cell></cell><cell cols="2">160.36G 1×</cell><cell></cell><cell>87.60</cell><cell>22.76</cell></row><row><cell cols="2">EfficientPose RT 0.46M</cell><cell>56×</cell><cell></cell><cell>0.87G</cell><cell>184×</cell><cell></cell><cell>82.88</cell><cell>23.56</cell></row><row><cell>EfficientPose I</cell><cell>0.72M</cell><cell>36×</cell><cell></cell><cell>1.67G</cell><cell>96×</cell><cell></cell><cell>85.18</cell><cell>26.49</cell></row><row><cell>EfficientPose II</cell><cell>1.73M</cell><cell>15×</cell><cell></cell><cell>7.70G</cell><cell>21×</cell><cell></cell><cell>88.18</cell><cell>30.17</cell></row><row><cell>EfficientPose III</cell><cell>3.23M</cell><cell>8.0×</cell><cell></cell><cell>23.35G</cell><cell>6.9×</cell><cell></cell><cell>89.51</cell><cell>30.90</cell></row><row><cell>EfficientPose IV</cell><cell>6.56M</cell><cell>4.0×</cell><cell></cell><cell>72.89G</cell><cell>2.2×</cell><cell></cell><cell>89.75</cell><cell>35.63</cell></row><row><cell>Model</cell><cell></cell><cell cols="8">Parameters Head Shoulder Elbow Wrist Hip Knee Ankle Mean</cell></row><row><cell cols="3">Pishchulin et al., ICCV'13 [35] −</cell><cell>74.3</cell><cell>49.0</cell><cell>40.8</cell><cell>32.1</cell><cell>36.5 34.4</cell><cell>35.2</cell><cell>44.1</cell></row><row><cell cols="2">Tompson et al., NIPS'14 [53]</cell><cell>−</cell><cell>95.8</cell><cell>90.3</cell><cell>80.5</cell><cell>74.3</cell><cell>77.6 69.7</cell><cell>62.8</cell><cell>79.6</cell></row><row><cell cols="2">Lifshitz et al., ECCV'16 [28]</cell><cell>76M</cell><cell>97.8</cell><cell>93.3</cell><cell>85.7</cell><cell>80.4</cell><cell>85.3 76.6</cell><cell>70.2</cell><cell>85.0</cell></row><row><cell cols="2">Tang et al., BMVC'18 [50]</cell><cell>10M</cell><cell>97.4</cell><cell>96.2</cell><cell>91.8</cell><cell>87.3</cell><cell>90.0 87.0</cell><cell>83.3</cell><cell>90.8</cell></row><row><cell cols="2">Newell et al., ECCV'16 [33]</cell><cell>26M</cell><cell>98.2</cell><cell>96.3</cell><cell>91.2</cell><cell>87.1</cell><cell>90.1 87.4</cell><cell>83.6</cell><cell>90.9</cell></row><row><cell cols="2">Zhang et al., CVPR'19 [60]</cell><cell>3M</cell><cell>98.3</cell><cell>96.4</cell><cell>91.5</cell><cell>87.4</cell><cell>90.9 87.1</cell><cell>83.7</cell><cell>91.1</cell></row><row><cell cols="2">Bulat et al., FG'20 [5]</cell><cell>9M</cell><cell>98.5</cell><cell>96.4</cell><cell>91.5</cell><cell>87.2</cell><cell>90.7 86.9</cell><cell>83.6</cell><cell>91.1</cell></row><row><cell cols="2">Yang et al., ICCV'17 [57]</cell><cell>27M</cell><cell>98.5</cell><cell>96.7</cell><cell>92.5</cell><cell>88.7</cell><cell>91.1 88.6</cell><cell>86.0</cell><cell>92.0</cell></row><row><cell cols="2">Tang et al., ECCV'18 [49]</cell><cell>16M</cell><cell>98.4</cell><cell>96.9</cell><cell>92.6</cell><cell>88.7</cell><cell>91.8 89.4</cell><cell>86.2</cell><cell>92.3</cell></row><row><cell cols="2">Sun et al., CVPR'19 [44]</cell><cell>29M</cell><cell>98.6</cell><cell>96.9</cell><cell>92.8</cell><cell>89.0</cell><cell>91.5 89.0</cell><cell>85.7</cell><cell>92.3</cell></row><row><cell cols="2">Zhang et al., arXiv'19 [61]</cell><cell>24M</cell><cell>98.6</cell><cell>97.0</cell><cell>92.8</cell><cell>88.8</cell><cell>91.7 89.8</cell><cell>86.6</cell><cell>92.5</cell></row><row><cell>OpenPose [6]</cell><cell></cell><cell>25.94M</cell><cell>97.7</cell><cell>94.7</cell><cell>89.5</cell><cell>84.7</cell><cell>88.4 83.6</cell><cell>79.3</cell><cell>88.8</cell></row><row><cell>EfficientPose RT</cell><cell></cell><cell>0.46M</cell><cell>97.0</cell><cell>93.3</cell><cell>85.0</cell><cell>79.2</cell><cell>85.9 77.0</cell><cell>71.0</cell><cell>84.8</cell></row><row><cell>EfficientPose IV</cell><cell></cell><cell>6.56M</cell><cell>98.2</cell><cell>96.0</cell><cell>91.7</cell><cell>87.9</cell><cell>90.3 87.5</cell><cell>83.9</cell><cell>91.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>Model accuracy on the MPII validation dataset in relation to the number of training epochs</figDesc><table><row><cell>Model</cell><cell cols="2">Epochs PCK h @50</cell></row><row><cell>OpenPose [6]</cell><cell>100</cell><cell>80.47</cell></row><row><cell>OpenPose [6]</cell><cell>200</cell><cell>85.00</cell></row><row><cell>OpenPose [6]</cell><cell>400</cell><cell>87.60</cell></row><row><cell cols="2">EfficientPose II 100</cell><cell>87.05</cell></row><row><cell cols="2">EfficientPose II 200</cell><cell>88.18</cell></row><row><cell cols="2">EfficientPose II 400</cell><cell>88.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>). This means that the parameter-efficient MBConvs provide value in HPE as with other computer vision tasks, such as image classification and object de- tection. This, in turns, makes MBConv a very suitable component for HPE networks. For this reason, it would be interesting to investigate the effect of combining it with other novel HPE architectures, such as Hourglass and HRNet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Computer Vision and Pattern Recognition, pp. 7093-7102 (2020) 60. Zhang, F., Zhu, X., Ye, M.: Fast human pose estimation.In:Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3517-3526 (2019) 61. Zhang, H., Ouyang, H., Liu, S., Qi, X., Shen, X., Yang, R., Jia, J.: Human pose estimation with spatial contextual information. arXiv preprint arXiv:1901.01760 (2019) 62. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures for scalable image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8697-8710 (2018)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 Table 7 Table 8</head><label>678</label><figDesc>Model accuracy on the MPII validation dataset in relation to the use of cross-resolution features Model Cross-resolution features Parameters FLOPs PCK h @50 PCK h @10 Model accuracy on the MPII validation dataset in relation to the use of skeleton estimation Model Skeleton estimation Parameters FLOPs PCK h @50 PCK h @10 Model accuracy on the MPII validation dataset in relation to the number of detection passes Model Detection passes Parameters FLOPs PCK h @50 PCK h @10</figDesc><table><row><cell>EfficientPose I</cell><cell></cell><cell>0.72M</cell><cell>1.67G</cell><cell>83.56</cell><cell>26.35</cell></row><row><cell>EfficientPose I</cell><cell></cell><cell>0.68M</cell><cell>1.58G</cell><cell>83.64</cell><cell>25.79</cell></row><row><cell>EfficientPose II</cell><cell></cell><cell>1.73M</cell><cell>7.70G</cell><cell>87.05</cell><cell>29.87</cell></row><row><cell>EfficientPose II</cell><cell></cell><cell>1.69M</cell><cell>7.50G</cell><cell>86.93</cell><cell>29.16</cell></row><row><cell>EfficientPose I</cell><cell></cell><cell>0.72M</cell><cell>1.67G</cell><cell>83.56</cell><cell>26.35</cell></row><row><cell>EfficientPose I</cell><cell></cell><cell>0.54M</cell><cell>1.37G</cell><cell>81.13</cell><cell>25.00</cell></row><row><cell>EfficientPose II</cell><cell></cell><cell>1.73M</cell><cell>7.70G</cell><cell>87.05</cell><cell>29.87</cell></row><row><cell>EfficientPose II</cell><cell></cell><cell>1.27M</cell><cell>6.03G</cell><cell>85.75</cell><cell>29.67</cell></row><row><cell>EfficientPose I</cell><cell>1</cell><cell>0.52M</cell><cell>1.33G</cell><cell>81.85</cell><cell>24.51</cell></row><row><cell>EfficientPose I</cell><cell>2</cell><cell>0.72M</cell><cell>1.67G</cell><cell>83.56</cell><cell>26.35</cell></row><row><cell>EfficientPose I</cell><cell>3</cell><cell>0.92M</cell><cell>2.02G</cell><cell>84.35</cell><cell>26.42</cell></row><row><cell cols="2">EfficientPose II 1</cell><cell>1.24M</cell><cell>5.92G</cell><cell>85.42</cell><cell>28.01</cell></row><row><cell cols="2">EfficientPose II 2</cell><cell>1.73M</cell><cell>7.70G</cell><cell>87.05</cell><cell>29.87</cell></row><row><cell cols="2">EfficientPose II 3</cell><cell>2.22M</cell><cell>9.49G</cell><cell>87.55</cell><cell>29.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9</head><label>9</label><figDesc>Model accuracy on the MPII validation dataset in relation to the use of upscalingModelUpscaling Parameters FLOPs PCK h @50 PCK h @10 Optimization scheme displaying learning rates λ and σ values corresponding to the training of EfficientPose II over 100 epochs</figDesc><table><row><cell>EfficientPose I</cell><cell>0.72M</cell><cell>1.67G</cell><cell>83.56</cell><cell>26.35</cell></row><row><cell>EfficientPose I</cell><cell>0.71M</cell><cell>1.52G</cell><cell>82.42</cell><cell>14.02</cell></row><row><cell>EfficientPose II</cell><cell>1.73M</cell><cell>7.70G</cell><cell>87.05</cell><cell>29.87</cell></row><row><cell>EfficientPose II</cell><cell>1.73M</cell><cell>7.37G</cell><cell>86.56</cell><cell>20.66</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research is funded by RSO funds from the Faculty of Medicine and Health Sciences at the Norwegian University of Science and Technology. The experiments were carried out utilizing computational resources provided by the Norwegian Open AI Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gait analysis for gender classification in forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bisogni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freire-Obregón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castrillón-Santana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Dependability in Sensor, Cloud, and Big Data Systems and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="180" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Singleshot 3d multi-person pose estimation in complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107534</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Toward fast and accurate human pose estimation via softgated skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>FG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Developer</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<title level="m">NVIDIA TensorRT</title>
		<imprint>
			<date type="published" when="2020-02-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast sparse convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14629" to="14638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tracking by prediction: A deep generative model for mutli-person localisation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1122" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised emotional state classification through physiological parameters for social robotics applications. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mancioppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cavallo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page">105217</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing fingerspelling in sibi (sistem isyarat bahasa indonesia) using openpose and elliptical fourier descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Firdaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rakun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Advanced Information Science and System</title>
		<meeting>the International Conference on Advanced Information Science and System</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activation function optimizations for capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gagana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Athri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1172" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Siamese attentional keypoint network for high performance visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">105448</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning reinforced attentional representation for end-to-end visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">517</biblScope>
			<biblScope unit="page" from="52" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using shufflenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Computer Science &amp; Education (ICCSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">High-speed multi-person pose estimation with deep feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aslam</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Computer Vision and Image Understanding p</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">103010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hand gesture recognition based omnidirectional wheelchair control using imu and emg sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Lenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhaumik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="529" to="541" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning structures of interval-based bayesian networks in probabilistic generative model for human complex activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="545" to="561" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-shot multiperson 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prediction of basketball free throw shooting by openpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsunoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murakoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JSAI International Symposium on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="483" to="499" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A robust human activity recognition approach using openpose, motion features, and deep recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torresen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="299" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the estimation of children&apos;s poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sciortino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Distante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Mixconv: Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">Cu-net: coupled u-nets</title>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deploy machine learning models on mobile and IoT devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-02-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Clip-q: Deep network compression learning by in-parallel pruning-quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7873" to="7882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A new approach for medical assessment of patient&apos;s injured shoulder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maffioletti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</title>
		<imprint>
			<publisher>American Society of Mechanical Engineers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">59179</biblScope>
			<biblScope unit="page" from="1" to="02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Single image-based head pose estimation with spherical parametrization and 3d morphing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107316</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distributionaware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on</title>
		<meeting>the IEEE/CVF Conference on</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
