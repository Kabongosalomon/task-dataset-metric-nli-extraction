<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-View Attention Network for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whang</surname></persName>
							<email>2taesunwhang@wisenut.co.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Wisenut Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeochan</forename><surname>Yoon</surname></persName>
							<email>3ycyoon@etri.re.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Electronics and Telecommunications Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuiseok</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-View Attention Network for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual dialog is a challenging vision-language task in which a series of questions visually grounded by a given image are answered. To resolve the visual dialog task, a high-level understanding of various multimodal inputs (e.g., question, dialog history, and image) is required. Specifically, it is necessary for an agent to 1) determine the semantic intent of question and 2) align question-relevant textual and visual contents among heterogeneous modality inputs. In this paper, we propose Multi-View Attention Network (MVAN), which leverages multiple views about heterogeneous inputs based on attention mechanisms. MVAN effectively captures the question-relevant information from the dialog history with two complementary modules (i.e., Topic Aggregation and Context Matching), and builds multimodal representations through sequential alignment processes (i.e., Modality Alignment). Experimental results on VisDial v1.0 dataset show the effectiveness of our proposed model, which outperforms the previous state-of-theart methods with respect to all evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As a part of the interdisciplinary research that combines natural language processing with computer vision, a wide variety of vision-language tasks (e.g., visual question answering (VQA), image captioning, referring expressions, and etc.) have been introduced in recent years. Considerable efforts in this field have advanced the capabilities of artificial intelligence agents a step further, but the agent's comprehension of multimodal information is still far from human-level reasoning and cognitive ability <ref type="bibr" target="#b10">(Hudson and Manning 2019;</ref><ref type="bibr" target="#b34">Zellers et al. 2019</ref>).</p><p>The visual dialog task is similar to VQA in that it requires the agent to answer a question that is guided by an image, but this task differs in that the agent needs to answer a series of questions focusing on previous dialog as well as a given image. It is more challenging than other vision-language tasks because the agent is asked to selectively ground the visual contents related to the question topics, which change as the dialog proceeds. For example, <ref type="figure">Figure 1</ref> illustrates how the question topics change during each dialog turn (e.g., "household goods", "people", and "food"). Furthermore, answering * Equal Contribution † Work performed while at Korea University Cap: 2 small kids eating large carrots on a bed <ref type="figure">Figure 1</ref>: Example of a visual dialog task. The text color indicates the dialog topic (e.g., "people", "food", and "household goods"). some questions that contain ambiguous expressions (e.g., question 5 (Q5): "how old do they look?") can be more difficult because the agent should consider which entity "they" refers to (i.e., "kids" and "boys" in caption and Q4-A4 pair) and then ground the referent in the image. To address these issues, the dialog agent should be capable of determining the semantic intent of the question, clarifying referential ambiguities, and then leveraging grounded visual contents and identified semantic information. Several recent researches have been performed to solve the visual dialog task from the perspective of visual co-reference resolution <ref type="bibr" target="#b28">(Seo et al. 2017;</ref><ref type="bibr" target="#b15">Kottur et al. 2018;</ref><ref type="bibr" target="#b12">Kang, Lim, and Zhang 2019;</ref><ref type="bibr" target="#b22">Niu et al. 2019)</ref>. However, resolving the visual co-reference does not always lead to complete understanding of semantic intent and topics of the question. For example, when answering Q6 in <ref type="figure">Figure 1</ref>, the agent is required to not only resolve visual co-reference, but also explicitly understand the semantic intent of the question which is asking whether other "snacks" exist or not (i.e., the model should focus on "snacks" rather than "they"). From these observations, it is crucial to capture what the topic of the question is in order to accurately determine the semantic intent of the question.</p><p>To this end, this paper proposes Multi-View Attention Network (MVAN), which leverages question-guided contextual information and clues through the dialog history; and then effectively learns semantic alignments between visual and textual representations through the sequential alignment processes. MVAN consists of three main modules. First, the Context Matching module effectively represents contextual information of dialog history that is relevant to the question at sentence level. This is because, in general, the semantic intent of a sentence tends to be determined by the context of the entire sequence as well as some words that are directly connected to the topic. Second, the Topic Aggregation module is capable of capturing topic-guided clues from the dialog history at word level. This takes advantage of the fact that topic-related representation is well-constructed by directly using the original word embeddings of each word. Both modules adaptively propagate textual information that interacts with the semantic intent of the current question by attention mechanism and gate function. Lastly, the Modality Alignment module performs two sequential aligning processes to learn semantic alignments between each textual output of previous modules with visual features. Since the alignments between contextual representation and visual contents can be implicit and noisy, the Modality Alignment module first learns the soft mapping between heterogeneous inputs based on the topic-guided clues, then sequentially aligns them with the surrounding contextual information.</p><p>The main contributions of this paper are as follows. 1) We propose MVAN that consists of two complementary views and combine them with visual contents through multiple alignment steps. 2) Experimental results on VisDial v1.0 show that our proposed model outperforms the previous stateof-the-art methods with respect to all evaluation metrics. 3) Visualization of reasoning process for each module demonstrates that MVAN explicitly understands the semantic intent of the question, which leads to a reasonable interpretation of employing various multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Visual dialog is a task proposed by <ref type="bibr" target="#b4">Das et al. (2017)</ref> that requires the dialog agent to answer the current question by exploiting both the image and dialog history. <ref type="bibr" target="#b4">Das et al. (2017)</ref> also introduced encoder-decoder models such as late fusion, hierarchical recurrent network, and memory network as baseline methods. Most of the previous approaches are predominantly based on attention mechanisms to fuse representations of the given multimodal inputs <ref type="bibr" target="#b32">(Wu et al. 2018;</ref><ref type="bibr" target="#b9">Guo, Xu, and Tao 2019;</ref><ref type="bibr">Gan et al. 2019;</ref><ref type="bibr" target="#b13">Kim, Tan, and Bansal 2020)</ref>. Another direction of research that is inspired by graphical networks focuses on learning the inherent relations among image, dialog history, and question <ref type="bibr" target="#b35">(Zheng et al. 2019;</ref><ref type="bibr" target="#b27">Schwartz et al. 2019;</ref><ref type="bibr" target="#b8">Guo et al. 2020</ref>).</p><p>On the other hand, several approaches that explicitly resolve ambiguous references are based on visual co-reference resolution. <ref type="bibr" target="#b15">Kottur et al. (2018)</ref> used neural module networks <ref type="bibr" target="#b2">(Andreas et al. 2016)</ref> to effectively link references and ground relevant visual contents at word level. <ref type="bibr" target="#b12">Kang, Lim, and Zhang (2019)</ref> adapted a self-attention mechanism <ref type="bibr" target="#b29">(Vaswani et al. 2017</ref>) based on sentence-level representation to resolve referential ambiguity. <ref type="bibr" target="#b22">Niu et al. (2019)</ref> proposed a recursive attention mechanism to capture question-relevant dialog history and ground related visual contents to the image. Most existing works <ref type="bibr" target="#b15">(Kottur et al. 2018;</ref><ref type="bibr" target="#b12">Kang, Lim, and Zhang 2019)</ref> that use only word or sentence representations have limitations in identifying the semantic intent of the question. Unlike this, our proposed model considers both topic-related clues and contextual information to effectively capture the semantic intent of the question. In addition, MVAN adaptively integrates dialog history and visual contents by performing sequential alignment steps rather than exploiting only dialog history and visual contents that meet specific recursion conditions <ref type="bibr" target="#b22">(Niu et al. 2019)</ref>.</p><p>More recently, <ref type="bibr" target="#b20">Murahari et al. (2020)</ref> and <ref type="bibr" target="#b31">Wang et al. (2020)</ref> introduced a fine-tuning method using a pre-trained model, as it is observed that pre-trained language model architectures (e.g., BERT <ref type="bibr" target="#b5">(Devlin et al. 2019)</ref>) also effectively perform on vision-language tasks . Also, <ref type="bibr" target="#b25">Qi et al. (2020)</ref> proposed causal intervention algorithms that can be applicable to other visual dialog models and <ref type="bibr" target="#b0">Agarwal et al. (2020)</ref> proposed curriculum fine-tuning inspired by the work of <ref type="bibr" target="#b3">Bengio et al. (2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In the visual dialog task <ref type="bibr" target="#b4">(Das et al. 2017</ref>), a dialog agent is given a set of multimodal inputs for each dialog turn t. This input set consists of an image I, a current question Q t , the dialog history set H t = {C, (Q 1 , A gt 1 ), · · · , (Q t−1 , A gt t−1 )}, which contains image caption C and t − 1 consecutive question-answer pairs, and a set of answer candidates A t = {A 1 t , A 2 t , ..., A 100 t }. The agent then is required to answer the question by either discriminating or generating a correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Representation</head><p>Visual Features We employ a bottom-up attention mechanism <ref type="bibr" target="#b1">(Anderson et al. 2018)</ref> to represent the objects appearing in an image. Visual features of object regions {v k } nv k=1 ∈ R dv×nv , where n v is the number of detected objects ranging from 10 to 100, are adaptively extracted from a Faster-RCNN <ref type="bibr" target="#b26">(Ren et al. 2015)</ref> that is pre-trained with Visual Genome <ref type="bibr" target="#b16">(Krishna et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Features</head><p>We first embed three different text inputs: the current question, dialog history, and answer candidates. The word embedding layer is initialized with pretrained GloVe embeddings <ref type="bibr" target="#b24">(Pennington, Socher, and Manning 2014)</ref>. We then feed the word embeddings into a bidirectional long short-term memory (BiLSTM) to encode a sequential representation of each embedding. Specifically, each word in question Q is embedded as</p><formula xml:id="formula_0">{w q i } nq i=1 ∈ R dw×nq ,</formula><p>where n q is the number of words in the sequence. Each word embedding is fed into the BiLSTM layer as follows:  The sequential representation of each token is constructed by concatenating the hidden states of the forward and backward LSTMs, denoted as</p><formula xml:id="formula_1">− → u q i = LSTM(w q i , − → u q i−1 ) (1a) ← − u q i = LSTM(w q i , ← − u q i+1 ).<label>(1b)</label></formula><formula xml:id="formula_2">u q i = [ − → u q i , ← − u q i ]. Meanwhile, the sequential representation for each dialog history u h r = {u h r,j } n h j=1 (0 ≤ r ≤ t − 1)</formula><p>is constructed using the question construction process with different BiLSTM layers. For the answer candidates, we use a different uni-directional LSTM to represent them because their sequence lengths are shorter than those of the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-View Attention Network</head><p>We propose MVAN that considers the semantic intent and topic of the question simultaneously and effectively aligns the textual and visual information through multiple alignment processes. <ref type="figure" target="#fig_0">Figure 2</ref> describes the MVAN architecture, which consists of three components: 1) Context Matching, 2) Topic Aggregation, and 3) Modality Alignment.</p><p>Context Matching Module Generally, the semantic intent of a sentence not only relies on particular words that implicitly point to the topic of the sentence but also tends to be determined by the context of the entire sequence. Therefore, we build the Context Matching module that adaptively integrates the question and its relevant history at sentence level. Contextual representation is constructed by concatenating the last hidden states of the forward and backward LSTMs for the question and dialog history, denoted as</p><formula xml:id="formula_3">s q = [ − → u q 1 , ← − u q nq ] and s h r = [ − → u h r,1 , ← − u h r,</formula><p>n hr ], respectively. We then apply an attention mechanism to focus on questionrelevant history. The Context Matching module takes contextual representation of the question s q ∈ R ds×1 and dialog history s h = {s h 0 , s h 1 , ..., s h t−1 } ∈ R ds×t and outputs question-relevant history features as follows:</p><formula xml:id="formula_4">z S r = W (f S q (s q ) • f S h (s h r )) + b (2a) a S r = softmax(z S r ) (2b) s h = t−1 r=0 a S r s h r ,<label>(2c)</label></formula><p>where • is element-wise multiplication, W ∈ R d f ×1 is a projection matrix, and f S q (·) and f S h (·) denote non-linear transformation functions. We apply a gate function to automatically filter out dialog history that is irrelevant to current question as follows:</p><formula xml:id="formula_5">gate C = σ(W C gate [s q ,s h ] + b C gate ) (3a) e C = gate C • [s q ,s h ],<label>(3b)</label></formula><p>where σ(·) is a sigmoid function and W C gate ∈ R 2ds×2ds and b C gate ∈ R 2ds×1 are trainable parameters. Note that e C ∈ R 2ds×1 is a context-matching representation that selectively combines the contextual information of the question and question-relevant dialog history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Aggregation Module</head><p>The topic of the question is generally expressed in a single word or phrase and likely to be connected with clues (i.e., the topic of previous questions in the dialog history). We design the Topic Aggregation module to combine the clues associated with the question topic by exploiting the initial word embeddings (i.e., GloVe) to represent their original meaning. Specifically, this module leverages word-level sequential representation of the question and dialog history, {u q i } nq i=1 ∈ R du×nq and {u h r,j } n h j=1 ∈ R du×n h , respectively. The dot product attention mechanism is employed to selectively focus the words that are relevant to the question topic from the dialog history as follows: gate</p><formula xml:id="formula_6">z W r,ij = f W q (u q i ) f W h (u h r,j ) (4a) a W r,ij = exp(z W r,ij )/ n h j=1 exp(z W r,ij ) (4b) w h r,i = nr j=1 a W r,ij w h r,j ,<label>(4c)</label></formula><formula xml:id="formula_7">T i = σ(W T gate [w q i ,w h i ] + b T gate ) (6a) e T i = gate T i • [w q i ,w h i ], (6b) where W T gate ∈ R 2dw×2dw and b T gate ∈ R 2dw×1 are train- able parameters. Note that {e T i } nq i=1 ∈ R 2dw×nq</formula><p>is topicaggregation representations that encode adaptive information of the question topic and various history clues associated with it.</p><p>Modality Alignment Module Given the output representations of the Context Matching module e C ∈ R 2ds×1 and Topic Aggregation module</p><formula xml:id="formula_8">{e T i } nq i=1 ∈ R 2dw×nq</formula><p>, the Modality Alignment module aligns them with visual features {v k } nv k=1 ∈ R dv×nv via two-step alignments. First, topicview alignment performs soft alignment between heterogeneous modalities at topic level before mapping high-level contextual representation with the visual features. We utilize dot-product attention to represent the visual relevant topicaggregation embeddings as follows:</p><formula xml:id="formula_9">z T ik = f W (e T i ) f T v (v k ) (7a) a T ik = exp(z T ik )/ nq i=1 exp(z T ik ) (7b) e T k = nq i=1 a T ik e T i ,<label>(7c)</label></formula><p>where f W (·) and f W v (·) are non-linear transformation functions to embed two different modality representations into the same embedding space. Then, we obtain the fused feature vectors by concatenating the visual and attended topicaggregation features and using a multi-layer perceptron (MLP).</p><formula xml:id="formula_10">m T k = MLP([v k ,ẽ T k ]) (8) Note that {m T k } nv k=1</formula><p>∈ R dv×nv is a topic-view aligned representation that is aligned information across the visual contents with the salient question topics. This is passed to the context-view aligning step as follows:</p><formula xml:id="formula_11">m C k = [m T k , e C ]<label>(9a)</label></formula><formula xml:id="formula_12">z C k = L2Norm(f C m (m C k ) • f C (e C ))<label>(9b)</label></formula><formula xml:id="formula_13">a C k = softmax(W z C k + b)<label>(9c)</label></formula><formula xml:id="formula_14">m C = nv k=1 a C km C k ,<label>(9d)</label></formula><p>where f C m (·) and f C (·) are non-linear transformation functions and W ∈ R d f ×1 is a projection matrix. Note that m C ∈ R (dv+2ds)×1 is a context-view aligned representation, which is realigned using context-matching representation. These multiple alignment processes allow the model to understand the semantic intent of the question with complementary views, and effectively align the corresponding heterogeneous multimodal inputs. Finally, this enhanced feature is fed into a single-layer feed-forward neural network (FFNN) with a ReLU activation function.</p><formula xml:id="formula_15">m enc = max(0, W m C + b),<label>(10)</label></formula><p>where W ∈ R (dv+2ds)×denc and b ∈ R denc×1 are trainable parameters. Note that m enc ∈ R denc×1 is the multi-view aligned representation, which is fed into either a discriminative or generative decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Decoder</head><p>Discriminative Decoder We use the last hidden states of the forward LSTM to encode sentence representations of answer candidates, denoted as s a = { − → u a i,na } 100 i=1 ∈ R da×100 . We rank them according to the dot products of the candidates s a and multi-view aligned representation m enc , then apply the softmax function to obtain the probability distribution of the candidates, denoted as p = softmax((s a ) m enc ). Note that dimension of each answer candidate representation is same as that of the encoder output. We use multi-class cross entropy loss as the discriminative objective function, formulated as</p><formula xml:id="formula_16">L D = − 100 i=1 y i logp i ,<label>(11)</label></formula><p>where y i is a one-hot encoded vector of the ground truth answer.</p><p>Generative Decoder Unlike most previous approaches, which take only a discriminative approach, we also train our model in a generative manner <ref type="bibr" target="#b4">(Das et al. 2017)</ref>. During the training phase, we use a two-layer LSTM to predict the next token given the previous tokens in the answer sequence. The initial hidden state of the LSTM is initialized with the encoder output representation. For each answer candidate, we compute the likelihood of the ground truth of each token, denoted as {p k } na k=1 , and train the model by minimizing the summation of the negative log-likelihood as follows:</p><formula xml:id="formula_17">L G = − na k=1 logp k .<label>(12)</label></formula><p>Multi-task Learning We perform multi-task learning by combining both discriminative and generative decoders to classify answers, denoted as L = L D + L G . For the evaluation, we simply average the probability distributions of each decoder. Multi-task learning substantially improves performance with respect to the normalized discounted cumulative gain (NDCG) metric.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setup</head><p>Datasets We use the VisDial v0.9 and v1.0 datasets to evaluate our proposed model. VisDial v0.9 <ref type="bibr" target="#b4">(Das et al. 2017)</ref> consists of 123k MS-COCO <ref type="bibr" target="#b17">(Lin et al. 2014)</ref> images and their captions. The training and validation splits of VisDial v0.9 contain 83k and 40k images respectively, and each image has 10 consecutive question-answer pairs. VisDial v1.0, which was released by supplementing VisDial v0.9, has 123k images for training splits that combine the training and validation splits of VisDial v0.9. An additional 10k images from the Flickr dataset are utilized to construct the validation and test splits in VisDial v1.0, which contain 2k and 8k images, respectively. Unlike the previous version of the dataset, dense annotations for each candidate answer are added in the validation and test splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We evaluated our proposed model using several retrieval metrics, following the work of <ref type="bibr" target="#b4">Das et al. (2017)</ref>: 1) mean rank of the ground truth response (Mean), 2) recall at k (k={1,5,10}), which is denoted as R@k and evaluates where the ground truth is positioned in the sorted list, and 3) mean reciprocal rank (MRR) <ref type="bibr" target="#b30">(Voorhees et al. 1999)</ref>. NDCG was also introduced as a primary metric in the VisDial v1.0 dataset, and decreases when the model gives a low ranking to candidate answers with high relevance scores. MRR evaluates the precision of the model by ranking where a ground truth answer is positioned, whereas NDCG evaluates relative relevance of the predicted answers.  question and dialog history are set to 20 and 40, respectively. We set the batch size to 32 and apply the Adam optimizer <ref type="bibr" target="#b14">(Kingma and Ba. 2015)</ref> with an initial learning rate of 1e-5, which is gradually increased to 1e-3 until epoch 2, then decay at epochs 6 and 7 with the decay rate 0.1. Our code is publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>Baselines We compare the results of our proposed model with previously published results on the VisDial v1.0 and v0.9 datasets for the following methods: LF <ref type="formula">(</ref>  <ref type="bibr" target="#b33">(Yang, Zha, and Zhang 2019)</ref>, Synergistic <ref type="bibr" target="#b9">(Guo, Xu, and Tao 2019)</ref>, and DAN <ref type="bibr" target="#b12">(Kang, Lim, and Zhang 2019)</ref>.</p><p>Results on VisDial v1.0 and v0.9  <ref type="table">Table 3</ref>: Ablations of our approaches on the VisDial v1.0 validation dataset. in MRR from 64.22 to 64.84, compared to the state-of-theart baseline. In addition, we obtain better results for Mean from 4.11 to 3.97 and for R@k increased by approximately 0.4%. Similar results are obtained in the R@5 and Mean for VisDial v0.9. We also report the results for an ensemble of 10 independent models that were trained with random initial seeds, which yields average performance improvements of 1.3% for all metrics.</p><p>These results indicate that MVAN not only has accurate prediction ability, as indicated by the non-NDCG metric results (i.e., MRR, R@k, and Mean), but it has a powerful generalization capability given the result of NDCG score because this metric considers several relevant answers to be correct.</p><p>Results on multi-task learning As shown in <ref type="table" target="#tab_5">Table 2</ref>, we report the results of our MVAN model, which was trained using multi-task learning. Our proposed approach performs better with respect to all metrics than ReDAN <ref type="bibr">(Gan et al. 2019)</ref>, which averages the ranking results of the discriminative and generative model, and LTMI <ref type="bibr" target="#b21">(Nguyen, Suganuma, and Okatani 2020)</ref>, which employs multi-task learning but uses only discriminative decoder outputs for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of dialog history</head><p>We experimented with the amount of dialog history to evaluate the impact of dialog history on the model performance in the two major metrics (i.e., MRR and NDCG). The results in <ref type="figure" target="#fig_2">Figure 3</ref> show that as the amount of dialog history information increases, the MRR tends to gradually improve, but the NDCG score deteriorates. This result of the quantity analysis show that history information decreases the NDCG score but substantially boosts the other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>We conducted ablation studies on the VisDial v1.0 validation splits to evaluate the influence of each component in our model. Modality Alignment module is not ablated because this module handles the visual features. We use the same discriminative decoder model for all ablations to exclude the impact of multitask learning.</p><p>In <ref type="table">Table 3</ref>, the first rows of each block indicate the impact of each module in our model. Because the two modules (i.e., Context Matching and Topic Aggregation) are interdependent, we employ simple visual features instead of topic-aggregation representation for MVAN w/o Topic Aggregation, whereas we simply remove context-matching representation for MVAN w/o Context Matching. Both models obtain slightly lower performance with respect to all evaluation metrics than MVAN. We can hence infer that the two modules are complementary with respect to each other and our model integrates these complementary characteristics well for the task.</p><p>Recent approaches <ref type="bibr" target="#b20">(Murahari et al. 2020;</ref><ref type="bibr" target="#b13">Kim, Tan, and Bansal 2020;</ref><ref type="bibr" target="#b21">Nguyen, Suganuma, and Okatani 2020)</ref> reported that they observed a trade-off relationship between two primary metrics (i.e., NDCG and MRR) in the visual dialog task. We also found the trade-off relationship through ablative experiments with and without dialog history features (see <ref type="table">Table 3</ref>). Specifically, adding dialog history features improves the MRR score by 3.54% on average, whereas NDCG score is decreased by 1.92% on average. We observe that the model has a tendency to predict the answers more precisely (i.e., it has a better MRR score) when the dialog history features are added. This may imply that question-related clues in the dialog history are important factors in reasoning the ground truth, but they hinder the model's generalization ability (i.e., they lower the NDCG score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>To qualitatively demonstrate the advantages of our model, we visualize the attention scores of each module through examples from the VisDial v1.0 validation set in <ref type="figure" target="#fig_3">Figure 4</ref>. The attention scores of the Context Matching module, highlighted in blue, show that our model selectively focuses on contextual information as the semantic intent of the question changes. The tendency for the caption (i.e., H0) to receive the highest attention score implies that the caption contains global information describing the image. In addition, the top three visual contents with high attention scores in each image lead to the potential interpretation that our model is capable Q5: what are they drinking? H1: is this in a public setting? yes H2: can you see the type of pizza? appears to be cheese and a deep dish H3: how many males and how many females? 3 of each GT: sodas, water and beer <ref type="formula">(1)</ref>   of explicitly align the semantic intent (i.e., highlighted in yellow) of the question and visual contents through Modality Alignment module. In more detail, the attention scores of the dialog history, highlighted in red, indicate how our model captures topic-relevant clues through previous dialog history.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>, comparing two examples, we see that the model no longer focuses on "6" and "people" in H0 because those words are not related to the topic of the current question (i.e.,"drinking"). In the example in <ref type="figure" target="#fig_3">Figure 4(b)</ref>, when answering Q4 (the left dialog), the model pays more attention to the question-relevant clue such as "women" in H0, while no longer focusing on it when answering Q6 (the question topic changes from "tennis outfits" to "background"). These qualitative results show that our model successfully pays attention to visual and textual information connected to the semantic intent of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>We analyzed examples in the VisDial v1.0 validation set for which our model obtained a score of 0 for the R@10 metric. The errors can be categorized into three groups: 1) Subjective judgment: our model tends to make wrong predictions for the questions about age, weather, and appearance that could involve subjective judgment, but might be acceptable <ref type="figure">(Figures 5(a) and (b)</ref>). 2) Ambiguous questions: our model may focus on the wrong visual contents, for instance the left and right side walls rather than the rear wall when faced with an ambiguous question ( <ref type="figure">Figure 5(c)</ref>). 3) Wrong multimodal alignment: when the dialog history includes multiple entities (e.g., "boys", "pizzas", and "toppings") that can be referenced by a single pronoun (i.e., "them"), MVAN may be confused as to which entity the pronoun refers ( <ref type="figure">Figure 5(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) (d)</head><p>Q5: what color are the walls? GT: natural wood (28) Prediction: 1) white 2) beige 3) light blue Q5: are they twins? GT: maybe (12) Prediction: 1) no 2) no they're playing a game 3) i don't think so</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) (b)</head><p>Q2: how old is he? GT: looks like maybe 26 (20) Prediction: 1) 20s 2) he looks like he is a teen 3) 20's Q7: is the weather sunny?</p><p>GT: it appears overcast (23) Prediction: 1) yes but it is blue sky 2) yes very clear 3) yes it's sunny <ref type="figure">Figure 5</ref>: Error analysis on the VisDial v1.0 validation set. We analyze the examples for which the model scored 0 with respect to the R@10 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduced MVAN for the visual dialog task. MVAN can effectively determine the semantic intent of the current question and capture question-relevant information through complementary modules and sequential alignment processes. We used VisDial v1.0 to empirically evaluate our model, and as a result, our model outperforms existing stateof-the-art models. Moreover, we not only suggest plausible factors affecting a trade-off relationship of the evaluation metrics, but we enhance the interpretability of multi-level attention through detailed visualization. In future work, we aim to develop a complementary model by adding sequential information about the dialog history. Moreover, we plan to incorporate the latest pre-training methods to investigate if the performance of MVAN can be further improved.  <ref type="figure">Figure 7</ref>: Visualization of the reasoning process for each module in MVAN</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of Multi-View Attention Network (MVAN). BiLSTM layers for the question (blue) and the dialog history (green) are shared in Topic Aggregation Module and Context Matching Module, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Das et al. 2017), HRE (Das et al. 2017), MN (Das et al. 2017), HCIAE (Lu et al. 2017), AMEM (Seo et al. 2017), CoAtt (Wu et al. 2018), FGA (Schwartz et al. 2019), CorefNMN (Kottur et al. 2018), RVA (Niu et al. 2019), DualVD (Jiang et al. 2020), CAG (Guo et al. 2020), HACAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance of MVAN with different amounts of dialog history on the VisDial v1.0 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on the VisDial v1.0 validation set. We visualize the different attention scores for each module: 1) attention scores from Topic Aggregation module and Context Matching module are highlighted in red and blue, respectively; 2) semantic intent of the current question represented via the topic-view alignment step in yellow; and 3) the top three attention scores of visual features from the context-view alignment step, which are represented by the b-boxes with fine adjustment of transparency in the given image. The numbers in the brackets indicate the rank of the correct answer that our model predicts. Darker colors indicate higher attention scores. More qualitative results are described in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where f W q (·) and f W h (·) are non-linear transformation functions. The question-guided history feature for each roundw h</figDesc><table><row><cell></cell><cell></cell><cell>r,i</cell></row><row><cell cols="3">is computed by a weighted sum of their word embeddings,</cell></row><row><cell cols="3">which represent the original meanings of words. The attended representation {w h i } nq i=1 ∈ R dw×nq is computed by aggregat-ing overall all history {w h r,i } t−1 r=0 , weighted by the attention</cell></row><row><cell cols="3">scores of the Context Matching module a S r as follows:</cell></row><row><cell>w h i =</cell><cell>t−1 r=0 a S rw r,i . h</cell><cell>(5)</cell></row><row><cell cols="3">Similar to the Context Matching module, the gate operation</cell></row><row><cell cols="3">adaptively filters out irrelevant topic-guided clues at word</cell></row><row><cell>level.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on VisDial v1.0 (test-std) and v0.9 (val). † denotes ensembles.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results of different methods of combining discriminative and generative models on VisDial v1.0 (test-std). ‡ indicates that the model was trained using multi-task learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>reports the quan-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/taesunwhang/MVAN-VisDial</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">History for Visual Dialog: Do we really need it?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="6463" to="6474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative Context-Aware Graph Inference for Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-question-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modality-balanced models for visual dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Largescale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6679" to="6688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two Causal Principles for Improving Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factor graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3719" to="3729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13278</idno>
		<title level="m">Vd-bert: A unified vision and dialog transformer with bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6106" to="6115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making History Matter: History-Advantage Sequence Training for Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2561" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reasoning visual dialogs with structural and partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6669" to="6678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
