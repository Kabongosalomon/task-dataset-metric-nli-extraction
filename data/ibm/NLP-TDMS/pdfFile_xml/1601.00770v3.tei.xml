<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
							<email>makoto-miwa@toyota-ti.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute Nagoya</orgName>
								<address>
									<postCode>468-8511</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@ttic.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Toyota Technological Institute at Chicago Chicago, IL</orgName>
								<address>
									<postCode>60637</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the stateof-the-art feature-based model on end-toend relation extraction, achieving 12.1% and 5.7% relative error reductions in F1score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) <ref type="bibr" target="#b3">(Nadeau and Sekine, 2007;</ref><ref type="bibr" target="#b5">Ratinov and Roth, 2009</ref>) and relation extraction <ref type="bibr" target="#b15">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b16">Zhou et al., 2005)</ref>, but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance <ref type="bibr" target="#b16">(Li and Ji, 2014;</ref><ref type="bibr" target="#b2">Miwa and Sasaki, 2014)</ref> since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an Organization-Affiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models.</p><p>There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences <ref type="bibr">(Hammerton, 2001)</ref> and constituent/dependency trees <ref type="bibr" target="#b9">(Tai et al., 2015)</ref>. Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs <ref type="bibr" target="#b12">(Xu et al., 2015b;</ref> is worse than one using CNNs <ref type="bibr">(dos Santos et al., 2015)</ref>. These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures.</p><p>Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words are not enough to predict that source and U.S. have an ORG-AFF relation in the sentence "This is ...", one U.S. source said, and the context word said is required for this prediction. Many traditional, feature-based relation classification models extract features from both sequences and parse trees <ref type="bibr" target="#b16">(Zhou et al., 2005)</ref>. However, previous RNNbased models focus on only one of these linguistic structures <ref type="bibr" target="#b7">(Socher et al., 2012)</ref>.</p><p>We present a novel end-to-end model to extract relations between entities on both word sequence and dependency tree structures. Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTM-RNNs. Our model first detects entities and then extracts relations between the detected entities using a single incrementally-decoded NN structure, and the NN parameters are jointly updated using both entity and relation labels. Unlike traditional incremental end-to-end relation extraction models, our model further incorporates two enhancements into training: entity pretraining, which pretrains the entity model, and scheduled sampling <ref type="bibr" target="#b0">(Bengio et al., 2015)</ref>, which replaces (unreliable) predicted labels with gold labels in a certain probability. These enhancements alleviate the problem of low-performance entity detection in early stages of training, as well as allow entity information to further help downstream relation classification.</p><p>On end-to-end relation extraction, we improve over the state-of-the-art feature-based model, with 12.1% (ACE2005) and 5.7% (ACE2004) relative error reductions in F1-score. On nominal relation classification (SemEval-2010 Task 8), our model compares favorably to the state-of-the-art CNNbased model in F1-score. Finally, we also ablate and compare our various model components, which leads to some key findings (both positive and negative) about the contribution and effectiveness of different RNN structures, input dependency relation structures, different parsing models, external resources, and joint learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>LSTM-RNNs have been widely used for sequential labeling, such as clause identification <ref type="bibr">(Hammerton, 2001)</ref>, phonetic labeling <ref type="bibr">(Graves and Schmidhuber, 2005)</ref>, and NER <ref type="bibr">(Hammerton, 2003)</ref>. <ref type="bibr">Recently, Huang et al. (2015)</ref> showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the partof-speech (POS) tagging, chunking, and NER.</p><p>For relation classification, in addition to traditional feature/kernel-based approaches <ref type="bibr" target="#b15">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b0">Bunescu and Mooney, 2005)</ref>, several neural models have been proposed in the <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2010</ref>, including embedding-based models <ref type="bibr">(Hashimoto et al., 2015)</ref>, <ref type="bibr">CNN-based models (dos Santos et al., 2015)</ref>, and RNN-based models <ref type="bibr" target="#b7">(Socher et al., 2012)</ref>. Recently, <ref type="bibr" target="#b11">Xu et al. (2015a)</ref> and <ref type="bibr" target="#b12">Xu et al. (2015b)</ref> showed that the shortest dependency paths between relation arguments, which were used in feature/kernel-based systems <ref type="bibr" target="#b0">(Bunescu and Mooney, 2005)</ref>, are also useful in NN-based models. <ref type="bibr" target="#b12">Xu et al. (2015b)</ref> also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models.  compared separate sequence-based and tree-structured LSTM-RNNs on relation classification, using basic RNN model structures.</p><p>Research on tree-structured LSTM-RNNs <ref type="bibr" target="#b9">(Tai et al., 2015)</ref> fixes the direction of information propagation from bottom to top, and also cannot handle an arbitrary number of typed children as in a typed dependency tree. Furthermore, no RNNbased relation classification model simultaneously uses word sequence and dependency tree information. We propose several such novel model structures and training settings, investigating the simultaneous use of bidirectional sequential and bidirectional tree-structured LSTM-RNNs to jointly capture linear and dependency context for end-toend extraction of relations between entities.</p><p>As for end-to-end (joint) extraction of relations between entities, all existing models are featurebased systems (and no NN-based model has been proposed). Such models include structured prediction <ref type="bibr" target="#b16">(Li and Ji, 2014;</ref><ref type="bibr" target="#b2">Miwa and Sasaki, 2014)</ref>, integer linear programming <ref type="bibr" target="#b6">(Roth and Yih, 2007;</ref><ref type="bibr" target="#b13">Yang and Cardie, 2013)</ref>, card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models <ref type="bibr" target="#b14">(Yu and Lam, 2010;</ref><ref type="bibr" target="#b7">Singh et al., 2013)</ref>. Among these, structured prediction methods are state-of-the-art on several corpora. We present an improved, NN-based alternative for the end-to-end relation extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We design our model with LSTM-RNNs that represent both word sequences and dependency tree structures, and perform end-to-end extraction of relations between entities on top of these RNNs. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the overview of the model. The model mainly consists of three representation layers: a word embeddings layer (embedding layer), a word sequence based LSTM-RNN layer (sequence layer), and finally a dependency subtree based LSTM-RNN layer (dependency layer). During decoding, we build greedy, left-to-right entity detection on the sequence layer and realize relation classification on the dependency layers, where each subtree based LSTM-RNN corresponds to a relation candidate between two detected entities. After decoding the entire model structure, we update the parameters simultaneously via backpropagation through time (BPTT) <ref type="bibr" target="#b10">(Werbos, 1990)</ref>. The dependency layers are stacked on the sequence layer, so the embedding and sequence layers are shared by both entity detection and relation classification, and the shared parameters are affected by both entity and relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>The embedding layer handles embedding representations. n w , n p , n d and n e -dimensional vectors v <ref type="bibr">(w)</ref> , v (p) , v <ref type="bibr">(d)</ref> and v (e) are embedded to words, part-of-speech (POS) tags, dependency types, and entity labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence Layer</head><p>The sequence layer represents words in a linear sequence using the representations from the embedding layer. This layer represents sentential context information and maintains entities, as shown in bottom-left part of <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We represent the word sequence in a sentence with bidirectional LSTM-RNNs <ref type="bibr">(Graves et al., 2013)</ref>. The LSTM unit at t-th word consists of a collection of n ls -dimensional vectors: an input gate i t , a forget gate f t , an output gate o t , a memory cell c t , and a hidden state h t . The unit receives an n-dimensional input vector x t , the previous hidden state h t−1 , and the memory cell c t−1 , and calculates the new vectors using the following equations:</p><formula xml:id="formula_0">i t = σ W (i) x t + U (i) h t−1 + b (i) ,</formula><p>(1)</p><formula xml:id="formula_1">f t = σ W (f ) x t + U (f ) h t−1 + b (f ) , o t = σ W (o) x t + U (o) h t−1 + b (o) , u t = tanh W (u) x t + U (u) h t−1 + b (u) , c t = i t u t + f t c t−1 , h t = o t tanh(c t ),</formula><p>where σ denotes the logistic function, denotes element-wise multiplication, W and U are weight matrices, and b are bias vectors. The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector:</p><formula xml:id="formula_2">x t = v (w) t ; v (p) t</formula><p>. We also concatenate the hidden state vectors of the two directions' LSTM units corresponding to each word (denoted as − → h t and ← − h t ) as its output vector, s t = − → h t ; ← − h t , and pass it to the subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Detection</head><p>We treat entity detection as a sequence labeling task. We assign an entity tag to each word using a commonly used encoding scheme BILOU (Begin, Inside, Last, Outside, Unit) (Ratinov and <ref type="bibr" target="#b5">Roth, 2009)</ref>, where each entity tag represents the entity type and the position of a word in the entity. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, we assign B-PER and L-PER (which denote the beginning and last words of a person entity type, respectively) to each word in Sidney Yates to represent this phrase as a PER (person) entity type.</p><p>We perform entity detection on top of the sequence layer. We employ a two-layered NN with an n he -dimensional hidden layer h (e) and a softmax output layer for entity detection.</p><formula xml:id="formula_3">h (e) t = tanh W (e h ) [s t ; v (e) t−1 ] + b (e h ) (2) y t = softmax W (ey) h (e) t + b (ey)<label>(3)</label></formula><p>Here, W are weight matrices and b are bias vectors.</p><p>We assign entity labels to words in a greedy, left-to-right manner. 1 During this decoding, we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account. The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word <ref type="figure" target="#fig_0">(Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dependency Layer</head><p>The dependency layer represents a relation between a pair of two target words (corresponding to a relation candidate in relation classification) in the dependency tree, and is in charge of relationspecific representations, as is shown in top-right part of <ref type="figure" target="#fig_0">Fig. 1</ref>. This layer mainly focuses on the shortest path between a pair of target words in the dependency tree (i.e., the path between the least common node and the two target words) since these paths are shown to be effective in relation classification <ref type="bibr" target="#b11">(Xu et al., 2015a)</ref>. For example, we show the shortest path between Yates and Chicago in the bottom of <ref type="figure" target="#fig_0">Fig. 1</ref>, and this path well captures the key phrase of their relation, i.e., born in.</p><p>We employ bidirectional tree-structured LSTM-RNNs (i.e., bottom-up and top-down) to represent a relation candidate by capturing the dependency structure around the target word pair. This bidirectional structure propagates to each node not only the information from the leaves but also information from the root. This is especially important for relation classification, which makes use of argument nodes near the bottom of the tree, and our top-down LSTM-RNN sends information from the top of the tree to such near-leaf nodes (unlike in standard bottom-up LSTM-RNNs). 2 Note that the two variants of tree-structured LSTM-RNNs by <ref type="bibr" target="#b9">Tai et al. (2015)</ref> are not able to represent our target structures which have a variable number of typed children: the Child-Sum Tree-LSTM does not deal with types and the N -ary Tree assumes a fixed number of children. We thus propose a new variant of tree-structured LSTM-RNN that shares weight matrices U s for same-type children and also allows variable number of children. For this variant, we calculate n lt -dimensional vectors in the LSTM unit at t-th node with C(t) children using following equations:</p><formula xml:id="formula_4">i t = σ   W (i) x t + l∈C(t) U (i) m(l) h tl + b (i)   , (4) f tk = σ   W (f ) x t + l∈C(t) U (f ) m(k)m(l) h tl + b (f )   , o t = σ   W (o) x t + l∈C(t) U (o) m(l) h tl + b (o)   , u t = tanh   W (u) x t + l∈C(t) U (u) m(l) h tl + b (u)   , c t = i t u t + l∈C(t) f tl c tl , h t = o t tanh(c t ),</formula><p>where m(·) is a type mapping function.</p><p>To investigate appropriate structures to represent relations between two target word pairs, we experiment with three structure options. We primarily employ the shortest path structure (SP-Tree), which captures the core dependency path between a target word pair and is widely used in relation classification models, e.g., <ref type="bibr" target="#b0">(Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b11">Xu et al., 2015a)</ref>. We also try two other dependency structures: SubTree and Full-Tree. SubTree is the subtree under the lowest common ancestor of the target word pair. This provides additional modifier information to the path and the word pair in SPTree. FullTree is the full dependency tree. This captures context from the entire sentence. While we use one node type for SPTree, we define two node types for SubTree and FullTree, i.e., one for nodes on shortest paths and one for all other nodes. We use the type mapping function m(·) to distinguish these two nodes types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Stacking Sequence and Dependency Layers</head><p>We stack the dependency layers (corresponding to relation candidates) on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output. The dependency-layer LSTM unit at the t-th word receives as input</p><formula xml:id="formula_5">x t = s t ; v (d) t ; v (e) t ,</formula><p>i.e., the concatenation of its corresponding hidden state vectors s t in the sequence layer, dependency type embedding v  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Relation Classification</head><p>We incrementally build relation candidates using all possible combinations of the last words of detected entities, i.e., words with L or U labels in the BILOU scheme, during decoding. For instance, in <ref type="figure" target="#fig_0">Fig. 1</ref>, we build a relation candidate using Yates with an L-PER label and Chicago with an U-LOC label. For each relation candidate, we realize the dependency layer d p (described above) corresponding to the path between the word pair p in the relation candidate, and the NN receives a relation candidate vector constructed from the output of the dependency tree layer, and predicts its relation label. We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation. We represent relation labels by type and direction, except for negative relations that have no direction.</p><p>The relation candidate vector is constructed as the concatenation d p = [↑h p A ; ↓h p 1 ; ↓h p 2 ], where ↑h p A is the hidden state vector of the top LSTM <ref type="bibr">3</ref> We use the dependency to the parent since the number of children varies. Dependency types can also be incorporated into m(·), but this did not help in initial experiments. unit in the bottom-up LSTM-RNN (representing the lowest common ancestor of the target word pair p), and ↓h p 1 , ↓h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top-down LSTM-RNN. <ref type="bibr">4</ref> All the corresponding arrows are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Similarly to the entity detection, we employ a two-layered NN with an n hr -dimensional hidden layer h (r) and a softmax output layer (with weight matrices W , bias vectors b).</p><formula xml:id="formula_6">h (r) p = tanh W (r h ) d p + b (r h )<label>(5)</label></formula><formula xml:id="formula_7">y p = softmax W (ry) h (r) t + b (ry)<label>(6)</label></formula><p>We construct the input d p for relation classification from tree-structured LSTM-RNNs stacked on sequential LSTM-RNNs, so the contribution of sequence layer to the input is indirect. Furthermore, our model uses words for representing entities, so it cannot fully use the entity information. To alleviate these problems, we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification, i.e., d p =</p><formula xml:id="formula_8">d p ; 1 |Ip 1 | i∈Ip 1 s i ; 1 |Ip 2 | i∈Ip 2 s i (Pair)</formula><p>, where I p 1 and I p 2 represent sets of word indices in the first and second entities. <ref type="bibr">5</ref> Also, we assign two labels to each word pair in prediction since we consider both left-to-right and right-to-left directions. When the predicted labels are inconsistent, we select the positive and more confident label, similar to <ref type="bibr" target="#b11">Xu et al. (2015a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Training</head><p>We update the model parameters including weights, biases, and embeddings by BPTT and Adam (Kingma and Ba, 2015) with gradient clipping, parameter averaging, and L2-regularization (we regularize weights W and U , not the bias terms b). We also apply dropout <ref type="bibr" target="#b8">(Srivastava et al., 2014)</ref> to the embedding layer and to the final hidden layers for entity detection and relation classification.</p><p>We employ two enhancements, scheduled sampling <ref type="bibr" target="#b0">(Bengio et al., 2015)</ref> and entity pretraining, to alleviate the problem of unreliable prediction of entities in the early stage of training, and to encourage building positive relation instances from the detected entities. In scheduled sampling, we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal. As for i , we choose the inverse sigmoid decay i = k/(k + exp(i/k)), where k(≥ 1) is a hyper-parameter that adjusts how often we use the gold labels as prediction. Entity pretraining is inspired by <ref type="bibr" target="#b5">(Pentina et al., 2015)</ref>, and we pretrain the entity detection model using the training data before training the entire model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Task Settings</head><p>We evaluate on three datasets: ACE05 and ACE04 for end-to-end relation extraction, and SemEval-2010 Task 8 for relation classification. We use the first two datasets as our primary target, and use the last one to thoroughly analyze and ablate the relation classification part of our model. ACE05 defines 7 coarse-grained entity types and 6 coarse-grained relation types between entities. We use the same data splits, preprocessing, and task settings as <ref type="bibr" target="#b16">Li and Ji (2014)</ref>. We report the primary micro F1-scores as well as micro precision and recall on both entity and relation extraction to better explain model performance. We treat an entity as correct when its type and the region of its head are correct. We treat a relation as correct when its type and argument entities are correct; we thus treat all non-negative relations on wrong entities as false positives.</p><p>ACE04 defines the same 7 coarse-grained entity types as ACE05 <ref type="bibr" target="#b16">(Doddington et al., 2004)</ref>, but defines 7 coarse-grained relation types. We follow the cross-validation setting of Chan and <ref type="bibr">Roth (2011) and</ref><ref type="bibr" target="#b16">Ji (2014)</ref>, and the preprocessing and evaluation metrics of ACE05.</p><p>SemEval-2010 Task 8 defines 9 relation types between nominals and a tenth type Other when two nouns have none of these relations <ref type="bibr" target="#b16">(Hendrickx et al., 2010)</ref>. We treat this Other type as a negative relation type, and no direction is considered. The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentences from the training set as our development set. We followed the official task setting, and report the official macro-averaged F1-score (Macro-F1) on the 9 relation types.</p><p>For more details of the data and task settings, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>We implemented our model using the cnn library. <ref type="bibr">6</ref> We parsed the texts using the Stanford neural dependency parser 7 (Chen and Manning, 2014) with the original Stanford Dependencies. Based on preliminary tuning, we fixed embedding dimensions n w to 200, n p , n d , n e to 25, and dimensions of intermediate layers (n ls , n lt of LSTM-RNNs and n he , n hr of hidden layers) to 100. We initialized word vectors via word2vec <ref type="bibr">(Mikolov et al., 2013)</ref> trained on Wikipedia 8 and randomly initialized all other parameters. We tuned hyper-parameters using development sets for ACE05 and SemEval-2010 Task 8 to achieve high primary (Micro-and Macro-) F1-scores. 9 For ACE04, we directly employed the best parameters for ACE05. The hyperparameter settings are shown in the supplementary material. For SemEval-2010 Task 8, we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types. Our statistical significance results are based on the Approximate Randomization (AR) test <ref type="bibr" target="#b4">(Noreen, 1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">End-to-end Relation Extraction Results</head><p>Table 1 compares our model with the state-of-theart feature-based model of Li and Ji (2014) 10 on final test sets, and shows that our model performs better than the state-of-the-art model.</p><p>To analyze the contributions and effects of the various components of our end-to-end relation extraction model, we perform ablation tests on the ACE05 development set <ref type="table" target="#tab_1">(Table 2</ref>). The performance slightly degraded without scheduled sampling, and the performance significantly degraded when we removed entity pretraining or removed both (p&lt;0.05). This is reasonable because the model can only create relation instances when both of the entities are found and, without these enhancements, it may get too late to find some relations. Removing label embeddings did not affect 6 https://github.com/clab/cnn 7 http://nlp.stanford.edu/software/ stanford-corenlp-full-2015-04-20.zip 8 https://dumps.wikimedia.org/enwiki/ 20150901/ <ref type="bibr">9</ref> We did not tune the precision-recall trade-offs, but doing so can specifically improve precision further. <ref type="bibr">10</ref> Other work on ACE is not comparable or performs worse than the model by Li and Ji (2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity</head><p>Relation P R F1 P R F1 ACE05 Our Model (SPTree) 0.829 0.839 0.834 0.572 0.540 0.556</p><p>Li and Ji (2014) 0.852 0.769 0.808 0.654 0.398 0.495 ACE04 Our Model (SPTree) 0.808 0.829 0.818 0.487 0.481 0.484</p><p>Li and Ji (2014) 0.835 0.762 0.797 0.608 0.361 0.453    the entity detection performance, but this degraded the recall in relation classification. This indicates that entity label information is helpful in detecting relations.</p><p>We also show the performance without sharing parameters, i.e., embedding and sequence layers, for detecting entities and relations (−Shared parameters); we first train the entity detection model, detect entities with the model, and build a separate relation extraction model using the detected entities, i.e., without entity detection. This setting can be regarded as a pipeline model since two separate models are trained sequentially. Without the shared parameters, both the performance in entity detection and relation classification drops slightly, although the differences are not significant. When we removed all the enhancements, i.e., scheduled sampling, entity pretraining, label embedding, and shared parameters, the performance is significantly worse than SP-Tree (p&lt;0.01), showing that these enhancements provide complementary benefits to end-to-end relation extraction.</p><p>Next, we show the performance with different LSTM-RNN structures in <ref type="table" target="#tab_2">Table 3</ref>. We first compare the three input dependency structures (SPTree, SubTree, FullTree) for tree-structured LSTM-RNNs. Performances on these three structures are almost same when we distinguish the nodes in the shortest paths from other nodes, but when we do not distinguish them (-SP), the information outside of the shortest path, i.e., FullTree (-SP), significantly hurts performance (p&lt;0.05). We then compare our tree-structured LSTM-RNN (SPTree) with the Child-Sum treestructured LSTM-RNN on the shortest path of <ref type="bibr" target="#b9">Tai et al. (2015)</ref>. Child-Sum performs worse than our SPTree model, but not with as big of a decrease as above. This may be because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child.</p><p>We finally show results with two counterparts of sequence-based LSTM-RNNs using the shortest path (last two rows in <ref type="table" target="#tab_2">Table 3</ref>). SPSeq is a bidirectional LSTM-RNN on the shortest path. The LSTM unit receives input from the sequence layer concatenated with embeddings for the surrounding dependency types and directions. We concatenate the outputs of the two RNNs for the relation candidate. SPXu is our adaptation of the shortest path LSTM-RNN proposed by <ref type="bibr" target="#b12">Xu et al. (2015b)</ref> to match our sequence-layer based model. 11 This has two LSTM-RNNs for the left and right subpaths of the shortest path. We first calculate the max pooling of the LSTM units for each of these two RNNs, and then concatenate the outputs of the pooling for the relation candidate. The comparison with these sequence-based LSTM-RNNs indicates that a tree-structured LSTM-RNN is comparable to sequence-based ones in representing shortest paths.</p><p>Overall, the performance comparison of the LSTM-RNN structures in <ref type="table" target="#tab_2">Table 3</ref> show that for end-to-end relation extraction, selecting the appropriate tree structure representation of the input (i.e., the shortest path) is more important than the choice of the LSTM-RNN structure on that input (i.e., sequential versus tree-based).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relation Classification Analysis Results</head><p>To thoroughly analyze the relation classification part alone, e.g., comparing different LSTM structures, architecture components such as hidden layers and input information, and classification task settings, we use the SemEval-2010 Task 8. This dataset, often used to evaluate NN models for relation classification, annotates only relation-related nominals (unlike ACE datasets), so we can focus cleanly on the relation classification part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>Macro-F1 No External Knowledge Resources Our Model (SPTree) 0.844 dos <ref type="bibr">Santos et al. (2015)</ref> 0.841 <ref type="bibr" target="#b11">Xu et al. (2015a)</ref> 0.840 +WordNet Our Model (SPTree + WordNet) 0.855 <ref type="bibr" target="#b11">Xu et al. (2015a)</ref> 0.856 <ref type="bibr" target="#b12">Xu et al. (2015b)</ref> 0.837  We first report official test set results in Table 4. Our novel LSTM-RNN model is comparable to both the state-of-the-art CNN-based models on this task with or without external sources, i.e., WordNet, unlike the previous best LSTM-RNN model <ref type="bibr" target="#b12">(Xu et al., 2015b)</ref>. <ref type="bibr">12</ref> Next, we compare different LSTM-RNN structures in <ref type="table" target="#tab_4">Table 5</ref>. As for the three input dependency structures (SPTree, SubTree, FullTree), FullTree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes, which hints that the information outside of the shortest path significantly hurts the performance (p&lt;0.05). We also compare our treestructured LSTM-RNN (SPTree) with sequencebased LSTM-RNNs (SPSeq and SPXu) and treestructured LSTM-RNNs (Child-Sum). All these LSTM-RNNs perform slightly worse than our SP-12 When incorporating WordNet information into our model, we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger <ref type="bibr">(Ciaramita and Altun, 2006)</ref> and concatenated the embeddings to the input vector (the concatenation of word and POS embeddings) of the sequence LSTM. We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset.  <ref type="bibr" target="#b11">(Xu et al., 2015a)</ref> 0.848  <ref type="table" target="#tab_4">Table 5</ref> produces different results on FullTree as compared to the results on ACE05 in <ref type="table" target="#tab_2">Table 3</ref>, the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM-RNN structure on that input.</p><p>Finally, <ref type="table" target="#tab_6">Table 6</ref> summarizes the contribution of several model components and training settings on SemEval relation classification. We first remove the hidden layer by directly connecting the LSTM-RNN layers to the softmax layers, and found that this slightly degraded performance, but the difference was small. We then skip the sequence layer and directly use the word and POS embeddings for the dependency layer. Removing the sequence layer 13 or entity-related information from the sequence layer (−Pair) slightly degraded performance, and, on removing both, the performance dropped significantly (p&lt;0.05). This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task.</p><p>When we replace the Stanford neural dependency parser with the Stanford lexicalized PCFG parser (Stanford PCFG), the performance slightly dropped, but the difference was small. This indicates that the selection of parsing models is not critical. We also included WordNet, and this slightly improved the performance (+WordNet), but the difference was small. Lastly, for the generation of relation candidates, generating only leftto-right candidates slightly degraded the perfor-mance, but the difference was small and hence the creation of right-to-left candidates was not critical. Treating the inverse relation candidate as a negative instance (Negative sampling) also performed comparably to other generation methods in our model <ref type="bibr">(unlike Xu et al. (2015a)</ref>, which showed a significance improvement over generating only left-to-right candidates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel end-to-end relation extraction model that represents both word sequence and dependency tree structures by using bidirectional sequential and bidirectional tree-structured LSTM-RNNs. This allowed us to represent both entities and relations in a single model, achieving gains over the state-of-the-art, feature-based system on end-to-end relation extraction (ACE04 and ACE05), and showing favorably comparable performance to recent state-of-the-art CNNbased models on nominal relation classification (SemEval-2010 Task 8).</p><p>Our evaluation and ablation led to three key findings. First, the use of both word sequence and dependency tree structures is effective. Second, training with the shared parameters improves relation extraction accuracy, especially when employed with entity pretraining, scheduled sampling, and label embeddings. Finally, the shortest path, which has been widely used in relation classification, is also appropriate for representing tree structures in neural LSTM models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our incrementally-decoded end-to-end relation extraction model, with bidirectional sequential and bidirectional tree-structured LSTM-RNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>type of dependency to the parent 3 ), and label embedding v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>predicted entity label).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>parameters (Shared) 0.796 0.820 0.808 0.541 0.482 0.510 −EP, SS 0.781 0.804 0.792 0.509 0.479 0.494* −EP, SS, LE, Shared 0.800 0.815 0.807 0.520 0.452 0.484**</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state-of-the-art on the ACE05 test set and ACE04 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation tests on the ACE05 development dataset. * denotes significance at p&lt;0.05, ** denotes p&lt;0.01.</figDesc><table><row><cell>Settings</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>SPTree</cell><cell cols="2">0.815 0.821</cell><cell cols="4">0.818 0.506 0.529 0.518</cell></row><row><cell>SubTree</cell><cell cols="2">0.812 0.818</cell><cell cols="4">0.815 0.525 0.506 0.515</cell></row><row><cell>FullTree</cell><cell cols="2">0.806 0.816</cell><cell cols="4">0.811 0.536 0.507 0.521</cell></row><row><cell cols="3">SubTree (-SP) 0.803 0.816</cell><cell cols="4">0.810 0.533 0.495 0.514</cell></row><row><cell cols="3">FullTree (-SP) 0.804 0.817</cell><cell cols="4">0.811 0.517 0.470 0.492*</cell></row><row><cell>Child-Sum</cell><cell cols="6">0.806 0.819 0.8122 0.514 0.499 0.506</cell></row><row><cell>SPSeq</cell><cell cols="2">0.801 0.813</cell><cell cols="4">0.807 0.500 0.523 0.511</cell></row><row><cell>SPXu</cell><cell cols="2">0.809 0.818</cell><cell cols="4">0.813 0.494 0.522 0.508</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of LSTM-RNN structures on the ACE05 development dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Comparison with state-of-the-art models</cell></row><row><cell cols="2">on SemEval-2010 Task 8 test-set.</cell></row><row><cell>Settings</cell><cell>Macro-F1</cell></row><row><cell>SPTree</cell><cell>0.851</cell></row><row><cell>SubTree</cell><cell>0.839</cell></row><row><cell>FullTree</cell><cell>0.829 *</cell></row><row><cell>SubTree (-SP)</cell><cell>0.840</cell></row><row><cell>FullTree (-SP)</cell><cell>0.828 *</cell></row><row><cell>Child-Sum</cell><cell>0.838</cell></row><row><cell>SPSeq</cell><cell>0.844</cell></row><row><cell>SPXu</cell><cell>0.847</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of LSTM-RNN structures on SemEval-2010 Task 8 development set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Model setting ablations on SemEval-</cell></row><row><cell>2010 development set.</cell></row><row><cell>Tree model, but the differences are small.</cell></row><row><cell>Overall, for relation classification, although</cell></row><row><cell>the performance comparison of the LSTM-RNN</cell></row><row><cell>structures in</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also tried beam search but this did not show improvements in initial experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also tried to use one LSTM-RNN by connecting the root (Paulus et al., 2014), but preparing two LSTM-RNNs showed slightly better performance in our initial experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the order of the target words corresponds to the direction of the relation, not the positions in the sentence. 5 Note that we do not show this Pair inFig.1for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">This is different from the original one in that we use the sequence layer and we concatenate the embeddings for the input, while the original one prepared individual LSTM-RNNs for different inputs and concatenated their outputs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Note that this setting still uses some sequence layer information since it uses the entity-related information (Pair).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Qi Li, Kevin Gimpel, and the anonymous reviewers for dataset details and helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the conference on Human Language Technology and Empirical Methods in</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth2015] Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>September. ACL. [Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki2014] Makoto Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computer-Intensive Methods for Testing Hypotheses : An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen ; Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno>Paulus et al.2014</idno>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D</editor>
		<imprint>
			<date type="published" when="1989-04" />
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
	<note>Global belief recursive neural networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Weinberger ; Anastasia Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boston, MA, USA, June; Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Global Inference for Entity and Relation Identification via a Linear Programming Formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Roth and Yih2007</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2013 workshop on Automated knowledge base construction</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
	<note>September. ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="Au-gust.ACL" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam2010] Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zelenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">We removed DISC and did 5-fold CV on bnews and nwire subsets (348 documents). We use the same preprocessing and evaluation metrics of ACE05. SemEval-2010 Task 8 defines 9 relation types between nominals ( Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection and Message-Topic), and a tenth type Other when two nouns have none of these relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">but defines 7 coarse-grained relation types: PYS, PER-SOC, Employment / Membership / Subsidiary (EMP-ORG), ART, PER/ORG affiliation (Other-AFF), GPE affiliation (GPE-AFF), and Discourse (DISC)</title>
		<editor>Lu and Roth</editor>
		<meeting><address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>We treat this Other type as a negative relation type, and no direction is considered. The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentences from the training set as our development set. We followed the official task setting, and report the official macro-averaged F1-score (Macro-F1) on the 9 relation types</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hyper-parameter Settings Here we show the hyper-parameters and the range tried for the hyper-parameters in parentheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">the number of epochs for training and entity pretraining (≤ 100), and the embedding dimension of WordNet hypernym</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>Hyper-parameters include the initial learning rate</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
