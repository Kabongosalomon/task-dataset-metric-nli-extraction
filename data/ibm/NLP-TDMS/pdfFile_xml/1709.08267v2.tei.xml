<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HDLTex: Hierarchical Deep Learning for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Gerber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
							<email>lbarnes@virginia.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HDLTex: Hierarchical Deep Learning for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Text Mining</term>
					<term>Document Classification</term>
					<term>Deep Neural Networks</term>
					<term>Hierarchical Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Each year scientific researchers produce a massive number of documents. In 2014 the 28,100 active, scholarly, peerreviewed, English-language journals published about 2.5 million articles, and there is evidence that the rate of growth in both new journals and publications is accelerating <ref type="bibr" target="#b0">[1]</ref>. The volume of these documents has made automatic organization and classification an essential element for the advancement of basic and applied research. Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees, naïve Bayes, support vector machines (SVM), neural nets, and ensemble methods. Classification trees and naïve Bayes approaches provide good interpretability but tend to be less accurate than the other methods.</p><p>However, automatic classification has become increasingly challenging over the last several years due to growth in corpus sizes and the number of fields and sub-fields. Areas of research that were little known only five years ago have now become areas of high growth and interest. This growth in sub-fields has occurred across a range of disciplines including biology (e.g., CRISPR-CA9), material science (e.g., chemical programming), and health sciences (e.g., precision medicine). This growth in sub-fields means that it is important to not just label a document by specialized area but to also organize it within its overall field and the accompanying sub-field. This is hierarchical classification.</p><p>Although many existing approaches to document classification can quickly identify the overall area of a document, few of them can rapidly organize documents into the correct subfields or areas of specialization. Further, the combination of top-level fields and all sub-fields presents current document classification approaches with a combinatorially increasing number of class labels that they cannot handle. This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification (HDLTex). <ref type="bibr" target="#b0">1</ref> HDLTex combines deep learning architectures to allow both overall and specialized learning by level of the document hierarchy. This paper reports our experiments with HDLTex, which exhibits improved accuracy over traditional document classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Document classification is necessary to organize documents for retrieval, analysis, curation, and annotation. Researchers have studied and developed a variety of methods for document classification. Work in the information retrieval community has focused on search engine fundamentals such as indexing and dictionaries that are considered core technologies in this field <ref type="bibr" target="#b1">[2]</ref>. Considerable work has built on these foundational methods to provide improvements through feedback and query reformulation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>More recent work has employed methods from data mining and machine learning. Among the most accurate of these techniques is the support vector machine (SVM) <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. SVMs use kernel functions to find separating hyperplanes in high-dimensional spaces. Other kernel methods used for information retrieval include string kernels such as the spectrum kernel <ref type="bibr" target="#b7">[8]</ref> and the mismatch kernel <ref type="bibr" target="#b8">[9]</ref>, which are widely used with DNA and RNA sequence data.</p><p>SVM and related methods are difficult to interpret. For this reason many information retrieval systems use decision trees <ref type="bibr" target="#b2">[3]</ref> and naïve Bayes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> methods. These methods are easier to understand and, as such, can support query reformulation, but they lack accuracy. Some recent work has investigated topic modeling to provide similar interpretations as naïve Bayes methods but with improved accuracy <ref type="bibr" target="#b11">[12]</ref>. This paper uses newer methods of machine learning for document classification taken from deep learning. Deep learning is an efficient version of neural networks <ref type="bibr" target="#b12">[13]</ref> that can perform unsupervised, supervised, and semi-supervised learning <ref type="bibr" target="#b13">[14]</ref>. Deep learning has been extensively used for image processing, but many recent studies have applied deep learning in other domains such as text and data mining. The basic architecture in a neural network is a fully connected network of nonlinear processing nodes organized as layers. The first layer is the input layer, the final layer is the output layer, and all other layers are hidden. In this paper, we will refer to these fully connected networks as Deep Neural Networks (DNN). Convolutional Neural Networks (CNNs) are modeled after the architecture of the visual cortex where neurons are not fully connected but are spatially distinct <ref type="bibr" target="#b14">[15]</ref>. CNNs provide excellent results in generalizing the classification of objects in images <ref type="bibr" target="#b15">[16]</ref>. More recent work has used CNNs for text mining <ref type="bibr" target="#b16">[17]</ref>. In research closely related to the work in this paper, Zhang et al. <ref type="bibr" target="#b17">[18]</ref> used CNNs for text classification with character-level features provided by a fully connected DNN. Regardless of the application, CNNs require large training sets. Another fundamental deep learning architecture used in this paper is the Recurrent Neural Network (RNN). RNNs connect the output of a layer back to its input. This architecture is particularly important for learning time-dependent structures to include words or characters in text <ref type="bibr" target="#b18">[19]</ref>. Deep learning for hierarchical classification is not new with this paper, although the specific architectures, the comparative analyses, and the application to document classification are new. Salakhutdinov <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> used deep learning to hierarchically categorize images. At the top level the images are labeled as animals or vehicles. The next level then classifies the kind of animal or vehicle. This paper describes the use of deep learning approaches to create a hierarchical document classification approach. These deep learning methods have the promise of providing greater accuracy than SVM and related methods. Deep learning methods also provide flexible architectures that we have used to produce hierarchical classifications. The hierarchical classification our methods produce is not only highly accurate but also enables greater understanding of the resulting classification by showing where the document sits within a field or area of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BASELINE TECHNIQUES</head><p>This paper compares fifteen methods for performing document classification. Six of these methods are baselines since they are used for traditional, non-hierarchical document classification. Of the six baseline methods three are widely used for document classification: term-weighted support vector machines <ref type="bibr" target="#b21">[22]</ref>, multi-word support vector machines <ref type="bibr" target="#b22">[23]</ref>, and naïve Bayes classification (NBC). The other three are newer deep learning methods that form the basis for our implementation of a new approach for hierarchical document classification. These deep learning methods are described in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Support Vector Machines (SVMs)</head><p>Vapnik and Chervonenkis introduced the SVM in 1963 <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and in 1992 Boser et al. introduced a nonlinear version to address more complex classification problems <ref type="bibr" target="#b25">[26]</ref>. The key idea of the nonlinear SVM is the generating kernel shown in Equation 1, followed by Equations 2 and 3:</p><formula xml:id="formula_0">K(x, x ) =&lt; φ(x), φ(x ) &gt; (1) f (x) = xi∈training α i K(x, x i ) + b (2) max α1,...,αn n i=1 α i − 1 2 n j=1 n k=1 α j α k y j y k K(x j , x k ) ∀α i ≥ 0i ∈ 1, .., n.<label>(3)</label></formula><p>Multi-Class SVM: Text classification using string kernels within SVMs has been successful in many research projects <ref type="bibr" target="#b26">[27]</ref>. The original SVM solves a binary classification problem; however, since document classification often involves several classes, the binary SVM requires an extension. In general, the multi-class SVM (MSVM) solves the following optimization:</p><formula xml:id="formula_1">min w1,w2,..,w k ,ζ 1 2 k w T k w k + C (xi,yi)∈D ζ i (4) st. w T yi x − w T k x ≤ i − ζ i , ∀(x i , y i ) ∈ D, k ∈ {1, 2, ..., K}, k = y i<label>(5)</label></formula><p>where k indicates number of classes, ζ i are slack variables, and w is the learning parameter. To solve the MSVM we construct a decision function of all k classes at once <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>. One approach to MSVM is to use binary SVM to compare each of the k(k − 1) pairwise classification labels, where k is the number of labels or classes. Yet another technique for MSVM is one-versus-all, where the two classes are one of the k labels versus all of the other k − 1 labels. Stacking Support Vector Machines (SVM): We use Stacking SVMs as another baseline method for comparison with HDL-Tex. The stacking SVM provides an ensemble of individual SVM classifiers and generally produces more accurate results than single-SVM models <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Naïve Bayes classification</head><p>Naïve Bayes is a simple supervised learning technique often used for information retrieval due to its speed and interpretability <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Suppose the number of documents is n and each document has the label c, c ∈ {c 1 , c 2 , ..., c k }, where k is the number of labels. Naïve Bayes calculates   where d is the document, resulting in</p><formula xml:id="formula_2">P (c | d) = P (d | c)P (c) P (d)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of area</head><formula xml:id="formula_3">Ψ 1 Ψ . . . Ψ0, Ψ0, Ψ0,0 Ψ0, Ψ0, Ψ ,0 Ψ , Ψ , Ψ ,0 Ψ , Ψ ,</formula><formula xml:id="formula_4">C M AP = arg max c∈C P (d | c)P (c) = arg max c∈C P (x 1 , x 2 , ..., x n | c)P (c).<label>(7)</label></formula><p>The naïve Bayes document classifier used for this study uses word-level classification <ref type="bibr" target="#b10">[11]</ref>. Letθ j be the parameter for word j, then</p><formula xml:id="formula_5">P (c j | d i ;θ) = P (c j |θ)P (d i | c j ;θ j ) P (d i |θ) .<label>(8)</label></formula><p>IV. FEATURE EXTRACTION Documents enter our hierarchical models via features extracted from the text. We employed different feature extraction approaches for the deep learning architectures we built. For CNN and RNN, we used the text vector-space models using 100 dimensions as described in Glove <ref type="bibr" target="#b31">[32]</ref>. A vector-space model is a mathematical mapping of the word space, defined as</p><formula xml:id="formula_6">d j = (w 1,j , w 2,j , ..., w i,j ..., w lj ,j )<label>(9)</label></formula><p>where l j is the length of the document j, and w i,j is the Glove word embedding vectorization of word i in document j.</p><p>For DNN, we used count-based and term frequency-inverse document frequency (tf-idf) for feature extraction. This approach uses counts for N -grams, which are sequences of N words <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. For example, the text "In this paper we introduced this technique" is composed of the following Ngrams:</p><p>• Feature count (1): { (In, 1) , (this, 2), (paper, 1), (we, 1), (introduced, 1), (technique, 1) } • Feature count (2): { (In, 1) , (this, 2), (paper, 1), (we, 1), (introduced, 1), (technique, 1), (In this, 1), (This paper, 1), (paper we, 1),...} Where the counts are indexed by the maximum N -grams. So Feature count (2) includes both 1-grams and 2-grams. The resulting DNN feature space is</p><formula xml:id="formula_7">f j,n =[x (j,0) , ..., x (j,k−1) , x j,{0,1} , ..., x j,{k−2,k−1} , ..., x j,{k−n,...,k−1} ]<label>(10)</label></formula><p>where f is the feature space of document j for n-grams of size n, n ∈ {0, 1, ..., N }, and x is determined by word or ngram counts. Our algorithm is able to use N-grams for features within deep learning models <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DEEP LEARNING NEURAL NETWORKS</head><p>The methods used in this paper extend the concepts of deep learning neural networks to the hierarchical document classification problem. Deep learning neural networks provide efficient computational models using combinations of nonlinear processing elements organized in layers. This organization of simple elements allows the total network to generalize (i.e., predict correctly on new data) <ref type="bibr" target="#b35">[36]</ref>. In the research described here, we used several different deep learning techniques and combinations of these techniques to create hierarchical document classifiers. The following subsections provide an overview of the three deep learning architectures we used: Deep Neural Networks (DNN), Recurrent Neural Networks(RNN), and Convolutional Neural Networks (CNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Neural Networks (DNN)</head><p>In the DNN architecture each layer only receives input from the previous layer and outputs to the next layer. The layers are fully connected. The input layer consists of the text features (see IV) and the output layer has a node for each classification label or only one node if it is a binary classification. This architecture is the baseline DNN. Additional details on this architecture can be found in <ref type="bibr" target="#b36">[37]</ref>.   This paper extends this baseline architecture to allow hierarchical classification. <ref type="figure" target="#fig_0">Figure 1</ref> shows this new architecture. The DNN for the first level of classification (on the left side in <ref type="figure" target="#fig_0">Figure 1</ref>) is the same as the baseline DNN. The second level classification in the hierarchy consists of a DNN trained for the domain output in the first hierarchical level. Each second level in the DNN is connected to the output of the first level. For example, if the output of the first model is labeled computer science then the DNN in the next hierarchical level (e.g., Ψ 1 in <ref type="figure" target="#fig_0">Figure 1</ref>) is trained only with all computer science documents. So while the first hierarchical level DNN is trained with all documents, each DNN in the next level of the document hierarchy is trained only with the documents for the specified domain.</p><formula xml:id="formula_8">Ψ 1 ... Ψ Ψ , Ψ ,1 Ψ ,0 Ψ 1 , Ψ 1 ,0 Ψ 1 ,1 Ψ 1 ,0 Ψ 1 , Ψ 1 , Ψ ,0 Ψ , Ψ ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM/GRU</head><p>The DNNs in this study are trained with the standard backpropagation algorithm using both sigmoid (Equation 11) and ReLU (Equation 12) as activation functions. The output layer uses softmax (Equation 13).</p><formula xml:id="formula_9">f (x) = 1 1 + e −x ∈ (0, 1),<label>(11)</label></formula><formula xml:id="formula_10">f (x) = max(0, x),<label>(12)</label></formula><formula xml:id="formula_11">σ(z) j = e zj K k=1 e z k ,<label>(13)</label></formula><p>∀ j ∈ {1, . . . , K} Given a set of example pairs (x, y), x ∈ X, y ∈ Y the goal is to learn from the input and target spaces using hidden layers.</p><p>In text classification, the input is generated by vectorization of text (see Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recurrent Neural Networks (RNN)</head><p>The second deep learning neural network architecture we use is RNN. In RNN the output from a layer of nodes can reenter as input to that layer. This approach has advantages for text processing <ref type="bibr" target="#b37">[38]</ref>. The general RNN formulation is given in <ref type="figure" target="#fig_0">Equation 14</ref> where x t is the state at time t and u t refers to the input at step t.</p><formula xml:id="formula_12">x t = F (x t−1 , u t , θ)<label>(14)</label></formula><p>We use weights to reformulate Equation 14 as shown in <ref type="figure" target="#fig_0">Equation 15</ref> below:</p><formula xml:id="formula_13">x t = W rec σ(x t−1 ) + W in u t + b.<label>(15)</label></formula><p>In Equation <ref type="bibr" target="#b14">15</ref>, W rec is the recurrent matrix weight, W in are the input weights, b is the bias, and σ is an element-wise function. Again we have modified the basic architecture for use in hierarchical classification. <ref type="figure" target="#fig_3">Figure 2</ref> shows this extended RNN architecture. Several problems (e.g., vanishing and exploding gradients) arise in RNNs when the error of the gradient descent algorithm is back-propagated through the network <ref type="bibr" target="#b38">[39]</ref>. To deal with these problems, long short-term memory (LSTM) is a special type of RNN that preserves long-term dependencies in a more effective way compared with the basic RNN. This is particularly effective at mitigating the vanishing gradient problem <ref type="bibr" target="#b39">[40]</ref>.  <ref type="figure" target="#fig_4">Figure 3</ref> shows the basic cell of an LSTM model. Although LSTM has a chain-like structure similar to RNN, LSTM uses multiple gates to regulate the amount of information allowed into each node state. A step-by-step explanation the LSTM cell and its gates is provided below: 1) Input Gate:</p><formula xml:id="formula_14">i t = σ(W i [x t , h t−1 ] + b i ),<label>(16)</label></formula><p>2) Candid Memory Cell Value:</p><formula xml:id="formula_15">C t = tanh(W c [x t , h t−1 ] + b c ),<label>(17)</label></formula><p>3) Forget Gate Activation:</p><formula xml:id="formula_16">f t = σ(W f [x t , h t−1 ] + b f ),<label>(18)</label></formula><p>4) New Memory Cell Value:</p><formula xml:id="formula_17">C t = i t * C t + f t C t−1 ,<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Output Gate Values:</head><formula xml:id="formula_18">o t =σ(W o [x t , h t−1 ] + b o ), h t =o t tanh(C t ),<label>(20)</label></formula><p>In the above description, b is a bias vector, W is a weight matrix, and x t is the input to the memory cell at time t. The i, c, f and o indices refer to input, cell memory, forget and output gates, respectively. <ref type="figure" target="#fig_4">Figure 3</ref> shows the structure of these gates with a graphical representation.</p><p>An RNN can be biased when later words are more influential than the earlier ones. To overcome this bias convolutional neural network (CNN) models (discussed in Section V-C) include a max-pooling layer to determine discriminative phrases in text <ref type="bibr" target="#b40">[41]</ref>. A gated recurrent unit (GRU) is a gating mechanism for RNNs that was introduced in 2014 <ref type="bibr" target="#b41">[42]</ref>. GRU is a simplified variant of the LSTM architecture, but there are differences as follows: GRUs contain two gates, they do not possess internal memory (the C t−1 in <ref type="figure" target="#fig_4">Figure 3)</ref>, and a second non-linearity is not applied (tanh in <ref type="figure" target="#fig_4">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convolutional Neural Networks (CNN)</head><p>The final deep learning approach we developed for hierarchical document classification is the convolutional neural network (CNN). Although originally built for image processing, as discussed in Section II, CNNs have also been effectively used for text classification <ref type="bibr" target="#b14">[15]</ref>. The basic convolutional layer in a CNN connects to a small subset of the inputs usually of size 3 × 3. Similarly the next convolutional layer connects to only a subset of its preceding layer. In this way these convolution layers, called feature maps, can be stacked to provide multiple filters on the input. To reduce computational complexity, CNNs use pooling to reduce the size of the output from one stack of layers to the next in the network. Different pooling techniques are used to reduce outputs while preserving important features <ref type="bibr" target="#b42">[43]</ref>. The most common pooling method is max-pooling where the maximum element is selected in the pooling window. In order to feed the pooled output from stacked featured maps to the next layer, the maps are flattened into one column. The final layers in a CNN are typically fully connected. In general during the back-propagation step of a CNN not only the weights are adjusted but also the feature detector filters. A potential problem of CNNs used for text is the number of channels or size of the feature space. This might be very large (e.g., 50K words) for text, but for images this is less of a problem (e.g., only 3 channels of RGB) <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hierarchical Deep Learning</head><p>The primary contribution of this research is hierarchical classification of documents. A traditional multi-class classification technique can work well for a limited number classes, but performance drops with increasing number of classes, as is present in hierarchically organized documents. In our hierarchical deep learning model we solve this problem by creating architectures that specialize deep learning approaches for their level of the document hierarchy (e.g., see <ref type="figure" target="#fig_0">Figure 1</ref>). The structure of our Hierarchical Deep Learning for Text (HDLTex) architecture for each deep learning model is as follows: DNN: 8 hidden layers with 1024 cells in each hidden layer. RNN: GRU and LSTM are used in this implementation, 100 cells with GRU with two hidden layers. CNN: Filter sizes of {3, 4, 5, 6, 7} and max-pool of 5, layer sizes of {128, 128, 128} with max pooling of {5, 5, 35}, the CNN contains 8 hidden layers. All models used the following parameters: Batch Size = 128, learning parameters = 0.001, β 1 =0.9, β 2 =0.999, = 1e 08 , decay = 0.0, Dropout=0.5 (DNN) and Dropout=0.25 (CNN and RNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation</head><p>We used the following cost function for the deep learning models:</p><formula xml:id="formula_19">Acc(X) = Acc(X Ψ ) k − 1 Ψ∈{Ψ1,..Ψ k } Acc(X Ψi ).n Ψ k<label>(21)</label></formula><p>where is the number of levels, k indicates number of classes for each level, and Ψ refers to the number of classes in the child's level of the hierarchical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Optimization</head><p>We used two types of stochastic gradient optimization for the deep learning models in this paper: RMSProp and Adam. These are described below.</p><p>RMSProp Optimizer: The basic stochastic gradient descent (SGD) is shown below:</p><formula xml:id="formula_20">θ ← θ − α∇ θ J(θ, x i , y i ) (22) θ ← θ − γθ + α∇ θ J(θ, x i , y i )<label>(23)</label></formula><p>For these equations, θ is the learning parameter, α is the learning rate, and J(θ, x i , y i ) is the objective or cost function. The history of updates is defined by γ ∈ (0, 1). To update parameters, SGD uses a momentum term on a rescaled gradient, which is shown in Equation <ref type="formula" target="#formula_0">(23)</ref>. This approach to the optimization does not perform bias correction, which is a problem for a sparse gradient. Adam Optimizer: Adam is another stochastic gradient optimizer, which averages over only the first two moments of the gradient, v and m, as shown below:</p><formula xml:id="formula_21">θ ←θ − α √v + m<label>(24)</label></formula><p>where</p><formula xml:id="formula_22">g i,t = ∇ θ J(θ i , x i , y i ) (25) m t = β 1 m t−1 + (1 − β 1 )g i,t (26) m t = β 2 v t−1 + (1 − β 2 )g 2 i,t<label>(27)</label></formula><p>In these equations, m t and v t are the first and second moments, respectively. Both are estimated asm t = mt</p><formula xml:id="formula_23">1−β t 1 andv t = vt 1−β t 2 .</formula><p>This approach can handle the non-stationarity of the objective function as can RMSProp, but Adam can also overcome the sparse gradient issue that is a drawback in RMSProp <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>Our document collection had 134 labels as shown in Table I. <ref type="bibr" target="#b1">2</ref> The target value has two levels, k 0 ∈ {1, .., 7} which are k 0 ∈ { Computer Science, Electrical Engineering, Psychology, Mechanical Engineering, Civil Engineering, Medical Science, biochemistry} and children levels of the labels, k , which contain {17, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">53</ref>, 9} specific topics belonging to k 0 , respectively. To train and test the baseline methods described in Section III and the new hierarchical document classification methods described in Section V, we collected data and meta-data on 46, 985 published papers available from the Web Of Science <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. To automate collection we used Selenium <ref type="bibr" target="#b46">[47]</ref> with ChoromeDriver <ref type="bibr" target="#b47">[48]</ref> for the Chrome web browser. To extract the data from the site we used Beautiful Soup <ref type="bibr" target="#b48">[49]</ref>. We specifically extracted the abstract, domain, and keywords of this set of published papers. The text in the abstract is the input for classification while the domain name provides the label for the top level of the hierarchy. The keywords provide the descriptors for the next level in the classification hierarchy. <ref type="table" target="#tab_2">Table I</ref> shows statistics for this collection. For example, Medical Sciences is one of the top-level domain classifications and there are 53 sub-classifications within this domain. There are also over 14k articles or documents within the domain of health sciences in this data set. We divided the data set into three parts as shown in <ref type="table" target="#tab_2">Table II</ref>. Data set W OS − 46985 is the full data set with 46,985 documents, and data sets W OS −11967 and W OS −5736 are subsets of this full data set with the number of training and testing documents shown as well as the number of labels or classes in each of the two levels. For dataset W OS − 11967, each of the seven level-1 classes has five sub-classes. For data set W OS − 5736, two of the three higher-level classes have four sub-classes and the last high-level class has three subclasses. We removed all special characters from all three data sets before training and testing. B. Hardware and Implementation</p><p>The following results were obtained using a combination of central processing units (CPUs) and graphical processing units (GPUs). The processing was done on a Xeon E5 − 2640 (2.6GHz) with 32 cores and 64GB memory, and the GPU cards were N vidia Quadro K620 and N vidia T esla K20c. We implemented our approaches in Python using the Compute Unified Device Architecture (CUDA), which is a parallel computing platform and Application Programming Interface (API) model created by N vidia. We also used Keras and TensorFlow libraries for creating the neural networks <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. <ref type="table" target="#tab_2">Table III</ref> shows the results from our experiments. The baseline tests compare three conventional document classification approaches (naïve Bayes and two versions of SVM) and stacking SVM with three deep learning approaches (DNN, RNN, and CNN). In this set of tests the RNN outperforms the others for all three W OS data sets. CNN performs secondbest for three data sets. SVM with term weighting <ref type="bibr" target="#b21">[22]</ref> is third for the first two sets while the multi-word approach of <ref type="bibr" target="#b22">[23]</ref> is in third place for the third data set. The third data set is the smallest of the three and has the fewest labels so the differences among the three best performers are not large. These results show that overall performance improvement for general document classification is obtainable with deep learning approaches compared to traditional methods. Overall, naïve Bayes does much worse than the other methods throughout these tests. As for the tests of classifying these documents within a hierarchy, the HDLTex approaches with stacked, deep learning architectures clearly provide superior performance. For data set W OS − 11967, the best accuracy is obtained by the combination RNN for the first level of classification and DNN for the second level. This gives accuracies of 94% for the first level, 92% for the second level and 86% overall. This is significantly better than all of the others except for the combination of CNN and DNN. For data set W OS − 46985 the best scores are again achieved by RNN for level one but this time with RNN for level 2. The closest scores to this are obtained by CNN and RNN in levels 1 and 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Empirical Results</head><p>Finally the simpler data set W OS − 5736 has a winner in CNN at level 1 and CNN at level 2, but there is little difference between these scores and those obtained by two other HDLTex architectures: DNN with CNN and RNN with CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE WORK</head><p>Document classification is an important problem to address, given the growing size of scientific literature and other document sets. When documents are organized hierarchically, multi-class approaches are difficult to apply using traditional supervised learning methods. This paper introduces a new approach to hierarchical document classification, HDLTex, that combines multiple deep learning approaches to produce hierarchical classifications. Testing on a data set of documents obtained from the Web of Science shows that combinations of RNN at the higher level and DNN or CNN at the lower level produced accuracies consistently higher than those obtainable by conventional approaches using naïve Bayes or SVM. These results show that deep learning methods can provide improvements for document classification and that they provide flexibility to classify documents within a hierarchy. Hence, they provide extensions over current methods for document classification that only consider the multi-class problem.</p><p>The methods described here can improved in multiple ways. Additional training and testing with other hierarchically structured document data sets will continue to identify architectures that work best for these problems. Also, it is possible to extend the hierarchy to more than two levels to capture more of the complexity in the hierarchical classification. For example, if keywords are treated as ordered then the hierarchy continues down multiple levels. HDLTex can also be applied to unlabeled documents, such as those found in news or other media outlets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>HDLTex: Hierarchical Deep Learning for Text Classification. This is our Deep Neural Network (DNN) approach for text classification. The left figure depicts the parent-level of our model, and the right figure depicts child-level models defined by Ψ i as input documents in the parent level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>HDLTex: Hierarchical Deep Learning for Text Classification. This is our structure of recurrent neural networks (RNN) for text classification. The left figure is the parent level of our text leaning model. The right figure depicts child-level learning models defined by Ψ i as input documents in the parent levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>The top sub-figure is a cell of GRU, and the bottom Figure is a cell of LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Details of the document set used in this paper.</figDesc><table><row><cell>Domain</cell><cell>Number of Document</cell><cell>Number of Area</cell></row><row><cell>Biochemistry</cell><cell>5,687</cell><cell>9</cell></row><row><cell>Civil Engineering</cell><cell>4,237</cell><cell>11</cell></row><row><cell>Computer Science</cell><cell>6,514</cell><cell>17</cell></row><row><cell>Electrical Engineering</cell><cell>5,483</cell><cell>16</cell></row><row><cell>Medical Sciences</cell><cell>14,625</cell><cell>53</cell></row><row><cell>Mechanical Engineering</cell><cell>3,297</cell><cell>9</cell></row><row><cell>Psychology</cell><cell>7,142</cell><cell>19</cell></row><row><cell>Total</cell><cell>46,985</cell><cell>134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Details of three data sets used in this paper.</figDesc><table><row><cell>Data Set</cell><cell>Training</cell><cell cols="2">Testing Level 1</cell><cell>Level 2</cell></row><row><cell>WOS-11967</cell><cell>8018</cell><cell>3949</cell><cell>7</cell><cell>35</cell></row><row><cell>WOS-46985</cell><cell>31479</cell><cell>15506</cell><cell>7</cell><cell>134</cell></row><row><cell>WOS-5736</cell><cell>4588</cell><cell>1148</cell><cell>3</cell><cell>11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>HDLTex and Baseline Accuracy of three WOS datasets</figDesc><table><row><cell></cell><cell cols="2">WOS-11967</cell><cell></cell><cell cols="2">WOS-46985</cell><cell></cell><cell cols="2">WOS-5736</cell><cell></cell></row><row><cell></cell><cell>Methods</cell><cell></cell><cell>Accuracy</cell><cell>Methods</cell><cell></cell><cell>Accuracy</cell><cell>Methods</cell><cell></cell><cell>Accuracy</cell></row><row><cell></cell><cell>DNN</cell><cell></cell><cell>80.02</cell><cell>DNN</cell><cell></cell><cell>66.95</cell><cell>DNN</cell><cell></cell><cell>86.15</cell></row><row><cell></cell><cell cols="2">CNN (Yang el. et. 2016)</cell><cell>83.29</cell><cell cols="2">CNN (Yang el. et. 2016)</cell><cell>70.46</cell><cell cols="2">CNN (Yang el. et. 20016)</cell><cell>88.68</cell></row><row><cell>Baseline</cell><cell cols="2">RNN (Yang el. et. 2016) NBC</cell><cell>83.96 68.8</cell><cell cols="2">RNN (Yang el. et. 2016) NBC</cell><cell>72.12 46.2</cell><cell cols="2">RNN (Yang el. et. 2016) NBC</cell><cell>89.46 78.14</cell></row><row><cell></cell><cell cols="2">SVM (Zhang el. et. 2008)</cell><cell>80.65</cell><cell cols="2">SVM (Zhang el. et. 2008)</cell><cell>67.56</cell><cell cols="2">SVM (Zhang el. et. 2008)</cell><cell>85.54</cell></row><row><cell></cell><cell cols="2">SVM (Chen el et. 2016)</cell><cell>83.16</cell><cell cols="2">SVM (Chen el et. 2016)</cell><cell>70.22</cell><cell cols="2">SVM (Chen el et. 2016)</cell><cell>88.24</cell></row><row><cell></cell><cell cols="2">Stacking SVM</cell><cell>79.45</cell><cell cols="2">Stacking SVM</cell><cell>71.81</cell><cell cols="2">Stacking SVM</cell><cell>85.68</cell></row><row><cell></cell><cell>DNN 91.43</cell><cell>DNN 91.58</cell><cell>83.73</cell><cell>DNN 87.31</cell><cell>DNN 80.29</cell><cell>70.10</cell><cell>DNN 97.97</cell><cell>DNN 90.21</cell><cell>88.37</cell></row><row><cell></cell><cell>DNN 91.43</cell><cell>CNN 91.12</cell><cell>83.32</cell><cell>DNN 87.31</cell><cell>CNN 82.35</cell><cell>71.90</cell><cell>DNN 97.97</cell><cell>CNN 92.34</cell><cell>90.47</cell></row><row><cell></cell><cell>DNN 91.43</cell><cell>RNN 89.23</cell><cell>81.58</cell><cell>DNN 87.31</cell><cell>RNN 84.66</cell><cell>73.92</cell><cell>DNN 97.97</cell><cell>RNN 90.25</cell><cell>88.42</cell></row><row><cell></cell><cell>CNN 93.52</cell><cell>DNN 91.58</cell><cell>85.65</cell><cell>CNN 88.67</cell><cell>DNN 80.29</cell><cell>71.20</cell><cell>CNN 98.47</cell><cell>DNN 90.21</cell><cell>88.83</cell></row><row><cell>HDLTex</cell><cell>CNN 93.52</cell><cell>CNN 91.12</cell><cell>85.23</cell><cell>CNN 88.67</cell><cell>CNN 82.35</cell><cell>73.02</cell><cell>CNN 98.47</cell><cell>CNN 92.34</cell><cell>90.93</cell></row><row><cell></cell><cell>CNN 93.52</cell><cell>RNN 89.23</cell><cell>83.45</cell><cell>CNN 88.67</cell><cell>RNN 84.66</cell><cell>75.07</cell><cell>CNN 98.47</cell><cell>RNN 90.25</cell><cell>88.87</cell></row><row><cell></cell><cell>RNN 93.98</cell><cell>DNN 91.58</cell><cell>86.07</cell><cell>RNN 90.45</cell><cell>DNN 80.29</cell><cell>72.62</cell><cell>RNN 97.82</cell><cell>DNN 90.21</cell><cell>88.25</cell></row><row><cell></cell><cell>RNN 93.98</cell><cell>CNN 91.12</cell><cell>85.63</cell><cell>RNN 90.45</cell><cell>CNN 82.35</cell><cell>74.46</cell><cell>RNN 97.82</cell><cell>CNN 92.34</cell><cell>90.33</cell></row><row><cell></cell><cell>RNN 93.98</cell><cell>RNN 89.23</cell><cell>83.85</cell><cell>RNN 90.45</cell><cell>RNN 84.66</cell><cell>76.58</cell><cell>RNN 97.82</cell><cell>RNN 90.25</cell><cell>88.28</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">WOS dataset is shared at http://archive.ics.uci.edu/index.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scoring here could be performed on small sets using human judges.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The stm report: An overview of scientific and scholarly journal publishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A classification approach to boolean query reformulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="694" to="706" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Construction of fuzzyfind dictionary using golay coding transformation for searching applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yammahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vichr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alsaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Berkovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications(IJACSA)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="81" to="87" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The spectrum kernel: A string kernel for svm protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific symposium on biocomputing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="566" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mismatch string kernels for svm protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1441" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-98 workshop on learning for text categorization</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">752</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some effective techniques for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Myaeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1457" to="1466" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An effective and interpretable method for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Linh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Than</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="763" to="793" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03827</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Medsker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Design and Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning with hierarchical-deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1958" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2518" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Turning from tf-idf to tf-igm for term weighting in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text classification based on multiword with support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="879" to="886" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A class of algorithms for pattern recognition learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomat. i Telemekh</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="945" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early history of support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Inference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
		<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gakco: a fast gapped k-mer string kernel using counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<idno>CSD-TR-98-04</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="1998-05" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Royal Holloway, University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical text classification and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Conference on</title>
		<meeting>IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note>Data Mining</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Naive (bayes) at forty: The independence assumption in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">N-gram-based text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Arbor MI</title>
		<imprint>
			<biblScope unit="volume">48113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Comparing input words to a word dictionary for correct spelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glickman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985-05" />
			<biblScope unit="page">148</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 4,498</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">N-gram-based author profiles for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference pacific association for computational linguistics, PACLING</title>
		<meeting>the conference pacific association for computational linguistics, PACLING</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (3)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Web of science R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poungpair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pootong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tongtawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Songserm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tapchaisri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chaicumpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMMUNICATIONS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Web of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reuters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automated data collection with R: A practical guide to web scraping and text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Munzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rubba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meißner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nyhuis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Selenium Testing Tools Cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gundecha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Beautiful soup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richardson</surname></persName>
		</author>
		<ptr target="https://www.crummy.com/software/BeautifulSoup/" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Keras: Deep learning library for theano and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
