<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTINGENCY-AWARE EXPLORATION IN REINFORCEMENT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
							<email>guoyijie@umich.edumoczulski@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
							<email>junhyuk@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Wu</surname></persName>
							<email>nealwu@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONTINGENCY-AWARE EXPLORATION IN REINFORCEMENT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. 1 For example, we report a state-of-the-art score of &gt;11,000 points on MONTEZUMA'S REVENGE without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations. * Equal contributions, listed in alphabetical order. † Now at DeepMind. 1 Examples of the learned policy and the contingent regions are available at https://coex-rl.github.io/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The success of reinforcement learning (RL) algorithms in complex environments hinges on the way they balance exploration and exploitation. There has been a surge of recent interest in developing effective exploration strategies for problems with high-dimensional state spaces and sparse rewards <ref type="bibr" target="#b36">(Schmidhuber, 1991;</ref><ref type="bibr" target="#b31">Oudeyer &amp; Kaplan, 2009;</ref><ref type="bibr" target="#b17">Houthooft et al., 2016;</ref><ref type="bibr" target="#b8">Bellemare et al., 2016;</ref><ref type="bibr" target="#b29">Osband et al., 2016;</ref><ref type="bibr" target="#b32">Pathak et al., 2017;</ref><ref type="bibr" target="#b33">Plappert et al., 2018;</ref><ref type="bibr" target="#b44">Zheng et al., 2018)</ref>. Deep neural networks have seen great success as expressive function approximators within RL and as powerful representation learning methods for many domains. In addition, there have been recent studies on using neural network representations for exploration <ref type="bibr" target="#b41">(Tang et al., 2017;</ref><ref type="bibr" target="#b21">Martin et al., 2017;</ref><ref type="bibr" target="#b32">Pathak et al., 2017)</ref>. For example, count-based exploration with neural density estimation <ref type="bibr" target="#b8">(Bellemare et al., 2016;</ref><ref type="bibr" target="#b41">Tang et al., 2017;</ref><ref type="bibr" target="#b30">Ostrovski et al., 2017)</ref> presents one of the state-of-the-art techniques on the most challenging Atari games with sparse rewards.</p><p>Despite the success of recent exploration methods, it is still an open question on how to construct an optimal representation for exploration. For example, the concept of visual similarity is used for learning density models as a basis for calculating pseudo-counts <ref type="bibr" target="#b8">(Bellemare et al., 2016;</ref><ref type="bibr" target="#b30">Ostrovski et al., 2017)</ref>. However, as <ref type="bibr" target="#b41">Tang et al. (2017)</ref> noted, the ideal way to represent states should be based on what is relevant to solving the MDP, rather than only relying on visual similarity. In addition, there remains another question on whether the representations used for recent exploration works are easily interpretable. To address these questions, we investigate whether we can learn a complementary, more intuitive, and interpretable high-level abstraction that can be very effective in exploration by using the ideas of contingency awareness and controllable dynamics.</p><p>The key idea that we focus on in this work is the notion of contingency awareness <ref type="bibr" target="#b42">(Watson, 1966;</ref><ref type="bibr" target="#b6">Bellemare et al., 2012)</ref> -the agent's understanding of the environmental dynamics and recognizing that some aspects of the dynamics are under the agent's control. Intuitively speaking, this can represent the segmentation mask of the agent operating in the 2D or 3D environments (yet one can think of more abstract and general state spaces). In this study, we investigate the concept of contingency awareness based on self-localization, i.e., the awareness of where the agent is located in the abstract state space. We are interested in discovering parts of the world that are directly dependent on the agent's immediate action, which often reveal the agent's approximate location.</p><p>For further motivation on the problem, we note that contingency awareness is a very important concept in neuroscience and psychology. In other words, being self-aware of one's location is an important property within many observed intelligent organisms and systems. For example, recent breakthroughs in neuroscience, such as the Nobel Prize winning work on the grid cells <ref type="bibr" target="#b27">(Moser et al., 2015;</ref><ref type="bibr" target="#b4">Banino et al., 2018)</ref>, show that organisms that perform very well in spatially-challenging tasks are self-aware of their location. This allows rats to navigate, remember paths to previously visited places and important sub-goals, and find shortcuts. In addition, the notion of contingency awareness has been shown as an important factor in developmental psychology <ref type="bibr" target="#b42">(Watson, 1966;</ref><ref type="bibr" target="#b2">Baeyens et al., 1990)</ref>. We can think of self-localization (and more broadly self-awareness) as a principled and fundamental direction towards intelligent agents.</p><p>Based on these discussions, we hypothesize that contingency awareness can be a powerful mechanism for tackling exploration problems in reinforcement learning. We consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). For example, in the context of 2D Atari games, contingency-awareness roughly corresponds to understanding the notion of controllable entities (e.g., the player's avatar), which <ref type="bibr" target="#b6">Bellemare et al. (2012)</ref> refer to as contingent regions. More concretely, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in the game FREEWAY, only the chicken sprite is under the agent's control and not the multiple moving cars; therefore the chicken's location should be an informative element for exploration <ref type="bibr" target="#b6">(Bellemare et al., 2012;</ref><ref type="bibr" target="#b32">Pathak et al., 2017)</ref>.</p><p>In this study, we also investigate whether contingency awareness can be learned without any external annotations or supervision. For this, we provide an instantiation of an algorithm for automatically learning such information and using it for improving exploration on a 2D ALE environment <ref type="bibr" target="#b7">(Bellemare et al., 2013)</ref>. Concretely, we employ an attentive dynamics model (ADM) to predict the agent's action chosen between consecutive states. It allows us to approximate the agent's position in 2D environments, but unlike other approaches such as <ref type="bibr" target="#b6">(Bellemare et al., 2012)</ref>, it does not require any additional supervision to do so. The ADM learns in an online and self-supervised fashion with pure observations as the agent's policy is updated and does not require hand-crafted features, an environment simulator, or supervision labels for training.</p><p>In experimental evaluation, our methods significantly improve the performance of A2C on hardexploration Atari games in comparison with competitive methods such as density-based exploration <ref type="bibr" target="#b8">(Bellemare et al., 2016;</ref><ref type="bibr" target="#b30">Ostrovski et al., 2017)</ref> and SimHash <ref type="bibr" target="#b41">(Tang et al., 2017)</ref>. We report very strong results on sparse-reward Atari games, including the state-of-the-art performance on the notoriously difficult MONTEZUMA'S REVENGE, when combining our proposed exploration strategy with PPO , without using expert demonstrations, explicit high-level information (e.g., RAM states), or resetting the environment to an arbitrary state.</p><p>We summarize our contributions as follows:</p><p>• We demonstrate the importance of learning contingency awareness for efficient exploration in challenging sparse-reward RL problems. • We develop a novel instance of attentive dynamics model using contingency and controllable dynamics to provide robust localization abilities across the most challenging Atari environments. • We achieve a strong performance on difficult sparse-reward Atari games, including the state-ofthe-art score on the notoriously challenging MONTEZUMA'S REVENGE.</p><p>Overall, we believe that our experiments confirm the hypothesis that contingency awareness is an extremely powerful concept for tackling exploration problems in reinforcement learning, which opens up interesting research questions for further investigations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Self-Localization. The discovery of grid cells <ref type="bibr" target="#b27">(Moser et al., 2015)</ref> motivates working on agents that are self-aware of their location. <ref type="bibr" target="#b4">Banino et al. (2018)</ref> emphasize the importance of self-localization and train a neural network which learns a similar mechanism to grid cells to perform tasks related to spatial navigation. The presence of grid cells is correlated with high performance. Although grid cells seem tailored to 2D or 3D problems that animals encounter in their life, it is speculated that their use can be extended to more abstract spaces. A set of potential approaches to self-localization ranges from ideas specific to a given environment, e.g., SLAM <ref type="bibr" target="#b15">(Durrant-Whyte &amp; Bailey, 2006)</ref>, to methods with potential generalizability <ref type="bibr" target="#b23">(Mirowski et al., 2017;</ref><ref type="bibr" target="#b18">Jaderberg et al., 2017;</ref>.</p><p>Self-supervised Dynamics Model and Controllable Dynamics. Several works have used forward and/or inverse dynamics models of the environment <ref type="bibr" target="#b28">(Oh et al., 2015;</ref><ref type="bibr" target="#b1">Agrawal et al., 2016;</ref><ref type="bibr" target="#b38">Shelhamer et al., 2017)</ref>. <ref type="bibr" target="#b32">Pathak et al. (2017)</ref> employ a similar dynamics model to learn feature representations of states that captures controllable aspects of the environment. This dense representation is used to design a curiosity-driven intrinsic reward. The idea of learning representations on relevant aspects of the environment by learning auxiliary tasks is also explored in <ref type="bibr" target="#b18">(Jaderberg et al., 2017;</ref><ref type="bibr" target="#b9">Bengio et al., 2017;</ref><ref type="bibr" target="#b35">Sawada, 2018)</ref>. Our presented approach is different as we focus on explicitly discovering controllable aspects using an attention mechanism, resulting in better interpretability.</p><p>Exploration and Intrinsic Motivation. The idea of providing an exploration bonus reward depending on the state-action visit-count was proposed by <ref type="bibr" target="#b40">Strehl &amp; Littman (2008)</ref>   <ref type="bibr" target="#b21">Martin et al. (2017)</ref> also construct a visitation density model over a compressed feature space rather than the raw observation space. Alternatively, <ref type="bibr" target="#b41">Tang et al. (2017)</ref> propose a localitysensitive hashing (LSH) method to cluster states and maintain a state-visitation counter based on a form of similarity between frames. We train an agent with a similar count-based exploration bonus, but the way of maintaining state counter seems relatively simpler in that key feature information (i.e., controllable region) is explicitly extracted from the observation and directly used for counting states. Another popular family of exploration strategies in RL uses intrinsic motivation <ref type="bibr" target="#b36">(Schmidhuber, 1991;</ref><ref type="bibr" target="#b39">Singh et al., 2004;</ref><ref type="bibr" target="#b31">Oudeyer &amp; Kaplan, 2009;</ref><ref type="bibr" target="#b5">Barto, 2013)</ref>. These methods encourage the agent to look for something surprising in the environment which motivates its search for novel states, such as surprise <ref type="bibr" target="#b0">(Achiam &amp; Sastry, 2017)</ref>, curiosity <ref type="bibr" target="#b32">(Pathak et al., 2017;</ref><ref type="bibr" target="#b10">Burda et al., 2018)</ref>, and diversity <ref type="bibr" target="#b16">(Eysenbach et al., 2018)</ref>, or via feature control <ref type="bibr" target="#b18">(Jaderberg et al., 2017;</ref><ref type="bibr" target="#b14">Dilokthanakul et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISCOVERING CONTINGENCY VIA ATTENTIVE DYNAMICS MODEL</head><p>To discover the region of the observation that is controllable by the agent, we develop an instance of attentive dynamics model (ADM) based on inverse dynamics f inv . The model takes two consecutive input frames (observations) s t−1 , s t ∈ S as input and aims to predict the action (a t−1 ∈ A) taken by the agent to transition from s t−1 to s t : a t−1 = f inv (s t−1 , s t ).</p><p>(1) Our key intuition is that the inverse dynamics model should attend to the most relevant part of the observation, which is controllable by the agent, to be able to classify the actions. We determine whether each region in a H × W grid is controllable, or in other words, useful for predicting the agent's action, by using a spatial attention mechanism <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b43">Xu et al., 2015)</ref>. An overview of the model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Model. To perform action classification, we first compute a convolutional feature map φ s t = φ(s t ) ∈ R H×W ×K based on the observation s t using a convolutional neural network φ. We estimate a set of logit (score) vectors, denoted e t (i, j) ∈ R |A| , for action classification from each grid cell (i, j) of the convolutional feature map. The local convolution features and feature differences for consecutive frames are fed into a shared multi-layer perceptron (MLP) to derive the logits as:</p><formula xml:id="formula_0">e t (i, j) = MLP φ s t (i, j) − φ s t−1 (i, j); φ s t (i, j) ∈ R |A| .<label>(2)</label></formula><p>We then compute an attention mask α t ∈ R H×W corresponding to frame t, which indicates the controllable parts of the observation s t . Such attention masks are computed via a separate MLP from the features of each region (i, j), and then converted into a probability distribution using softmax or sparsemax operators <ref type="bibr" target="#b22">(Martins &amp; Astudillo, 2016)</ref>:</p><formula xml:id="formula_1">α t = sparsemax( α t ) where α t (i, j) = MLP φ s t (i, j) ,<label>(3)</label></formula><p>so that i,j α t (i, j) = 1. The sparsemax operator is similar to softmax but yields a sparse attention, leading to more stable performance. Finally, the logits e t (i, j) from all regions are linearly combined using the attention probabilities α t :</p><formula xml:id="formula_2">p( a t−1 | s t−1 , s t ) = softmax i,j α t (i, j) · e t (i, j) ∈ R |A| .<label>(4)</label></formula><p>Training. The model can be optimized with the standard cross-entropy loss L action (a * t−1 , a t−1 ) with respect to the ground-truth action a * t−1 ∈ A that the agent actually has taken. Based on this formulation, the attention probability α t (i, j) should be high only on regions (i, j) that are predictive of the agent's actions. Our formulation enables learning to localize controllable entities in a selfsupervised way without any additional supervisory signal, unlike some prior work (e.g., <ref type="bibr" target="#b6">(Bellemare et al., 2012)</ref>) that adopts simulators to collect extra supervisory labels.</p><p>Optimizing the parameters of ADM on on-policy data is challenging for several reasons. First, the ground-truth action may be unpredictable for given pairs of frames, leading to noisy labels. For example, actions taken in uncontrollable situations do not have any effect (e.g., when the agent is in the middle of jumping in MONTEZUMA'S REVENGE). Second, since we train the ADM online along with the policy, the training examples are not independently and identically distributed, and the data distribution can shift dramatically over time. Third, the action distribution from the agent's policy can run into a low entropy 2 , being biased towards certain actions. These issues may prevent the ADM from generalization to novel observations, which hurts exploration. Generally, we prefer models that quickly adapt to the policy and learn to localize the controllable regions in a robust manner.</p><p>To mitigate the aforementioned issues, we adopt a few additional objective functions. We encourage the attention distribution to attain a high entropy by including an attention entropy regularization loss, i.e., L ent = −H(α t ). This term penalizes over-confident attention masks, making the attention closer to uniform whenever action prediction is not possible. We also train the logits corresponding to each grid cell independently using a separate cross-entropy loss: p( a i,j t−1 | e t (i, j)) = softmax(e t (i, j)). These additional cross-entropy losses, denoted L i,j cell , allow the model to learn from unseen observations even when attention fails to perform well at first. The entire training objective becomes:</p><formula xml:id="formula_3">L ADM = L action + i,j L i,j cell + λ ent L ent (5)</formula><p>where λ ent is a mixing hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COUNT-BASED EXPLORATION WITH CONTINGENT REGIONS</head><p>One natural way to take advantage of discovered contingent regions for exploration is count-based exploration. The ADM can be used to localize the controllable entity (e.g., the agent's avatar) from an observation s t experienced by the agent. In 2D environments, a natural discretization (x, y) = argmax (j,i) α t (i, j) provides a good approximation of the agent's location within the current observation 3 . This provides a key piece of information about the current state of the agent.</p><p>Inspired by previous work <ref type="bibr" target="#b8">(Bellemare et al., 2016;</ref><ref type="bibr" target="#b41">Tang et al., 2017)</ref>, we add an exploration bonus of r + to the environment reward, where r + (s) = 1/ #(ψ(s)) and #(ψ(s)) denotes the visitation count of the (discrete) mapped state ψ(s), which consists of the contingent region (x, y). We want to find a policy π that maximizes the expected discounted sum of environment rewards r ext plus count-based exploration rewards r</p><formula xml:id="formula_4">+ , denoted R = E π t γ t (β 1 r ext (s t , a t ) + β 2 r + (s t ))</formula><p>, where β 1 , β 2 ≥ 0 are hyperparameters that balance the weight of environment reward and exploration bonus. For every state s t encountered at time step t, we increase the counter value #(ψ(s t )) by 1 during training. The full procedure is summarized in Algorithm 1 in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In the experiments below we investigate the following key questions:</p><p>• Does the contingency awareness in terms of self-localization provide a useful state abstraction for exploration? • How well can the self-supervised model discover the ground-truth abstract states?</p><p>• How well does the proposed exploration strategy perform against other exploration methods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTS WITH A2C</head><p>We evaluate the proposed exploration strategy on several difficult exploration Atari 2600 games from the Arcade Learning Environment (ALE) <ref type="bibr" target="#b7">(Bellemare et al., 2013)</ref>. We focus on 8 Atari games including FREEWAY, FROSTBITE, HERO, PRIVATEEYE, MONTEZUMA'S REVENGE, QBERT, SEAQUEST, and VENTURE. In these games, an agent without an effective exploration strategy can often converge to a suboptimal policy. For example, as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, the Advantage Actor-Critic (A2C) baseline <ref type="bibr" target="#b26">(Mnih et al., 2016)</ref> achieves a reward close to 0 on MONTEZUMA'S REVENGE, VENTURE, FREEWAY, FROSTBITE, and PRIVATEEYE, even after 100M steps of training. By contrast, our proposed technique, which augments A2C with count-based exploration with the location information learned by the attentive dynamics model, denoted A2C+CoEX (CoEX stands for "Contingency-aware Exploration"), significantly outperforms the A2C baseline on six out of the 8 games.</p><p>We compare our proposed A2C+CoEX technique against the following baselines: 4    <ref type="bibr" target="#b30">(Ostrovski et al., 2017)</ref>, and Curiosity-Driven <ref type="bibr" target="#b10">(Burda et al., 2018)</ref>. The numbers for DDQN+ were taken from <ref type="bibr" target="#b41">(Tang et al., 2017)</ref> or were read from a plot.</p><p>• A2C: an implementation adopted from OpenAI baselines  using the default hyperparameters, which serves as the building block of our more complicated baselines. • A2C+Pixel-SimHash: Following <ref type="bibr" target="#b41">(Tang et al., 2017)</ref>, we map 52×52 gray-scale observations to 128-bit binary codes using random projection followed by quantization <ref type="bibr" target="#b12">(Charikar, 2002)</ref>. Then, we add a count-based exploration bonus based on quantized observations.</p><p>As a control experiment, we evaluate A2C+CoEX+RAM * , our contingency-aware exploration method together with the ground-truth location information obtained from game's RAM. It is roughly an upper-bound of the performance of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMPLEMENTATION DETAILS</head><p>For the A2C <ref type="bibr" target="#b26">(Mnih et al., 2016)</ref> algorithm, we use 16 parallel actors to collect the agent's experience, with 5-step rollout, which yields a minibatch of size 80 for on-policy transitions. We use the last 4 observation frames stacked as input, each of which is resized to 84 × 84 and converted to grayscale as in <ref type="bibr" target="#b25">(Mnih et al., 2015;</ref>. We set the end of an episode to when the game ends, rather than when the agent loses a life. Each episode is initialized with a random number of no-ops <ref type="bibr" target="#b25">(Mnih et al., 2015)</ref>. More implementation details can be found in Appendix A and B.</p><p>For the ADM, we take observation frames of size 160×160 as input (resized from the raw observation of size 210 × 160). <ref type="bibr">5</ref> We employ a 4-layer convolutional neural network that produces a feature map φ(s t ) with a spatial grid size of H × W = 9 × 9. As a result, the prediction of location coordinates lies in the 9 × 9 grid.</p><p>In some environments, the contingent regions within the visual observation alone are not sufficient to determine the exact location of the agent within the game; for example, the coordinate cannot solely distinguish between different rooms in HERO, MONTEZUMA'S REVENGE, and PRIVATEEYE, etc. Therefore, we introduce a discrete context representation c ∈ Z that summarizes the high-level visual context in which the agent currently lies. We use a simple clustering method similar to <ref type="bibr" target="#b19">(Kulis &amp; Jordan, 2012)</ref>, which we refer to as observation embedding clustering that clusters the random projection vectors of the input frames as in <ref type="bibr" target="#b41">(Tang et al., 2017)</ref>, so that different contexts are assigned to different clusters. We further explain this heuristic approach more in detail in Appendix D.</p><p>In sparse-reward problems, the act of collecting a reward is rare but frequently instrumental for the future states of the environment. The cumulative reward R t = t−1 t =0 r ext (s t , a t ) from the beginning of the episode up to the current step t, can provide a useful high-level behavioral context because collecting rewards can trigger significant changes to the agent's state and as a result the optimal behavior can change as well. In this sense, the agent should revisit the previously visited location for exploration when the context changes. For example, in MONTEZUMA'S REVENGE, if the agent is in the first room and the cumulative reward is 0, we know the agent has not picked up the key and the optimal policy is to reach the key. However, if the cumulative reward in the first room is 100, it means the agent has picked up the key and the next optimal goal is to open a door and move on to the next room. Therefore, we could include the cumulative reward as a part of state abstraction for exploration, which leads to empirically better performance.</p><p>To sum up, for the purpose of count-based exploration, we utilize the location (x, y) of the controllable entity (i.e., the agent) in the current observation discovered by ADM (Section 3.1), a context representation c ∈ Z that denotes the high level visual context, and a cumulative environment reward R ∈ Z that represents the exploration behavioral state. In such setting, we may denote ψ(s) = (x, y, c, R). <ref type="figure" target="#fig_1">Figure 2</ref> shows the learning curves of the proposed methods on 8 Atari games. The performance of our method A2C+CoEX and A2C+CoEX+RAM as well as the baselines A2C and A2C+Pixel-SimHash are summarized in <ref type="table" target="#tab_2">Table 1</ref>. In order to find a balance between the environment reward and the exploration bonus reward, we perform a hyper-parameter search for the proper weight of the environment reward β 1 and the exploration reward β 2 for A2C+CoEX+RAM, as well as for A2C+CoEX. The hyper-parameters for the two ended up being the same, which is consistent with our results. For fair comparison, we also search for the proper weight of environment reward for A2C baseline. The best hyper-parameters for each game are shown in <ref type="table" target="#tab_7">Table 5</ref> in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PERFORMANCE OF COUNT-BASED EXPLORATION</head><p>Compared to the vanilla A2C, the proposed exploration strategy improves the score on all the hard-exploration games. As shown in <ref type="table" target="#tab_2">Table 1</ref>, provided the representation (x, y, c, R) is perfect, A2C+CoEX+RAM achieves a significant improvement over A2C by encouraging the agent to visit novel locations, and could nearly solve these hard exploration games as training goes on. Furthermore, A2C+CoEX using representations learned with our proposed attentive dynamics model and observation embedding clustering also outperforms the A2C baseline. Especially on FREEWAY, FROSTBITE, HERO, MONTEZUMA'S REVENGE, QBERT and SEAQUEST, the performance is comparable with A2C+CoEX+RAM, demonstrating the usefulness of the contigency-awareness information discovered by ADM.</p><p>Comparison to other count-based exploration methods. <ref type="table" target="#tab_3">Table 2</ref> compares the proposed method with previous state-of-the-art results, where our proposed method outperforms the other methods on 5 out of 8 games. DQN-PixelCNN is the strongest alternative achieving a state-of-the-art performance on some of the most difficult sparse-reward games. We argue that using Q-learning as the base learner with DQN-PixelCNN makes the direct comparison with A2C+CoEX not completely adequate. Note that the closest alternative count-based exploration method to A2C+CoEX would be A3C+ (Bellemare </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS OF ATTENTIVE DYNAMICS MODEL</head><p>We also analyze the performance of the ADM that learns the controllable dynamics of the environment. As a performance metric, we report the average distance between the ground-truth agent location (x * , y * ) and the predicted location (x, y) within the 9 × 9 grid: (x, y) − (x * , y * ) 2 . The ground-truth location of the agent is extracted from RAM 6 , then rescaled so that the observation image frame fits into the 9 × 9 grid. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results on 4 Atari games (MONTEZUMA'S REVENGE, SEAQUEST, HERO, and VENTURE). The ADM is able to quickly capture the location of the agent without any supervision of localization, despite the agent constantly visiting new places. Typically the predicted location is on average 1 or 2 grid cells away from the ground-truth location. Whenever a novel scene is encountered (e.g., the second room in MONTEZUMA'S REVENGE at around 10M steps), the average distance temporarily increases but quickly drops again as the model learns the new room. We provide videos of the agents playing and localization information as the supplementary material. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ANALYSIS OF OBSERVATION EMBEDDING CLUSTERING</head><p>To make the agent aware of a change in high-level visual context (i.e., rooms in Atari games) in some games such as MONTEZUMA'S REVENGE, VENTURE, HERO, and PRIVATEEYE, we obtain a representation of the high-level context and use it for exploration. The high-level visual contexts are different from each other (different layouts, objects, colors, etc.), so the embedding generated by a random projection is quite distinguishable and the clustering is accurate and robust.</p><p>For evaluation, given an observation in Atari games, we compare the discrete representation (i.e., which cluster it is assigned to) based on the embedding from random projection to the ground-truth room number extracted from RAM. The Adjusted Rand Index (ARI) <ref type="bibr" target="#b34">(Rand, 1971)</ref> measures the similarity between these two data clusterings. The ARI may only yield a value between 0 and 1, and is exactly 1 when the clusterings are identical.</p><p>The curves of the Adjusted Rand Index are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. For MONTEZUMA'S REVENGE and VENTURE, the discrete representation as room number is roughly as good as the ground-truth. For HERO and PRIVATEEYE, since there are many rooms quite similar to one another, it is more challenging to accurately cluster the embeddings. The samples shown in <ref type="figure" target="#fig_5">Figure 7</ref> in Appendix D show reasonable performances of the clustering method on all these games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ADDITIONAL EXPERIMENTS WITH PPO</head><p>We also evaluate the proposed exploration algorithm on MONTEZUMA'S REVENGE using the sticky actions environment setup <ref type="bibr" target="#b20">(Machado et al., 2017)</ref> identical to the setup found in <ref type="bibr" target="#b11">(Burda et al., 2019)</ref>. In the sticky action setup, the agent randomly repeats the previous action with probability of 0.25, preventing the algorithm from simply memorizing the correct sequence of actions and relying on determinism. The agent is trained with Proximal Policy Optimization (PPO)  in conjunction with the proposed exploration method using 128 parallel actors to collect the experience. We used reward normalization and advantage normalization as in <ref type="bibr" target="#b10">(Burda et al., 2018)</ref>.   The method, denoted PPO+CoEX, achieves the score of 11,618 at 500M environment steps (2 billion frames) on MONTEZUMA'S REVENGE, when averaged over 3 runs. The learning curve is illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. Since the vanilla PPO baseline achieves a score near 0 (our runs) or 1,797 <ref type="bibr" target="#b11">(Burda et al., 2019)</ref>, this result is not solely due to the benefits of PPO. There is another approach "Exploration by Random Network Distillation" <ref type="bibr" target="#b11">(Burda et al., 2019)</ref> concurrent with our work which achieves similar performance by following a slightly different philosophy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">DISCUSSIONS AND FUTURE WORK</head><p>This paper investigates whether discovering controllable dynamics via an attentive dynamics model (ADM) can help exploration in challenging sparse-reward environments. We demonstrate the effectiveness of this approach by achieving significant improvements on notoriously difficult video games. That being said, we acknowledge that our approach has certain limitations. Our currently presented instance of state abstraction method mainly focuses on controllable dynamics and employs a simple clustering scheme to abstract away uncontrollable elements of the scene. In more general setting, one can imagine using attentive (forward or inverse) dynamics models to learn an effective and compact abstraction of the controllable and uncontrollable dynamics as well, but we leave this to future work.</p><p>Key elements of the current ADM method include the use of spatial attention and modelling of the dynamics. These ideas can be generalized by a set of attention-based dynamics models (ADM) operating in forward, inverse, or combined mode. Such models could use attention over a lowerdimensional embedding that corresponds to an intrinsic manifold structure from the environment (i.e., intuitively speaking, this also corresponds to being self-aware of (e.g., locating) where the agent is in the abstract state space). Our experiments with the inverse dynamics model suggest that the mechanism does not have to be perfectly precise, allowing for some error in practice. We speculate that mapping to such subspace could be obtained by techniques of embedding learning.</p><p>We note that RL environments with different visual characteristics may require different forms of attention-based techniques and properties of the model (e.g., partial observability). Even though this paper focuses on 2D video games, we believe that the presented high-level ideas of learning contingency-awareness (with attention and dynamics models) are more general and could be applicable to more complex 3D environments with some extension. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a method of providing contingency-awareness through an attentive dynamics model (ADM). It enables approximate self-localization for an RL agent in 2D environments (as a specific perspective). The agent is able to estimate its position in the space and therefore benefits from a compact and informative representation of the world. This idea combined with a variant of countbased exploration achieves strong results in various sparse-reward Atari games. Furthermore, we report state-of-the-art results of &gt;11,000 points on the infamously challenging MONTEZUMA'S REVENGE without using expert demonstrations or supervision. Though in this work we focus mostly on 2D environments in the form of sparse-reward Atari games, we view our presented high-level concept and approach as a stepping stone towards more universal algorithms capable of similar abilities in various RL environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SUMMARY OF TRAINING ALGORITHM</head><p>Algorithm 1 A2C+CoEX Initialize parameter θ ADM for attentive dynamics model f ADM Initialize parameter θ A2C for actor-critic network Initialize parameter θ c for context embedding projector if applicable (which is not trainable) Initialize transition buffer E ← ∅ for each iteration do Collect on-policy transition samples, distributed over K parallel actors for each step t do s t ← Observe state a t ∼ π θ (a t |s t ) s t+1 , r ext t ← Perform action a t in the environment Compute the contingent region information α t+1 ← Compute the attention map of s t+1 using f ADM c(s t+1 ) ← Compute the observation embedding cluster of s t+1 (Algorithm 2)</p><p>Increment state visitation counter based on the representation ψ(s t+1 ) ← (argmax <ref type="bibr">(i,j)</ref> </p><formula xml:id="formula_5">α t+1 (i, j), c(s t+1 ), t k=0 r ext k ) #(ψ(s t+1 )) ← #(ψ(s t+1 )) + 1 r + t ← 1 √ #(ψ(st+1)) Store transition E ← E ∪ (s t , a t , s t+1 , β 1 clip(r ext t , −1, 1) + β 2 r + t ) end for</formula><p>Perform actor-critic using on-policy samples in E θ A2C ← θ A2C − η∇ θA2C L A2C</p><p>Train the attentive dynamics model using on-policy samples in E θ ADM ← θ ADM − η∇ θADM L ADM Clear transition buffer E ← ∅ end for</p><p>The learning objective L ADM is from Equation <ref type="formula">(5)</ref>. The objective L A2C of Advantage Actor-Critic (A2C) is as in <ref type="bibr" target="#b26">(Mnih et al., 2016;</ref>:</p><formula xml:id="formula_6">L A2C = E (s,a,r)∼E L A2C policy + 1 2 L A2C value (6) L A2C policy = − log π θ (a t |s t )(R n t − V θ (s t )) − αH t (π θ )<label>(7)</label></formula><formula xml:id="formula_7">L A2C value = 1 2 V θ (s t ) − R n t 2 (8) H t (π θ ) = − a π θ (a|s t ) log π θ (a|s t )<label>(9)</label></formula><p>where R n t = n−1 i=0 γ i r t+i + γ n V θ (s t+n ) is the n-step bootstrapped return and α is a weight for the standard entropy regularization loss term H t (π θ ). We omit the subscript as θ = θ A2C when it is clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ARCHITECTURE AND HYPERPARAMETER DETAILS</head><p>The architecture details of the attentive dynamics model (ADM), the policy network, and hyperparameters are as follows.   <ref type="figure">Figure 6</ref>: Learning curves on several Atari games: A2C, A2C+CoEX, and A2C+CoEX+RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENT WITH RAM INFORMATION</head><p>In order to understand the performance of exploration with perfect representation, we extract the ground-truth location of the agent and the room number from RAM, and then run count-based exploration with the perfect (x, y, c, R). <ref type="figure">Figure 6</ref> shows the learning curves of the experiments; we could see A2C+CoEX+RAM acts as an upper bound performance of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D OBSERVATION EMBEDDING CLUSTERING</head><p>We describe the detail of a method to obtain the observation embedding. Given an observation of shape (84, 84, 3), we flatten the observation and project it to an embedding of dimension 64. We randomly initialize the parameter of the fully-connected layer for projection, and keep the values unchanged during the training to make the embedding stationary.</p><p>For the embedding of these observations, we cluster them based on a threshold value τ . The value of τ for each game with change of rooms is listed in <ref type="table" target="#tab_7">Table 5</ref>. If the distance between the current embedding and the center mean(c) of a cluster c is less than the threshold, we assign this embedding to the cluster with the smallest distance and update its center with the mean value of all embeddings belonging to this cluster. If the distance between the current embedding and the center of any cluster is larger than the threshold, we create a new cluster and this embedding is assigned to this new cluster. In <ref type="figure" target="#fig_5">Figure 7</ref>, we also show the samples of observation in each cluster. We could see observations from the same room are assigned to the same cluster and different clusters correspond to different rooms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ABLATION STUDY ON ATTENTIVE DYNAMICS MODEL</head><p>We conduct a simple ablation study on the learning objectives of ADM, described in Equation <ref type="formula">(5)</ref>. We evaluate the performance of ADM when trained on the same trajectory data under different combinations of loss terms, simulating batches of on-policy transition data to be replayed. The sample trajectory was obtained from an instance of A2C+CoEX+RAM and kept same across all the runs, which allows a fair comparison between different variants. We compare the following four methods:</p><p>• ADM (action) : train ADM using L action only • ADM (action, cell) : train ADM using L action and L cell • ADM (action, ent) : train ADM using L action and L ent • ADM (action, cell, ent) : train ADM using all losses (L action , L cell , L ent ) <ref type="figure">Figure 8</ref> shows the average distance between the ground-truth location of the agent and the predicted one by ADM during the early stages of training. On MONTEZUMA'S REVENGE, there is only little difference between the variants although the full model worked slightly better on average. On FREEWAY, the effect of loss terms is more clear; in the beginning the agent tends to behave suboptimally by taking mostly single actions only (UP out of three action choices -UP, DOWN, and NO-OP), hence very low entropy H(π(·|s)), which can confuse the ADM of telling which part is actually controllable as the action classifier would give correct answer regardless of attention. We can observe additional loss terms help the model quickly correct the attention to localize the controllable object among the uncontrollable clutters with better stability. <ref type="figure">Figure 8</ref>: Performance of ADM in terms of mean distance under different loss combinations in early stages, trained using the same online trajectory data. Plots were obtained by averaging runs over 5 random seeds.</p><p>In another ablation study, we compare the end performance of the A2C+CoEX agent with the ADM jointly trained under different loss objectives on these three games (MONTEZUMA'S REVENGE, FREEWAY and SEAQUEST). In our experiments, the variant with full ADM worked best on MON-TEZUMA'S REVENGE and FREEWAY. The minimal training objective of ADM (i.e., L action ) also solely works reasonably well, but with the combination of other loss terms we can attain a more stable performance.    <ref type="table" target="#tab_9">Table 6</ref>: Summary of the results of the ablation study of the state representation. We report the maximum mean score (averaged over 40 recent episodes) achieved over 100M environment steps, averaged over 3 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ABLATION STUDY ON THE STATE REPRESENTATION</head><p>We present a result of additional ablation study on the state representation ψ(s) used in count-based exploration. The following baselines are considered:</p><p>• A2C+CoEX(c): Uses only the context embedding for exploration, i.e., ψ(s) = (c).</p><p>• A2C+CoEX(c, R): Uses only the context embedding and the cumulative reward for exploration without contingent region information, i.e., ψ(s) = (c, R). <ref type="figure" target="#fig_1">• A2C+CoEX(x, y, c)</ref>: Uses the contingent region information (x, y) as well as the context embedding c, however without the cumulative reward component, i.e., ψ(s) = (x, y, c).</p><p>One can also consider another baseline similar to A2C+CoEX(c, R) with ψ(s) = (x, y, c, R), where the location information (x, y) is replaced with random coordinates uniformly sampled from the grid. It ablates the learned contingent regions. However, we found that it performs similarly to the presented A2C+CoEX(c, R) baseline.</p><p>The experimental results are summarized in <ref type="table" target="#tab_9">Table 6</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>. The variants without contingent regions (i.e., A2C+CoEX(c) and A2C+CoEX(c, R) performed significantly worse in most of the games than A2C+CoEX(x, y, c) and A2C+CoEX(x, y, c, R) giving little improvement over the A2C baseline. Most notably, in the games with the hardest exploration such as MONTEZUMA'S REVENGE and VENTURE, the performance is hardly better than the vanilla A2C or a random policy, achieving a score as low as zero. The variants with contingent region information worked best and comparable to each other. We observe that using the cumulative reward (total score) for exploration gives a slight improvement on some environments. These results support the effectiveness of the learned contingency-awareness information in count-based exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Contingent region in FREEWAY; an object in a red box denotes what is under the agent's control, whereas the rest is not. Right: A diagram for the proposed ADM architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Learning curves on several Atari games: A2C+CoEX and A2C. The x-axis represents total environment steps and the y-axis the mean episode reward averaged over 40 recent episodes. The mean curve is obtained by averaging over 3 random seeds, each shown in a light color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance plot of ADM trained using on-policy samples from the A2C+CoEX agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Curves of ARI score during training of A2C+CoEX, averaged over 100 recent observations. et al., 2016), which augments A3C<ref type="bibr" target="#b26">(Mnih et al., 2016)</ref> with exploration bonus derived from pseudocount, because A2C and A3C share a similar policy learning method. With that in mind, one can observe a clear improvement of A2C+CoEX over A3C+ on all of the 8 Atari games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The learning curve of PPO+CoEX on several Atari games with sticky actions setup. The x-axis represents the total number of environment steps and the y-axis the mean episode reward averaged over 40 recent episodes. The mean curve is obtained by averaging over 3 random seeds, each shown in a light color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Sample of clustering results for VENTURE, HERO, PRIVATEEYE, and MONTEZUMA'S REVENGE. Each column is one cluster, and we show 3 random samples assigned into this cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Learning curves of A2C+CoEX with ADM trained under different training objectives. The curve in solid line shows the mean episode over 40 recent episodes, averaged over 3 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Learning curves for the ablation study of state representation. The exploration algorithm without the contingent region information (purple) performs significantly worse, yielding almost no improvement on hard-exploration games such as MONTEZUMA'S REVENGE, VENTURE, and FROSTBITE. The mean curve is obtained by averaging over 3 random seeds. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of our method and its baselines on Atari games: maximum mean scores (averaged over 40 recent episodes) achieved over total 100M environment timesteps (400M frames) of training, averaged over 3 seeds. The best entry in the group of experiments without supervision is shown in bold. * denotes that A2C+CoEX+RAM acts as a control experiment, which includes some supervision. More experimental results on A2C+CoEX+RAM are shown in Appendix C.</figDesc><table><row><cell>Method</cell><cell cols="9">#Steps Freeway Frostbite Hero Montezuma PrivateEye Qbert Seaquest Venture</cell></row><row><cell cols="2">A2C+CoEX (Ours) 50M</cell><cell>33.9</cell><cell cols="2">3900 31367</cell><cell>4100</cell><cell cols="2">5316 17724</cell><cell>2620</cell><cell>128</cell></row><row><cell cols="2">A2C+CoEX (Ours) 100M</cell><cell>34.0</cell><cell cols="2">4260 36827</cell><cell>6635</cell><cell cols="2">5316 23962</cell><cell>5169</cell><cell>204</cell></row><row><cell>DDQN+</cell><cell>25M</cell><cell>29.2</cell><cell cols="2">-20300</cell><cell>3439</cell><cell>1880</cell><cell>-</cell><cell>-</cell><cell>369</cell></row><row><cell>A3C+</cell><cell>50M</cell><cell>27.3</cell><cell cols="2">507 15210</cell><cell>142</cell><cell cols="2">100 15805</cell><cell>2274</cell><cell>0</cell></row><row><cell cols="2">TRPO-AE-SimHash 50M</cell><cell>33.5</cell><cell>5214</cell><cell>-</cell><cell>75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>445</cell></row><row><cell>Sarsa-φ-EB</cell><cell>25M</cell><cell>0.0</cell><cell>2770</cell><cell>-</cell><cell>2745</cell><cell cols="2">-4112</cell><cell>-</cell><cell>1169</cell></row><row><cell>DQN-PixelCNN</cell><cell>37.5M</cell><cell>31.7</cell><cell>-</cell><cell>-</cell><cell>2514</cell><cell cols="2">15806 5501</cell><cell>-</cell><cell>1356</cell></row><row><cell>Curiosity-Driven</cell><cell>25M</cell><cell>32.8</cell><cell>-</cell><cell>-</cell><cell>2505</cell><cell>3037</cell><cell>-</cell><cell>-</cell><cell>416</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance of our method and state-of-the-art exploration methods on Atari games. For fair comparison, we report the maximum mean score achieved over the specific number of timesteps during training, averaged over 3 seeds. The best entry is shown in bold. Baselines (for reference) are: DDQN+ and A3C+ (Bellemare et al., 2016), TRPO-AE-SimHash (Tang et al., 2017), Sarsa-φ-EB (Martin et al., 2017), DQN-PixelCNN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance of PPO and PPO+CoEX: maximum mean scores (average over 40 recent episodes) achieved over total 500M environment steps (2B frames) of training, averaged over 3 seeds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Network architecture and hyperparameters</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell><cell></cell></row><row><cell>Policy and Value Network Architecture</cell><cell>Input: 84x84x1</cell><cell></cell></row><row><cell></cell><cell>-Conv(32-8x8-4)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-Conv(64-4x4-2)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-Conv(64-3x3-1)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(512)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(|A|), FC(1)</cell><cell></cell></row><row><cell>ADM Encoder Architecture</cell><cell>Input: 160x160x3</cell><cell></cell></row><row><cell></cell><cell>-Conv(8-4x4-2)</cell><cell>/LeakyReLU</cell></row><row><cell></cell><cell>-Conv(8-3x3-2)</cell><cell>/LeakyReLU</cell></row><row><cell></cell><cell>-Conv(16-3x3-2)</cell><cell>/LeakyReLU</cell></row><row><cell></cell><cell>-Conv(16-3x3-2)</cell><cell>/LeakyReLU</cell></row><row><cell>MLP Architecture for et(i, j)</cell><cell>FC(1296,256)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(256,128)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(128,|A|)</cell><cell></cell></row><row><cell>MLP Architecture for αt(i, j)</cell><cell>FC(1296,64)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(64,64)</cell><cell>/ReLU</cell></row><row><cell></cell><cell>-FC(64,1)</cell><cell></cell></row><row><cell>λent for Loss</cell><cell>0.001</cell><cell></cell></row><row><cell>A2C Discount Factor γ</cell><cell>0.99</cell><cell></cell></row><row><cell>Learning Rate (RMSProp)</cell><cell>0.0007</cell><cell></cell></row><row><cell>Number of Parallel Environments</cell><cell>16</cell><cell></cell></row><row><cell>Number of Roll-out Steps per Iteration</cell><cell>5</cell><cell></cell></row><row><cell>Entropy Regularization of Policy (α)</cell><cell>0.01</cell><cell></cell></row><row><cell>PPO Discount Factor γ</cell><cell>0.99</cell><cell></cell></row><row><cell>λ for GAE</cell><cell>0.95</cell><cell></cell></row><row><cell>Learning rate (Adam)</cell><cell>0.00001</cell><cell></cell></row><row><cell>Number of Parallel Environments</cell><cell>128</cell><cell></cell></row><row><cell>Rollout Length</cell><cell>128</cell><cell></cell></row><row><cell>Number of Minibatches</cell><cell>4</cell><cell></cell></row><row><cell>Number of Optimization Epochs</cell><cell>4</cell><cell></cell></row><row><cell cols="2">Coefficient of Extrinsic and Intrinsic reward β1 = 2, β2 = 1</cell><cell></cell></row><row><cell>Entropy Regularization of Policy (α)</cell><cell>0.01</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The list of hyperparameters used for A2C+CoEX in each game. For the four games where there is no change of high-level visual context (FREEWAY, FROSTBITE, QBERT and SEAQUEST), we do not include c in the state representation ψ(s), hence there is no τ . The same values of τ are used in PPO+CoEX.</figDesc><table><row><cell>Games</cell><cell cols="4">β1 in A2C+CoEX β2 in A2C+CoEX β1 in A2C τ for clustering</cell></row><row><cell>FREEWAY</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>-</cell></row><row><cell>FROSTBITE</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>-</cell></row><row><cell>HERO</cell><cell>1</cell><cell>0.1</cell><cell>1</cell><cell>0.7</cell></row><row><cell cols="2">MONTEZUMA'S REVENGE 10</cell><cell>10</cell><cell>10</cell><cell>0.7</cell></row><row><cell>PRIVATEEYE</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>0.55</cell></row><row><cell>QBERT</cell><cell>1</cell><cell>0.5</cell><cell>1</cell><cell>-</cell></row><row><cell>SEAQUEST</cell><cell>1</cell><cell>0.5</cell><cell>10</cell><cell>-</cell></row><row><cell>VENTURE</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Algorithm 2 Observation Embedding Clustering Initialize parameter θ c for context embedding projector if applicable (which is not trainable) Initialize threshold τ for clustering Initialize clusters set C ← ∅ for each observation s do Get embedding of the observation from the random projection v ← f θc (s) Find a cluster to which the current embedding fits, if any Find a cluster c ∈ C with smallest mean(c) − v ≤ τ , or NIL if there is no such if c = NIL then c ← c ∪ v else if there's no existing cluster that v should be assigned to, create a new one</figDesc><table><row><cell>C ← C ∪ {v}</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>for numbers.</figDesc><table><row><cell>Method</cell><cell cols="8">Freeway Frostbite Hero Montezuma PrivateEye Qbert Seaquest Venture</cell></row><row><cell>A2C</cell><cell>7.2</cell><cell>1099</cell><cell>34352</cell><cell>12.5</cell><cell>574</cell><cell>19620</cell><cell>2401</cell><cell>0</cell></row><row><cell>A2C+CoEX (c)</cell><cell>10.7</cell><cell>1313</cell><cell>34269</cell><cell>14.7</cell><cell>2692</cell><cell>20942</cell><cell>1810</cell><cell>94</cell></row><row><cell>A2C+CoEX (c, R)</cell><cell>34.0</cell><cell>941</cell><cell>34046</cell><cell>9.2</cell><cell>5458</cell><cell>21587</cell><cell>2056</cell><cell>77</cell></row><row><cell>A2C+CoEX (x, y, c)</cell><cell>33.7</cell><cell>5066</cell><cell>36934</cell><cell>6558</cell><cell>5377</cell><cell>21130</cell><cell>1978</cell><cell>1374</cell></row><row><cell>A2C+CoEX (x, y, c, R)</cell><cell>34.0</cell><cell>4260</cell><cell>36827</cell><cell>6635</cell><cell>5316</cell><cell>23962</cell><cell>5169</cell><cell>204</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that an entropy regularization term (e.g., Eq.(9)) is used when learning the policy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To obtain more accurate localization by taking temporal correlation into account, we can use exponential smoothing as αt(i, j) = (1 − ωt)αt−1(i, j) + ωtαt(i, j), where ωt = max (i,j) {αt(i, j)}.4 In Section 4.6, we also report experiments using Proximal Policy Optimization (PPO)) as a baseline, where our PPO+CoEX achieves the average score of &gt;11,000 on MONTEZUMA'S REVENGE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In some games such as Venture, the agent is depicted in very small pixels, which might be hardly recognizable in rescaled 84 × 84 images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Please note that the location from RAM is used only for analysis and evaluation purposes. 7 A demo video of the learnt policy and localization is available at https://coex-rl.github.io/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Poke by Poking: Experiential Learning of Intuitive Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contingency awareness in evaluative conditioning: A case for unaware affective-evaluative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Baeyens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Eelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and emotion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like representations in artificial agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caswell</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">557</biblScope>
			<biblScope unit="issue">7705</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">In Intrinsically motivated learning in natural and artificial systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="17" to="47" />
		</imprint>
	</monogr>
	<note>Intrinsic motivation and reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating Contingency Awareness Using Atari 2600 Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unifying Count-Based Exploration and Intrinsic Motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07718</idno>
		<title level="m">Doina Precup, and Yoshua Bengio. Independently Controllable Features</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale study of curiosity-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04355</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploration by random network distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1lJJnR5Ym" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thiry-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the thiry-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><forename type="middle">Openai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baselines</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kaplanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06769</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous Localization and Mapping: Part I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics &amp; automation magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diversity is All You Need: Learning Skills without a Reward Function</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VIME: Variational Information Maximizing Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reinforcement Learning with Unsupervised Auxiliary Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting k-means: New Algorithms via Bayesian Nonparametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Count-Based Exploration in Feature Space for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarryd</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suraj Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sasikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Navigate in Complex Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Navigate in Cities Without a Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Koichi</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shane Legg, and Demis Hassabis</title>
		<meeting><address><addrLine>Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><forename type="middle">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asynchronous Methods for Deep Reinforcement Learning. In ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Place Cells, Grid Cells, and Memory. Cold Spring Harbor perspectives in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">May-Britt</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edvard I</forename><surname>Moser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action-Conditional Video Prediction using Deep Networks in Atari Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Exploration via Bootstrapped DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Aaron van den Oord, and Remi Munos. Count-Based Exploration with Neural Density Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? A typology of computational approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curiosity-driven Exploration by Self-supervised Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pieter Abbeel, and Marcin Andrychowicz. Parameter Space Noise for Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamim</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Disentangling Controllable and Uncontrollable Factors of Variation by Interacting with the World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihide</forename><surname>Sawada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06955</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adaptive confidence and adaptive curiosity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Loss is its own Reward: Self-Supervision for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Mahmoudieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07307</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Intrinsically Motivated Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuttapong</forename><surname>Chentanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An analysis of model-based interval estimation for markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The development and generalization of &quot;contingency awareness&quot; in early infancy: Some hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Merrill-Palmer Quarterly of Behavior and Development</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="135" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On Learning Intrinsic Rewards for Policy Gradient Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
