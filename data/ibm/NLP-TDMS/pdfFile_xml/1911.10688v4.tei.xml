<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETHINKING SOFTMAX WITH CROSS-ENTROPY: NEURAL NETWORK CLASSIFIER AS MUTUAL INFORMATION ESTIMATOR A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyue</forename><surname>Qin</surname></persName>
							<email>zhenyue.qin@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
							<email>dongwookim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RETHINKING SOFTMAX WITH CROSS-ENTROPY: NEURAL NETWORK CLASSIFIER AS MUTUAL INFORMATION ESTIMATOR A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal contribution and correspondence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-entropy loss with softmax output is a standard choice to train neural network classifiers. While it is reasonable to reduce the cross-entropy between outputs of a neural network and labels, the implication of cross-entropy with softmax on the relation between inputs and labels remains to be better explained. We show that training a neural network with cross-entropy maximises the mutual information between inputs and labels through a variational form of mutual information. Our result provides an alternative view: neural network classifiers are mutual information estimators. The new view leads us to develop an informative class activation map (infoCAM). Given a classification task, infoCAM can locate the most informative features of the input toward a label. When applied to an image classification task, infoCAM performs better than the traditional class activation map in the weakly supervised object localisation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network classifiers play an important role in contemporary machine learning and computer vision <ref type="bibr" target="#b15">[16]</ref>. Since the emergence of AlexNet <ref type="bibr" target="#b14">[15]</ref>, much research has been done to improve the performance of neural network classifiers. To overcome the vanishing gradient in deep networks, the residual connection and various activation functions have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. To improve generalisation, better regularisation techniques such as dropout have been developed <ref type="bibr" target="#b25">[26]</ref>. To reach better local minima, various optimisation techniques have been suggested <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. Although many architectural choices and optimisation methods have been explored, relatively fewer considerations have been shown on the final layer of the neural network classifier: the cross-entropy loss with the softmax output.</p><p>The combination of softmax with cross-entropy is a standard choice to train neural network classifiers. It measures the cross-entropy between the ground truth label y and the output of the neural networkŷ. The network's parameters are then adjusted to reduce the cross-entropy via back-propagation. While it seems sensible to reduce the cross-entropy between the labels and predicted probabilities, it still remains a question as to what relation the network aims to model between input x and label y via this loss function, i.e., , softmax with cross-entropy.</p><p>In this work, for neural network classifiers, we explorer the connection between cross-entropy with softmax and mutual information between inputs and labels. From a variational form of mutual information, we prove that optimising model parameters using the softmax with cross-entropy is equal to maximising the mutual information between input data and labels when the distribution over labels is uniform. This connection provides an alternative view on neural network classifiers: they are mutual information estimators. We further propose a probability-corrected version of softmax that relaxes the uniform distribution condition.</p><p>This new information-theoretic view of neural network classifiers being mutual information estimators allows us to directly access the most informative regions of input with respect to the labels, given classification tasks. The access A PREPRINT -SEPTEMBER 18, 2020    to the most informative regions for the labels leads us to develop infoCAM that can locate the most relevant regions for the labels within an image, given an object classification task. Compared to the traditional class activation map, infoCAM exhibits better performance in the weakly supervised object localisation task.</p><p>In summary, we outline our contributions as follows:</p><p>• The previous view on cross-entropy with softmax only reflects the relationship between the outputs and the labels. We show that with minor modifications to softmax, neural network classifiers then become mutual information estimators. As a result, these mutual information estimators exhibit the information-theoretic relationship between the inputs and the labels. • We empirically demonstrate that our mutual information estimators can accurately evaluate mutual information.</p><p>We also show mutual information estimators can perform classification more accurately than traditional neural network classifiers. When the dataset is imbalanced, the estimators outperform the state-of-the-art classifier for our example. • We propose the informative class activation map (infoCAM) which locates the most informative regions for the labels within an image via mutual information. For the weakly supervised object localisation task, we achieve a new state-of-the-art result on Tiny-ImageNet with infoCAM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we first define the notations used throughout this paper. We then introduce the definition of mutual information and variational forms of mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>We let training data consist of M classes and N labelled instances as {(x i , y i )} N i=1 , where y i ∈ Y = {1, ..., M } is a class label of input x i . We let n φ (x) : X → R M be a neural network parameterised by φ, where X is a space of input x. Without additional clarification, we assume X to be a compact subset of D-dimensional Euclidean space. We denote by P XY some joint distribution over X × Y, with (X, Y ) ∼ P XY being a pair of random variables. P X and P Y are the marginal distributions of X and Y , respectively. We remove a subscript from the distribution if it is clear from context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Bounds of Mutual Information</head><p>Mutual information evaluates the mutual dependence between two random variables. The mutual information between X and Y can be expressed as:</p><formula xml:id="formula_0">I(X, Y ) = x∈X y∈Y</formula><p>P (x, y) log P (x, y) P (x)P (y) dx.</p><p>Equivalently, following <ref type="bibr" target="#b24">[25]</ref>, we may express the definition of mutual information in Equation 1 as:</p><formula xml:id="formula_2">I(X, Y ) = E (X,Y ) log P (y|x) P (y) ,<label>(2)</label></formula><p>where E (X,Y ) is the abbreviations of E (X,Y )∼P XY . Computing mutual information directly from the definition is, in general, intractable due to integration.</p><p>Variational form: Barber and Agakov introduce a commonly used lower bound of mutual information via a variational distribution Q <ref type="bibr" target="#b2">[3]</ref>, derived as: ≥ E (X,Y ) log Q(x, y) P (x)P (y) .</p><formula xml:id="formula_3">I(X, Y ) = E (X,</formula><p>The inequality in <ref type="bibr">Equation 3</ref> holds since KL divergence maintains non-negativity. This lower bound is tight when variational distribution Q(x, y) converges to joint distribution P (x, y), i.e., Q(x, y) = P (x, y).</p><p>The form in <ref type="bibr">Equation 3</ref> is, however, still hard to compute since it is not easy to make a tractable and flexible variational distribution Q(x, y). Variational distribution Q(x, y) can be considered as a constrained function which has to satisfy the probability axioms. Especially, the constraint is challenging to model with a function estimator such as a neural network. To relax the function constraint, McAllester et al. <ref type="bibr" target="#b20">[21]</ref> further apply reparameterisation and define Q(x, y) in terms of an unconstrained function f φ parameterised by φ as:</p><formula xml:id="formula_5">Q(x, y) = P (x)P (y) E y ∼P Y [exp(f φ (x, y ))]</formula><p>exp(f φ (x, y)).</p><p>As a consequence, the variational lower bound of mutual information I(X, Y ) can be rewritten with function f φ as:</p><formula xml:id="formula_7">I(X, Y ) ≥ E (X,Y ) log exp(f φ (x, y)) E y [exp(f φ (x, y ))] .<label>(5)</label></formula><p>Thus, one can estimate mutual information without any constraint on f . Through the reparameterisation, the MI estimation can be recast as an optimisation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NN Classifiers as MI Estimators</head><p>In this section, we prove that a neural network classifier with cross entropy loss and softmax output estimates the mutual information between inputs and labels.</p><p>To view neural network classifiers as mutual information estimators, we need to discuss two separate cases related to the dataset: whether it is balanced or imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Softmax with Balanced Dataset</head><p>Softmax is widely used to map outputs of neural networks into a categorical probabilistic distribution for classification. Given neural network n(x) :</p><formula xml:id="formula_8">X → R M , softmax σ : R M → R M is defined as: σ(n(x)) y = exp(n(x) y ) M y =1 exp(n(x) y ) .<label>(6)</label></formula><p>Expected cross-entropy is often employed to train a neural network with softmax output. The expected cross-entropy loss is</p><formula xml:id="formula_9">L = −E (X,Y ) [n(x) y − log( M y =1 exp(n(x) y ))],<label>(7)</label></formula><p>where the expectation is taken over the joint distribution of X and Y . Given a training set, one can train the model with an empirical distribution of the joint distribution. We present an interesting connection between cross-entropy with softmax and mutual information in the following theorem. In a bid for conciseness, we only provide proof sketches for Theorem 1 and Theorem 2 here. Please refer to the appendix for rigorous proofs. Theorem 1. Let f φ (x, y) be n(x) y . Infimum of the expected cross-entropy loss with softmax outputs is equivalent to the mutual information between input and output variables up to constant log M under uniform label distribution.</p><p>Proof. Let f φ (x, y) = n(x) y , then the lower bound is</p><formula xml:id="formula_10">E (X,Y ) log exp(n(x) y ) E y [exp(n(x) y )] .<label>(8)</label></formula><p>If the distribution of the label is uniform then, it can be rewritten as</p><formula xml:id="formula_11">E (X,Y ) log exp(n(x) y ) 1/M M y =1 exp(n(x) y ) = E (X,Y ) log exp(n(x) y ) M y =1 exp(n(x) y ) + log M,<label>(9)</label></formula><p>which is equivalent to the negative expected cross-entropy loss <ref type="bibr" target="#b6">(7)</ref> up to constant log M . Hence, the infimum of the expected cross entropy is equal to the mutual information between input and output variables since the supremum of r.h.s in Equation 5 is the mutual information.  <ref type="table">Table 1</ref>: Synthetic dataset description. µ is a mean vector for each Gaussian distribution. # samples denotes the number (resp. prior distribution) of samples with the non-uniform prior assumption. For the test with the uniform prior assumption, we use 12,000 samples from each distribution.</p><p>Note that the constant does not change the gradient of the objective. Consequently, the solutions of both the mutual information maximisation and the softmax cross-entropy minimisation optimisation problems are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Softmax with Imbalanced Dataset</head><p>The uniform label distribution assumption in Theorem 1 is restrictive since we cannot access the true label distribution, often assumed to be non-uniform. To relax the restriction, we propose a probability-corrected softmax (PC-softmax):</p><formula xml:id="formula_12">σ p (n(x)) y = exp(n(x) y ) M y =1 P (y ) exp(n(x) y ) ,<label>(10)</label></formula><p>where P (y ) is a distribution over label y . In experiments, we optimise the revised softmax with empirical distribution on P (y ) estimated from the training set. We show the equivalence between optimising the classifier and maximising mutual information with the new softmax below. Theorem 2. The mutual information between two random variables X and Y can be obtained via the infimum of cross-entropy with PC-softmax in Equation 10, using a neural network. Such an evaluation is strongly consistent.</p><p>See the proofs in the appendix for the proof of Theorem 2.</p><p>Mutual information is often used in generative models to find the maximally informative representation of an observation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>, whereas its implication in classification has been unclear so far. The results of this section imply that the neural network classifier with softmax optimises its weights to maximise the mutual information between inputs and labels under the uniform label assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Impact of PC-softmax on Classification</head><p>In this section, we measure the empirical performance of PC-softmax as mutual information (MI) and the influence of PC-softmax on the classification task. Since it is impossible to obtain correct MI from real-world datasets, we first construct synthetic data with known properties to measure the MI estimation performance, and then we use two real-world datasets to measure the impact of PC-softmax on classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mutual information estimation task</head><p>To construct a synthetic dataset with a pair of continuous and discrete variables, we employ a Gaussian mixture model:</p><formula xml:id="formula_13">P (x) = M y=1 P (y)N (x|µ y , Σ y ) P (x|y) = N (x|µ y , Σ y ),</formula><p>where P (y) is a prior distribution over the labels. To form a classification task, we use x as an input variable, and y as a label.</p><p>For the experiments, we use five mixtures of isotropic Gaussian, each of which has a unit diagonal covariance matrix with different means. We set the parameters of the mixtures to make them overlap in significant proportions of their distributions.  We generate two sets of datasets: one with uniform prior and the other with non-uniform prior distribution over labels, p(y). For the uniform prior, we sample 12,000 data points from each Gaussian, and for the non-uniform prior, we sample unequal number of data points from each Gaussian. In addition, we vary the dimension of Gaussian distribution from 1 to 10. The detailed statistics for the Gaussian parameters and the number of samples are available in <ref type="table">Table 1</ref>. To train classification models, we divide the dataset into training, validation and test sets. We use the validation set to find the best parameter configuration of the classifier.</p><p>We aim to compare the difference of true and softmax-based estimated mutual information I(X, Y ). The mutual information is, however, intractable. We thus approximate it via Monte Carlo (MC) methods using the true probability density function, expressed as:</p><formula xml:id="formula_14">I(X, Y ) ≈ 1 N N i=1 log P (x i |y i ) P (x i ) ,<label>(11)</label></formula><p>where (x i , y i ) forms a paired sample. Equation 11 attains equality as N approaches infinity.</p><p>We use four layers of a feed-forward neural network with the ReLU as an activation for internal layers and softmax as an output layer <ref type="bibr" target="#b0">1</ref> . We train the model with softmax on balanced dataset and with PC-softmax on unbalanced dataset. We compare the experimental results against mutual information neural estimator (MINE) proposed in <ref type="bibr" target="#b3">[4]</ref>. Note that MINE requires having a pair of input and label variables as an input of an estimator network, the classification-based MI-estimator seems more straightforward for measuring mutual information between inputs and labels of classification tasks. <ref type="table" target="#tab_2">Table 2a</ref> summarises the experimental results with the balanced dataset. With the balanced dataset, there is no difference between softmax and PC-softmax. Note that the MC estimator has access to explicit model parameters for estimating mutual information, whereas the softmax estimator measures mutual information based on the model outputs without accessing the true distribution. We could not find a significant difference between MC and the softmax estimator. Additionally, we report the accuracy of the trained model on the classification task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification task</head><p>We test the classification performance of softmax and PC-softmax with two real-world datasets: MNIST <ref type="bibr" target="#b17">[18]</ref> and CUB-200-2011 <ref type="bibr" target="#b28">[29]</ref>.</p><p>We construct balanced and unbalanced versions of the MNIST dataset. For the balanced-MNIST, we use a subset of the original dataset. For the unbalanced-MNIST, we randomly subsample one tenth of instances for digits 0, 2, 4, 6 and 8 from the balanced-MNIST. With CUB-200-2011, we follow the same training and validation splits as in <ref type="bibr" target="#b6">[7]</ref>. As a result of such splitting, the training set is approximately balanced, where out of the total 200 classes, 196 of them contain 30 instances and the remaining 6 classes include 29 instances. To construct an unbalanced dataset, similar to MNIST, we randomly drop one half of the instances from one half of the bird classes.</p><p>We adopt a simple convolutional neural network as a classifier for MNIST. The model contains two convolutional layers with max pooling layer and the ReLU activation, followed by two fully connected layers with the final softmax. For CUB-200-2011, we apply the same architecture as Inception-V3 <ref type="bibr" target="#b6">[7]</ref>.We measure both the micro accuracy and the average per-class accuracy of the two softmax versions on both datasets. The average per-class accuracy alleviates the dominance of the majority classes in unbalanced datasets. The classification results are shown in <ref type="table" target="#tab_4">Table 3</ref>. PC-softmax is significantly more accurate than softmax on unbalanced datasets in terms of the average per-class accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Informative Class Activation Maps: Estimating Mutual Information Between Regions and Labels</head><p>In this section, we show that viewing neural network classifiers as mutual information estimators contributes to a more interpretable neural network classifier, via identifying regions of an image that contain high mutual information with a label. There exist previous work exhibiting how to identify regions of an image corresponding to particular labels, known as class activation maps (CAM). We further formalise CAMs to be related to information theory. Furthermore, with the new view of neural network classifiers as mutual information evaluators, we are able to depict the quantitative relationship between the information of the entire image and its local regions about a label. We call our new CAM Informative Class Activation Map (infoCAM), since it is based on information theory. Moreover, infoCAM can also improve the performance of the weakly supervised object localisation (WSOL) task than the original CAM.</p><p>To explain infoCAM, we first introduce the concept and definition of the class activation map. We then show how to apply it to the weakly supervised object localisation (WSOL) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CAM: Class Activation Map</head><p>Contemporary classification CNNs such as AlexNet <ref type="bibr" target="#b14">[15]</ref> and Inception <ref type="bibr" target="#b26">[27]</ref> consist of stacks of convolutional layers interleaved with pooling layers for extracting visual features. These convolutional layers result in feature maps. A <ref type="figure">Figure 1</ref>: A visualisation of the infoCAM procedure for the WSOL task. The task aims to draw a bounding box for the target object in the original image. The procedure includes: 1) feed input image into a CNN to extract its feature maps, 2) evaluate PMI difference between the true and the other labels of input image for each region within the feature maps, 3) generate the bounding box by keeping the regions exceeding certain infoCAM values and find the largest connected region and 4) interpolate and map the bounding box to the original image.</p><p>feature map is a collection of 2-dimensional grids. The size of the feature map depends on the structure of convolution and pooling layers. Generally the feature map is smaller than the original image. The number of feature maps corresponds to the number of convolutional filters. The feature maps from the final convolutional layer are usually averaged, flattened and fed into the fully-connected layer for classification <ref type="bibr" target="#b18">[19]</ref>. Given K feature maps g 1 , .., g K , the fully-connected layer consists of weight matrix W ∈ R M ×K , where w y k represents the scalar weight corresponding to class y for feature k. We use g k (a, b) to denote a value of 2-dimensional spatial point (a, b) with feature k in map g k . In <ref type="bibr" target="#b5">[6]</ref>, the authors propose a way to interpret the importance of each point in feature maps. The importance of spatial point (a, b) for class y is defined as a weighted sum over features:</p><formula xml:id="formula_15">M y (a, b) = k w y k g k (a, b).<label>(12)</label></formula><p>We redefine M y (a, b) as an intensity of the point (a, b). The collection of these intensity values over all grid points forms a class activation map (CAM). CAM highlights the most relevant region in the feature space for classifying y.</p><p>The input going to the softmax layer corresponding to the class label y is:</p><formula xml:id="formula_16">a,b M y (a, b) = n(x) y .<label>(13)</label></formula><p>Intuitively, weight w y k indicates the overall importance of the kth feature to class y, and intensity M y (a, b) implies the importance of the feature map at spatial location (a, b) leading to the classification of image x to y.</p><p>The aim of WSOL is to identify the region containing the target object in an image given a label, without any pixel-level supervision. Previous approaches tackle the WSOL task by creating a bounding box from the CAM <ref type="bibr" target="#b5">[6]</ref>. Such a CAM contains all important locations that exceed a certain intensity threshold. The box is then upsampled to match the size of the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">InfoCAM: Informative Class Activation Map</head><p>In section 3, we show that softmax classifier carries an explicit implication between inputs and labels in terms of information theory. We extend the notion of mutual information from being a pair of an input image and a label to regions of the input image and labels to capture the regions that have high mutual information with labels.</p><p>To simplify the discussion, we assume here that there is only one feature map, i.e., K = 1. However, the following results can be easily applied to the general cases where K &gt; 1 without loss of generality. We introduce a region R containing a subset of grid points in feature map g.</p><p>Mutual information is an expectation of the point-wise mutual information (PMI) between two variables, i.e., I(X, Y ) = E[PMI(x, y)]. Given two instances of variables, we can estimate their PMI via Equation 9, i.e.,</p><formula xml:id="formula_17">PMI(x, y) = n(x) y − log M y =1 exp(n(x) y ) + log M.</formula><p>The PMI is close to log M if y is the maximum argument in log-sum-exp. To find a region which is the most beneficial to the classification, we compute the difference between PMI with true label and the average of the other labels and decompose it into a point-wise summation as</p><formula xml:id="formula_18">Diff(PMI(x)) = PMI(x, y * ) − 1 M − 1 y =y * PMI(x, y ) = (a,b)∈g w y * g(a, b) − 1 M − 1 y =y * w y g(a, b).</formula><p>The point-wise decomposition suggests that we can compute the PMI differences with respect to a certain region. Based on this observation, we propose a new CAM, named informative CAM or infoCAM, with the new intensity function M Diff y (R) between region R and label y defined as follows:</p><formula xml:id="formula_19">M Diff y (R) = (a,b)∈R w y g(a, b) − 1 M − 1 y =y w y g(a, b).<label>(14)</label></formula><p>The infoCAM highlights the region which decides the classification boundary against the other labels. Moreover, we further simplify Equation 14 to be the difference between PMI with the true and the most-unlikely labels according to the classifier's outputs, denoting as infoCAM+, with the new intensity:</p><formula xml:id="formula_20">M Diff + y (R) = (a,b)∈R w y g(a, b) − w y g(a, b),<label>(15)</label></formula><p>where y = arg min m (a,b)∈R w m g(a, b).</p><p>The complete procedure of WSOL with infoCAM is visually illustrated in <ref type="figure">Figure 1</ref>. We first feed an input image into a CNN to extract its feature maps. Then instead of computing the CAM of the feature map, we compute infoCAM of varying regions from the input image and the class label. Afterwards, we generate the bounding box for the object by preserving regions surpassing a certain intensity level. Then, we generate the bounding box that covers the largest connected remaining regions <ref type="bibr" target="#b31">[32]</ref>. Finally, we interpolate the generated bounding box to the original image size and merge the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Object Localisation with InfoCAM</head><p>In this section, we demonstrate experimental results with infoCAM for WSOL. We first describe the experimental settings and then present the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental settings</head><p>We evaluate WSOL performance on CUB-200-2011 <ref type="bibr" target="#b28">[29]</ref> and Tiny-ImageNet <ref type="bibr" target="#b0">[1]</ref>. CUB-200-2011 consists of 200 bird specifies, including 5,994 training and 5,794 validation images. Each bird class contains roughly the same number of instances, thus the dataset is approximately balanced. Since the dataset only depicts birds, not including other kinds of objects, variations due to class difference are subtle <ref type="bibr" target="#b7">[8]</ref>. Therefore, CNN-based classifiers tend to concentrate on the most discriminative areas within an image while disregarding other regions that are similar among all the birds <ref type="bibr" target="#b29">[30]</ref>. Such nuance-only detection can lead to localisation accuracy degradation <ref type="bibr" target="#b5">[6]</ref>.</p><p>Tiny-ImageNet is a reduced version of ImageNet in terms of both class number, number of instances per class and image resolution. It includes 200 classes, and each consists of 500 training and 50 validation images, and is balanced. Unlike CUB-200-2011 comprising only birds, Tiny-ImageNet contains a wide range of objects from animals to daily supplies. Compared with the full ImageNet, training classifiers on Tiny-ImageNet is faster due to image resolution reduction and quantity shrinkage, yet classification becomes more challenging <ref type="bibr" target="#b22">[23]</ref>.</p><p>To perform an evaluation on localisation, we first need to generate a bounding box for the object within an image. We generate a bounding box in the same way as in <ref type="bibr" target="#b31">[32]</ref>. Specifically, after evaluating infoCAM within each region of an   image, we only retain the regions whose infoCAM values are more than 20% of the maximum infoCAM and abandon all the other regions. Then, we draw the smallest bounding box that covers the largest connected component.</p><p>We follow the same evaluation metrics in <ref type="bibr" target="#b5">[6]</ref> to evaluate localisation performance with two accuracy measures: 1) localisation accuracy with known ground truth class (GT Loc.), and 2) top-1 localisation accuracy (Top-1 Loc.). GT Loc. draws the bounding box from the ground truth of image labels, whereas Top-1 Loc. draws the bounding box from the predicted most likely image label and also requires correct classification. The localisation of an image is judged to be correct when the intersection over union of the estimated bounding box and the ground-truth bounding box is greater than 50%.</p><p>We adopt the same network architectures and hyper-parameters as in <ref type="bibr" target="#b5">[6]</ref>, which shows the current state-of-the-art performance. Specifically, the network backbone is ResNet50 <ref type="bibr" target="#b10">[11]</ref> and a variation of VGG16 <ref type="bibr" target="#b26">[27]</ref>, in which the fully These models are tested with the Attention-based Dropout Layer (ADL) to tackle the localisation degradation problem <ref type="bibr" target="#b5">[6]</ref>. ADL is designed to randomly abandon some of the most discriminative image regions during training to ensure CNN-based classifiers cover the entire object. The ADL-based approaches demonstrate state-of-the-art performance in CUB-200-2011 <ref type="bibr" target="#b5">[6]</ref> and Tiny-ImageNet <ref type="bibr" target="#b4">[5]</ref> for the WSOL task and are computationally efficient. We test ADL with infoCAMs to enhance WSOL capability.</p><p>To prevent overfitting in the test dataset, we evenly split the original validation images to two data piles, one still used for validation during training and the other acting as the final test dataset. We pick the trained model from the epoch that demonstrates the highest top-1 classification accuracy in the validation dataset and report the experimental results with the test dataset. All experiments are run on two Nvidia 2080-Ti GPUs, with the PyTorch deep learning framework <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_6">Table 4</ref> shows the localisation results on CUB-200-2011 and Tiny-ImageNet. The results demonstrate that infoCAM can consistently improve accuracy over the original CAM for WSOL under a wide range of networks and datasets. Both infoCAM and infoCAM+ perform comparably to each other. ADL improves the performance of both models with CUB-200-2011 datasets, but it reduces the performance with Tiny-ImageNet. We conjecture that dropping any part of a Tiny-ImageNet image with ADL significantly influences classification since the images are relatively small. <ref type="figure" target="#fig_1">Figure 2</ref> highlights the difference between CAM and infoCAM. The figure suggests that infoCAM gives relatively high intensity on the object to compare with that of CAM, which only focuses on the head part of the bird. <ref type="figure">Figure 9</ref> in the Appendix presents additional examples of visualisation for comparing localisation performance of CAM to infoCAM, both without the assistance of ADL 3 . From these visualisations, we notice that the bounding boxes generated from infoCAM are formed closer to the objects than the original CAM. That is, infoCAM tends to precisely cover the areas where objects exist, with almost no extraneous or lacking areas. For example, CAM highlights the bird heads in CUB-200-2011, whereas infoCAM also covers the bird bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results</head><p>Ablation Study: InfoCAM differs from CAM in two ways: 1) the new intensity function and 2) region-based intensity smoothing with parameter R. We conduct an ablation study to investigate which feature(s) help to localise objects. The results suggest that both components are indispensable to improve the performance of the localisation. For the detailed results, please refer to the ablation study table in the Appendix.  1.00 0.84 0.86 0.94 0.89 0.87 0.87 0.86 1.00 1.00 PC-sigmoid 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 <ref type="table">Table 5</ref>: Comparison between the classification accuracy results with sigmoid and PC-sigmoid on the double-digit MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Localisation of multiple objects with InfoCAM</head><p>So far, we have shown the results of localisation from a multi-class classification problem. We further extend our experiments on localisation to multi-label classification problems.</p><p>A softmax function is a generalisation of its binary case, a sigmoid function. Therefore, we can apply infoCAM to each label for a multi-label classification problem, which is a collection of binary classification tasks.</p><p>For the experiment, we construct a double-digit MNIST dataset where each image contains up to two digits randomly sampled from the original MNIST dataset <ref type="bibr" target="#b17">[18]</ref>. We locate one digit on the left-side, and the other on the right-side. Some of the images only contain a single digit. For each side, we first decide whether to include a digit from a Bernoulli distribution with mean of 0.7. Then each digit is randomly sampled from a uniform distribution. However, we remove the images that contain no digits. Random samples from the double-digit MNIST are shown in <ref type="figure" target="#fig_3">Figure 4a</ref>.</p><p>We first compare the classification accuracy results between using the original sigmoid and PC-sigmoid. As shown in <ref type="table">Table 5</ref>, PC-sigmoid increases the classification accuracy for each digit type on the test set. InfoCAM improves the localisation accuracy for the WSOL task as well. CAM achieves the localisation accuracy of 91%. InfoCAM enhances the localisation accuracy to 98%. Qualitative visualisations are displayed in <ref type="figure" target="#fig_4">Figure 4</ref>. We aim to preserve the regions of an image that are most relevant to a digit, and erase all the other regions. From the visualisation, one can see that infoCAM localises digits more accurately than CAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown the connection between mutual information estimators and neural network classifiers through the variational form of mutual information. The connection explains the rationale behind the use of sigmoid, softmax and cross-entropy from an information-theoretic perspective. The connection also brings a new insight to understand neural network classifiers. There exists previous work that called the negative log-likelihood (NLL) loss as maximum mutual information estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Despite this naming similarity, that work does not show the relationship between softmax and mutual information that we have shown here.</p><p>The connection between neural network classifiers and mutual information evaluators provides more than an alternative view on neural network classifiers. Via converting neural network classifiers to mutual information estimators, we receive two positive consequences for practical applications. First, we improve the classification accuracy, in particular when the datasets are unbalanced. The new mutual information estimators even outperform the prior state-of-the-art neural network classifiers. Second, using the pointwise mutual information between the inputs and labels, we can locate the objects within images more precisely. We also provide a more information-theoretic interpretation of class activation maps. We believe that this opens new ways to understand how neural network classifiers work and improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Rethinking Softmax with Cross-Entropy:</p><p>Neural Network Classifier as Mutual Information Estimator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network Architectures</head><p>In this section, we illustrate neural network architectures that have been utilised in the previous experiments. <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates the architecture of the softmax mutual information neural estimator in section 4. <ref type="figure" target="#fig_6">Figure 6</ref> demonstrates the architecture of the network that are utilised to show PC-softmax leads to higher classification accuracy on the unbalanced MNIST dataset as in section 4. We explain in section 6 on how to convert the VGG16 architecture to the VGG16-GAP architecture. Such VGG16-GAP is used in infoCAM. We illustrate in <ref type="figure" target="#fig_7">Figure 7</ref> on how to convert the former to the latter architecture. For ResNet50 and Inception-V3, the architectures are identical to <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b27">[28]</ref>.</p><p>For more detailed information, please refer to the actual implementation, which we plan to make public.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualisation of Data Distributions</head><p>We show both theoretically and experimentally in section 4 and section 3 that neural network classifiers can be considered as mutual information estimators. In this section, we provide visualisation on the distributions of the data that are used to test the effectiveness of the softmax-based mutual information estimator. In such visualisation as <ref type="figure" target="#fig_8">Figure 8</ref>, data points are stratified subsets of the test datasets, so that it can reflect the dataset imbalance. Since it is impossible to visualise data whose dimension is greater or equal to three, we apply principle component analysis (PCA) to reduce the dimension to two. Furthermore, data of different class labels become more distinguishable as dimension increases. This can account for the reason why classification accuracy increases as the dimension rises. <ref type="figure" target="#fig_8">Figure 8</ref>: Illustration of the synthetic dataset for evaluating the softmax-based mutual information estimator. For data whose dimension is greater or equal to three, the visualisation is on the results of PCA. The same colour represents the identical class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Result</head><p>In this section, we present some further results on localisation and classification. <ref type="table">Table 7</ref> is a reproduction of main result with the classification results. Note that the classification performances of CAM and infoCAM is the same since we do not modify the training objective of infoCAM. The result can be used to understand the effect of ADL on the classification task. <ref type="table">Table 8a</ref> shows the result of ablation study. We have tested the importance of three features: 1) ADL, 2) region parameter R and 3) the second subtraction term in Equation <ref type="bibr" target="#b13">14</ref>. To combine the result in the main text, the result suggests that both region parameter and subtraction term are necessary to increase the performance of localisation. The choice of ADL depends on the dataset. We conjecture that ADL is inappropriate to apply Tiny-ImageNet since the removal of any part of tiny image, which is what ADL does during training, affects the performance of the localisation to compare with its application to relatively large images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Localisation and Classification Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Localisation Examples from Tiny-ImageNet</head><p>We present examples from the Tiny-ImageNet dataset in <ref type="figure">Figure 9</ref>. Such examples show the infoCAM draws tighter bound toward target objects.   <ref type="table">Table 7</ref>: Evaluation results of CAM and infoCAM on Tiny-ImageNet. Note that the classification accuracy of infoCAM is the same as those of CAM. InfoCAM always outperforms CAM on localisation of objects under the same model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs</head><p>In this section, we provide rigorous proofs of Theorem 1 and Theorem 2. The structure of proof is similar to the proof used in <ref type="bibr" target="#b3">[4]</ref>. We assume the input space Ω = X × Y being a compact domain of R d , where all measures are Lebesgue and are absolutely continuous. We restrict neural networks to produce a single continuous output, denoted as n(x) y . We restate the two theorems for quick reference.</p><p>Theorem 1. Let f φ (x, y) be n(x) y . Minimising the cross-entropy loss of softmax-normalised neural network outputs is equivalent to maximising Equation 5, i.e., the lower bound of mutual information, under the uniform label distribution. That is, if the dataset is balanced, then training a neural network via minimising cross-entropy with softmax equals enhancing a estimator toward more accurately evaluating the mutual information between data and label.</p><p>Theorem 2. The mutual information between two random variable X and Y can be obtained via the infimum of cross-entropy with PC-softmax in Equation <ref type="bibr" target="#b9">10</ref>. Such an evaluation is strongly consistent.</p><p>The proof technique that we have used to prove Theorem 2 is similar to the one used in <ref type="bibr" target="#b3">[4]</ref>. Lemma 1. Let η &gt; 0. There exists a family of neural network functions n φ with parameter φ in some compact domain such that   where</p><formula xml:id="formula_21">|I(X; Y ) − I φ (X; Y )| ≤ η,<label>(16)</label></formula><formula xml:id="formula_22">I φ (X; Y ) = sup φ E (X,Y ) n φ − E X log E Y [exp(n φ ) y ].<label>(17)</label></formula><p>Proof. Let n * φ (X, Y ) = PMI(X, Y ) = log P (X,Y ) P (X)P (Y ) . We then have: E (X,Y ) [n * φ (x) y ] = I(X; Y ) and E X E Y [exp (n * φ (x) y )] = 1.</p><p>(18) Then, for neural network n φ , the gap I(X; Y ) − I φ (X; Y ): <ref type="figure" target="#fig_3">Equation 19</ref> is positive since the neural mutual information estimator evaluates a lower bound. The equation uses Jensen's inequality and the inequality log x ≤ x − 1.</p><formula xml:id="formula_23">I(X; Y ) − I φ (X; Y ) = E (X,Y ) [n * φ (X, Y ) − n φ (X, Y )] + E X log E Y [exp(n φ ) y ] ≤ E (X,Y ) [n * φ (X, Y ) − n φ (X, Y )] + log E X E Y [exp(n φ (x) y )] ≤ E (X,Y ) [n * φ (X, Y ) − n φ (X, Y )] + E X E Y [exp(n φ (x) y ) − exp (n * φ (x) y )].<label>(19)</label></formula><p>We assume η &gt; 0 and consider n * φ (x) y is bounded by a positive constant M . Via the universal approximation theorem <ref type="bibr" target="#b12">[13]</ref>, there exists n φ (x) y ≤ M such that</p><formula xml:id="formula_24">E (X,Y ) |n * φ (X, Y ) − n φ (X, Y )| ≤ η 2 and E X E Y |n φ (x) y − n * φ (x) y | ≤ η 2 exp (−M ).<label>(20)</label></formula><p>By utilising that exp is Lipschitz continuous with constant exp(M ) over (−∞, M ], we have</p><formula xml:id="formula_25">E X E Y | exp(n φ (x) y ) − exp(n * φ (x) y )| ≤ exp(M ) · E X E Y |n φ (x) y − n * φ (x) y | ≤ η 2 .<label>(21)</label></formula><p>Combining Equation <ref type="bibr" target="#b18">19</ref>, Equation <ref type="bibr" target="#b19">20</ref> and Equation 21, we then obtain </p><formula xml:id="formula_26">|I(X; Y ) − I φ (X; Y )| ≤ E (X,Y ) |n * φ (X, Y ) − n φ (X, Y )| + E X E Y | exp(n φ (x) y ) − exp(n * φ (x) y )| = η 2 + η 2 = η.<label>(22)</label></formula><p>Proof. We start by employing the triangular inequality:</p><formula xml:id="formula_28">| I n (X; Y ) − I φ (X; Y )| ≤ sup φ |E (X,Y ) [n * φ (X, Y )] − E (X,Y )n [n * φ (X, Y )]| +sup φ |E X log E Y [exp(n φ ) y ] − E Xn log E Yn [exp(n φ ) y ]|<label>(24)</label></formula><p>We have stated previously that neural network n φ is bounded by M , i.e., n φ (x) y ≤ M . Using the fact that log is Lipschitz continuous with constant exp(M ) over the interval [exp(−M ), exp(M )]. We have</p><formula xml:id="formula_29">| log E Y [exp(n φ ) y ] − log E Yn [exp(n φ ) y ]| ≤ exp(M ) · |E Y [exp(n φ ) y ] − E Yn [exp(n φ ) y ]|<label>(25)</label></formula><p>Using the uniform law of large numbers <ref type="bibr" target="#b9">[10]</ref>, we can choose N ∈ N such that for ∀n ≥ N and with probability one</p><formula xml:id="formula_30">sup φ |E Y [exp(n φ ) y ] − E Yn [exp(n φ ) y ]| ≤ η 4 exp(−M ).<label>(26)</label></formula><p>That is,</p><formula xml:id="formula_31">| log E Y [exp(n φ ) y ] − log E Yn [exp(n φ ) y ]| ≤ η 4<label>(27)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>=</head><label></label><figDesc>E (X,Y ) log Q(y|x) P (y) + E (X,Y ) log P (x, y) Q(x, y) D KL (P (x,y)||Q(x,y)) − E (X) log P (x) Q(x) D KL (P (x)||Q(x))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualisation of comparison between CAM and infoCAM+. Red and green boxes represent the ground truth and prediction, respectively. Brighter regions represent higher CAM or infoCAM+ values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualisation of localisation with ResNet50 without using ADL on CUB-200-2011. Images in the second and the third row correspond to CAM and infoCAM+, respectively. Estimated (green) and ground-truth (red) bounding boxes are shown separately.connected layers are replaced with global average pooling (GAP) layers to reduce the number of parameters. The traditional softmax is used as the final layer since both datasets are well balanced. InfoCAM requires the region parameter R. We apply a square region for the region parameter R. The size of the region R is set as 5 and 4 for VGG and ResNet in CUB-200-2011, respectively, and 3 for both VGG and ResNet in Tiny-ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Original input images. (b) CAM localisation. (c) InfoCAM localisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualisation of comparison between CAM and infoCAM for the multi-MNIST dataset. Each image has one or two digits in the left and/or right. We aim to extract digit 0 in each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The neural network architecture of the softmax mutual information estimator. The softmax in the last layer can be either the traditional or the PC one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The neural network architecture that is utilised to show PC-softmax leads to higher classification accuracy on the unbalanced MNIST dataset. The softmax in the last layer can be either the traditional or the PC one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Illustration on the conversion from VGG16 to VGG16-GAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 :</head><label>8</label><figDesc>Ablation study results on the importance of the region parameter R and the subtraction term within the formulation of infoCAM. Y and N indicates the use of corresponding feature. Arrows indicates the relative performance against the case where both features are not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Lemma 2 .</head><label>92</label><figDesc>Visualisation of localisation with ResNet50 on CUB-200-2011 and TinyImageNet, without the assistance of ADL. The images in the second row are generated from the original CAM approach and the ones in the third row correspond to infoCAM. The red and green bounding boxes are ground truth and estimations, respectively. Let η &gt; 0. Given a family of neural networks n φ with parameter φ in some compact domain, there exists N ∈ N such that ∀n ≥ N, Pr | I n (X; Y ) − n φ (X; Y )| ≤ η = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mutual information estimation results with softmax-based classification neural networks. MC represents the estimated mutual information via Monte Carlo methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2b</head><label>2b</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="4">MNIST Balanced Unbalanced Balanced Unbalanced CUB-200-2011</cell></row><row><cell>softmax</cell><cell>97.95</cell><cell>96.81</cell><cell>89.23</cell><cell>89.21</cell></row><row><cell>PC-softmax</cell><cell>97.91</cell><cell>96.86</cell><cell>89.18</cell><cell>89.73*</cell></row><row><cell></cell><cell cols="3">(a) Classification accuracy (%).</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">MNIST Balanced Unbalanced Balanaced Unbalanced CUB-200-2011</cell></row><row><cell>softmax</cell><cell>97.95</cell><cell>95.05</cell><cell>89.21</cell><cell>84.63</cell></row><row><cell>PC-softmax</cell><cell>97.91</cell><cell>96.30</cell><cell>89.16</cell><cell>87.69</cell></row></table><note>summarises the experimental results with the unbalanced dataset. The results show that the PC-softmax slightly under-estimates mutual information when compared with the other two approaches. It is worth noting that the classification accuracy of PC-softmax consistently outperforms the original softmax. The results show that the MINE slightly under-estimate the MI as the input dimension increases.(b) Average per-class accuracy (%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy of using softmax and PC-softmax. Numbers of instances for different labels are the same for a balanced dataset and are significantly distinct for an unbalanced dataset. Bold values denote p-values less than 0.05 with the Mann-Whitney U test 2 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Localisation results of CAM and infoCAM on CUB-2011-200 and Tiny-ImageNet. InfoCAM outperforms CAM on localisation of objects with the same model architecture. Bold values represent the highest accuracy for a certain metric.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Evaluation results of CAM and infoCAM on CUB-2011-200. Note that the classification accuracy of infoCAM is the same as those of CAM. InfoCAM always outperforms CAM on localisation of objects under the same model architecture.</figDesc><table><row><cell></cell><cell></cell><cell>GT</cell><cell>Top-1</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell>Loc. (%)</cell><cell>Loc. (%)</cell><cell>Cls (%)</cell><cell>Cls (%)</cell></row><row><cell></cell><cell>CAM</cell><cell>53.49</cell><cell>33.48</cell><cell>55.25</cell><cell>79.19</cell></row><row><cell>VGG-16-GAP</cell><cell>CAM (ADL) infoCAM infoCAM (ADL) infoCAM+</cell><cell>52.75 55.50 53.95 55.25</cell><cell>32.26 34.27 33.05 34.27</cell><cell>52.48 ---</cell><cell>78.75 ---</cell></row><row><cell></cell><cell>infoCAM+ (ADL)</cell><cell>53.91</cell><cell>32.94</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CAM</cell><cell>54.56</cell><cell>40.55</cell><cell>66.45</cell><cell>86.22</cell></row><row><cell></cell><cell>CAM (ADL)</cell><cell>52.66</cell><cell>36.88</cell><cell>63.21</cell><cell>83.47</cell></row><row><cell>ResNet-</cell><cell>infoCAM</cell><cell>57.79</cell><cell>43.34</cell><cell>-</cell><cell>-</cell></row><row><cell>50</cell><cell>infoCAM (ADL)</cell><cell>54.18</cell><cell>37.79</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>infoCAM+</cell><cell>57.71</cell><cell>43.07</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>infoCAM+ (ADL)</cell><cell>53.70</cell><cell>37.71</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All model details used in this paper are available in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Accuracy with * is higher than the current state-of-the-art<ref type="bibr" target="#b6">[7]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Please refer to the supplementary material for more Tiny-ImageNet visualisation results.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Therefore, using the triangle inequality we can rewrite Equation 24 as:</p><p>Using the uniform law of large numbers again, we can choose N ∈ N such that for ∀n ≥ N and with probability one</p><p>and: </p><p>Now, combining the above two lemmas, we prove that our mutual information evaluator is strongly consistent.</p><p>Proof. Using the triangular inequality, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<ptr target="https://tiny-imagenet.herokuapp.com/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum mutual information estimation of hidden markov model parameters for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter V De</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The im algorithm: a variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix V Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">page None</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved techniques for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07888</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domainspecific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical Processes in M-estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Geer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu Jie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Formal limitations on the measurement of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Statos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04251</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Camdrop: A new explanation of dropout and a guided regularization method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
