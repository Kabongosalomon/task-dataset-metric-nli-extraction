<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial Fusion for Single-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
							<email>liusongtao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<email>dhuang@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial Fusion for Single-Shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>The code is available at https://github.com/ruinmessi/ASFF.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pyramidal feature representation is the common practice to address the challenge of scale variation in object detection. However, the inconsistency across different feature scales is a primary limitation for the single-shot detectors based on feature pyramid. In this work, we propose a novel and data driven strategy for pyramidal feature fusion, referred to as adaptively spatial feature fusion (ASFF). It learns the way to spatially filter conflictive information to suppress the inconsistency, thus improving the scale-invariance of features, and introduces nearly free inference overhead. With the ASFF strategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy trade-off on the MS COCO dataset, reporting 38.1% AP at 60 FPS, 42.4% AP at 45 FPS and 43.9% AP at 29 FPS. The code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the most fundamental components in various downstream vision tasks. In recent years, the performance of object detectors has been remarkably improved thanks to the rapid development of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35]</ref> and wellannotated datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. However, handling multiple objects across a wide range of scales still remains a challenging problem. To achieve scale invariance, recent state-of-the-art detectors construct feature pyramids or multi-level feature towers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The Single Shot Detector (SSD) <ref type="bibr" target="#b24">[25]</ref> is one of the first attempts to generate convolutional pyramidal feature representations for object detection. It reuses the multi-scale feature maps from different layers computed in the forward pass to predict objects of various sizes. However, this bottomup pathway suffers from low accuracies on small instances as the shallow-layer feature maps contain insufficient semantic information. To address the disadvantage of SSD, Feature Pyramid Network (FPN) <ref type="bibr" target="#b19">[20]</ref> sequentially combines two adjacent layers in feature hierarchy in the backbone model with a top-down pathway and lateral connections. The low-resolution, semantically strong features are up-sampled and combined with high-resolution, semantically weak features to build a feature pyramid that shares rich semantics at all levels. FPN and other similar top-down structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31]</ref> are simple and effective, but they still leave much room for improvement. Indeed, many recent models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> with advanced cross-scale connections show accuracy gains through strengthening feature fusion. Besides the manually designed fusion structures, NAS-FPN <ref type="bibr" target="#b7">[8]</ref> applies Neural Architecture Search (NAS) techniques to pursue a better architecture, producing significant improvements upon many backbones. Although these advanced studies deliver more powerful feature pyramids, they still leave room for scale-invariant prediction. Some evidences are given by SNIP <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, which adopts a scale normalization method that selectively trains and infers the objects of appropriate sizes in each image scale of the multi-scale image pyramids, achieving further improvements on the results of pyramidal feature based detectors with multi-scale testing. However, image pyramid solutions sharply increase the inference time, which makes them not applicable to real-world applications.</p><p>Meanwhile, compared to image pyramids, one main drawback of feature pyramids is the inconsistency across different scales, in particular for single-shot detectors. Specifically, when detecting objects with feature pyramids, a heuristicguided feature selection is adopted: large instances are typically associated with upper feature maps and small instances with lower feature maps. When an object is assigned and treated as positive in the feature maps at a certain level, the corresponding areas in the feature maps of other levels are viewed as background. Therefore, if an image contains both small and large objects, the conflict among features at different levels tends to occupy the major part of the feature pyramid. This inconsistency interferes gradient computation during training and downgrades the effectiveness of feature pyramids. Some models adopt several tentative strategies to deal with this problem. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref> set the corresponding areas of feature maps at adjacent levels as ignore regions (i.e. zero gradients), but this alleviation may increase inferior predictions at the the adjacent levels of features. TridentNet <ref type="bibr" target="#b18">[19]</ref> creates multiple scale-specific branches with different receptive fields for scale-aware training and inference. It breaks away from feature pyramids to avoid inconsistency, but also misses reusing its higher-resolution maps, limiting the accuracy of small instances.</p><p>In this paper, we propose a novel and effective approach, named adaptively spatial feature fusion (ASFF), to address the inconsistency in feature pyramids of single-shot detectors. The proposed approach enables the network to directly learn how to spatially filter features at other levels so that only useful information is kept for combination. For the features at a certain level, features of other levels are first integrated and resized into the same resolution and then trained to find the optimal fusion. At each spatial location, features at different levels are fused adaptively, i.e., some features may be filter out as they carry contradictory information at this location and some may dominate with more discriminative clues. ASFF offers several advantages: (1) as the operation of searching the optimal fusion is differential, it can be conveniently learned in back-propagation; (2) it is agnostic to the backbone model and it is applied to single-shot detectors that have a feature pyramid structure; and (3) its implementation is simple and the increased computational cost is marginal.</p><p>Experiments on the COCO <ref type="bibr" target="#b21">[22]</ref> benchmark confirm the effectiveness of our method. We first adopt the recent advanced training tricks <ref type="bibr" target="#b42">[43]</ref> and anchor-guiding pipeline <ref type="bibr" target="#b37">[38]</ref> to provide a solid baseline for YOLOv3 <ref type="bibr" target="#b30">[31]</ref> (i.e., 38.8% mAP with 50 FPS). We then employ ASFF to further improve this enhanced YOLOv3 and another strong singlestage detector, RetinaNet <ref type="bibr" target="#b20">[21]</ref>, equipped with different backbones by a large margin, while keeping computational cost under control. Especially, we boost the YOLOv3 baseline to 42.4% mAP with 45 FPS and 43.9% mAP with 29 FPS, which is a state-of-the-art speed and accuracy trade-off among all the existing detectors on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feature pyramid representations or multi-level feature towers are the basis of solutions of multi-scale processing in recent object detectors. SSD <ref type="bibr" target="#b24">[25]</ref> is one of the first attempts to predict class scores and bounding boxes from multiple feature scales in a bottom-up manner. FPN <ref type="bibr" target="#b19">[20]</ref> builds feature pyramid by sequentially combining two adjacent level of features with top-down pathway and lateral connections. Such connections effectively enhance feature representations and the rich semantics from depp and low-resolution features are shared at all levels.</p><p>Following FPN, many other models with similar topdown structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31]</ref>appear, which achieve substantial improvements for object detection. Recently, more advanced investigations have attempted to ameliorate such multi-scale feature representations. For instance, PANet <ref type="bibr" target="#b23">[24]</ref> proposes an additional bottom-up pathway based on FPN to increase the low-level information in deep layers. Chen et al. <ref type="bibr" target="#b3">[4]</ref> build a pyramid based on SSD that weaves features across different level of feature layers. DLA <ref type="bibr" target="#b39">[40]</ref> introduces iterative deep aggregation and hierarchical deep aggregation structures to better fuse semantic and spatial information. Kim et al. <ref type="bibr" target="#b12">[13]</ref> show a parallel feature pyramid by adopting spatial pyramid pooling and widening the network. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> present a feature selective anchor-free module to dynamically choose the most suitable feature level for each instance. Kong et al. <ref type="bibr" target="#b14">[15]</ref> aggregate feature maps at all scales to a specific scale and then produce features at each scale by a global attention operation on the combined features. Libra R-CNN <ref type="bibr" target="#b28">[29]</ref> also integrates features at all levels to generate more balanced semantical features. In addition to manually designing the fusion structure, NAS-FPN <ref type="bibr" target="#b7">[8]</ref> applies the Neural Architecture Search algorithm to seek a more powerful fusion architecture, delivering the best single-shot detector.</p><p>In spite of competitive scores, those feature pyramid based methods still suffer from the inconsistency across different scales, which limits the further performance gain. To address this, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref> set the corresponding regions of adjacent levels as ignore regions (i.e. zero gradients), but the relaxation in the adjacent levels tends to cause more inferior predictions as false positives. TridentNet <ref type="bibr" target="#b18">[19]</ref> drops out the structure of feature pyramids and creates multiple scalespecific branches with different receptive fields to adopt scale-aware training and inferencing, but the performance of small instances may suffer from the missing of its higherresolution maps.</p><p>The proposed ASFF approach alleviates this issue by learning connections among different feature maps. Actually, this idea is not new within the domain of computer vi- sion. <ref type="bibr" target="#b0">[1]</ref> adopts element-wise product in two adjacent feature maps to form a gate unit in a top-down manner for dense label prediction. The element-wise product operation reduces the categorical ambiguity from shallow layers and highlights the discriminability from deeper layers. This gating mechanism succeeds in semantic segmentation. However, the task of dense labeling does not need heuristic-guided feature selection required in object detection, since the features at all levels predict the same label map at different scales. It thus does not reduce spatial contradiction in object detection. <ref type="bibr" target="#b27">[28]</ref> proposes a sigmoid gating unit in the skip connection between convolutional and deconvolutional layers of features at each single level for visual counting. It optimizes the flow of information within the feature maps at the same level, but does not deal with the inconsistency in feature pyramids. ACNet <ref type="bibr" target="#b36">[37]</ref> employs a flexible way to switch global and local inference in processing the feature representations by adaptively determining the connection status among the feature nodes from CNNs, classical multi-layer perceptron and non-local network. In contrast to them, ASFF adaptively learns the import degrees for different levels of features on each location to avoid spatial contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we instantiate our adaptively spatial feature fusion (ASFF) approach by showing how it works on the single-shot detectors with feature pyramids, such as SSD <ref type="bibr" target="#b24">[25]</ref>, RetinaNet <ref type="bibr" target="#b20">[21]</ref>, and YOLOv3 <ref type="bibr" target="#b30">[31]</ref>. Taking YOLOv3 as an example, we apply ASFF to it and demonstrate the resulting detector in the following steps. First, we push YOLOv3 to a baseline, much stronger than the origin <ref type="bibr" target="#b30">[31]</ref>, by adopt-ing the recent advanced training tricks <ref type="bibr" target="#b42">[43]</ref> and anchor-free pipeline <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref>. Then, we present the formulation of ASFF and give a qualitative analysis of the consistency property of the pyramid feature fusion and ASFF. Finally, we display the details of training, testing, and implementing the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Strong Baseline</head><p>We take the YOLOv3 <ref type="bibr" target="#b30">[31]</ref> framework because it is simple and efficient. In YOLOv3, there are two main components: an efficient backbone (DarkNet-53) and a feature pyramid network of three levels. A recent work <ref type="bibr" target="#b42">[43]</ref> significantly improves the performance of YOLOv3 without modifying network architectures and bringing extra inference cost. Moreover, a number of studies <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14]</ref> indicate that the anchor-free pipeline contributes to considerably better performance with simpler designs. To better demonstrate the effectiveness of our proposed ASFF approach, we build a baseline, much stronger than the origin <ref type="bibr" target="#b30">[31]</ref>, based on these advanced techniques.</p><p>Following <ref type="bibr" target="#b42">[43]</ref>, we introduce a bag of tricks in the training process, such as the mixup algorithm <ref type="bibr" target="#b11">[12]</ref>, the cosine <ref type="bibr" target="#b25">[26]</ref> learning rate schedule, and the synchronized batch normalization technique <ref type="bibr" target="#b29">[30]</ref>. Besides those tricks, we further add an anchor-free branch to run jointly with anchor-based ones as <ref type="bibr" target="#b44">[45]</ref> does and exploit the anchor guiding mechanism proposed by <ref type="bibr" target="#b37">[38]</ref> to refine the results. Moreover, an extra Intersection over Union (IoU) loss function <ref type="bibr" target="#b40">[41]</ref> is employed on the original smooth L1 loss for better bounding box regression. More details can be found in the supplemental material.</p><p>With these advanced techniques mentioned above, we achieve 38.8% mAP on the COCO <ref type="bibr" target="#b21">[22]</ref> 2017 val set at a speed of 50 FPS (on Tesla V100), improving the original YOLOv3-608 baseline (33.0% mAP with 52 FPS <ref type="bibr" target="#b30">[31]</ref>) by a large margin without heavy computational cost in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptively Spatial Feature Fusion</head><p>Different from the former approaches that integrate multilevel features using element-wise sum or concatenation, our key idea is to adaptively learn the spatial weight of fusion for feature maps at each scale. The pipeline is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, and it consists of two steps: identically rescaling and adaptively fusing.</p><p>Feature Resizing. We denote the features of the resolution at level l (l ∈ {1, 2, 3} for YOLOv3) as x l . For level l, we resize the features x n at the other level n (n = l) to the same shape as that of x l . Because the features at three levels in YOLOv3 have different resolutions as well as different numbers of channels, we accordingly modify the up-sampling and down-sampling strategies for each scale. For up-sampling, we first apply a 1 × 1 convolution layer to compress the number of channels of features to that in level l, and then upscale the resolutions respectively with interpolation. For down-sampling with 1/2 ratio, we simply use a 3 × 3 convolution layer with a stride of 2 to modify the number of channels and the resolution simultaneously. For the scale ratio of 1/4, we add a 2-stride max pooling layer before the 2-stride convolution.</p><p>Adaptive Fusion. Let x n→l ij denote the feature vector at the position (i, j) on the feature maps resized from level n to level l. We propose to fuse the features at the corresponding level l as follows:</p><formula xml:id="formula_0">y l ij = α l ij · x 1→l ij + β l ij · x 2→l ij + γ l ij · x 3→l ij ,<label>(1)</label></formula><p>where y l ij implies the (i, j)-th vector of the output feature maps y l among channels. α l ij , β l ij and γ l ij refer to the spatial importance weights for the feature maps at three different levels to level l, which are adaptively learned by the network. Note that α l ij , β l ij and γ l ij can be simple scalar variables, which are shared across all the channels. Inspired by <ref type="bibr" target="#b36">[37]</ref>, we force α l ij + β l ij + γ l ij = 1 and α l ij , β l ij , γ l ij ∈ [0, 1], and define</p><formula xml:id="formula_1">α l ij = e λ l α ij e λ l α ij + e λ l β ij + e λ l γ ij .<label>(2)</label></formula><p>Here α l ij , β l ij and γ l ij are defined by using the softmax function with λ l αij , λ l βij and λ l γij as control parameters respectively. We use 1 × 1 convolution layers to compute the weight scalar maps λ l α , λ l β and λ l γ from x 1→l , x 2→l and x 3→l respectively, and they can thus be learned through standard back-propagation.</p><p>With this method, the features at all the levels are adaptively aggregated at each scale. The outputs {y 1 , y 2 , y 3 } are used for object detection following the same pipeline of YOLOv3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Consistency Property</head><p>In this section, we analyze the consistency property of the proposed ASFF approach and the other alternatives of feature fusion. Without loss of generality, we focus on the gradient at a certain position (i, j) of the unresized feature maps at level 1 x 1 in YOLOv3. Following the chain rule, the gradient is computed as:</p><formula xml:id="formula_2">∂L ∂x 1 ij = ∂y 1 ij ∂x 1 ij · ∂L ∂y 1 ij + ∂x 1→2 ij ∂x 1 ij · ∂y 2 ij ∂x 1→2 ij · ∂L ∂y 2 ij + ∂x 1→3 ij ∂x 1 ij · ∂y 3 ij ∂x 1→3 ij · ∂L ∂y 3 ij<label>(3)</label></formula><p>It is worth to note that feature resizing usually uses interpolation for up-sampling and pooling for down-sampling.</p><p>We thus assume that ∂x 1→l ij ∂x 1 ij ≈ 1 for simplicity. Then Eq. <ref type="formula" target="#formula_2">(3)</ref> can be written as:</p><formula xml:id="formula_3">∂L ∂x 1 ij = ∂y 1 ij ∂x 1 ij · ∂L ∂y 1 ij + ∂y 2 ij ∂x 1→2 ij · ∂L ∂y 2 ij + ∂y 3 ij ∂x 1→3 ij · ∂L ∂y 3 ij<label>(4)</label></formula><p>For the two common fusion operations used in RetinaNet <ref type="bibr" target="#b20">[21]</ref>, YOLOv3 <ref type="bibr" target="#b30">[31]</ref> and other pyramidal feature based detectors (i.e. element-wise sum and concatenation), we can further simplify the equation to the following with One typical way to deal with this problem is to set the corresponding positions of the other levels as ignore regions (i.e. ∂L ∂y 2 ij = ∂L ∂y 3 ij = 0) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref>. However, although the conflict in x 1 ij is eliminated, the relaxation in y 2 ij and y 3 ij tends to cause more inferior predictions as false positives at the suboptimal levels. For ASFF, it is straightforward to calculate the gradient from Eq. (1) and Eq. (4) as follows:</p><formula xml:id="formula_4">∂L ∂x 1 ij = α 1 ij · ∂L ∂y 1 ij + α 2 ij · ∂L ∂y 2 ij + α 3 ij · ∂L ∂y 3 ij ,<label>(6)</label></formula><p>where α 1 ij , α 2 ij , α 3 ij ∈ [0, 1]. With these three coefficients, the inconsistency of gradient can be harmonized if α 2 ij → 0 and α 3 ij → 0. Since the fusion parameters can be learned by the standard back-propagation algorithm, a well-tuned training process can yield such effective coefficients (see some qualitative results in <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_5">Figure 4)</ref>. Meanwhile, the supervision information of the background in ∂L ∂y 2 ij and ∂L ∂y 2 ij is kept, avoiding generating more false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training, Inference, and Implementation</head><p>Training. Let Θ denote the set of network parameters (e.g., the weights of convolution filters) and Φ = {λ l α , λ l β , λ l γ | l = 1, 2, 3} be the set of fusion parameters that control the spatial fusion of each scale. We jointly optimize the two sets of parameters by minimizing a loss function L(Θ, Φ), where L is the original YOLOv3 objective function plus the IoU regression loss <ref type="bibr" target="#b40">[41]</ref> for both anchor shape prediction and bounding box regression. Following <ref type="bibr" target="#b42">[43]</ref>, we apply mixup on the classification pretraining of DarkNet53, and all the new convolution layers are employed with the MSRA weight initialization method <ref type="bibr" target="#b9">[10]</ref>. To reduce the risk of overfitting and improve generalization of network predictions, we follow the approach of random shapes training as in YOLOv3 <ref type="bibr" target="#b30">[31]</ref>. More specifically, a mini-batch of N training images is resized to N ×3×H×W , where H = W is randomly picked in {320, 352, 384, 416, 448, 480, 512, 544, 576, 608}.</p><p>Inference. During inference, the detection header at each level first predicts the shape of anchors, and then conducts classification and box regression following the same pipeline as that in YOLOv3 <ref type="bibr" target="#b30">[31]</ref>. Next, non-maximum suppression (NMS) with the threshold at 0.6 is applied to each class separately. For simplicity and fair comparison against other counterparts, we do not use the advanced testing tricks such as Soft-NMS <ref type="bibr" target="#b1">[2]</ref> or test-time image augmentations. Implementation. We implement the modified YOLOv3 as well as ASFF using the existing PyTorch v1.0.1 framework with CUDA 10.0 and CUDNN v7.1. The entire network is trained with stochastic gradient descent (SGD) on 4 GPUs (NVDIA Tesla V100) with 16 images per GPU. All models are trained for 300 epochs with the first 4 epochs of warmup and the cosine learning rate schedule <ref type="bibr" target="#b25">[26]</ref> from 0.001 to 0.00001. The weight decay is 0.0005 and the momentum is 0.9. We also follow the implementation of <ref type="bibr" target="#b42">[43]</ref> to turn off mixup augmentation for the last 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform all the experiments on the bounding box detection track of the challenging MS COCO 2017 benchmark <ref type="bibr" target="#b21">[22]</ref>. We follow the common practice <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref> and use the COCO train-2017 split (consisting of 115k images) for training. We conduct ablation and sensitivity studies according to the evaluation on the val-2017 split (5k images). For our main results, we report COCO AP on the test-dev split (20k images), which has no public labels and requires uploading detection results to the evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Solid Baseline. We first evaluate the contribution of several elements to our baseline detector for better reference. Results are reported in <ref type="table" target="#tab_0">Table 1</ref>, where BoF denotes all the training tricks mentioned in <ref type="bibr" target="#b42">[43]</ref>, GA denotes the guided anchoring strategy <ref type="bibr" target="#b37">[38]</ref>, and IoU is the additional IoU loss <ref type="bibr" target="#b40">[41]</ref> in bounding box regression. From <ref type="table" target="#tab_0">Table 1</ref>, we can see that all the techniques contribute to accuracy gain, and thanks to them, we deliver a final baseline which reaches an AP of 38.8%. It is worth to note that the improvement of almost all the components is cost free, as BoF and IoU do not add any additional computation and GA introduces only two 1 × 1 convolution layers for each level of feature maps. The final baseline achieves the speed of 50 FPS on a single Graphics Card of NVIDIA Tesla V100.</p><p>Effectiveness of Adjacent Ignore Regions. To avoid gradient inconsistency, some works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref> ignore the corresponding areas on the two adjacent levels of the chosen level for each target, and the ignored area is the same size as that of the positive one in the chosen level. In YOLOv3, only the center location of the chosen area is positive, and we thus ignore the corresponding center location at the two adjacent levels to follow the ignoring rule. Besides, we denote ignore as the ratio of the widths and lengths of the ignored area to that of the target object area, and carry out some experiments with different values of ignore to show the effectiveness of the ignoring strategy. <ref type="table">Table 2</ref> reports the study results. We can see that the larger ignore area indeed hurt the performance of the detector, by bringing more false positives.</p><p>Adaptively Spatial Feature Fusion. ASFF significantly improves the box AP from 38.8% to 40.6% as shown in <ref type="table">Table 3</ref>. To be more specific, most of the improvements come from AP S and AP M , yielding increases of 2.9% and 2.9% respectively compared with corresponding reference scores. It validates that the representation of high-resolution features is largely improved by the proposed adaptively fusion strategy. Moreover, ASFF only incurs 2 ms additional inference time, keeping the detector run efficiently with 46 FPS.</p><p>As described in Sec. 3.2, to adaptively fuse the features for each scale, the features at other levels are firstly resized to the same shape before fusion. To make fair comparison, we further report the accuracies of another two common YOLOv3 @608 BoF <ref type="bibr" target="#b42">[43]</ref> GA <ref type="bibr" target="#b37">[38]</ref>   fusion operations (i.e. element-wise sum and concatenation) with resized features in <ref type="table">Table 3</ref>. We can see in the table that, these two operations improve the accuracy on AP S and AP M as ASFF does, but they both sharply downgrade the performance on AP L . These results indicate that the inconsistency across different levels in feature pyramids  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ignore area AP AP50 AP70</p><p>YOLOv3 @608 brings negative influence on the training process and thus leaves the potential of pyramidal feature representation from being fully exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visual Analysis</head><p>In order to understand how the features are adaptively fused, we visualize some qualitative results in <ref type="figure" target="#fig_3">Figure 3</ref> and 4. The detection results are in the left column. The heat maps of the learned weight scalars and the fused feature activation maps at each level are in the right column. For fused feature maps, we sum up the values among all the channels to visualize the activation maps. The red numbers near the boxes indicate the fused feature level that detects the object. Note that the actual resolutions of the three levels are different, and we resize them to a uniform size for better visualization.</p><p>Specifically, in <ref type="figure" target="#fig_3">Figure 3</ref>, we investigate how ASFF works when all objects in the image have roughly the same size. It is also worth to note that YOLOv3 only takes the center point of the object in the corresponding feature maps as a positive. For the image in the first row, all the three zebras are predicted from the fused feature maps of level 1. It indicates that their center areas are dominated by the original features of level 1 and the resized features within those areas from level 2 and 3 are filtered out. This filtering guarantees that the features of these three zebras at level 2 and 3 are treated as background and do not receive positive gradients in training. Regarding ASFF, in the fusion process of level 2 and 3, the central areas at the resized features from level 1 are also filtered out, and the original features of level 1 will receive no negative gradients in training. For the image in the second row, all the sheeps are predicted by the fused feature maps of level 3. We zoom in the heat maps of level 3 within the red box for better visualization. In fusion, the features from level 1 are kept in the object areas as they contain stronger semantic information, and the features from level 3 are extracted around each object since they are more sensitive for localization.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>, we exhibit the images that have several objects of different sizes. Most of the fusion cases at the corresponding level are similar to the ones in <ref type="figure" target="#fig_3">Figure 3</ref>. Meanwhile, one may notice that the tennis racket in the second  image is predicted from level 1, but the heat maps show that the main features within its central area are taken from the resized feature of level 2. We speculate that although the tennis racket is predicted from level 1 due to heuristic size selection, the features from level 2 are more discriminative in detecting it since they contain richer clues of lines and shapes. Thanks to our ASFF module, the final feature can be adaptively learned from optimal fusion, which contributes in particular to detecting challenging objects. Please see more visual results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Other Single-Shot Detectors</head><p>To better evaluate the performance of the proposed approach, we carry out additional experiments with another representative single-shot detector, namely RetinaNet <ref type="bibr" target="#b20">[21]</ref>. First, we directly adopt the official implementation <ref type="bibr" target="#b26">[27]</ref> to reproduce the baseline. We then add ASFF behind the pyramid feature maps from P3 to P5 on FPN, similar to <ref type="figure" target="#fig_1">Figure 2</ref>. As shown in <ref type="table" target="#tab_4">Table 5</ref>, ASFF consistently increases the accuracy of RetinaNet with different backbones (i.e. ResNet-50 and ResNet-101).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to State of the Art</head><p>We evaluate our detector on the COCO test-dev split to compare with recent state-of-the-art methods in <ref type="table" target="#tab_3">Table 4</ref> final model is YOLOv3 with ASFF*, which is an enhanced ASFFversion by integrating other lightweight modules (i.e. DropBlock <ref type="bibr" target="#b6">[7]</ref> and RFB <ref type="bibr" target="#b22">[23]</ref>) with 1.5× longer training time than the models in Section 4.1. Keeping the high efficiency of YOLOv3, we successfully uplift its performance to the same level as the state-of-the-art single-shot detectors (e.g., FCOS <ref type="bibr" target="#b35">[36]</ref>, CenterNet <ref type="bibr" target="#b43">[44]</ref>, and NAS-FPN <ref type="bibr" target="#b7">[8]</ref>), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Note that YOLOv3 can be evaluated at different input resolutions with the same weights, and when we lower the resolution of input images to pursue much faster detector, ASFF improves the performance more significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work identifies the inconsistency across different feature scales as a primary limitation for single-shot detectors with feature pyramids. To address this, we propose a novel ASFF strategy which learns the adaptive spatial fu-sion weight to filter out the inconsistency during training. It significantly improves strong baselines with tiny inference overhead and achieves a state-of-the-art speed and accuracy trade-off among all single-shot detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Speed-accuracy trade-off on COCO test-dev for real-time detectors. The proposed ASFF helps YOLOv3 outperform a range of state-of-the-art algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the adaptively spatial feature fusion mechanism. For each level, the features of all the other levels are resized to the same shape and spatially fused according to the learned weight maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 ij ( 5 )</head><label>35</label><figDesc>Suppose position (i, j) at level 1 is assigned as the center of an object according to a certain scale matching mechanism and ∂L ∂y 1 ij is the gradient from the positive sample. As the corresponding positions are viewed as background in the other levels, from negative samples. This inconsistency disturbs the gradient of ∂L∂x 1 ij and downgrades the training efficiency of the original feature maps x 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of detection results on COCO val-2017 as well as the learned weight scalar maps at each level. We zoom in the heat maps of level 3 within the red box for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>More qualitative examples when one image has several objects with different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>IoU [41] AP AP 50 AP 70 AP S AP M AP Effect of each component on the baseline. Results in terms of AP (%) and FPS are reported on COCO val-2017.</figDesc><table><row><cell>L FPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Detection performance in terms of AP (%) and FPS on COCO test-dev.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Contribution of ASFF to RetinaNet. APs (%) are reported on COCO val-2017.</figDesc><table><row><cell>. Our</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soft-nms: improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weaving multi-scale context for single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03149</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Hongyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cisse</forename><surname>Moustapha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dauphin Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyong-Keun</forename><surname>Kook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jee-Young</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun-Cheon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01892</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning short-cut connections for object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Oñoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto J López-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptively connected neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Soonmin Hwang, and In So Kweon. Stairnet: Top-down semantic aggregation for accurate one shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bag of freebies for training object detection neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
