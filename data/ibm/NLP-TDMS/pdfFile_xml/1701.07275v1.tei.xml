<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advent of large labelled datasets and highcapacity models, the performance of machine vision systems has been improving rapidly. However, the technology has still major limitations, starting from the fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on the target data. The human visual system, in stark contrast, learns a universal representation for vision in the early life of an individual. This representation works well for an enormous variety of vision problems, with little or no change, with the major advantage of requiring little training data to solve any of them.</p><p>In this paper we investigate whether neural networks may work as universal representations by studying their capacity in relation to the "size" of a large combination of vision problems. We do so by showing that a single neural network can learn simultaneously several very different visual domains (from sketches to planktons and MNIST digits) as well as, or better than, a number of specialized networks. However, we also show that this requires to carefully normalize the information in the network, by using domainspecific scaling factors or, more generically, by using an instance normalization layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While the performance of machine vision systems is nowadays believed to be comparable or even superior to the one of human vision in certain tasks <ref type="bibr" target="#b15">[16]</ref>, the very narrow scope of these systems remains a major limitation. In fact, while vision in a human works well for an enormous variety of different problems, different neural networks are required in order to recognize faces <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>, classify <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref>, detect <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35]</ref> or segment <ref type="bibr" target="#b8">[9]</ref> high-level object categories, read text <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, recognize bird, flower, dog, or cat species <ref type="bibr" target="#b33">[34]</ref>, interpret a radiography, an echography, or a MRI image of different parts of the human anatomy <ref type="bibr" target="#b23">[24]</ref>, Humans posses an internal visual representation that, out of the box, works very well any number of visual domains, from objects and faces to planktons and characters. In this paper we investigate such universal representations by constructing neural networks that work simultaneously on many domains, learning to share common visual structure where no obvious commonality exists. Our goal is to contrast the capacity of such model against the total size of the combined vision problems. and so on. Differently from machines, humans develop a powerful internal representation of images in the early years of their development <ref type="bibr" target="#b0">[1]</ref>. While this representation is subject to slight refinements even later in life, it changes little. This is possible because the representation has a universal valence and works equally well for any number of problems, from reading text to recognizing people and contemplating art.</p><p>The existence of non-trivial general-purpose representations means that an significant part of vision can essentially be learned once for all. However, the nature and scope of such universal representations remains unclear. In this paper, we shed some light on this question by investigating to which extent deep neural networks can be shared between extremely diverse visual domains ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>We start our investigation by asking whether it is possible to learn neural networks simultaneously from a large number of different problems <ref type="figure" target="#fig_0">(Fig. 1</ref>). Several authors <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b57">58]</ref> have shown that neural networks can transfer knowledge between tasks through a process of adaptation called fine tuning. While this is encouraging, here we look at the much more challenging problem of learning a single network that works well for all the problems simultaneously.</p><p>Several authors have considered multi-task scenarios before us, where the task is to extract multiple labels from the same visual domain (e.g. image classification, object and part detection and segmentation, and boundary extraction, all in PASCAL VOC <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25]</ref>). Such tasks are expected to partially overlap since they look at the same object types. Our goal, instead, is to check whether extreme visual diversity still allows a sharing of information. In order to do so, we fix the labelling task to image classification, and look at combining numerous and diverse domains (e.g. text, faces, animals, objects, sketches, planktons, etc.).</p><p>While the setup is simple, it allows to investigate an important question: what is the capacity of models in relation to the "size" of the combination of multiple vision problems. If problems are completely independent, the total size should grow proportionally, which should be matched by an equally unbounded increase in model capacity. On the other hand, if problems overlap, then the complexity growth gradually slows down, allowing model complexity to catch up, so that, in the limit, universal representations become possible.</p><p>Our first contribution is to show, through careful experimentation (section 4), that the capacity of neural networks is large even when contrasted to the complexity generated by combining numerous and very diverse visual domains. For example, it is possible to share all layers of a CNN, including classification ones, between datasets as diverse as CIFAR-10, MNIST and SVHN, without loss in performance (section 4.1). In general, extensive sharing of parameters works very well for combination of up to ten diverse domains.</p><p>Our second contribution is to show that, while sharing is possible, it notelessly requires to normalize information carefully, in order to compensate for the different dataset statistics (section 3). We test various schemes, including domain-oriented batch and instance normalization, and find (section 4) that the best method uses domain-specific scaling parameters learned to compensate for the statistical differences between datasets. However, we also show that instance normalization can be used to construct a representation that works well for all domains while using a single set of parameters, without any domain-specific tuning at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer learning and domain adaptation. Our work is related to methods that transfer knowledge between different tasks and between same tasks from different domains.</p><p>Long et al. <ref type="bibr" target="#b35">[36]</ref> propose the Deep Adaptation Network, a multi-task and multi-branch network that matches hidden representations of task-specific layers in a reproducing kernel Hilbert space to reduce domain discrepancy. Misra et al. <ref type="bibr" target="#b36">[37]</ref> propose Cross-Stitch units that combine the activations from multiple networks and can be trained end-to-end. Ganin and Lempitsky <ref type="bibr" target="#b12">[13]</ref> and Tzeng et al. <ref type="bibr" target="#b53">[54]</ref> propose deep neural networks that are simultaneously optimized to obtain domain invariant hidden representations by maximising the confusion of domain classifiers. Yosinski et al. <ref type="bibr" target="#b57">[58]</ref> study transferability of features in deep neural networks between different tasks from a single domain. The authors investigate which layers of a pre-trained deep networks can be adapted to new tasks in a sequential manner. The previous work explore various methods to transfer between different networks, here we look at learning universal representations from very diverse domains with a single neural network.</p><p>Our work is also related to methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6]</ref> that transfer the information between networks. Hinton et al. <ref type="bibr" target="#b18">[19]</ref> propose a knowledge distillation method that transfers the information from an ensemble of models (teacher) to a single one (student) by enforcing it to generate similar predictions to the existing ones. Romero et al. <ref type="bibr" target="#b44">[45]</ref> extend this strategy by encouraging similarity between not only the predictions but also between intermediate hidden representations of different networks. Chen et al. <ref type="bibr" target="#b5">[6]</ref> address the slow process of sequential training both teacher and student networks from scratch. The authors accelerate the learning process by simultaneously training teacher and student networks. This line of work focuses on learning compact and accurate networks on the same task by transferring knowledge between different networks, while our work aims to learn a single network that can perform well in multiple domains.</p><p>Multi-task learning. Multi-task learning <ref type="bibr" target="#b3">[4]</ref> has been extensively studied over two decades by the machine learning community. It is based on the key idea that the tasks share a common representation which is jointly learnt along with the task specific parameters. Multi-task learning is applied to various computer vision problems and reported to achieve performance gains in object tracking <ref type="bibr" target="#b59">[60]</ref>, faciallandmark detection <ref type="bibr" target="#b60">[61]</ref>, surface normals and edge labels <ref type="bibr" target="#b56">[57]</ref>, object detection and segmentation <ref type="bibr" target="#b9">[10]</ref> object and part detection <ref type="bibr" target="#b2">[3]</ref>. In contrast to our work, multi-task learning typically focuses on different tasks in the same datasets.</p><p>Life-long learning. Never Ending Learning <ref type="bibr" target="#b37">[38]</ref> and Lifelong Learning <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b47">48]</ref> aim at learning many tasks sequentially while retaining the previously learnt knowledge. Terekhov et al. <ref type="bibr" target="#b51">[52]</ref> propose Deep Block-Modular Neural Networks that allow a previously trained network learn a new task by adding new nodes while freezing the original network parameters. Li and Hoiem <ref type="bibr" target="#b32">[33]</ref> recently proposed the Learning without Forgetting method that can learn a new task while retaining the responses of the original network on the new task. The main focus in this line of research is to preserve information about old tasks as new tasks are learned, while our work is aimed at exploring the capacity of models when multiple tasks are learned jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We call a representation a vectorial function φ : x → φ(x) ∈ R C mapping an image x ∈ R H×W ×3 to a Cdimensional code vector φ(x) (often this vector is also a 3D tensor). As representations we consider here deep convolutional neural networks (DCNNs). A DCNN can be decomposed as a sequence φ(x) = φ N • · · · • φ 2 • φ 1 (x) of linear and non-linear functions φ n , called layers, or, in more sophisticated cases <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b16">17]</ref>, as a feed-forward computational graph where such functions are used as nodes.</p><p>A concept that will be important later is the one of data batches. Neural networks are almost invariably learned by considering batches of example images together. Here we follow the standard practice of representing a batch of T images by adding a fourth index t = 1, . . . , T to the data tensors x ∈ R H×W ×3×T . As the information propagates through the network, all intermediate tensors are also batches of T data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning from multiple domains</head><p>Here we consider the problem of learning neural networks from multiple domains D 1 , . . . , D D . For simplicity, we limit ourselves to image classification problems.</p><formula xml:id="formula_0">Hence, a domain D d = (X d , Y d , p d , L d ) consists of an input space X d = R H d ×W d ×C d , a discrete label (output) space Y d = {1, 2, . . . , K d },</formula><p>an (unknown) joint probability distribution p d (x, y) over inputs and labels, and a loss function L d : Y × Y → R measuring the quality L d (y,ŷ) of a label predictionŷ against its ground truth value y. As usual, the quality of a predictorŷ</p><formula xml:id="formula_1">= φ d (x) is measured in terms of its expected risk E[L d (y, φ d (x))].</formula><p>For each domain, furthermore, we also have a training set</p><formula xml:id="formula_2">T d = {(x (1,d) , y (1,d) ), . . . , (x (N d ,d) , y (N d ,d) )} of N d train- ing pairs, which results in the empirical risk V d (φ d ) = 1 N d N d t=1 L d (y (t,d) , φ d (x (t,d) ))</formula><p>We also assume that a similar but disjoint validation set V d is available for each domain.</p><p>Our goal is to learn D predictors φ 1 , . . . , φ D , one for each task, in order to minimize their overall risk. While balancing different tasks is an interesting problem in its own right, here we simply choose to minimize the average risk across domains:</p><formula xml:id="formula_3">(φ * 1 , . . . , φ * D ) = argmin φ1,...,φ D λR(φ 1 , . . . , φ D ) + 1 D D d=1 V d (φ d ). (1)</formula><p>The term R(φ 1 , . . . , φ D ) encodes both regularization terms as well as hard constraints, defining the structure of the learning problem.</p><p>No sharing. As a baseline, separate neural networks are learned for each domain. This is obtained when the regularizer in Eq. 1 decomposes additively</p><formula xml:id="formula_4">R(φ 1 , . . . , φ D ) = D d=1 R(φ d ).</formula><p>In this case, there is no sharing between domains.</p><p>Feature sharing. The baseline is set against the case in which part of the neural networks φ d are shared. In the simplest instance, this means that one can write</p><formula xml:id="formula_5">φ 1 = φ 1 • φ 0 , . . . , φ D = φ D • φ 0 ,</formula><p>where φ 0 is a common subset of the networks. For example, following the common intuition that early layers of a neural networks have are less specialized and hence less domainspecific <ref type="bibr" target="#b6">[7]</ref>, φ 0 may contain all the early layers up to some depth, after which the different networks branch off. <ref type="bibr" target="#b0">1</ref> We call this ordinary feature sharing.</p><p>Adapted feature sharing. In this paper, we propose and study alternatives to ordinary feature sharing. More abstractly, we are interested in minimizing the difference between the individual representations φ 1 , . . . , φ D and a universal representation φ 0 used as a common blueprint. For example, when domains differ substantially in their statistics (e.g. text vs natural images), such differences may have a significant impact in the response of neurons, but it may be possible to compensate for such differences by slightly adjusting the representation parameters. Another intuition is that not all features in the universal representation φ 0 may be useful in all domains, so that some could be deactivated depending on the problem. In order to explore these ideas, we will consider the case in which representations</p><formula xml:id="formula_6">decompose as φ d (x) = φ d • φ 0 (x|w d ),</formula><p>where w d is a small number of domain-dependent parameters and φ 0 (·; w d ) is the universal representation blueprint. We call this adapted feature sharing.</p><p>For adapted feature sharing, we consider in particular an extremely simple form of parametrization for φ 0 (·; w d ) ( <ref type="figure" target="#fig_1">Fig. 2</ref>). In order to be able to adjust for different mean responses of neurons for each domain, as well as to potentially select subset of features, we consider adding a domain-dependent scaling factor s d and a bias b d after each convolutional or linear layer in a CNN. This is implemented by a scaling layer φ scale :</p><formula xml:id="formula_7">∀vuc : y vuc = [φ scale (x; s, b)] uvc = s c x vuc + b c .</formula><p>All together, the scale and bias parameters form collections</p><formula xml:id="formula_8">S = (s 1 , . . . , s D ) and B = (b 1 , . . . , b D ).</formula><p>Since all domains are trained jointly, we introduce also a muxer ( <ref type="figure" target="#fig_1">Fig. 2)</ref>, namely a layer that extracts the corresponding parameter set given the index d of the current domain:</p><formula xml:id="formula_9">φ mux (d; S, B) = (s d , b d ).</formula><p>For networks that include batch or instance normalization layers (Section 3.2), a scaling layer already follows each occurrence of such blocks. In this case, we simply adapt the corresponding parameters rather than introducing new scaling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Batch and instance normalization</head><p>Batch normalization (BN) <ref type="bibr" target="#b20">[21]</ref> is a simple yet powerful technique that can substantially improve the learnability of deep neural networks. The batch normalization layer is defined as</p><formula xml:id="formula_10">y vuct = [φ BN (x)] vuct = x vuct − µ c (x) σ 2 c (x) + .<label>(2)</label></formula><p>where the batch means µ and variances σ 2 are given by</p><formula xml:id="formula_11">µ c (x) = 1 HW T vut x vuct , σ 2 c (x) = 1 HW T vut (x vuct − µ c ) 2 .</formula><p>Recently, <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b1">2]</ref> noted that it is sometimes advantageous to simplify batch normalization further, and consider instead instance (or layer) normalization. The instance normalization (IN) layer, in particular, has almost exactly the same expression as Eq. 2, but mean and covariance are instance rather than batch averages:</p><formula xml:id="formula_12">y vuct = [φ IN (x)] vuct = x vuct − µ ct (x) σ 2 ct (x) + ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_13">µ ct (x) = 1 HW vu x vuct , σ 2 ct (x) = 1 HW vu (x vuct − µ ct (x)) 2 .</formula><p>In practice, both batch normalization and instance normalization layers are always immediately followed by a scaling layer ( <ref type="figure" target="#fig_1">Fig. 2</ref>). As discussed earlier in section 3.1, in this paper we consider either fixing the same scaling and bias parameters across domains, or make them domain specific.</p><p>Batch purity. When the model is trained or tested, batches are always pure, i.e. composed of data points from a single domain. This simplifies the implementation, and, most importantly, has an important effect on the BN layer. For a pure batch, BN can in fact aggressively normalize datasetspecific biases, which would not be possible for mixed batches. IN, instead, operates on an image-by-image basis, and is not affected by the choice of pure or mixed batches.</p><p>An important detail is how the BN and IN blocks are used in testing, after the network has been learned. Upon "deploying" the architecture for testing, the BN layers are usually removed by fixing means and variances to fixed averages accumulated over several training batches <ref type="bibr" target="#b20">[21]</ref>. Unless this is done, BN cannot be evaluated on individual images at test time; furthermore, removing BN usually slightly improves the test performance and is also slightly faster.</p><p>Dropping BN requires some care in our architecture due to the difference between pure batches from different domains D d . In the experiments, we test computing domainspecific means and variances (µ d , σ 2 d ), selected by a muxer from collections (U, Σ) ( <ref type="figure" target="#fig_1">Fig. 2</ref>), or share a single set of means and variances (µ, σ 2 ) between all domains. We also consider an alternative setting, BN+, in which BN is applied at training and test times unchanged. The disadvantage of BN+ is that it can only operate on pure batches and not single images; the advantage is that moments are estimated onthe-fly for each test batch instead of being pre-computed.</p><p>Note that the IN layer is similar to the BN+ layer in that it estimates means and variances on the fly, both at training and testing time, and applies unchanged in both phases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training regime</head><p>As noted above, training always consider pure batches. In more detail, all models are learned by means of SGD, alternating batches from each of the domain D i , in a roundrobin fashion. This automatically balances the datasets when these have different sizes, as learning visits an equal number of training samples for each domain, regardless of the different training set sizes. This corresponds to weighing the domain-specific loss functions equally.</p><p>This design also has some practical advantages. In our implementation, different domains are assigned to different GPUs. In this case, each GPU computes the model parameter gradients with respect to a pure batch extracted from a particular dataset. Gradients are then accumulated before the descent step.</p><p>Finally, note that architectures may only partially share features, up to some depth. Obviously, domain-specific parameters are updated only from the pure batches corresponding to that domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experiments focus on image classification problems in two scenarios. In the first one, different architectures and learning strategies are evaluated on a portfolio of 10 very diverse image classification datasets (section 4.1), from planktons to street numbers. For computational reasons, these experiments consider relatively small 64 × 64 pixels images and tens of thousands training images per domain. In the second scenario, we test similar ideas on larger datasets, including ImageNet, but in a less extensive manner due to the computational cost (section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Small datasets</head><p>Data. We choose 10 image classification tasks from very diverse domains including objects, hand-written digit and characters, pedestrians, sketches, traffic signs, planktons and house numbers. The dataset statistics are summarized in <ref type="table">Table 1</ref> and a few example images are given in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>In more detail, Animals with Attributes (AwA) <ref type="bibr" target="#b28">[29]</ref> contains 30475 images of 50 animal species. While the dataset is introduced for zero-shot learning, it provides class labels for each image. Caltech-256 <ref type="bibr" target="#b13">[14]</ref> is a standard object classification benchmark that consists of 256 object categories and an additional background class. CI-FAR10 <ref type="bibr" target="#b25">[26]</ref> consists of 60000 32 × 32 colour object classes in 10 classes. Daimler Mono Pedestrian Classification Benchmark <ref type="bibr" target="#b38">[39]</ref> contains a collection of pedestrian and non-pedestrian images. Pedestrians are cropped and resized to 18 × 36 pixels. The German Traffic Sign Recognition (GTSR) Benchmark <ref type="bibr" target="#b48">[49]</ref> contains cropped images of 43 traffic signs. Sizes of the traffic signs vary between 15 × 15 and 222 × 193 pixels. MNIST <ref type="bibr" target="#b29">[30]</ref> contains 70000 handwritten digits which are centred in 28 × 28 images. Omniglot <ref type="bibr" target="#b27">[28]</ref> consists of 1623 different handwritten characters from 50 different alphabets. The dataset is originally designed for one shot learning. Instead we include all the character categories in train and test time. Plankton imagery data <ref type="bibr" target="#b7">[8]</ref> is a classification benchmark that contains 30336 images of various organisms ranging from the smallest single-celled protists to copepods, larval fish, and larger jellies. Human Sketch dataset <ref type="bibr" target="#b11">[12]</ref> contains 20000 human sketches of every day objects such as "book", "car", "house", "sun". The Street View House Numbers (SVHN) <ref type="bibr" target="#b39">[40]</ref> is a real-world digit recognition dataset with around 70,000 images which are centred around a single character and resized into 32 × 32 pixels.</p><p>As the majority of datasets differ in terms of image resolutions and characteristics, images are resized to 64 × 64 pixels, greyscale ones are converted into RGB by setting the three channels to the same value. Though it would be possible to maintain the images in the original resolution, using a single scale simplifies the network design. Each dataset is also whitened, by subtracting its mean and dividing it by its standard deviation per channel. For the datasets that do not have a fixed train and test splits, we use 80% to 20% ratio for train and test data respectively.</p><p>Architectures. We choose to use the state-of-the-art Residual Networks <ref type="bibr" target="#b17">[18]</ref> due to their remarkable capacity and performance. More specifically, for this experiment we select the ResNet-38 model. This network has a stack of 4 residual units with 3 × 3 convolutions for each feature map size ({64, 32, 16, 8}) and with number of filters {16, 32, 128, 256} respectively. The network ends with a global average pooling layer and a fully connected layer followed by softmax for classification. As the majority of the datasets have a different number of classes, we use a dataset-specific fully connected layer in our experiments unless otherwise stated.</p><p>As explained in section 3.3, datasets are balanced by sampling batches from different ones in a round-robin fashion during training. We follow the same data augmentation strategy in <ref type="bibr" target="#b17">[18]</ref>, the 64 × 64 size whitened image is padded with 8 pixels on all sides and a 64×64 patch randomly sam-Dataset AwA</p><p>Caltech <ref type="table" target="#tab_0">CIFAR10  Daimler  GTSR  MNIST Omniglot Plankton Sketches SVHN   # classes  50  257  10  2  43  10  1623  121  250  10  # images  30k  31k  60k  49k  52k  70k  32k  30k  20k  99k  content  animal  object  object  pedestrian traffic sign  digit  character plankton  sketch  digit  Table 1</ref>  pled from the padded image or its horizontal flip. Note that as MNIST, Omniglot and SVHN contain digits and characters, we do not augment flipped images from these datasets. The networks are trained using stochastic gradient descent with momentum. The learning range is set to 0.1 and gradually reduced to 0.0001 after a short warm-up training with a learning rate of 0.01 as in <ref type="bibr" target="#b17">[18]</ref>. The weight decay and momentum are set to 0.9 and 0.0001 respectively. In test time, we use only the central crop of images and report percentages of top-1 error rate. Next, we experiment with sharing features up to different depths in the architectures. To this end, the ResNet-38 model φ d = φ d • φ 0 is modified to have a branch φ d for each task D, stemming from a common trunk φ 0 .</p><p>Baseline: no sharing. As a baseline, a different ResNet-38 model (i.e. φ d = φ d ) is trained from scratch for each dataset until convergence (25k iterations are sufficient) using a batch size of 128 images and BN. Although our focus is not obtaining state-of-the-art results but demonstrating the effectiveness of sharing a representation among different domains, the chosen CNN provides a good speed and performance trade-off and achieves comparable results to state-of-the-art methods (see <ref type="table" target="#tab_2">Table 3</ref>). The much deeper ResNet-1001 <ref type="bibr" target="#b17">[18]</ref> obtains 4.62% error rate in CIFAR-10 (compared to our 9.4%), DropConnect <ref type="bibr" target="#b55">[56]</ref> with a heavier multi-column network obtains 0.21% in MNIST (compared to our 0.3%) and Lee et al. <ref type="bibr" target="#b30">[31]</ref> report 1.69% (compared to our 3.7%) in SVHN by using more sophisticated pooling mechanisms. While the network yields relatively low error rates in the majority of the datasets, the absolute performance is less good in AwA and Caltech256. However, this is inline with the results reported in the literature, where good performance on such datasets was shown to require pre-training on a very large dataset such as Ima-geNet <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b58">59]</ref>. In short, this validates our ResNet-38 baseine as a good and representative architecture.</p><p>Full sharing. Next, we consider the opposite of no sharing and share all the parameters of the network (i.e. φ d = φ 0 ). A common belief is that only the relatively shallow layers of a CNN are shareable, whereas deeper ones are more domain-specific <ref type="bibr" target="#b31">[32]</ref>. Full sharing challenges this notion.</p><p>In this experiment, ResNet-38 is configured with BN with domain-specific scaling parameters (s d , b d ) and moments (µ d , σ 2 d ). A single CNN is trained on three domains, CIFAR, MNIST, and SVHN, because such domains happens to contain exactly 10 classes each. Although CIFAR objects and MNIST/SVHN digits have nothing in common, we randomly pair digits with objects. This allows to share all filter parameters, including the final classification layer, realising full sharing.</p><p>As shown in <ref type="table" target="#tab_0">Table 2</ref>, evaluated on the different datasets, the performance of this network is nearly the same as learning three independent models. This surprising result means that the model has sufficient capacity to learn classifiers that respond strongly either to a digit in MNIST or SVHN, or to an object in CIFAR, essentially learning an or operator. The question then is whether combining more problems together can eventually exceed the capacity of the network.</p><p>Deep sharing. Next, we experiment with sharing all layers except the last one, which performs classification. In this case, therefore, φ d is a single convolutional layer and φ 0 contains the rest of the network, including all but the last fully connected layer. This setup is similar to full sharing, but allows to combine classification tasks even when these do not have an equal number of classes.</p><p>The results in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_0">Table 2</ref> show that the shared CNN performs better than training domain-specific networks. Remarkably, this is true for all the tasks, and reduces the average error rate by 1%. Remarkably, this improvement is obtained while reducing the overall number of parameters by a factor of 10. Partial sharing. Here, we investigate whether there can be a benefit in specializing at least part of the shared model for individual tasks. We test two settings. In the first setting, the network has dataset-specific parameters in the shallower block (i.e. the first stack of 4 residual units) -this should be beneficial to compensate for different low-level statistics in the domains. In the second setting, instead, the network specializes the last block-this should be beneficial in order to capture different higher-level concepts in for different tasks. Interestingly, the results in <ref type="table" target="#tab_2">Table 3</ref> show that deep sharing is, for this choice of datasets and model, the best configuration. Specializing the last block is only marginally worse (−0.3%) and better (+0.9%) than specializing the first block. This may indicate that high-level specialization is preferable.</p><p>Network capacity. Experiments so far suggested that the model has sufficient capacity to accommodate all the tasks, despite their significant diversity. In fact, ten individual networks perform worse than a single, shared one. Next, we increase the capacity of the model, but we keep sharing all such parameters between tasks. In order to do so, we increase the number of convolutional filters twice ({64, 128, 256, 512}) and four times ({128, 256, 512, 1024}), which increases the number of parameters 4 and 16 times. Differently from learning 10 independent networks, this setup allows the model to better use the added capacity to accommodate the different tasks, reducing the mean error rate by 0.6% and 1.2% points, respectively. The fact that joint training can exploit the added capacity better suggests that the different domains overlap synergistically, despite their apparent differences.</p><p>Normalization strategies. So far, we have shown that learning a single CNN for the 10 domains is not only possible, but in fact preferable to learining individual models. However, this CNN used a specific normalization strategy, BN, as well as domain-specific scaling parameters (s d , b d ) and moments (µ d , σ 2 d ). In <ref type="table">Table 4</ref> we examine the importance of these design decisions. First, we note that BN with domain-agnostic scaling (s, b) and moments (µ d , σ 2 ) performs very poorly on the test set, comparable to random chance, clearly due to its inability to compensate for the large variance among the different domains. If BN is applied at test time (BN+), such that moments are computed on the fly but domain-agnostic scaling is still used, results are better but still poor (46.3%). Domain-agnostic scaling works well as long as at least the moments are domain-specific (27.3%). However, the best combination is to use domain-specific moments and scalings (25%). In contrast to BN, IN, which normalizes images individually, works just as well with domain-specific and domainagnostic scaling. The price to pay is a 5% drop in performance compared to BN with domain-specific parameters. However, this strategy has a significant practical advantage: IN with domain-agnostic scaling effectively uses only a single set of parameters, including all filter weights and scaling factors, for all domains. This suggest that such a representation may be applicable to novel domain without requiring any domain-specific tuning of the normalization parameters at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Large datasets</head><p>Data. In this part, we consider three large scale computer vision tasks: object classification in ImageNet <ref type="bibr" target="#b45">[46]</ref>, face identification in VGG-Face <ref type="bibr" target="#b41">[42]</ref>, and word classification in Synth90k <ref type="bibr" target="#b21">[22]</ref> dataset <ref type="table" target="#tab_4">(Table 5</ref>). ImageNet contains 1000 object categories and 1.2 million images. VGG-Face dataset consists of 2.6 million face images of 2622 different people which are centered and resized into a fixed height of ImageNet VGG-Face Synth90k 128 pixels and a variable width. The Synth90k dataset contains approximately 9 million synthetic images for a 90k word lexicon which are generated with a fixed height of 32 pixels and variable width. We show example images from these datasets in <ref type="table">Table 4</ref>.</p><p>Implementation details. ImageNet images are resized to 256 pixels on their shortest side and maintaining the original aspect ratio. During training, random 224 × 224, 112 × 56 and 32 × 128 patches are cropped from ImageNet, VGG-Face and Synth90k respectively. As different input sizes lead to different feature map sizes at the last convolutional layer (a 6 × 6 map for ImageNet down to a tiny 1 × 6 map for the smallest Synth90k images -see below), we share the convolutional feature maps among the three tasks but use domain-specific fully connected layers. In training, we augment the data by randomly cropping, flipping and varying aspect ratio with the exception that we do not flip the images from Synth90k as they contain words. At test time, we only use a single center crop and report top1-error rates. The best normalization strategy (BN with domain specific scaling) identified in 4.1 is used.</p><p>Results. We conduct two experiments. In the first one, a network is trained simultaneously on the ImageNet and VGG-Face datasets, with a significant difference in content as well as resolution between domains. We use an AlexNet model <ref type="bibr" target="#b26">[27]</ref>, adapt the dimensionality of the first fully connected layer (fc6) for the VGG-Face dataset, and train the networks from scratch. For this setting, the baseline is obtained by training two individual networks without any parameter sharing, obtaining 40.5% and 25.7% top-1 error rates on the ImageNet and VGG-Face respectively (see <ref type="table" target="#tab_4">Table 5</ref> -this is the same as published results). Sharing the convolutional weights between these tasks achieve comparable performance (there is a marginal drop of 1% accuracy), illustrating once more the high degree of shareability of such representations. In the second setting, we push the envelope by adding the Synth90k dataset which contains synthetically gener- ated words for 90k different word classes. For this experiment, we use the higher-capacity model VGG-M-128 from <ref type="bibr" target="#b4">[5]</ref>. This model has only 128 filters in the second to last fully connected layer (fc7), instead of 4096. As the Synth90k dataset contains 90k classes, having a small 128-dimensional bottleneck is necessary in order to maintain the size of the 90k classes classifier matrix (which is 128 × 90k) reasonable. Since Synth90k images are much smaller than the other two datasets, the last downsampling layer (pool5) is not used for this domain. Without parameter sharing, this network performs similarly to AlexNet (due to the bottleneck which partially offsets the higher capacity). As before, the convolutional layer parameters are shared, and the fully-connected layers parameters are not. The results (see the bottom row in Table 5) show that the capacity of the model is pushed to its limit. Performance on ImageNet and VGG-Face is still very good, with a minor hit of 2-3%, but there is a larger drop for Synth90k (26.9% error). Note that the total number of parameters in the joint network is a third of the sum of the individual network parameters. In order to have a fair comparison, we evaluate the performance of three independent models with a third of the parameters each (see the 4 th row in <ref type="table" target="#tab_4">Table 5</ref>). We show that the jointly trained model performs dramatically better than the individual models, despite the fact that the total number of parameters is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>As machine vision consolidates, the challenge of developing universal models that, similarly to human vision, can be trained once and solve a large variety of problems, will come into focus. A component of such systems will be universal representations, i.e. feature extractors that work well for all visual domains, despite the significant diversity of the latter.</p><p>In this paper, we have shown that standard deep neural networks are already capable of learning very differ-ent visual domains together, with a high degree of information sharing. However, we have also shown that successful sharing requires tuning certain normalization parameters in the networks, preferably by using domain-specific scaling factors, in order to compensate for inter-domain statistical shifts. Alternatively, techniques such as instance normalization can compensate for such difference on the fly, in a domain-agnostic manner.</p><p>Overall, our findings are very encouraging. Universal representations seem to be within the grasp of current technology, at least for a wide array of real-world problems. In fact, while our most convincing results have been obtained for smaller datasets, we believe that larger problems can be addressed just as successfully by a moderate increase of model capacity and other refinements to the technology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Humans posses an internal visual representation that, out of the box, works very well any number of visual domains, from objects and faces to planktons and characters. In this paper we investigate such universal representations by constructing neural networks that work simultaneously on many domains, learning to share common visual structure where no obvious commonality exists. Our goal is to contrast the capacity of such model against the total size of the combined vision problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>From left to right, three example modules: instance normalization, batch normalization, and batch normalization with domainspecific scaling building modules. The shaded blocks indicate learnable parameters. Other variants are tested, not shown for compactness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example images from various datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Example images from the large-scale datasets are shown in their relative sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Top-1 error rate (%) for three datasets. The top row is for individually trained networks per dataset. Deep sharing corresponds to sharing all the convolutional but the last classifier layer. Full sharing corresponds to sharing all parameters including final classifier parameters with domain-specific scale and bias parameters. Note that all three datasets have ten classes and this allows us to share classifier parameters.</figDesc><table><row><cell></cell><cell cols="3">CIFAR10 MNIST SVHN</cell></row><row><cell>No sharing</cell><cell>9.4</cell><cell>0.34</cell><cell>3.7</cell></row><row><cell>Deep sharing</cell><cell>10.2</cell><cell>0.37</cell><cell>3.7</cell></row><row><cell>Full sharing</cell><cell>10.2</cell><cell>0.38</cell><cell>3.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Statistics of various datasets.</figDesc><table><row><cell>AwA</cell><cell>Caltech</cell><cell>CIFAR10</cell><cell>Daimler</cell><cell>GTSR</cell><cell>MNIST</cell><cell>Omniglot</cell><cell>Plankton</cell><cell>Sketches</cell><cell>SVHN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Top-1 error rate(%) for various tasks. The table shows the results in case of no feature sharing between different domains (first row), deep feature sharing of all convolutional weights (deep), partial sharing for selected convolutional weights in block 1-3 and block 2-4 and deep sharing with more convolutional filters (×2 and 4× number of filters).</figDesc><table><row><cell>Sharing</cell><cell cols="11">AwA Caltech CIFAR10 Daimler GTSR MNIST Omniglot Plankton Sketches SVHN mean</cell></row><row><cell>no sharing</cell><cell>77.9</cell><cell>85.1</cell><cell>9.4</cell><cell>10.4</cell><cell>4.2</cell><cell>0.34</cell><cell>13</cell><cell>25.7</cell><cell>31.1</cell><cell>3.7</cell><cell>26.1</cell></row><row><cell>deep</cell><cell>73.7</cell><cell>82.6</cell><cell>11.7</cell><cell>5.2</cell><cell>3.5</cell><cell>0.38</cell><cell>13.1</cell><cell>24.9</cell><cell>30.8</cell><cell>4.1</cell><cell>25.0</cell></row><row><cell>partial (block 1-3)</cell><cell>76.6</cell><cell>84.0</cell><cell>12.4</cell><cell>7.0</cell><cell>4.0</cell><cell>0.29</cell><cell>13.8</cell><cell>26.2</cell><cell>33.1</cell><cell>4.4</cell><cell>26.2</cell></row><row><cell>partial (block 2-4)</cell><cell>76.6</cell><cell>84.0</cell><cell>9.9</cell><cell>7.7</cell><cell>3.3</cell><cell>0.49</cell><cell>12.1</cell><cell>25.3</cell><cell>30.7</cell><cell>3.5</cell><cell>25.3</cell></row><row><cell>deep (×2 params)</cell><cell>74.0</cell><cell>81.7</cell><cell>9.1</cell><cell>4.2</cell><cell>4.1</cell><cell>0.43</cell><cell>12.3</cell><cell>25.8</cell><cell>29.1</cell><cell>3.5</cell><cell>24.4</cell></row><row><cell>deep (×4 params)</cell><cell>73.1</cell><cell>81.5</cell><cell>7.2</cell><cell>4.1</cell><cell>3.5</cell><cell>0.35</cell><cell>11.8</cell><cell>25</cell><cell>27.6</cell><cell>3.5</cell><cell>23.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Top-1 error rate (%). The first and second rows show the results for the AlexNet without and with sharing parameters on ImageNet and VGG-Face datasets respectively. The third and fourth depict the results for a VGG-M-128 without parameter sharing in different capacities. 1 /3 indicates the number of parameters reduced to a thir in each individual network. The last one show the results for the same network with sharing parameters on Ima-geNet, VGG-Face and MJSynth datasets respectively.</figDesc><table><row><cell></cell><cell cols="3">ImageNet VGG-Face Synth90k</cell></row><row><cell>No sharing</cell><cell>40.5</cell><cell>25.7</cell><cell>-</cell></row><row><cell>Deep (conv1-5)</cell><cell>41.8</cell><cell>26.3</cell><cell>-</cell></row><row><cell>No sharing</cell><cell>40.8</cell><cell>25.7</cell><cell>12.1</cell></row><row><cell>No sharing ( 1 / 3 )</cell><cell>49.2</cell><cell>57.1</cell><cell>39.0</cell></row><row><cell>Deep (conv1-5)</cell><cell>43.0</cell><cell>27.8</cell><cell>26.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Such a constraint can be incorporated in Eq. 1 by requiring that R(φ 1 , . . . , φ d ) &lt; ∞ ⇔ ∃φ 0 : ∀d = 1, . . . , D ∃φ d :φ d = φ d • φ 0 .</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Developing Visual Brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. In arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05641</idno>
		<title level="m">Net2net: Accelerating learning via knowledge transfer</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6836</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Planktonset 1.0: Plankton imagery data collected from f.g. walton smith in straits of florida from 2014-06-03 to 2014-06-06 and used in the 2015 national data science bowl. NODC Accession 127422</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sponaugle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<idno>44:1- 44:10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852v1</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faces in places: Compound query retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spinenet: Automatically pinpointing classification evidence in spinal mris</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ubernet: Training auniversal&apos;convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02132</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1502.02791</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">DTIC Document</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An experimental study on pedestrian classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Munder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1863" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CORR</title>
		<meeting>CORR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832v1</idno>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifelong machine learning systems: Beyond learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Lifelong Machine Learning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Knowledge transfer in deep block-modular neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Terekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Oregan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Biomimetic and Biohybrid Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? Proc. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
