<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa</forename><forename type="middle">L</forename><surname>Powalski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Borchmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Adam Mickiewicz University</orgName>
								<address>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Adam Mickiewicz University</orgName>
								<address>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><forename type="middle">L</forename><surname>Pietruszka</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Jagiellonian University</orgName>
								<address>
									<settlement>Cracow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><forename type="middle">Pa</forename><surname>Lka</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Adam Mickiewicz University</orgName>
								<address>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 Applica.ai</addrLine>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Natural Language Processing · Transfer learning · Docu- ment understanding · Layout analysis · Deep learning · Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the challenging problem of Natural Language Comprehension beyond plain-text documents by introducing the TILT neural network architecture which simultaneously learns layout information, visual features, and textual semantics. Contrary to previous approaches, we rely on a decoder capable of unifying a variety of problems involving natural language. The layout is represented as an attention bias and complemented with contextualized visual information, while the core of our model is a pretrained encoder-decoder Transformer. Our novel approach achieves state-of-the-art results in extracting information from documents and answering questions which demand layout understanding (DocVQA, CORD, WikiOps, SROIE). At the same time, we simplify the process by employing an end-to-end model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most tasks in Natural Language Processing (NLP) can be unified under one framework by casting them as triplets of the question, context, and answer [30, <ref type="bibr" target="#b17">40,</ref><ref type="bibr">27]</ref>. We consider such unification of Document Classification, Key Information Extraction, and Question Answering in a demanding scenario where context extends beyond the text layer. This challenge is prevalent in business cases since contracts, forms, applications, and invoices cover a wide selection of document types and complex spatial layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Importance of Spatio-Visual Relations</head><p>The most remarkable successes achieved in NLP involved models that map raw textual input into raw textual output, which usually were provided in a digital form. An important aspect of real-world oriented problems is the presence of scanned paper records and other analog materials that became digital.</p><p>Consequently, there is no easily accessible information regarding the document layout or reading order, and these are to be determined as part of the process. Furthermore, interpretation of shapes and charts beyond the layout may help answer the stated questions. A system cannot rely solely on text but requires incorporating information from the structure and image. 95 90 PERCENT MORTALITY 70 90-DOSE TEST 70 50 30 10 5 <ref type="figure">Fig. 1</ref>. The same document perceived differently depending on modalities. Respectively: its visual aspect, spatial relationships between the bounding boxes of detected words, and unstructured text returned by OCR under the detected reading order.</p><p>Thus, it takes three to solve this fundamental challenge -the extraction of key information from richly formatted documents lies precisely at the intersection of NLP, Computer Vision, and Layout Analysis <ref type="figure">(Figure 1</ref>). These challenges impose extra conditions beyond NLP that we sidestep by formulating layoutaware models within an encoder-decoder framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Limitations of Sequence Labeling</head><p>Sequence labeling models can be trained in all cases where the token-level annotation is available or can be easily obtained. Limitations of this approach are strikingly visible on tasks framed in either key information extraction or property extraction paradigms <ref type="bibr">[20,</ref><ref type="bibr" target="#b8">9]</ref>. Here, no annotated spans are available, and only property-value pairs are assigned to the document. Occasionally, it is expected from the model to mark some particular subsequence of the document. However, problems where the expected value is not a substring of the considered text are unsolvable assuming sequence labeling methods ( <ref type="table" target="#tab_0">Table 1)</ref>. As a result, authors applying state-of-the-art entity recognition models were forced to rely on human-made heuristics and time-consuming rule engineering.</p><p>Particular problems one has to solve when employing a sequence-labeling method can be divided into three groups. We investigate them below to precisely point out the limitations of this approach. Take, for example, the total amount assigned to a receipt in the SROIE dataset <ref type="bibr">[20]</ref>. Suppose there is no exact match for the expected value in the document, e.g., due to an OCR error, incorrect reading order or the use of a different decimal separator. Unfortunately, a sequence labeling model cannot be applied off-the-shelf. Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results <ref type="bibr" target="#b33">[56,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b32">55,</ref><ref type="bibr">37]</ref>. Moreover, when receipts with one item listed are considered, the total amount is equal to a single item price, which is the source of yet another problem. Precisely, if there are multiple matches for the value in the document, it is ambiguous whether to tag all of them, part or none.</p><p>Another problem one has to solve is which and how many of the detected entities to return, and whether to normalize the output somehow. Consequently, the authors of Kleister proposed a set of handcrafted rules for the final selection of the entity values <ref type="bibr" target="#b11">[12]</ref>. These and similar rules are either labor-intensive or prone to errors <ref type="bibr" target="#b18">[41]</ref>.</p><p>Finally, the property extraction paradigm does not assume the requested value appeared in the article in any form since it is sufficient for it to be inferable from the content, as in document classification or non-extractive question answering <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Resorting to Encoder-Decoder Models</head><p>Since sequence labeling-based extraction is disconnected from the final purpose the detected information is used for, a typical real-world scenario demands the setting of Key Information Extraction.</p><p>To address this issue, we focus on the applicability of the encoder-decoder architecture since it can generate values not included in the input text explicitly [17] and performs reasonably well on all text-based problems involving natural language <ref type="bibr" target="#b22">[45]</ref>. Additionally, it eliminates the limitation prevalent in sequence labeling, where the model output is restricted by the detected word order, previously addressed by complex architectural changes (Section 2).</p><p>Furthermore, this approach potentially solves all identified problems of sequence labeling architectures and ties various tasks, such as Question Answering or Text Classification, into the same framework. For example, the model may deduce to answer yes or no depending on the question form only. Its end-to-end elegance and ease of use allows one to not rely on human-made heuristics and to get rid of time-consuming rule engineering required in the sequence labeling paradigm.</p><p>Obviously, employing a decoder instead of a classification head comes with some known drawbacks related to the autoregressive nature of answer generation. This is currently investigated, e.g., in the Neural Machine Translation context, and can be alleviated by methods such as lowering the depth of the decoder <ref type="bibr" target="#b25">[48,</ref><ref type="bibr">25]</ref>. However, the datasets we consider have target sequences of low length; thus, the mentioned decoding overhead is mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We aim to bridge several fields, with each of them having long-lasting research programs; thus, there is a large and varied body of related works. We restrict ourselves to approaches rooted in the architecture of Transformer <ref type="bibr" target="#b31">[54]</ref> and focus on the inclusion of spatial information or different modalities in text-processing systems, as well as on the applicability of encoder-decoder models to Information Extraction and Question Answering.</p><p>Spatial-aware Transformers. Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings <ref type="bibr">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">56]</ref> or indirectly by allowing them to be contextualized on their spatial neighborhood <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">57,</ref><ref type="bibr" target="#b15">16]</ref>. Further improvements focused on the training and inference aspects by the inclusion of the area masking loss function or achieving independence from sequential order in decoding respectively <ref type="bibr">[19,</ref><ref type="bibr">21]</ref>. In contrast to the mentioned methods, we rely on a bias added to self-attention instead of positional embeddings and propose its generalization to distances on the 2D plane. Additionally, we introduce a novel word-centric masking method concerning both images and text. Moreover, by resorting to an encoder-decoder, the independence from sequential order in decoding is granted without dedicated architectural changes.</p><p>Encoder-decoder for IE and QA. Most NLP tasks can be unified under one framework by casting them as Language Modeling, Sequence Labeling or Question Answering <ref type="bibr" target="#b21">[44,</ref><ref type="bibr">26]</ref>. The QA program of unifying NLP frames all the problems as triplets of question, context and answer [30, <ref type="bibr" target="#b17">40,</ref><ref type="bibr">27]</ref> or item, property name and answer <ref type="bibr">[17]</ref>. Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture <ref type="bibr" target="#b31">[54,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">45]</ref>. The T5 is a prominent example of large-scale Transformers achieving state-of-the-art results on varied NLP benchmarks <ref type="bibr" target="#b22">[45]</ref>. We extend this approach beyond the text-to-text scenario by making it possible to consume a multimodal input.</p><p>Multimodal Transformers. The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, Video-Grounded Dialogue, Speech, and Visual Question Answering <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b2">3]</ref>. In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them <ref type="bibr">[38,</ref><ref type="bibr">34,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b30">53,</ref><ref type="bibr" target="#b33">56]</ref>. We differ from the mentioned approaches, as in our model, visual features added to word embeddings are already contextualized on an image's multiple resolution levels (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>Our starting point is the architecture of the Transformer, initially proposed for Neural Machine Translation, which has proven to be a solid baseline for all generative tasks involving natural language <ref type="bibr" target="#b31">[54]</ref>.</p><p>Let us begin from the general view on attention in the first layer of the Transformer. If n denotes the number of input tokens, resulting in a matrix of embeddings X, then self-attention can be seen as:</p><formula xml:id="formula_0">softmax Q X K X √ n + B V X<label>(1)</label></formula><p>where Q X , K X and V X are projections of X onto query, keys, and value spaces, whereas B stands for an optional attention bias. There is no B term in the original Transformer, and information about the order of tokens is provided explicitly to the model, that is: where S and P are respectively the semantic embeddings of tokens and positional embedding resulting from their positions <ref type="bibr" target="#b31">[54]</ref>. 0 n×d denote a zero matrix.</p><formula xml:id="formula_1">X = S + P B = 0 n×d (A) Vanilla Transformer (B) T5 Architecture K Q V (C) Our model Pairwise 1+2D distances Semantics Contextualized Vision × × + + Sequential word index K Q V Semantics × × + K Q V Pairwise sequential distances Semantics × × +</formula><p>In contrast to the original formulation, we rely on relative attention biases instead of positional embeddings. These are further extended to take into account spatial relationships between tokens <ref type="figure" target="#fig_1">(Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial Bias</head><p>Authors of the T5 architecture disregarded positional embeddings <ref type="bibr" target="#b22">[45]</ref>, by setting X = S. They used relative bias by extending self-attention's equation with the sequential bias term B = B 1D , a simplified form of positional signal inclusion. Here, each logit used for computing the attention head weights has some learned scalar added, resulting from corresponding token-to-token offsets.</p><p>We extended this approach to spatial dimensions. In our approach, biases for relative horizontal and vertical distances between each pair of tokens are calculated and added to the original sequential bias, i.e.:</p><formula xml:id="formula_2">B = B 1D + B H + B V</formula><p>Such bias falls into one of 32 buckets, which group similarly-distanced tokenpairs. The size of the buckets grows logarithmically so that greater token pair distances are grouped into larger buckets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amount:</head><p>100.00 2020 relative distance <ref type="figure">Fig. 4</ref>. Document excerpt with distinguished vertical buckets for the Amount token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextualized Image Embeddings</head><p>Contextualized Word Embeddings are expected to capture context-dependent semantics and return a sequence of vectors associated with an entire input sequence <ref type="bibr" target="#b9">[10]</ref>. We designed Contextualized Image Embeddings with the same objective, i.e., they cover the image region semantics in the context of its entire visual neighborhood.</p><p>Visual features. To produce image embeddings, we use a convolutional network that consumes the whole page image of size 512×384 and produces a feature map of 64×48×128. We rely on U-Net as a backbone visual encoder network <ref type="bibr" target="#b26">[49]</ref> since this architecture provides access to not only the information in the near neighborhood of the token, such as font and style but also to more distant regions of the page, which is useful in cases where the text is related to other structures, i.e., is the description of a picture. This multi-scale property emerges from the skip connections within chosen architecture ( <ref type="figure" target="#fig_2">Figure 5</ref>). Then, each token's bounding box is used to extract features from U-Net's feature map with ROI pooling <ref type="bibr" target="#b4">[5]</ref>. The obtained vector is then fed into a linear layer which projects it to the model embedding dimension. Embeddings. In order to inject visual information to the Transformer, a matrix of contextualized image-region embeddings U is added to semantic embeddings, i.e. we define X = S + U in line with the convention from Section 3 (see <ref type="figure" target="#fig_1">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Regularization Techniques</head><p>In the sequence labeling scenario, each document leads to multiple training instances (token classification), whereas in Transformer sequence-to-sequence mod-els, the same document results in one training instance with feature space of higher dimension (decoding from multiple tokens).</p><p>Since most of the tokens are irrelevant in the case of Key Information Extraction and contextualized word embeddings are correlated by design, one can suspect our approach to overfit easier than its sequence labeling counterparts. To improve the model's robustness, we introduced a regularization technique for each modality.</p><p>Case Augmentation. Subword tokenization <ref type="bibr" target="#b27">[50,</ref><ref type="bibr">29]</ref> was proposed to solve the word sparsity problem and keep the vocabulary at a reasonable size. Although the algorithm proved its efficiency in many NLP fields, the recent work showed that it performs poorly in the case of an unusual casing of text <ref type="bibr" target="#b20">[43]</ref>, for instance, when all words are uppercased. The problem occurs more frequently in formated documents (FUNSD, CORD, DocVQA), where the casing is an important visual aspect. We overcome both problems with a straightforward regularization strategy, i.e., produce augmented copies of data instances by lower-casing or upper-casing both the document and target text simultaneously.</p><p>Spatial Bias Augmentation. Analogously to Computer Vision practices of randomly transforming training images, we augment spatial biases by multiplying the horizontal and vertical distances between tokens by a random factor. Such transformation resembles stretching or squeezing document pages in horizontal and vertical dimensions. Factors used for scaling each dimension were sampled uniformly from range [0.8, 1.25].</p><p>Affine Vision Augmentation. To account for visual deformations of realworld documents, we augment images with affine transformation, preserving parallel lines within an image but modifying its position, angle, size, and shear. When we perform such modification to the image, the bounding box of every token is updated accordingly. The exact hyperparameters were subject to an optimization. We use 0.9 probability of augmenting and report the following boundaries for uniform sampling work best: [−5, 5] degrees for rotation angle, [−5%, 5%] for translation amplitude, [0.9, 1.1] for scaling multiplier, [−5, 5] degrees for the shearing angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts. The following datasets represented the broad spectrum of tasks and were selected for the evaluation process (see <ref type="table">Table 2</ref> for additional statistics).</p><p>Datasets. The CORD dataset <ref type="bibr" target="#b19">[42]</ref> includes images of Indonesian receipts collected from shops and restaurants. The dataset is prepared for the information extraction task and consists of four categories, which fall into thirty subclasses. The main goal of the SROIE dataset [20] is to extract values for four categories (company, date, address, total) from scanned receipts. The DocVQA dataset <ref type="bibr" target="#b16">[39]</ref> is focused on the visual question answering task. The RVL-CDIP dataset <ref type="bibr" target="#b14">[15]</ref> contains gray-scale images and assumes classification into 16 categories such as letter, form, invoice, news article, and scientific publication. The WikiOps dataset <ref type="bibr" target="#b0">[1]</ref> consists of tables extracted from Wikipedia and natural language questions corresponding to them. Each has an operand information assigned. For DocVQA, we relied on Amazon Textract OCR; for RVL-CDIP, we used Microsoft Azure OCR, and for WikiOps, SROIE and CORD, we depended on the original OCR.  <ref type="table">Table 2</ref>. Comparison of datasets considered for supervised pretraining and evaluation process. Statistics given in thousands of documents or questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Procedure</head><p>The training procedure consists of three steps. First, the model is initialized with vanilla T5 model weights and is pretrained on numerous documents in an unsupervised manner. It is followed by training on a set of selected supervised tasks. Finally, the model is finetuned solely on the dataset of interest. We trained two size variants of TILT models, starting from T5-Base and T5-Large models.</p><p>Our models grew to 230M and 780M parameters due to the addition of Visual Encoder weights.</p><p>Unsupervised Pretraining. We constructed a corpus of documents with rich structure, based on RVL-CDIP (275k docs), UCSF Industry Documents Library (480k), * and PDF files from Common Crawl (350k). The latter were filtered according to the score obtained from a simple SVM business document classifier. Then, a T5-like masked language model pretraining objective is used, but in a salient span masking scheme, i.e., named entities are preferred rather than random tokens <ref type="bibr" target="#b22">[45,</ref><ref type="bibr" target="#b12">13]</ref>. Additionally, regions in the image corresponding to the randomly selected text tokens are masked with the probability of 80%. Models are trained for 100, 000 steps with batch size of 64, AdamW optimizer and linear scheduler with an initial learning rate of 2e − 4. At this stage, the model is trained on each dataset for 10,000 steps or 5 epochs, depending on the dataset size: the goal of the latter condition was to avoid a quick overfitting.</p><p>We estimated each dataset's value concerning a downstream task, assuming a fixed number of pretraining steps followed by finetuning. The results of this investigation are demonstrated in <ref type="figure">Figure 6</ref>, where the group of WikiTable, WikiOps, SQuAD, and infographicsVQA performed robustly, convincing us to rely on them as a solid foundation for further experiments.</p><p>Model pretrained in unsupervised, and then supervised manner, is at the end finetuned for two epochs on a downstream task with AdamW optimizer and hyperparameters presented in <ref type="table">Table 3</ref>.  <ref type="table">Table 3</ref>. Parameters used during the finetuning on a downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The TILT model achieved state-of-the-art results on four out of five considered tasks <ref type="table" target="#tab_4">(Table 4</ref>). We have confirmed that unsupervised layout-and vision-aware pretraining leads to good performance on downstream tasks that require comprehension of tables and other structures within the documents. Additionally, we successfully leveraged supervised training from both plain-text datasets and these involving layout information.</p><p>DocVQA. We improved SOTA results on this dataset by 0.33 points. Moreover, detailed results show that model gained the most in CORD. Since the complete inventory of entities is not present in all examples, we force the model to generate a None output for missing entities. Our model achieved SOTA results on this challenge and improved the previous best score by 0.3 points. Moreover, after the manual review of the model errors, we noticed that model's score could be higher since the model output and the reference differ insignificantly e.g. "2.00 ITEMS" and "2.00".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SROIE.</head><p>Following the same evaluation procedure as the top submission (LAM-BERT), we excluded OCR mismatches and fixed total entity annotations discrepancies. We achieved results indistinguishable from the SOTA (98.10 vs. 98.17).</p><p>Significantly better results are impossible due to ocr mismatches in the test-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation study</head><p>In the following section, we analyze the design choices in our architecture, considering the base model pretrained in an unsupervised manner and the same hyperparameters for each run. The DocVQA was used as the most representative and challenging for Document Intelligence since its leaderboard reveals a large gap to human performance. We report average results over two runs of each model varying only in the initial random seed to account for the impact of different initialization and data order <ref type="bibr" target="#b6">[7]</ref>. Significance of Modalities. We start with the removal of the 2D layout positional bias. <ref type="table" target="#tab_5">Table 5</ref> demonstrates that information that allows models to recognize spatial relations between tokens is a crucial part of our architecture. It is consistent with the previous works on layout understanding <ref type="bibr" target="#b32">[55,</ref><ref type="bibr" target="#b10">11]</ref>. Removal of the UNet-based convolutional feature extractor results in a less significant ANLS decrease than the 2D bias. This permits the conclusion that contextualized image embeddings are beneficial to the encoder-decoder.</p><p>Justifying Regularization. Aside from removing modalities from the network, we can also exclude regularization techniques. To our surprise, the results suggest that the removal of case augmentation decreases performance most severely. Our baseline is almost one point better than the equivalent non-augmented model. Simultaneously, model performance tends to be reasonably insensitive to the bounding boxes' and image alterations. It was confirmed that other modalities are essential for the model's success on real-world data, whereas regularization techniques we propose slightly improve the results, as they prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>In this paper, we introduced a novel encoder-decoder framework for layout-aware models. Compared to the sequence labeling approach, the proposed method achieved better results while operating in an end-to-end manner. Moreover, the framework can handle various tasks such as Key Information Extraction, Question Answering or Document Classification, while the need for complicated preprocessing and postprocessing steps is eliminated. We established state-ofthe-art results on three datasets (DocVQA, CORD, WikiOps) and performed on par with the previous best scores on SROIE and RVL-CDIP, albeit having a much simpler workflow. Spatial and image enrichment of the Transformer model allowed the TILT to combine information from text, layout, and image modalities. We showed that the proposed regularization methods significantly improve the results.</p><p>17. Hewlett, D., Lacoste, A., Jones, L., Polosukhin, I., Fandrianto, A., Han, J., Kelcey, M., Berthelot, D.: WikiReading: A novel large-scale language understanding task over Wikipedia. In: ACL (2016) 18. Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multidimensional transformers (2019) 19. Hong, T., Kim, D., Ji, M., Hwang, W., Nam, D., Park, S.: BROS: A pre-trained language model for understanding texts in document (2021) 20. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.: IC-DAR2019 competition on scanned receipt OCR and information extraction. In: ICDAR (2019) 21. Hwang, W., Yim, J., Park, S., Yang, S., Seo, M.: Spatial dependency parsing for semi-structured document information extraction (2020) 22. Jaume, G., Ekenel, H.K., Thiran, J.P. H.: UnifiedQA: Crossing format boundaries with a single QA system. In: EMNLP-Findings (2020) 28. Khot, T., Clark, P., Guerquin, M., Jansen, P., Sabharwal, A.: QASC: A dataset for question answering via sentence composition. In: AAAI (2020) 29. Kudo, T.: Subword regularization: Improving neural network translation models with multiple subword candidates. In: ACL (2018) 30. Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., Zhong, V., Paulus, R., Socher, R.: Ask me anything: Dynamic memory networks for natural language processing. In: ICML (2016) 31. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: A benchmark for question answering research. TACL (2019) 32. Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale ReAding comprehension dataset from examinations. In: EMNLP (2017) 33. Le, H., Sahoo, D., Chen, N., Hoi, S.: Multimodal transformer networks for end-toend video-grounded dialogue systems. In: ACL (2019) 34. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text matching (2018) 35. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In: ACL (2020) 36. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: VisualBERT: A simple and performant baseline for vision and language (2019) 37. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal information extraction from visually rich documents. In: NAACL-HLT (2019) 38. Ma, J., Qin, S., Su, L., Li, X., Xiao, L.: Fusion of image-text attention for transformer-based multimodal machine translation. In: IALP (2019)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Our work in relation to encoder-decoder models, multi-modal transformers, and models for text that are able to comprehend spatial relationships between words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(A) In the original Transformer, information about the order of tokens is provided explicitly to the model by positional embeddings added to semantic embeddings. (B) T5 introduces sequential bias, thus separating semantics from sequential distances. (C) We maintain this clear distinction, extending biases with spatial relationships and providing additional image semantics at the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Truncated U-Net network. conv max-pool up-conv residual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>: FUNSD: A dataset for form understanding in noisy scanned documents. In: ICDAR-OST (2019) 23. Kafle, K., Price, B.L., Cohen, S., Kanan, C.: DVQA: understanding data visualizations via question answering. In: CVPR (2018) 24. Kahou, S.E., Michalski, V., Atkinson, A., Kádár,Á., Trischler, A., Bengio, Y.: FigureQA: An annotated figure dataset for visual reasoning. In: ICLR (2018) 25. Kasai, J., Pappas, N., Peng, H., Cross, J., Smith, N.A.: Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation (2020) 26. Keskar, N., McCann, B., Xiong, C., Socher, R.: Unifying question answering and text classification via span extraction (2019) 27. Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Hajishirzi,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of extraction tasks. Expected values are always present in a substring of a document in NER, but not elsewhere. Our estimation.</figDesc><table><row><cell>Task</cell><cell></cell><cell>Annotation</cell><cell>Exact match</cell><cell>Layout</cell></row><row><cell>CoNLL 2003</cell><cell></cell><cell>word-level</cell><cell>100%</cell><cell>−</cell></row><row><cell>SROIE WikiReading</cell><cell> </cell><cell>document-level</cell><cell>93% 20%</cell><cell>+ −</cell></row><row><cell>Kleister</cell><cell></cell><cell></cell><cell>27%</cell><cell>+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 6. Scores on CORD, DocVQA, SROIE, WikiOps and RVL-CDIP compared to the baseline without supervised pretraining. The numbers represent the differences in the metrics, orange text denote datasets chosen for the final supervised pretraining run.Supervised Training. To obtain a general-purpose model which can reason about documents with rich layout features, we constructed a dataset relying on a large group of tasks, representing diverse types of information conveyed by a document (seeTable 2for datasets comparison). Datasets, which initially had been plain-text, had their layout produced, assuming some arbitrary font size and document dimensions. Some datasets, such as WikiTable Questions, come with original HTML code -for the others, we render text alike. Finally, an image and computed bounding boxes of all words are used.</figDesc><table><row><cell>CORD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>RVL CDIP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SROIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>WikiOps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>docVQA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>docVQA</cell><cell>WikiOps</cell><cell>SROIE</cell><cell>RVL CDIP</cell><cell>CORD</cell><cell>WikiTable Questions</cell><cell>WikiReading</cell><cell>TyDi QA</cell><cell>TextVQA</cell><cell>TextCaps</cell><cell>SQuAD 1.1</cell><cell>RACE Middle</cell><cell>RACE High</cell><cell>QuAC</cell><cell>QASC</cell><cell>Natural Questions</cell><cell>infographics VQA</cell><cell>FUNSD</cell><cell>DVQA</cell><cell>DROP</cell><cell>CoQA</cell><cell>−4 −2 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>table-like categories, i.e., forms (89.5 → 94.<ref type="bibr" target="#b5">6</ref>) and tables (87.7 → 89.8), which proved its ability to understand the spatial structure of the document. Besides, we see a vast improvement in the yes/no category (55.2 → 69.0). † In such a case, our architecture generates simply yes or no answer, while sequence labeling based models require additional components such as an extra classification head. We noticed that model achieved lower results in the image/photo category, which can be explained by the low presence of image-rich documents in our datasets. † Per-category test set scores are available after submission on the competition web page: https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1. Results of previous state-of-the-art methods in relation to our base and large models. Bold indicates the best score in each category. All results on the test set.RVL-CDIP.Part of the documents to classify does not contain any readable text. Because of this shortcoming, we decided to guarantee there are at least 16 image tokens that would carry general image information. Precisely, we act as there were tokens with bounding boxes covering 16 adjacent parts of the document. These have representations from U-Net, exactly as they were regular text tokens. Our model places second, 0.12 below the best model, achieving the similar accuracy of 95.52.</figDesc><table><row><cell></cell><cell>CORD</cell><cell>SROIE</cell><cell>DocVQA</cell><cell>WikiOps</cell><cell>RVL-CDIP</cell></row><row><cell>Model</cell><cell>F1</cell><cell>F1</cell><cell>ANLS</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>LayoutLMv2 [55]</cell><cell>96.01</cell><cell>97.81</cell><cell>86.72</cell><cell>-</cell><cell>95.64</cell></row><row><cell>LAMBERT [11]</cell><cell>96.06</cell><cell>98.17</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NeOp [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.50</cell><cell>-</cell></row><row><cell>TILT-Base</cell><cell>95.11</cell><cell>97.65</cell><cell>83.92</cell><cell>69.16</cell><cell>95.25</cell></row><row><cell>TILT-Large</cell><cell>96.33</cell><cell>98.10</cell><cell>87.05</cell><cell>73.80</cell><cell>95.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results of ablation study. The minus sign indicates removal of the mentioned part from the base model.</figDesc><table><row><cell>Model</cell><cell>Score</cell><cell>Relative change</cell></row><row><cell>TILT-Base</cell><cell>82.9 ± 0.3</cell><cell>-</cell></row><row><cell>-Spatial Bias</cell><cell>81.1 ± 0.2</cell><cell>−1.8</cell></row><row><cell>-Visual Embeddings</cell><cell>81.2 ± 0.3</cell><cell>−1.7</cell></row><row><cell>-Case Augmentation</cell><cell>82.2 ± 0.3</cell><cell>−0.7</cell></row><row><cell>-Spatial Augmentation</cell><cell>82.6 ± 0.4</cell><cell>−0.3</cell></row><row><cell>-Vision Augmentation</cell><cell>82.8 ± 0.2</cell><cell>−0.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* http://www.industrydocuments.ucsf.edu/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Filip Graliński, Tomasz Stanis lawek, and Lukasz Garncarek for fruitful discussions regarding the paper and our managing directors at Applica.ai. Moreover, Dawid Jurkiewicz pays due thanks to his son for minding the deadline and generously coming into the world a day after.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<title level="m">Adversarial TableQA: Attention supervision for question answering on tables</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">QuAC: Question answering in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SpeechBERT: An audio-and-text jointly learned language model for end-to-end spoken question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ISCA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERTgrid: Contextualized embedding for 2d document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Finetuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From dataset recycling to multi-property extraction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietruszka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ch Ledowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Graliński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">LAMBERT: Layout-aware (language) modeling using bert for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stanis Lawek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Graliński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Graliński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stanis Lawek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wróblewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lipiński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaliska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biecek</surname></persName>
		</author>
		<title level="m">Kleister: A novel task for information extraction involving long documents with complex layout</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">REALM: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>A survey on visual transformer</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenschlos</surname></persName>
		</author>
		<editor>ACL. Online</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<title level="m">DocVQA: A dataset for vqa on document images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">The natural language decathlon: Multitask learning as question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cloudscan -a configuration-free invoice analysis system using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Palm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laws</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICDAR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CORD: A consolidated receipt dataset for post-ocr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Intelligence Workshop at NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unicase -rethinking casing in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stanislawek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CoQA: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A study of nonautoregressive model for sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">TextCaps: A dataset for image captioning with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards VQA models that can read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Layoutlmv2: Multi-modal pre-training for visuallyrich document understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">TaBERT: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
