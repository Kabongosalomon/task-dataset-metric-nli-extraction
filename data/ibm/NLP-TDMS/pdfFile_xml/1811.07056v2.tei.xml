<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptive Transfer Learning with Specialist Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
							<email>jngiam@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
							<email>daiyip@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
							<email>skornblith@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
							<email>rpang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">Domain Adaptive Transfer Learning with Specialist Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning is a widely used method to build high performing computer vision models. In this paper, we study the efficacy of transfer learning by examining how the choice of data impacts performance. We find that more pre-training data does not always help, and transfer performance depends on a judicious choice of pre-training data. These findings are important given the continued increase in dataset sizes. We further propose domain adaptive transfer learning, a simple and effective pre-training method using importance weights computed based on the target dataset. Our method to compute importance weights follow from ideas in domain adaptation, and we show a novel application to transfer learning. Our methods achieve state-of-the-art results on multiple fine-grained classification datasets and are well-suited for use in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transfer learning using pre-trained models is one of the most successfully applied methods in the field of computer vision. In practice, a model is first trained on a large labeled dataset such as ImageNet <ref type="bibr" target="#b26">[27]</ref>, and then fine-tuned on a target dataset. During fine-tuning, a new classification layer is learned from scratch, but the parameters for the rest of the network layers are initialized from the ImageNet pre-trained model. This method to initialize training of image models has proven to be highly successful and is now a central component of object recognition <ref type="bibr" target="#b23">[24]</ref>, detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>, and segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>By initializing networks with ImageNet pre-trained parameters, models converge faster, requiring less training time, and often achieve higher test accuracy. They can also achieve good performance when the target dataset is small. Most prior work have considered only ImageNet as the source of pre-training data due its large size and availability. In this work, we explore how the choice of pretraining data can impact the accuracy of the model when fine-tuned on a new dataset.</p><p>To motivate the problem, consider a target task where the goal is to classify images of different food items (e.g., 'hot dog' v.s. 'hamburger') for a mobile application <ref type="bibr" target="#b1">[2]</ref>. A straight-forward approach to applying transfer learning would be to employ an ImageNet pre-trained model finetuned on a food-specific dataset. However, we might wonder whether the pre-trained model, having learned to discriminate between irrelevant categories (e.g., 'dogs' vs. 'cats'), would be helpful in this case of food classification. More generally, if we have access to a large database of images, we might ask: is it more effective to pre-train a classifier on all the images, or just a subset that reflect food-like items? Furthermore, instead of making a hard decision when selecting pre-training images, we can consider a soft decision that weights each example based on their relevancy to the target task. This could be estimated by comparing the distributions of the source pre-training data and the target dataset. This approach has parallels to the covariate shift problem often encountered in survey and experimental design <ref type="bibr" target="#b29">[30]</ref>.</p><p>We study different choices of source pre-training data and show that a judicious choice can lead to better performance on all target datasets we studied. Furthermore, we propose domain adaptive transfer learning -a simple and effective pre-training method based on importance weights computed based on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Summary of findings</head><p>More pre-training data does not always help. We find that using the largest pre-training dataset does not always result in the best performance. By comparing results of transfer learning on different subsets of pre-training data, we find that the best results are obtained when irrelevant examples are discounted. This effect is particularly pronounced with fine-grained classification datasets.</p><p>Matching to the target dataset distribution improves transfer learning. We demonstrate a simple and computationally-efficient method to determine relevant examples for pre-training. Our method computes importance weights for examples on a pre-training dataset and is competitive with hand-curated pre-training datasets. Using this method, we obtain state-of-the-art results on the finegrained classification datasets we studied (e.g., Birdsnap, Oxford Pets, Food-101).</p><p>Fine-grained target tasks require fine-grained pretraining. We find that transfer learning performance is dependent on whether the pre-training data captures similar discriminative factors of variations to the target data. When features are learned on coarse grained classes, we do not observe significant benefits transferred to fine-grained datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The success of applying convolution neural networks to the ImageNet classification problem <ref type="bibr" target="#b18">[19]</ref> led to the finding that the features learned by a convolutional neural network perform well on a variety of image classification problems <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>. Further fine-tuning of the entire model was found to improve performance <ref type="bibr" target="#b0">[1]</ref>.</p><p>Yosinski et al. <ref type="bibr" target="#b35">[36]</ref> conducted a study of how transferable ImageNet features are, finding that the higher layers of the network tend to specialize to the original task, and that the neurons in different layers in a network were highly coadapted. They also showed that distance between tasks matters for transfer learning and examined two different subsets (man-made v.s. natural objects). Azizpour et al. <ref type="bibr" target="#b2">[3]</ref> also examined different factors of model design such as depth, width, data diversity and density. They compared data similarity to ImageNet based on the task type: whether it was classification, attribute detection, fine-grained classification, compositional, or instance retrieval.</p><p>Pre-training on weakly labeled or noisy data was also found to be effective for transfer learning. Krause et al. <ref type="bibr" target="#b16">[17]</ref> obtained additional noisy training examples by searching the web with the class labels. We note that our method does not use the class labels to collect additional data. Mahajan et al. <ref type="bibr" target="#b19">[20]</ref> were able to attain impressive ImageNet performance by pre-training on 3 billion images from Instagram. Notably, they found that it was important to select appropriate hash-tags (used as weak labels) for source pre-training, and suggested a heuristic for their specific dataset.</p><p>Understanding the similarity between datasets based on their content was studied by Cui et al. <ref type="bibr" target="#b7">[8]</ref>, who suggest using the Earth Mover's Distance (EMD) as a distance measure between datasets. They constructed two pre-training datasets by selecting subsets of ImageNet and iNaturalist, and showed that selecting an appropriate pre-training subset was important for good performance. Ge and Yu <ref type="bibr">[</ref>  <ref type="bibr" target="#b32">[33]</ref> dataset statistics. We hand-picked several subsets by hand-selecting labels based on the target dataset. We compare our methods against these hand-picked subsets.</p><p>ity between tasks on the same input; our work focuses on computing relationships between different input datasets.</p><p>In a comprehensive comparison, Kornblith et al. <ref type="bibr" target="#b14">[15]</ref> studied fine-tuning a variety of models on multiple datasets, and showed that performance on ImageNet correlated well with fine-tuning performance. Notably, they found that transfer learning with ImageNet was ineffective for small, fine-grained datasets.</p><p>Our approach is related to domain adaptation which assumes that the training and test set have differing distributions <ref type="bibr" target="#b29">[30]</ref>. We adopt similar ideas of importance weighting examples <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> and adapt them to the pre-training step instead, showing that this is an effective approach. While our derivation of importance weights is similar to past work in domain adaptation, the application to the transfer learning setting is novel.</p><p>In this work, we show that transfer learning to finegrained datasets is sensitive to the choice of pre-training data, and demonstrate how to select pre-training data to significantly improve transfer learning performance. We build on the work of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, demonstrating the effectiveness of constructing pre-training datasets. Furthermore, we present a simple, scalable, and computationally-efficient way to construct pre-training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transfer learning setup</head><p>We use the JFT <ref type="bibr" target="#b32">[33]</ref> and ImageNet <ref type="bibr" target="#b26">[27]</ref> datasets as our source pre-training data and consider a range of target datasets for fine-tuning (Section 3.2). For each target dataset, we consider different strategies for selecting pretraining data, and compare the fine-tuned accuracy. We do not perform any label alignment between the source and target datasets. During fine-tuning, the classification layer in the network is trained from random initialization. The following sections describe the datasets and experiments in further detail. Pre-train new model on JFT using importance weights Use JFT trained model as initialization for fine-tuning on target dataset <ref type="figure">Figure 1</ref>. To compute importance weights, we start by evaluating images from the target dataset using an image model pre-trained on the entire JFT dataset. For each image, this gives us a prediction over the 18,291 classes in JFT. We average these predictions to obtain pt(y).</p><p>We estimate ps(y) from the source pre-training dataset directly by dividing the number of times a label appears in the source pre-training dataset by the total number of examples in the source pre-training dataset. The ratio pt(y)/ps(y) provides the importance weight for a given label in the source pre-training dataset. A model is pre-trained on the entire JFT dataset using these importance weights and then fine-tuned on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Source pre-training data</head><p>The JFT dataset has 300 million images and 18,291 classes. Each image can have multiple labels and on average, each image has 1.26 labels. The large number of labels include many fine-grained categories, for example, there are 1,165 different categories for animals. While the labels are noisy and sometimes missing, we do not find this to a be a problem for transfer learning in practice. The labels form a semantic hierarchy: for example, the label 'mode of transport' includes the label 'vehicle', which in turn includes 'car'.</p><p>The semantic hierarchy of the labels suggests a straightforward approach to constructing different subsets of JFT as source pre-training data. Given a label, we can select all of its child labels in the hierarchy to form a label set, with the corresponding set of training examples. We created 7 subsets of JFT across a range of labels 1 <ref type="table">(Table 1)</ref>.</p><p>However, creating subsets using the label hierarchy can be limiting for several reasons: (a) the number of examples per label are pre-defined by the JFT dataset; (b) not all child labels may be relevant; (c) a union over different sub-trees of the hierarchy may be desired; and (d) not all source datasets have richly-defined label hierarchies. In section 3.3, we discuss a domain adaptive transfer learning approach that automatically selects and weights the relevant pre-training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Domain adaptive transfer learning by importance weighting</head><p>In this section, we propose domain adaptive transfer learning, a simple and effective way to weight examples during pre-training. Let us start by considering a simplified setting where our source and target datasets are over the same set of values in pixels x, and labels y; we will relax this assumption later in this section.</p><p>During pre-training, we usually optimize parameters θ to minimize a loss function E x,y∼Ds [L(f θ (x), y)] computed empirically over a source dataset D s . L(f θ (x), y) is often the cross entropy loss between the predictions of the model f θ (x) and the ground-truth labels y. However, the distribution of source pre-training dataset D s may differ from the target dataset D t . This could be detrimental as the model may emphasize features which are not relevant to the target dataset. We will mitigate this by up-weighting the examples that are most relevant to the target dataset. This is closely Target Dataset # Training Examples # Test Examples # Classes CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> 50,000 10,000 10 Birdsnap <ref type="bibr" target="#b3">[4]</ref> 47,386 2,443 500 Stanford Cars <ref type="bibr" target="#b15">[16]</ref> 8,144 8,041 196 FGVC Aircraft <ref type="bibr" target="#b20">[21]</ref> 6,667 3,333 100 Oxford-IIIT Pets <ref type="bibr" target="#b22">[23]</ref> 3,680 3,369 37 Food-101 <ref type="bibr" target="#b4">[5]</ref> 75,750 25,250 101 <ref type="table">Table 2</ref>. Target datasets for fine-tuning. related 3 to prior probability shift <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref> also known as target shift <ref type="bibr" target="#b38">[39]</ref>. We start by considering optimizing the loss function over the target dataset, D t instead:</p><formula xml:id="formula_0">E x,y∼Dt L(f θ (x), y) = x,y P t (x, y)L(f θ (x), y)</formula><p>where we use P s and P t to denote distributions over the source and target datasets respectively. We first reformulate the loss to include the source dataset D s :</p><formula xml:id="formula_1">= x,y P s (x, y) P t (x, y) P s (x, y) L(f θ (x), y) = x,y P s (x, y) P t (y)P t (x|y) P s (y)P s (x|y) L(f θ (x), y)</formula><p>Next, we make the assumption that P s (x|y) ≈ P t (x|y), that is the distribution of examples given a particular label in the source dataset is approximately the same as that of the target dataset. We find this assumption reasonable in practice: for example, the distribution of 'bulldog' images from a large natural image dataset can be expected to be similar to that of a smaller animal-only dataset. This assumption also allows us to avoid having to directly model the data distribution P (x).</p><p>Cancelling out the terms, we obtain:</p><formula xml:id="formula_2">≈ x,y P s (x, y) P t (y) P s (y) L(f θ (x), y) = E x,y∼Ds P t (y) P s (y) L(f θ (x), y)</formula><p>Intuitively, P t (y) describes the distribution of labels in the target dataset, and P t (y)/P s (y) reweights classes during source pre-training so that the class distribution statis-tics match P t (y). We refer to P t (y)/P s (y) as importance weights and call this approach of pre-training Domain Adaptive Transfer Learning.</p><p>For this approach to be applicable in practice, we need to relax the earlier assumption that the source and target datasets share the same label space. Our goal is to estimate P t (y)/P s (y) for each label in the source dataset. The challenge is that the source and target datasets have different sets of labels. Our solution is to estimate both P t (y) and P s (y) for labels in the source domain. The denominator P s (y) is obtained by dividing the number of times a label appears by the total number of source dataset examples. To estimate P t (y), we use a classifier to compute the probabilities of labels from source dataset on examples from the target dataset.</p><p>Our method is described in <ref type="figure">Figure 1</ref>. Concretely, we first train an image classification model on the entire source dataset. Next, we feed only the images from the target dataset into this model to obtain a prediction for each target example. The predictions are averaged across target examples, providing an estimate of P t (y), where y is specified over the source label space. We emphasize that this method does not use the target labels when computing importance weights.</p><p>Our approach is in contrast to Ge and Yu <ref type="bibr" target="#b9">[10]</ref>, which is computationally expensive as they compute a similarity metric between every pair of images in the source dataset and target dataset. It is also more adaptive than Cui et al. <ref type="bibr" target="#b7">[8]</ref>, which suggests selecting appropriate labels to pretrain on, without specifying a weight on each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We used the Inception v3 <ref type="bibr" target="#b33">[34]</ref>, and AmoebaNet-B <ref type="bibr" target="#b24">[25]</ref> models in our experiments.</p><p>For Inception v3 models, we pre-train from random initialization for 2,000,000 steps using Stochastic Gradient Descent (SGD) with Nesterov momentum. Each mini-batch contained 1,024 examples. The same weight regularization and learning rate parameters were used for all pretrained models and were selected based on a separate holdout dataset. We used a learning rate schedule that first starts with a linear ramp up for 20,000 steps, followed by cosine decay.</p><p>AmoebaNet-B models followed a similar setup with pretraining from random initialization for 250,000 steps using SGD and Nesterov momentum. We used larger minibatches of 2,048 examples to speed up training. The same weight regularization and learning rate parameters were used for all models, and matched the parameters that Real et al. <ref type="bibr" target="#b24">[25]</ref> used for ImageNet training. We chose to use AmoebaNet-B with settings (N=18, F=512), resulting in over 550 million parameters when trained on ImageNet, so as to evaluate our methods on a large model.</p><p>During fine-tuning, we used a randomly initialized classification layer in place of the pre-trained classification layer. Models were trained for 20,000 steps using SGD with momentum. Each mini-batch contained 256 examples. The weight regularization and learning rate parameters were determined using a hold-out validation set. We used a similar learning rate schedule with a linear ramp for 2,000 steps, followed by cosine decay.</p><p>For domain adaptive transfer learning, we found that adding a smooth prior when computing P t (y) helped performance with ImageNet as a source pre-training data. Hence, we used a temperature 4 of 2.0 when computing the softmax predictions for the computation of the importance weights. <ref type="bibr" target="#b3">4</ref> The logits are divided by the temperature before computing the softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training setup</head><p>While it is possible to directly perform pre-training with importance weights, we found it challenging as the importance weights varied significantly. When pre-training on a large dataset, this means that it is possible to have batches of data that are skewed in their weights with many examples weighted lightly. This is also computationally inefficient as the examples with very small weights contribute little to the gradients during training.</p><p>Hence, we created pre-training datasets by sampling examples from the source dataset using the importance weights. We start by choosing a desired pre-training dataset size, often large. We then sample examples from the source dataset at a rate proportional to the importance weights, repeating examples as needed. We report results that construct a pre-training dataset of 80 million examples for JFT, and 2 million examples for ImageNet. We used the same sampled pre-training dataset with both the Inception v3 and AmoebaNet-B experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer learning results</head><p>Domain adaptive transfer learning is better. When the source pre-training domain matches the target dataset, such as in JFT-Bird to Birdsnap or JFT-Cars to Stanford Cars, transfer learning is most effective <ref type="table">(Table 3</ref>). However, when the domains are mismatched, we observe negative transfer: JFT-Cars fine-tuned on Birdsnap performs poorly. Strikingly, this extends to categories which are intuitively close: aircrafts and cars. The features learned to discriminate be-  <ref type="figure">Figure 2</ref>. Distribution of importance weights for each target dataset when using ImageNet as a source pre-training dataset. The horizontal axis represents the top 100 ImageNet labels sorted by importance weight for each dataset; each dataset has a different order. The distributions vary widely between target datasets. FGVC Aircraft selects only a few labels that turn out to be coarse grained, whereas Oxford Pets selects a wider variety of fine-grained labels. This reflects the inherent bias in the ImageNet dataset.  We see that when dataset size increases, the performance of same distribution matcher increases and then saturates, while that of elastic distribution matcher drops after a peak. Notice that the elastic distribution matcher also has significantly more unique examples than same distribution matcher as the dataset size increases.</p><p>tween types of cars does not extend to aircrafts, and viceversa.</p><p>More data is not necessarily better. Remarkably, more data during pre-training can hurt transfer learning performance. In all cases, the model pre-trained on the entire JFT dataset did worse than models trained on more specific subsets. These results are surprising as common wisdom suggests that more pre-training data should improve transfer learning performance if generic features are learned. Instead, we find that it is important to determine how relevant additional data is. The ImageNet results with Domain Adaptive Transfer further emphasize this point. For ImageNet with Adaptive Transfer, each pre-training dataset only has around 450k unique examples. While this is less than half of the full Im-ageNet dataset of 1.2 million examples, the transfer learning results are slightly better than using the full ImageNet dataset for many of the target datasets.</p><p>Domain adaptive transfer is effective. When pretraining with JFT and ImageNet, we find that the domain adaptive transfer models are better or competitive with manually selected labels from the hierarchy. For datasets that are composed of multiple categories such as CIFAR-10 which includes animals and vehicles, we find further improved results since the constructed dataset includes multiple different categories.</p><p>In <ref type="figure">Figure 2</ref>, we observe that the distributions are much more concentrated with FGVC Aircraft and Stanford Cars: this arises from the fact that ImageNet has only coarsegrained labels for aircraft and cars. In effect, ImageNet captures less of the discriminative factors of variation that is captured in either FGVC Aircraft and Stanford Cars. Hence, we observe that transfer learning only improves the results slightly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing pre-training sampling mechanisms</head><p>In section 4.1, we described a method to construct pretraining datasets from sampling the source dataset. This process also allows us to study the effect of different distributions. Rather than sampling with replacement, as we did earlier, we could also sample without replacement when constructing the pre-training dataset. When sampling without replacement, we deviate from the importance weights assigned, but gain more unique examples to train on. We compare these two methods of sampling: (a) sampling with replacement -'same distribution matcher', and (b) sampling without replacement -'elastic distribution matcher'.</p><p>When sampling with replacement, we sample examples at a rate proportional to the importance weight computed before, repeating examples as needed. When sampling without replacement, we avoid selecting each example more than once. In order to keep the distribution as similar to the desired one, we consider a sequential approach: we start with the class with the highest importance weight and select all the samples available. Next, we recursively consider sampling a dataset of the remaining desired examples (original desired dataset size minus the number of examples just selected) from the rest of the classes.</p><p>By comparing model performances between these two sampling methods, we are able to study how the transfer learning performance varies when (a) there are more unique examples in pre-training, (b) when the distribution in pretraining deviates from the importance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparing sampling methods</head><p>We find that the performance of the same distribution matcher increases, and then saturates. Conversely, the elastic distribution matcher performance first increases then decreases. Note that at the low end of the dataset sizes, both methods will generate similar datasets. Thus, the later decrease in performance from the elastic distribution matcher comes from diverging from the original desired distribution. This indicates that using the importance weights during pretraining is more important than having more unique examples to train on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on large models</head><p>We studied our method on large models to understand if large models are better able to generalize, because the increased capacity enables them to capture more factors of variation. We conducted the same experiments on AmoebaNet-B, with over 550 million parameters.</p><p>We found that the general findings persisted with AmoebaNet-B: (a) using the entire JFT dataset was always worse compared to an appropriate subset and (b) our domain adaptive transfer method was better or competitive with the hand selected subsets.</p><p>Furthermore, we find that the large model was also able to narrow the performance gap between the more general subsets and specific subsets: for example, the performance on Birdsnap between JFT-Bird and JFT-Animal is smaller with AmoebaNet-B compared to Inception v3. We also observe better transfer learning between the transportation datasets compared to Inception v3.</p><p>Our results are competitive with the best published results <ref type="table" target="#tab_3">(Table 4</ref>). In our experiments, the performance of the AmoebaNet-B was also better in all cases than Inception v3, except for the FGVC Aircraft dataset. This is consistent with Kornblith et al. <ref type="bibr" target="#b14">[15]</ref> who also found that Inception v3 did slightly better than NasNet-A <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons to data selection methods</head><p>Cui et al. <ref type="bibr" target="#b7">[8]</ref> and Ge &amp; Yu <ref type="bibr" target="#b9">[10]</ref> recently proposed methods for improving transfer learning by selecting relevant data from the source pre-training dataset. We compared our methods to their reported results on the Oxford Flowers 102 <ref type="bibr" target="#b21">[22]</ref> dataset, following the same experimental setup in Section 4.1, using the Inception v3 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Method</head><p>Test Accuracy Entire ImageNet Dataset 97.1 ImageNet -Adaptive Transfer 97.7</p><p>ImageNet &amp; iNaturalist -EMD Subset <ref type="bibr" target="#b7">[8]</ref> 97.7 ImageNet -Selective Joint FT <ref type="bibr" target="#b9">[10]</ref> 97.0 In contrast to Cui et al. <ref type="bibr" target="#b7">[8]</ref>, our method not only selects the relevant categories from the source pre-training set, but also weights them. In order to understand the importance of the weighting scheme, we performed experiments to vary the weights assigned over a subset of selected categories.</p><p>We created JFT subsets over the top 4,000 matched labels on Oxford-IIIT Pets. We used three different weighting schemes in our comparison: (a) importance weights according to our adaptive transfer method, (b) uniform weights across all the labels, and (c) adaptive transfer weights reversed, where we assigned the highest weight to the label with originally the lowest weight.</p><p>Our results <ref type="table">(Table 6)</ref> show that uniform weights are inferior to the adaptive transfer weights, indicating that is it important to emphasize some labels more than others. The reversed weights perform the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Transfer learning appears most effective when the pretrained model captures the discriminative factors of variation present in the target dataset. This is reflected in the significant overlap in the classes between ImageNet and other datasets such as Caltech101, CIFAR-10, etc. where transfer learning with ImageNet is successful. Our domain adaptive transfer method is also able to identify the relevant examples in the source pre-training dataset that capture these discriminative factors.</p><p>Conversely, the cases where transfer learning is less effective are when it fails to capture the discriminative factors. In the case of the "FGVC Aircraft" dataset <ref type="bibr" target="#b20">[21]</ref>, the task is to discriminate between 100 classes over manufacturer and models of aircraft (e.g., Boeing 737-700). However, Ima-geNet only has coarse grained labels for aircraft (e.g., airliner, airship). In this case, ImageNet models tend to learn to "group" different makes of aircraft together rather than differentiate them. It turns out that the JFT dataset has finegrained labels for aircraft and is thus able to demonstrate better transfer learning efficacy.</p><p>Our results using AmoebaNet-B show that even large models transfer better when pre-trained on a subset of classes, suggesting that they make capacity trade-offs between the fine-grained classes when training on the entire dataset. This finding posits new research directions for developing large models that do not make such a trade-off.</p><p>We have seen an increase in dataset sizes since Ima-geNet; for example, the YFCC100M dataset <ref type="bibr" target="#b34">[35]</ref> has 100M examples. We have also seen developments of more efficient methods to train deep neural networks. Recent benchmarks <ref type="bibr" target="#b6">[7]</ref> demonstrate that it is possible to train a ResNet-50 model in half an hour, under fifty US dollars. This combination of data and compute will enable more opportunities to employ better methods for transfer learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Performance (top) and unique examples (bottom) of the same distribution matcher and elastic distribution matcher at different sampled dataset sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Transfer learning results with AmoebaNet-B. Each row corresponds ot a pre-training method. Adaptive transfer refers to our proposed method. Results reported are top-1 accuracy for all datasets except Oxford-IIIT Pets, where we report mean accuracy per class. All our results are averaged over 5 fine-tuning runs. Huang et al.<ref type="bibr" target="#b13">[14]</ref> also use the AmoebaNet-B model, but with a larger input image size at 480 × 480, while we used 331 × 331 image inputs instead.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Transfer learning performance on Oxford Flowers 102 with Inception v3. We report the mean per class accuracy, averaged over 5 runs. The first two rows are our results, showing that adaptive transfer improves over training over the entire ImageNet dataset. Our method performs as well as Cui et al.<ref type="bibr" target="#b7">[8]</ref> despite using less pre-training data.Our results(Table 5)show that our adaptive transfer method can effectively select appropriate examples to pretrain on. We observe an absolute gain of about 0.6% in accuracy, achieving results comparable to Cui et al.<ref type="bibr" target="#b7">[8]</ref> despite using less pre-training data. Transfer performance on Oxford-IIIT Pets from JFT subsets of the same size (80M), using the top 4k selected labels, but with different weighting schemes: Adaptive Transfer Weights are the weights from our method, Uniform is a uniform distribution on all selected labels, and Reverse is a distribution obtained by swapping the weights between the highest and the lowest weighted labels from the Adaptive Transfer distribution.</figDesc><table><row><cell cols="2">4.6. Understanding the importance of the pre-</cell></row><row><cell>training distribution</cell><cell></cell></row><row><cell>Weighting Scheme</cell><cell>Transfer Performance</cell></row><row><cell>Adaptive Transfer Weights</cell><cell>97.1</cell></row><row><cell>Uniform Weights</cell><cell>95.3</cell></row><row><cell>Reversed Weights</cell><cell>84.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The following parent-child relationships exists in the label hierarchy: bird ⊂ animal; car ⊂ vehicle ⊂ transport; aircraft ⊂ vehicle ⊂ transport. We note that<ref type="bibr" target="#b32">[33]</ref> excluded classes with too few training examples during training, while we include all classes available.3.2. Target training datasetWe evaluate the performance of transfer learning on a range of classification datasets (Table 2) that include both general and fine-grained classification problems. Using the same method as Krause et al.<ref type="bibr" target="#b16">[17]</ref>, we ensured that the source pre-training data did not contain any of the target training data by removing all near-duplicates of the target training and test data from the JFT dataset 2 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We used a CNN-based duplicate detector and chose a conservative threshold for computing near-duplicates to err on the side of ensuring that duplicates were removed. We removed a total of 48k examples from JFT, corresponding to duplicates that were found in target datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Prior work on prior probability shift usually considered shifts between train and test set, while we instead consider differences between the pretraining and training datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We wish to thank Yanping Huang, Jon Shlens, Sergey Ioffe, Tom Duerig, Tomas Pfister, Yin Cui, Vishy Tirumalashetty, and Chen Sun for helpful feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How HBOs Silicon Valley built Not Hotdog with mobile TensorFlow, Keras &amp; React Native</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anglade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale finegrained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="446" to="461" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<title level="m">Stanford DAWN Deep Learning Benchmark (DAWNBench)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3296" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? CoRR, abs/1805.08974</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Fine-Grained Visual Categorization (FGVC2)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>abs/1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft. CoRR, abs/1306</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing, ICVGIP &apos;08</title>
		<meeting>the 2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing, ICVGIP &apos;08<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000-10-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">When training and test sets are different: Characterising learning transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dataset Shift in Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Bünau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS&apos;07</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1707.07012</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
