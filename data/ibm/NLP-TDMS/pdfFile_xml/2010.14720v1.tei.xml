<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Second-Order Unsupervised Neural Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">Alibaba DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Second-Order Unsupervised Neural Dependency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the unsupervised dependency parsers are based on first-order probabilistic generative models that only consider local parent-child information. Inspired by second-order supervised dependency parsing, we proposed a second-order extension of unsupervised neural dependency models that incorporate grandparent-child or sibling information. We also propose novel design of the neural parameterization and optimization methods of the dependency models. In secondorder models, the number of grammar rules grows cubically with the increase of vocabulary size, making it difficult to train lexicalized models that may contain thousands of words. To circumvent this problem while still benefiting from both second-order parsing and lexicalization, we use the agreement-based learning framework to jointly train a second-order unlexicalized model and a first-order lexicalized model. Experiments on multiple datasets show the effectiveness of our second-order models compared with recent state-of-the-art methods. Our joint model achieves a 10% improvement over the previous state-of-the-art parser on the full WSJ test set. 1 * Corresponding Author 1 Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is a classical task in natural language processing. The head-dependent relations produced by dependency parsing can provide an approximation to the semantic relationship between words, which is useful in many downstream NLP tasks such as machine translation, information extraction and question answering. Nowadays, supervised dependency parsers can reach a very high accuracy <ref type="bibr" target="#b5">(Dozat and Manning, 2017;</ref><ref type="bibr" target="#b31">Zhang et al., 2020)</ref>. Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build. On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing. We focus on unsupervised dependency parsing in this paper.</p><p>Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref>, which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, <ref type="bibr" target="#b8">Grave and Elhadad (2015)</ref> uses the idea of disrciminative clustering, <ref type="bibr" target="#b2">Cai et al. (2017)</ref> uses a discriminative parser in the CRF-autoencoder framework, and <ref type="bibr" target="#b18">Li et al. (2018)</ref> uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, <ref type="bibr" target="#b10">Han et al. (2019)</ref> proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing <ref type="bibr" target="#b16">(Koo and Collins, 2010;</ref><ref type="bibr" target="#b21">Ma and Hai, 2012)</ref>.</p><p>A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs.</p><p>In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization <ref type="bibr" target="#b26">(Salakhutdinov et al., 2003;</ref><ref type="bibr" target="#b28">Tran et al., 2016)</ref> over the widely used expectationmaximization algorithm for training. One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words. Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework <ref type="bibr" target="#b20">(Liang et al., 2007)</ref>. The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization.</p><p>We conduct experiments on the Wall Street Journal (WSJ) dataset and seven languages on the Universal Dependencies (UD) dataset. The experimental results demonstrate that our models achieve state-ofthe-art accuracies on unsupervised dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dependency Model With Valence</head><p>The Dependency Model with Valence (DMV) <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref> is a probabilistic generative model of a sentence and its parse tree. It generates a dependency parse tree from the imaginary root node in a recursive top-down manner. There are three types of probabilistic grammar rules in a DMV, namely ROOT, CHILD and DECISION rules, each associated with a set of multinomial distributions P ROOT (c), P CHILD (c|p, dir, val) and P DECISION (dec|p, dir, val), where p is the parent token, c is the child token, dec is the continue/stop decision, dir indicates the direction of generation, and val indicates whether parent p has generated any child in direction dir. To generate a sequence of tokens along with its dependency parse tree, the DMV model generates a token c from the ROOT distribution P ROOT (c) firstly. Then for each token p that has already been generated, it generates a decision from the DECISION distribution P DECISION (dec|p, dir, val) to determine whether to generate a new child in direction dir. If dec is CONTINUE, then a new child p is generated from the CHILD distribution P CHILD (c|p, dir, val). If dec is STOP, then p stops generating children in direction dir. The joint probability of the sequence and its corresponding dependency parse tree can be calculated by taking product of the probabilities of all the generation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neuralized DMV Models</head><p>Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. <ref type="bibr" target="#b12">Jiang et al. (2016)</ref> proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly.</p><p>Lexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. <ref type="bibr" target="#b9">Han et al. (2017)</ref> proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS embedding and the word embedding as input. To reduce the vocabulary size, they replace low-frequency words with their POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Second-Order Parsing</head><p>In our proposed second-order NDMV, we calculate each rule probability based additionally on the information of the sibling or grandparent. We take sibling-NDMV for example to demonstrate the generative story.</p><p>• We start with the imaginary root token, generating its only child c with probability P ROOT (c) • For each token p, we decide whether to generate a new child or not with probability P DECISION (dec|p, s, dir, val), where s is the previous child token generated by p in direction dir. If p has not generated any child in direction dir yet, we use a special symbol NULL to represent s. • If decision dec is CONTINUE, p generates a new child c with probability P CHILD <ref type="bibr">(c|p, s, dir, val)</ref>.</p><p>If decision a is STOP, p stops generating children in direction dir.</p><p>For parsing, we design dynamic programming algorithms adapted from <ref type="bibr" target="#b16">Koo and Collins (2010)</ref>. Since the grandparent token is deterministic for each token, the parsing algorithm of our grand-NDMV model is similar to theirs. There are two options for determining the sibling token since the generation process of child tokens can be either from the inside out or from the outside in. Koo and Collins (2010) make the inside-out assumption, but in this paper, we make the outside-in assumption because it makes implementation easier and can achieve better performance empirically. We provide the pseudo code of the second-order inside algorithm and the second-order parsing algorithm in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterization</head><p>In a neural DMV, we compute the probability of a grammar rule using a neural network. Below we formulate the computation of CHILD rule probabilities. The full architecture of the neural network is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. ROOT and DECISION rule probabilities are computed in a similar way.</p><p>In our second-order neural DMV, each CHILD rule P CHILD (c|p, s, dir, val) involves three tokens: parent p, child c, and sibling (or grandparent) s. Denote the embedding of the parent, child and sibling (or grandparent) by x p , x c , x s ∈ R d , which are retrieved from a shared token embedding layer. We use three different linear transformations to produce the representations of a token as a parent, child, and sibling (or grandparent). e c = W c x c e p = W p x p e s = W s x s</p><p>We feed e c , e p , e s to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations h c , h p , h s (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skipconnections have been found very useful in unsupervised neural parsing <ref type="bibr" target="#b13">(Kim et al., 2019)</ref>. We then follow <ref type="bibr" target="#b30">Wang et al. (2019)</ref> and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors h c , h p , h s .</p><formula xml:id="formula_0">S(p, s, c) = q i=1 o p,i × o s,i × o c,i o p = C p h p o c = C c h c o s = C s h s where C p , C c , C s ∈ R q×d</formula><p>are the parameters of the decomposed trilinear function and × is scalar multiplication. Then we apply a softmax function to produce the final rule probability.</p><formula xml:id="formula_1">P CHILD (c|p, s, dir, val) = e S(p,s,c) c ∈C e S(p,s,c )</formula><p>where C is the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>The learning objective function L(θ) is the log-likelihood of training sentences X = {x 1 , ..., x n } where θ is the parameters of the neural networks. The probability of each sentence x is defined as:</p><formula xml:id="formula_2">L(θ) = n i=1 log p θ (x i )<label>(1)</label></formula><formula xml:id="formula_3">p θ (x) = z∈T (x) p θ (x, z)<label>(2)</label></formula><p>where T (x) is the set of all possible dependency parse trees for sentence x. We use c(r, x, z) to represent the number of times rule r is used in dependency parse tree z of sentence x. Then we have</p><formula xml:id="formula_4">p θ (x, z) = r∈R p θ (r) c(r,x,z)<label>(3)</label></formula><p>where R is the collection of all DECISION, CHILD and ROOT rules.</p><p>Learning via EM algorithm We can rewrite the log-likelihood of sentence x as follows.</p><formula xml:id="formula_5">log p θ (x) = E q(z) [log p θ (x, z)] + H [q(z)] + KL(q(z) p θ (z|x))<label>(4)</label></formula><p>where q(z) is an arbitrary distribution and H is the entropy function. In the E-step, we fix θ and set q(z) = p θ (z|x). In the M-step, we fix q(z) and update θ with the following objective:</p><formula xml:id="formula_6">Q(θ) = E q(z) [log p θ (x, z)] = E q(z) r∈R c(r, x, z) log p θ (r) = r∈R e(r, x) log p θ (r)<label>(5)</label></formula><p>where e(r, x) is the expected count of grammar rule r in sentence x based on q(z), which can be obtained using the inside-outside algorithm. We can use gradient descent to update θ.</p><formula xml:id="formula_7">∇ θ Q(θ) = r∈R e(r, x)∇ θ log p θ (r)<label>(6)</label></formula><p>Learning via direct marginal likelihood optimization We can also use gradient descent to maximize log p θ (x) directly. Based on the derivation of <ref type="bibr" target="#b26">Salakhutdinov et al. (2003)</ref> </p><formula xml:id="formula_8">2 , we have ∇ θ (log p θ (x)) = z∈T (x) p θ (z|x)∇ θ log p θ (x, z) = z∈T (x) p θ (z|x) r∈R c(r, x, z)∇ θ log p θ (r) = r∈R e(r, x)∇ θ log p θ (r)<label>(7)</label></formula><p>2 See Equation 8 in <ref type="bibr" target="#b26">Salakhutdinov et al. (2003)</ref>.</p><p>where e(r, x) is the expected count of grammar rule r in sentence x based on p θ (z|x). Traditionally, we use the inside-outside algorithm to obtain the expected count e(r, x). <ref type="bibr" target="#b6">Eisner (2016)</ref> points out that we can use back-propagation to calculate the expected count e(r, x).</p><formula xml:id="formula_9">e(r, x) = ∂ log p θ (x) ∂ log p θ (r)<label>(8)</label></formula><p>So we only need to use the inside algorithm to calculate log p θ (x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm.</p><p>Mini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log p θ (x), it is analogous to the online-EM algorithm <ref type="bibr" target="#b19">(Liang and Klein, 2009</ref>). To compute the gradient for each mini-batch, we first need to compute the expected counts from the training sentences in the mini-batch, which is exactly what the online E-step does; we then use the expected counts to compute the gradient and update the model parameters, which is similar to the M-step, except that here we only perform one update step, while in the EM algorithm multiple update steps may be taken based on the same expected counts. According to <ref type="bibr" target="#b19">Liang and Klein (2009)</ref>, online-EM has a faster convergence speed and can even find a better solution.</p><p>Empirically, we do find that direct marginal likelihood optimization outperforms the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Agreement-Based Learning</head><p>In our second-order DMV model, the number of grammar rules is 4 |V | 3 +4 |V | 2 +|V |, which is cubic in the vocabulary size |V |. When our model is lexicalized, the vocabulary may contain thousands of words or more, making the model size less manageable. Instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework <ref type="bibr" target="#b20">(Liang et al., 2007)</ref>. The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization. Empirically, we do find that the jointly trained models outperform lexicalized second-order models. Following <ref type="bibr" target="#b20">Liang et al. (2007)</ref>, we define the objective function for our jointly trained first-order L-NDMV and second-order NDMV as</p><formula xml:id="formula_10">O agree (θ) def = log z∈T (x) (p θ 0 (x, z) · p θ 1 (x, z))<label>(9)</label></formula><p>where θ 0 is parameters of L-NDMV and θ 1 is parameters of second-order NDMV. Intuitively, the objective requires the two models to reach agreement on the probability distribution of dependency parse tree z. We use joint decoding (parsing) to predict dependency parse tree z predict for sentence x.</p><formula xml:id="formula_11">z predict = argmax z∈T (x) p θ 0 (x, z) · p θ 1 (x, z)<label>(10)</label></formula><p>The inside and parsing algorithms for jointly trained models can be found in the appendix.</p><p>Learning via product EM algorithm <ref type="bibr" target="#b20">Liang et al. (2007)</ref> propose to optimize the objective using the product EM algorithm based on the following lower bound of the objective.</p><formula xml:id="formula_12">O agree = log z q(z) p θ 0 (x, z) · p θ 1 (x, z) q(z) ≥ E q(z) (log p θ 0 (x, z) · p θ 1 (x, z) q(z) ) def = L(θ, q)<label>(11)</label></formula><p>The product EM algorithm performs coordinate-wise ascent on L(θ, q). In the product E-step, we optimize L(θ, q) with respect to q.</p><formula xml:id="formula_13">L(θ, q) = −KL(q(z) p θ 0 (x, z) · p θ 1 (x, z)) + const<label>(12)</label></formula><p>where const does not depend on θ and q. In the product E-step, the maximum can be obtained by setting q(z) ∝ p θ 0 (x, z) · p θ 1 (x, z) to minimize the KL term. In the M-step, we optimize L(θ, q) with respect to θ.</p><formula xml:id="formula_14">L(θ, q) = E q log p θ 0 (x, z) + E q log p θ 1 (x, z) + const<label>(13)</label></formula><p>where const does not depend on θ. It consists of one term for each model. We update the parameters of each model separately based on the expected counts obtained from the product E-step, which can be calculated through the inside-outside algorithm.</p><p>Learning via direct marginal likelihood optimization O agree can be calculated through the inside algorithm. Similar to what we describe in Section 3.3, we can benefit from both agreement-based learning and the online-EM algorithm if we use gradient descent directly to optimize O agree instead of using the product EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setting</head><p>English Penn Treebank We conduct experiments on the Wall Street Journal (WSJ) corpus, with section 2-21 for training, section 22 for validation and section 23 for testing. We use sentences of length ≤ 10 in training and use sentences of length ≤ 10 (WSJ10) and all sentences (WSJ) in testing.</p><p>Universal Dependency Treebank Following the setting of <ref type="bibr" target="#b18">Li et al. (2018)</ref> and <ref type="bibr" target="#b10">Han et al. (2019)</ref>, we conduct experiments on selected languages from the Universal Dependency Treebank 1.4 <ref type="bibr" target="#b23">(Nivre et al., 2016)</ref>. We use sentences of length ≤ 15 in training and sentences of length ≤ 15 and ≤ 40 in testing.</p><p>Setting On the WSJ dataset, for fair comparison, we follow <ref type="bibr" target="#b9">Han et al. (2017)</ref> and <ref type="bibr" target="#b10">Han et al. (2019)</ref> and use HDP-DEP <ref type="bibr" target="#b22">(Naseem et al., 2010)</ref> to initialize our models. Specifically, we train the unsupervised HDP-DEP model on WSJ, use it to parse the training corpus, and then use the predicted parse trees to perform supervised learning of our model for several epochs. On the UD dataset, we use the K&amp;M initialization <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref>. We use direct marginal likelihood optimization (DMO) as the training method and use Adam (Kingma and Ba, 2015) as the optimizer with learning rate 0.001. The batch size is set to 64 for WSJ and 100 for UD. The hyperparameters of the neural networks, the setting of L-NDMV and more details can be found in the appendix. We apply early stopping based on the log-likelihood of the development data and report the mean accuracy over 5 random restarts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Result</head><p>Result on WSJ In <ref type="table">Table 1</ref>, we compare our methods with previous unsupervised dependency parsers.</p><p>Our sibling-NDMV model can outperform the previous state-of-the-art parser by 1.9 points on WSJ10 and 3.1 points on WSJ in the unlexicalized setting. Our lexicalized sibling-NDMV achieves further improvement over the unlexicalized sibling-NDMV. On the other hand, our grand-NDMV performs significantly worse than the sibling-NDMV and lexicalization hurts its performance. Why grandparent information is less useful than sibling information in unsupervised parsing is an intriguing question that we leave for feature research. Joint training with a first-order L-NDMV can increase the performance of unlexicalized sibling-NDMV from 77.5 to 79.9 and that of unlexicalized grand-NDMV from 71.4 to 76.0 on WSJ10. The jointly trained models also outperform the lexicalized second-order models.</p><p>Result on UD In <ref type="table" target="#tab_3">Table 2</ref>, we first compare our models with models which do not use the universal linguistic prior (UP) 3 . The variational variant of D-NDMV <ref type="bibr" target="#b10">(Han et al., 2019)</ref> is the recent state-ofthe-art model without UP. Our method outperforms theirs on six of the eight languages and also on average. We then compare our second-order models with recent state-of-the-art discriminative models, which rely heavily on the universal linguistic prior to achieve good performance (for example, <ref type="bibr" target="#b18">Li et al. (2018)</ref> reported bad results if they do not use the universal linguistic prior). We find that sibling-NDMV  <ref type="bibr" target="#b1">(Blunsom and Cohn, 2010)</ref> 65.9 53.1 UR-A E-DMV <ref type="bibr" target="#b29">(Tu and Honavar, 2012)</ref> 71.4 57.0 CRFAE <ref type="bibr" target="#b2">(Cai et al., 2017)</ref> 71.7 55.7 Neural DMV <ref type="bibr" target="#b12">(Jiang et al., 2016)</ref> 72.5 57.6 HDP-DEP <ref type="bibr" target="#b22">(Naseem et al., 2010)</ref> 73.8 -NVTP <ref type="bibr" target="#b18">(Li et al., 2018)</ref> 54.7 37.8 Variational variant D-NDMV * <ref type="bibr" target="#b10">(Han et al., 2019)</ref> 75.5 60.4 Deterministic variant D-NDMV * <ref type="bibr" target="#b10">(Han et al., 2019)</ref> 75.6 61.4 L-NDMV * <ref type="bibr" target="#b9">(Han et al., 2017)</ref> 75. can outperform these discriminative models while grand-NDMV can achieve comparable results, even though we do not utilize the universal linguistic prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Skip-Connections</head><p>From <ref type="table" target="#tab_5">Table 3</ref> and 4, we find that using skip-connections can achieve higher log-likelihood and better parsing accuracy in most cases. On UD, the performance is much better when using skip-connections except on Basque.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison of Training Methods</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, we find that the EM algorithm significantly underperforms DMO. On the other hand, <ref type="table">Table 4</ref> shows that the EM algorithm performs comparably to DMO on WSJ. We also compare the learning curves of these two methods. For fair comparison, we use the same batch-size for both methods. First we conduct an experiment using the joint L-NDMV and sibling-NDMV model on WSJ. In <ref type="figure">Figure 2</ref>, we find that DMO converges to a higher log-likelihood compared with EM and the convergence speed is roughly the same. In <ref type="figure">Figure 3</ref>, we find DMO can find a slightly better model compared with EM. Second, we conduct an experiment using sibling-NDMV model on the UD French dataset. In <ref type="figure">Figure 4</ref>, we find DMO converges faster than EM and converges to a higher log-likelihood. In <ref type="figure">Figure 4</ref>, we find that the model accuracy of DMO is much higher than that of EM at the beginning, but it drops significantly after epoch 23, suggesting that early-stop is necessary. We also find similar phenomena for other languages on UD.</p><p>It should be noted that we use HDP-DEP <ref type="bibr" target="#b22">(Naseem et al., 2010)</ref> for initialization on WSJ and use K&amp;M initialization <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref> on UD. We see that HDP-DEP initialization leads to a very high initial UAS of 75% <ref type="figure">(Figure 3)</ref>, while K&amp;M initialization leads to a low initial UAS of 38.5% ( <ref type="figure">Figure 5</ref>). It can be seen that EM is more sensitive to the initialization while DMO can achieve good results even if the initialization is bad.   <ref type="bibr" target="#b24">(Noji and Miyao, 2015)</ref>. DV,VV: The deterministic and variational variants of D-NDMV <ref type="bibr" target="#b10">(Han et al., 2019)</ref>. +sibling: Our second-order sibling-NDMV. +grand: Our second-order grand-NDMV. NVTP: Neural variational transition-based parser <ref type="bibr" target="#b18">(Li et al., 2018)</ref>. CM: Convex-MST <ref type="bibr" target="#b8">(Grave and Elhadad, 2015)</ref>.    <ref type="table">Table 4</ref>: Effect of skip-connections and training methods on WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Joint Training and Parsing</head><p>In <ref type="table" target="#tab_7">Table 5</ref>, we compare the performance with different training and parsing settings. We find that joint parsing is better than separate parsing in both training settings. With joint training, each individual model can achieve better performance compared with separate training, which shows the effectiveness of agreement-based joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Limitations</head><p>Our second-order NDMV model is more sensitive to the initialization compared with the first-order NDMV model. We fail to produce a good result under the K&amp;M initialization on WSJ: only 58.5% UAS for sibling-NDMV on WSJ10, while the first-order NDMV model can achieve 69.7% UAS. We rely on the parsing result of HDP-DEP to initialize our model in order to reach the state-of-the-art result on WSJ. This is similar to the case of L-NDMV, which performs badly when using the K&amp;M initialization according to <ref type="bibr" target="#b9">Han et al. (2017)</ref>. Because of the bad performance of L-NDMV with the K&amp;M initialization as well as the time constraint that prevents us from running HDP-DEP on UD, we did not conduct experiments of agreement-based learning with L-NDMV on the UD datasets. We leave this for future work.</p><p>Our second-order model is also quite sensitive to the design of the neural architecture, which is similar to case of unsupervised constituency parsing reported by <ref type="bibr" target="#b13">Kim et al. (2019)</ref>. We also try the third-order NDMV model (grand-sibling or tri-sibling) but are not able to get better results compared with sibling-NDMV.</p><p>Our second-order parsing algorithm has a theoretical time complexity of O(n 4 ), which is higher than the time complexity of O(n) of transition-based unsupervised parsers <ref type="bibr" target="#b18">(Li et al., 2018)</ref> and the time complexity of O(n 3 ) of first-order NDMV models, where n is the sentence length. However, transitionbased parsers are hard to batchify, while our model can be parallelized efficiently following the methods introduced by Torch-Struct <ref type="bibr" target="#b25">(Rush, 2020)</ref>. In practice, our second-order parser runs very fast on GPU, requiring only several minutes to train.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose second-order NDMV models, which incorporate sibling or grandparent information. We find that sibling information is very useful in unsupervised dependency parsing. We use agreement-based learning to combine the benefits of second-order parsing and lexicalization, achieving state-of-the-art results on the WSJ dataset. We also show the effectiveness of our neural parameterization architecture with skip-connections and the direct marginal likelihood optimization method.</p><p>d, valence = v)), S <ref type="bibr">[i, c, g, d, v]</ref> = log(p CHILD (child = x c |parent = x i , grand = x g , direction = d, valence = v)), and R[i] = log(p ROOT (child = x i )). Given these definitions, the inside algorithm of grand-NDMV is shown in Algorithm 1. For sibling-NDMV, g in C g h,e stands for the index of sibling instead of the index of grandparent. Given sentence x, we suppose that x 0 is a special NULL token which stands for no sibling and x 1 , ..x n are tokens. We denote D <ref type="bibr">[i, g, d, v, a]</ref> = log(p DECISION (decision = a|parent = x i , sibling = x g , direction = d, valence = v)), S <ref type="bibr">[i, c, g, d, v]</ref> = log(p CHILD (child = x c |parent = x i , sibling = x g , direction = d, valence = v)), and R[i] = log(p ROOT (child = x i )). Given these definitions, the inside algorithm of sibling-NDMV is shown in Algorithm 2.</p><p>For jointly trained L-NDMV and second-order NDMV model, we take jointly trained L-NDMV sibing-NDMV for example. We denote D [i, d, v, a] = log(p DECISION (decision = a|parent = x i , direction = d, valence = v)), S <ref type="bibr">[i, c, d, v]</ref> = log(p CHILD (child = x c |parent = x i , direction = d, valence = v)), R [i] = log(p ROOT (child = x i )) for L-NDMV where x is the sequence of word/POS pairs which starts indexing at 1. The inside algorithm of jointly trained L-NDMV and sibling-NDMV model is shown in Algorithm 3.</p><p>Following <ref type="bibr" target="#b6">Eisner (2016)</ref>, we use back-propagation to obtain expected counts of grammar rules. For the parsing algorithm, we can replace logsumexp with max in Algorithm 1, 2 and 3 to get the Viterbi log-likelihood of the sentence, then use back-propagation to get grammar rules which are used in the Viterbi parse tree, and finally reconstruct the parse tree based on these rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Full Parameterization</head><p>Denote the embedding of the parent, child and sibling (or grandparent) by x p , x c , x s ∈ R d . We use three different linear transformations to produce the representations of each token as a parent, child, and sibling (or grandparent). e c = W c x c e p = W p x p e s = W s x s</p><p>We feed e s , e c , e p to the same neural network which consists of three MLP with skip-connection layer. The first MLP aims at encoding valence information: v = ReLU (W 1 (ReLU (W val e + e)))</p><p>where val ∈ [HASCHILD, NOCHILD].</p><p>The second MLP aims at encoding direction information:</p><formula xml:id="formula_15">d = W 3 (ReLU (W 2 (W dir v + e))</formula><p>where dir ∈ [LEFT, RIGHT]</p><p>We use the final MLP to get final hidden representation h:</p><formula xml:id="formula_16">h = ReLU (W 4 d)</formula><p>For the UD dataset, we use a more expressive MLP to get the final hidden representation h since we find that the UD dataset is more difficult to train. h = ReLU (W 6 (ReLU (W 5 (ReLU (W 4 d + e)))))</p><p>For decision rules, we introduce two embedding x stop and x continue . We feed x stop and x continue to a fully connected layer W dec to get e stop and e continue . We use the same neural network to get hidden representation h stop and h continue . For root rules, we introduce x root . We feed x root to a fully connected layer W root to get e root . Also, we use the same neural network to get hidden representation h root . We use different decomposed trilinear function parameters for different types of rules. The calculation of the decision rule probability and root rule probability is similar to that of the child rule probability.</p><p>Algorithm 1: Inside algorithm for grand-NDMV notation LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 initialization ∀i, g C g,0 i,i,0 = D[i, g, 0, 0, 0] C g,1 i,i,0 = D[i, g, 0, 1, 0] C g,0 i,i,1 = D[i, g, 1, 0, 0] C g,1 i,i,1 = D[i, g, 1, 1, 0] for w = 1...(n − 1)</p><p>for i = 1...(n − w) j = i + w for g &lt; i or g &gt; j I g,0 i,j,0 = logsumexp i≤r&lt;j C j,1 i,r,1 + C g,0 r+1,j,0 + D[j, g, 0, 0, 1] + S[j, i, g, 0, 0] I g,1 i,j,0 = logsumexp i≤r&lt;j C j,1 i,r,1 + C g,0 r+1,j,0 + D[j, g, 0, 1, 1] + S[j, i, g, 0, 1]</p><formula xml:id="formula_17">I g,0 i,j,1 = logsumexp i≤r&lt;j C g,0 i,r,1 + C i,1 r+1,j,0 + D[i, g, 1, 0, 1] + S[i, j, g, 1, 0] I g,1 i,j,1 = logsumexp i≤r&lt;j C g,0 i,r,1 + C i,1 r+1,j,0 + D[i, g, 1, 1, 1] + S[i, j, g, 1, 1] C g,0 i,j,0 = logsumexp i≤r≤j C j,1 i,r,0 + I g,0 r,j,0 C g,1 i,j,0 = logsumexp i≤r≤j C j,1 i,r,0 + I g,1 r,j,0 C g,0 i,j,1 = logsumexp i≤r≤j I g,0 i,r,1 + C i,1 r,j,1 C g,1 i,j,1 = logsumexp i≤r≤j I g,1 i,r,1 + C i,1 r,j,1 for i = 1..(n) P [i] = R[i] + C 0,1 1,i,0 P = logsumexp 1≤i≤n P [i] + C 0,1 i,n,1</formula><p>return P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameters</head><p>We set the dimension of POS embedding to 100. The dimension of all linear layers to calculate hidden representation is set to 100. We set the size of decomposed trilinear function parameters to 30 for child and root rules and 10 for decision rules in the unlexicalized setting. For the lexicalized model, we set the dimension of word embedding to 100. We concatenate the POS embedding and word embedding as input. The dimension of all linear layers to calculate hidden representation is set to 200. We set the size of decomposed trilinear function parameters to 150 for child and root rules and 50 for decision rules. We use an additional dropout layer after the embedding layer to avoid over-fitting since the vocabulary size of the lexicalized model is much larger compared to the unlexicalized model. The dropout rate is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Setting of L-NDMV</head><p>The vocabulary consists of word/POS pairs that appear for at least two times in the WSJ10 dataset. We use random embedding to initialize the POS embedding and FastText embedding to initialize the word embedding, which is different from the setting in the original paper <ref type="bibr" target="#b9">(Han et al., 2017)</ref>. We train FastText on the whole WSJ dataset for 100 epochs with window size 3 and embedding dimension 100.</p><p>Algorithm 2: Inside algorithm for sibling-NDMV notation LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 initialization ∀i, g C g,0 i,i,0 = D[i, g, 0, 0, 0] C g,1 i,i,0 = D[i, g, 0, 1, 0] C g,0 i,i,1 = D[i, g, 1, 0, 0] C g,1 i,i,1 = D[i, g, 1, 1, 0] for w = 1...(n − 1)</p><p>for i = 1...(n − w) j = i + w for g &lt; i or g &gt; j I g,0 i,j,0 = logsumexp i≤r&lt;j C 0,1 i,r,1 + C i,0 r+1,j,0 + D[j, g, 0, 0, 1] + S[j, i, g, 0, 0] I g,1 i,j,0 = logsumexp i≤r&lt;j C 0,1 i,r,1 + C i,0 r+1,j,0 + D[j, g, 0, 1, 1] + S[j, i, g, 0, 1] I g,0 i,j,1 = logsumexp i≤r&lt;j C j,0 i,r,1 + C 0,1 r+1,j,0 + D[i, g, 1, 0, 1] + S[i, j, g, 1, 0] I g,1 i,j,1 = logsumexp i≤r&lt;j C j,0 i,r,1 + C 0,1 r+1,j,0 + D[i, g, 1, 1, 1] + S[i, j, g, 1, 1] C g,0 i,j,0 = logsumexp i≤r≤j C 0,1 i,r,0 + I i,0 r,j,0 C g,1 i,j,0 = logsumexp i≤r≤j C 0,1 i,r,0 + I i,1 r,j,0 C g,0 i,j,1 = logsumexp i≤r≤j I j,0 i,r,1 + C 0,1 r,j,1 C g,1 i,j,1 = logsumexp i≤r≤j I j,1 i,r,1 + C 0,1 for i = 1...(n − w) j = i + w for g &lt; i or g &gt; j I g,0 i,j,0 = logsumexp i≤r&lt;j C 0,1 i,r,1 + C i,0 r+1,j,0 + D[j, g, 0, 0, 1] + D [j, 0, 0, 1] + S[j, i, g, 0, 0] + S [j, i, 0, 0] I g,1 i,j,0 = logsumexp i≤r&lt;j C 0,1 i,r,1 + C i,0 r+1,j,0 + D[j, g, 0, 1, 1] + D [j, 0, 1, 1] + S[j, i, g, 0, 1] + S [j, i, 0, 1] I g,0 i,j,1 = logsumexp i≤r&lt;j C j,0 i,r,1 + C 0,1 r+1,j,0 + D[i, g, 1, 0, 1] + D [i, 1, 0, 1] + S[i, j, g, 1, 0] + S [i, j, 1, 0] I g,1 i,j,1 = logsumexp i≤r&lt;j C j,0 i,r,1 + C 0,1 r+1,j,0 + D[i, g, 1, 1, 1] + D [i, 1, 1, 1] + S[i, j, g, 1, 1] + S [i, j, 1, 1] C g,0 i,j,0 = logsumexp i≤r≤j C 0,1 i,r,0 + I i,0 r,j,0 C g,1 i,j,0 = logsumexp i≤r≤j C 0,1 i,r,0 + I i,1 r,j,0 C g,0 i,j,1 = logsumexp i≤r≤j I j,0 i,r,1 + C 0,1 r,j,1 C g,1 i,j,1 = logsumexp i≤r≤j I j,1 i,r,1 + C 0,1 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our neural architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 4 :Figure 5 :</head><label>245</label><figDesc>Comparison of training methods on log-likelihood for WSJ. Comparison of training methods on log-likelihood for UD (French). Comparison of training methods on UAS for UD (French).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r,j,1 for i = 1..(n) P [i] = R[i] + C 0,1 1,i,0 P = logsumexp 1≤i≤n P [i] + C 0Inside algorithm for joint L-NDMV and sibling-NDMV notation LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 initialization ∀i, g C g,0 i,i,0 = D[i, g, 0, 0, 0] + D [i, 0, 0, 0] C g,1 i,i,0 = D[i, g, 0, 1, 0] + D [i, 0, 1, 0] C g,0 i,i,1 = D[i, g, 1, 0, 0] + D [i, 1, 0, 0] C g,1 i,i,1 = D[i, g, 1, 1, 0] + D [i, 1, 1, 0] for w = 1...(n − 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P</head><label></label><figDesc>= logsumexp 1≤i≤n P [i] + C 0,1 i,n,1 return P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison on Universal Dependency Treebanks. No UP: System using the universal linguistic prior. +UP : Systems using the universal linguistic prior. LD: LC-DMV</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of skip-connections and training methods on UD.</figDesc><table><row><cell>MODELS</cell><cell cols="3">LOG-LIKELIHOOD</cell><cell cols="2">UAS ( WSJ10 / WSJ)</cell></row><row><cell></cell><cell></cell><cell>DMO</cell><cell>EM</cell><cell></cell><cell>DMO</cell><cell>EM</cell></row><row><cell></cell><cell cols="3">w. SC w.o. SC w. SC</cell><cell>w. SC</cell><cell>w.o. SC</cell><cell>w. SC</cell></row><row><cell>sibling-NDMV</cell><cell>-17.3</cell><cell>-17.7</cell><cell>-17.3</cell><cell cols="2">77.5/64.5 75.4/63.1 77.7/64.5</cell></row><row><cell>grand-NDMV</cell><cell>-16.9</cell><cell>-17.4</cell><cell>-17.0</cell><cell cols="2">71.4/57.3 68.4/55.1 72.7/61.8</cell></row><row><cell>sibling-NDMV + L-NDMV</cell><cell>-53.2</cell><cell>-55.3</cell><cell>-53.5</cell><cell cols="2">79.9/67.5 78.2/64.7 79.1/66.5</cell></row><row><cell>grand-NDMV + L-NDMV</cell><cell>-53.8</cell><cell>-57.5</cell><cell>-54.2</cell><cell cols="2">76.0/64.3 73.7/60.7 75.2/65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The effect of joint training and joint parsing</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The universal linguistic prior is a set of syntactic dependencies that are common in many languages, proposed by<ref type="bibr" target="#b22">Naseem et al. (2010)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Natural Science Foundation of China (61976139).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Inside Algorithm and Parsing Algorithm</head><p>We use the dynamic programming substructure proposed for second-order supervised dependency parsing. For grandparent-child model, Koo and Collins (2010) augment both complete and incomplete spans with grandparent indices. They called the augmented span g-spans. Formally, they denote a complete g-span as C g h,e , where C h,e is a normal complete span in the Eisner algorithm, g is the grandparent's index, with the implication that (g, h) is a dependency. Incomplete g-span is defined similarly.</p><p>For second-order NDMV, we further augment incomplete and complete g-spans with valence information. We distinguish the direction of span explicitly, denoting our augmented complete v-span as C g,v h,e,d , where d is the direction, v is the valence, h is the start index and e is the end index of span compared with g-span. Incomplete v-span is defined similarly.</p><p>For grand-NDMV, given sentence x, we suppose that x 0 is the imaginary root token and x 1 , ..x n are tokens. We denote D[i, g, d, v, a] = log(p DECISION (decision = a|parent = x i , grand = x g , direction =</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crf autoencoder for unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inside-outside and forward-backward algorithms are just backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPNLP@EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Posterior sparsity in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="455" to="490" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convex and feature-rich discriminative approach to dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dependency grammar induction with neural lexicalization and big training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhancing unsupervised generative dependency parser with contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compound probabilistic context-free grammars for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Anna Korhonen, David R. Traum, and Lluís Màrquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corpus-based induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient third-order dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing: Let&apos;s use supervised parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="651" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dependency grammar induction with a neural variational transition-based parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online em for unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Agreement-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Hai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<meeting><address><addrLine>Natalia Silveira, Reut Tsarfaty, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Left-corner parsing for dependency grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="251" to="288" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Torch-struct: Deep structured prediction library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization with em and expectationconjugate-gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vasant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Second-order semantic dependency parsing with end-to-end neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient second-order treecrf for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3295" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
