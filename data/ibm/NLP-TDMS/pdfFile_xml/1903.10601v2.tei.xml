<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghui</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-unsupervised domain adaptation</term>
					<term>zero-shot learning</term>
					<term>locality preserving projection</term>
					<term>subspace learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation aims to transfer knowledge from a source domain to a target domain so that the target domain data can be recognized without any explicit labelling information for this domain. One limitation of the problem setting is that testing data, despite having no labels, from the target domain is needed during training, which prevents the trained model being directly applied to classify unseen test instances. We formulate a new cross-domain classification problem arising from real-world scenarios where labelled data is available for a subset of classes (known classes) in the target domain, and we expect to recognize new samples belonging to any class (known and unseen classes) once the model is learned. This is a generalized zero-shot learning problem where the side information comes from the source domain in the form of labelled samples instead of class-level semantic representations commonly used in traditional zero-shot learning. We present a unified domain adaptation framework for both unsupervised and zero-shot learning conditions. Our approach learns a joint subspace from source and target domains so that the projections of both data in the subspace can be domain invariant and easily separable. We use the supervised locality preserving projection (SLPP) as the enabling technique and conduct experiments under both unsupervised and zero-shot learning conditions, achieving state-of-the-art results on three domain adaptation benchmark datasets: Office-Caltech, Office31 and Office-Home.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Training a visual recognition model for image classification requires large amount of annotated data which hinders its application in many real-world scenarios where very few or no labelled images exist in the target domain. One solution is to use the labelled data from a different domain for training and apply the model to the target task. For example, if we assume our task is to classify artistic images for which we do not have much labelled data, it maybe conversely easier to get access to many labelled natural images. Training a classifier with the natural images and applying it directly to the artistic data suffers due to the inherent domain shift. To address this problem, domain adaptation approaches have been proposed to transform original features so that the transformed source and target features can be aligned <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Recently, deep feature learning approaches have drawn much attention by using endto-end deep models to learn domain-invariant features from different domains <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>Although significant efforts have been made to address the domain adaptation problem, most focus on unsupervised domain adaptation for which the target domain data are assumed to be accessible for learning although no labelling information is available. This is a strong assumption and hinders a direct application of the learned model to out-ofsample classification. In many real-world scenarios, it is easier to get some labelled examples for some classes than the others in the target domain. With these limited labelled samples from the target domain, it is worth investigating the possibility of domain adaptation without accessing the testing samples. For this purpose, we formulate a novel domain adaptation problem under the zero-shot learning (ZSL) <ref type="bibr" target="#b16">[17]</ref> condition and subsequently propose a viable solution to it.</p><p>Specifically, we present a unified framework for visual domain adaptation under both unsupervised and zero-shot learning conditions. Our approach aims to learn a subspace in which the domain and target data can be aligned and wellseparated. To this end, a supervised locality preserving projection (LPP) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> is employed as an enabling technique for subspace learning. For unsupervised domain adaptation, we propose a confidence-aware pseudo label selection scheme to gradually align the domains in an iterative learning strategy. To evaluate the effectiveness of the proposed approaches, we conduct experiments on commonly used datasets for domain adaptation, achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>To facilitate our presentation in the following sections, we firstly formulate domain adaptation problems under the unsupervised learning and zero-shot learning conditions respectively. Given a labelled dataset D s = {(x s i , y s i )}, i = 1, 2, ..., n s from the source domain S, x s i ∈ R d s represents the feature vector of i-th training example in the source domain, d s is the feature dimension in the source domain and y s i ∈ Y s denotes the corresponding label. For the unsupervised domain adaptation problem, the task is to classify an unlabelled dataset D t = {x t i }, i = 1, 2, ..., n t from the target domain T , where x t i ∈ R d t represents the feature vector in the target domain and d t is the dimensionality of features. The target label space Y t is equal to the source label space Y s . It is assumed that both the labelled source domain data D s and the unlabelled target domain data D t are available for unsupervised domain adaptation learning.</p><p>For a zero-shot learning condition, we have D s as above as well as a labelled dataset D tl = {(x tl i , y tl i )}, i = 1, 2, ..., n tl from the target domain T . x tl i ∈ R d t and y tl i ∈ Y tl are the feature and label of the i-th labelled example respectively. The Blue and Red markers represent data from source and target domains respectively. The Black markers represent learned class-level representations. The shapes of "triangle", "diamond" and "square" denote three different classes whilst the shape of "circle" represent unlabelled samples. Filled and hollow markers represent ground truth labelling and predictions respectively. The main difference between unsupervised and zero-shot learning conditions is the access of target data for training as denoted by the presence and absence of the hollow markers in the left of the two conditions illustrated above.</p><p>task is to classify any given new instance x t from the target domain by learning an inference model</p><formula xml:id="formula_0">y = f (x t ) ∈ Y t . It is noteworthy that Y tl ⊂ Y t = Y s ,</formula><p>that is, only a subset of the target labels have labelled training examples available during learning, while the instance to classify could belong to any class in the whole target label space. The domain adaptation problem under zero-shot learning conditions is relatively new and under-explored. It differs from the traditional unsupervised domain adaptation in two ways. On one hand, unsupervised domain adaptation assumes there is no labelled data from target domain, while in the zero-shot learning problem it is assumed there exist some labelled examples from the target domain although the labelled examples are only restricted to a subset of the whole target label space. On the other hand, the testing samples in the zero-shot learning condition are not available during training while in the unsupervised condition they are used for training together with other labelled data. An illustration for the two conditions is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In addition, the zero-shot learning condition is also different from supervised domain adaptation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and semi-supervised domain adaptation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> where labelled examples in the target domain are assumed to be available for all classes (i.e. Y tl = Y t ).</p><p>Domain adaptation under zero-shot learning condition poses different challenge from other domain adaptation problems. It could be easy to classify target data from known classes but quite difficult for those from unseen classes since models learned under this condition could bias to known classes and mistakenly classify all target data as known classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>In this section, we firstly review existing work related to unsupervised domain adaptation. Subsequently, we briefly describe zero-shot learning problems, how we formulate the domain adaptation problem under the zero-shot learning condition and existing work related to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation has attracted much attention in recent years. Existing approaches to unsupervised domain adaptation in literature can be roughly categorized into two groups: feature transformation approaches [1]- <ref type="bibr" target="#b6">[7]</ref> and deep feature learning approaches <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>Feature transformation approaches aim to transform the source domain and/or target domain features such that transformed source and target domain data can be aligned. As such the classifier learned from labelled source data can be directly applied to target data. Usually linear transformations are used by learning the projection matrices with different optimization objectives and a kernel trick can help to explore the non-linear relations between source and target domain data if necessary. The most commonly employed objective for unsupervised domain adaptation is to align data distributions in source and target domains <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. For this purpose, Maximum Mean Discrepancy (MMD) based distribution matching has been used to reduce differences of the marginal distributions <ref type="bibr" target="#b1">[2]</ref>, conditional distributions or both <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Manifold Embedded Distribution Alignment (MEDA) <ref type="bibr" target="#b6">[7]</ref> learns a domain-invariant classifier based on the transformed features where the transformation aims to align both the marginal and conditional distributions with quantitative account for their relative importance. Joint Geometrical and Statistical Alignment (JGSA) <ref type="bibr" target="#b3">[4]</ref> learns two coupled projections that project the source and target domain data into a joint subspace where the geometrical and distribution shifts are reduced simultaneously. Apart from the distribution alignment, recent feature transformation based approaches also promote the discriminative properties in the transformed features. Scatter Component Analysis (SCA) <ref type="bibr" target="#b4">[5]</ref> aims to learn a feature transformation such that the transformed data from different domains have similar scattering and the labelled data are well separated. A Linear Discriminant Analysis (LDA) framework was proposed in <ref type="bibr" target="#b22">[23]</ref> by learning class-specific projections. Similarly, Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed an approach to feature transformation towards Domain Invariant and Class Discriminative (DICD) features.</p><p>The proposed approach in this paper falls under this category since it learns a joint subspace from source and target domains. To learn the projection matrix transforming both source and target features into the joint subspace, we take advantage of pseudo labels of target data and iteratively update the projection matrix with the combination of labelled source data and pseudo-labelled target data. This strategy of using the pseudo labels of the target domain data with iterative learning has been employed in many approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In contrast to the iterative learning in the existing approaches which use all the pseudo-labelled target data, we select a part of target data which have been labelled with relatively higher confidence whilst ignore the ones with lower confidence in each iteration (see Section IV-D).</p><p>Deep feature learning approaches to domain adaptation were inspired by the success of deep Convolutional Neural Networks (CNN) in visual recognition <ref type="bibr" target="#b24">[25]</ref>. Attempts have been made to take advantage of the powerful representation learning capability of CNN combined with a variety of feature learning objectives. Most deep feature learning approaches aim to learn domain-invariant features from raw image data in source and target domains in an end-to-end framework. Specifically, the objectives of feature transformation approaches have been incorporated in the deep learning models. To learn the domain-invariant features through a deep CNN, the gradient reversal layer was proposed in <ref type="bibr" target="#b7">[8]</ref> and used in other deep feature learning approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> as well. The gradient reversal layer connects the feature extraction layers and the domain classifier layers. During backpropagation, the gradients of this layer multiplies a certain negative constant to ensure the feature distributions over two domains are made similar (as indistinguishable as possible for the domain classifier). Deep Adaptation Networks (DAN) <ref type="bibr" target="#b8">[9]</ref> and Residual Transfer Network (RTN) <ref type="bibr" target="#b10">[11]</ref> aim to learn transferable features from two domains by matching the domain distributions of multiple hidden layer features based on MMD. Deep CORAL <ref type="bibr" target="#b25">[26]</ref> integrates the idea of CORAL <ref type="bibr" target="#b2">[3]</ref> into a deep CNN framework to learn features with favoured properties (i.e. aligned correlations over source and target distributions for multiple layer activations). These approaches only consider the alignment of marginal distributions and cannot ensure the separability of target data. Deep Reconstruction Classification Network (DRCN) <ref type="bibr" target="#b26">[27]</ref> trains a feature learning model using labelled source data and unlabelled target data in the supervised and unsupervised learning manners respectively. More recently, the prevalent Generative Adversarial Network (GAN) loss has been employed in Adversarial Discriminative Domain Adaption (ADDA) <ref type="bibr" target="#b27">[28]</ref> with promising results.</p><p>Though deep learning based approaches are able to train the models in an end-to-end way, their performance on benchmark datasets has not outperformed the feature transformation based approaches especially when the deep features are used for feature transformation itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Zero-Shot Learning</head><p>Zero-shot learning (ZSL) aims to recognize novel classes by transferring knowledge learned from known classes to unseen classes <ref type="bibr" target="#b16">[17]</ref>. ZSL has attracted much attention since it provides a promising solution to the sparse labelling issues in real world applications. In traditional zero-shot visual recognition tasks, the source domain data are usually of a different modality such as human-defined class attributes and large corpus hence it suffers from the semantic gap between visual and semantic representations <ref type="bibr" target="#b16">[17]</ref>. Since the domain adaptation problem under the zero-shot learning condition formulated in Section II assumes both source and target data are from visual domain, the semantic gap issue suffered in traditional zero-shot learning tasks can be alleviated though the domain shift still exists. Traditional ZSL approaches can only tackle the class-level semantic representations (e.g., attributes and word vectors) even the source domain data come with multiple labelled examples <ref type="bibr" target="#b28">[29]</ref>. As a result, most existing ZSL methods are not ready to be directly applied in our proposed problem.</p><p>Domain adaptation under the zero-shot learning condition has been investigated in <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b18">[19]</ref>. However, this work only focused on the conventional zero-shot learning <ref type="bibr" target="#b30">[31]</ref> where the test instances are restricted to be only from unseen classes. Our work aims to address the generalized zero-shot learning problem <ref type="bibr" target="#b30">[31]</ref> which arises from a more realistic situation where test instances can belong to any class (i.e. either known or unseen classes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>The proposed method aims to learn a subspace from the domain and target features so that the transformed features in the subspace are domain-invariant and well-separated. We have many options for this purpose including linear discriminant analysis (LDA) and supervised locality preserving projection (SLPP). According to <ref type="bibr" target="#b16">[17]</ref>, SLPP has favourable properties that data structures can be preserved after projection hence avoiding overfitting to the training data. It is therefore appropriate for the problems where test data and training data have different distributions. For unsupervised domain adaptation, there is no labelling information available for target domain data. To address this issue, we use pseudo labels generated by a classifier (e.g. nearest neighbour). To avoid the wrongly labelled target instances undermining the subspace learning process by propagating the errors to the next iteration, we propose a Confidence-Aware Pseudo Label Selection (CAPLS) scheme. For the zero-shot learning condition, we use the labelled source data and labelled target data to learn the subspace and project both source and target data into the subspace in which the out-of-sample classification can be done using nearest neighbour (to the learned class-level representations). In the following subsections, we describe each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preprocessing</head><p>As the first step of our approach, l2 normalization is applied to all data as follows:x = x/||x|| 2 .</p><p>(1)</p><p>The use of l2 normalization results in the data points distributed on the surface of a hyper-sphere which will help to align data from different domains <ref type="bibr" target="#b16">[17]</ref>. Our experimental results in this study also provide empirical evidence that sample normalization is beneficial to superior performance. Besides, feature normalization (e.g., Z-score normalization) might be needed depending on the features used, which, however, is not a must for deep features employed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Joint Subspace Learning</head><p>We aim to learn a subspace to which the source and target domain data can be projected by a projection matrix and the projected data are domain-invariant and well-separated. To this end, the supervised locality preserving projection <ref type="bibr" target="#b16">[17]</ref> is employed. We denote the projection matrix as P ∈ R d s ×d , where d is the dimensionality of the subspace and d s is assumed to be equal to d t in the following presentation for convenience without loss of generality.</p><p>In SLPP, the following cost function is employed to learn the projection matrix P :</p><formula xml:id="formula_1">L(P ; W, X l ) = i,j ||P T x i − P T x j || 2 2 W ij<label>(2)</label></formula><p>where x i is the i-th column of the labelled data matrix X l which is a collection of labelled data in both source and target domains and each column vector represents an instance. The similarity matrix W is defined as follows:</p><formula xml:id="formula_2">W ij = 1, y i = y j , 0, otherwise<label>(3)</label></formula><p>That is, W ij is set to 1 when x i and x j have the same label regardless of which domain they are from, otherwise the value is set 0. This is different from the original SLPP in <ref type="bibr" target="#b16">[17]</ref> where the distances between samples are considered to construct the similarity matrix W . Here we ignore the within-class sample distances since we aim to align the source and domain data in the learned subspace. It has been proved empirically that Eq. <ref type="formula" target="#formula_2">(3)</ref> is sufficient to capture the intrinsic data structures to learn a domain-invariant yet discriminative subspace by minimizing the cost function defined in Eq. <ref type="formula" target="#formula_1">(2)</ref>. Minimizing the cost function in Eq.(2) enables the instances of the same class stay close to each other in the learned subspace no matter whether they are from the same domain or different domains. Following the treatment in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the objective can be rewritten in the following form:</p><formula xml:id="formula_3">max P T r(P T X l DX l T P ) T r(P T (X l LX l T + I)P )<label>(4)</label></formula><p>where L = D−W is the laplacian matrix, D is a diagonal matrix with D ii = j W ij and the regularization term T r(P T P ) is added for penalizing extreme values in the projection matrix P . The problem defined in Eq.(4) is equivalent to the following generalized eigenvalue problem:</p><formula xml:id="formula_4">X l DX l T p = λ(X l LX l T + I)p,<label>(5)</label></formula><p>solving the generalized eigenvalue problem gives the optimal solution P = [p 1 , ..., p d ] where p 1 , ..., p d are the eigenvectors corresponding to the largest d eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recognition in Subspace</head><p>Once the joint subspace is learned, we can project data from either source or target domain into the subspace by:</p><formula xml:id="formula_5">z i = P T x i<label>(6)</label></formula><p>where z i is the projection of x i in the subspace.</p><p>Algorithm 1 Domain Adaptation Under Zero-Shot Learning condition Input: The labelled source data D s = {(x s i , y s i )}, i = 1, 2, ..., n s and labelled target data D tl = {(x tl i , y tl i )}, i = 1, 2, ..., n tl , dimensionality of subspace d, test instance from target domain x t . Output: The projection matrix P and predicted labelŷ t .</p><p>1: Do data pre-processing using Eq.(1); 2: Learn the joint subspace (projection matrix P ) using Eq.(5); 3: Predict the labelŷ t for test instance x t using Eq.(9).</p><p>To facilitate the separability of data projected into the subspace, we follow <ref type="bibr" target="#b16">[17]</ref> and apply the centralization (i.e. mean subtraction) and l2 normalization to all the projections:</p><formula xml:id="formula_6">z ← z −z,<label>(7)</label></formula><formula xml:id="formula_7">z ← z/||z|| 2 ,<label>(8)</label></formula><p>wherez is the mean of all the projected training data (i.e., all the source and target data for unsupervised domain adaptation condition; source data and labelled target data for zero-shot learning condition). Given any instance x t from the target domain, we now predict its corresponding label y t . We firstly project the instance from target domain into the subspace by Eq.(6) and then apply the centralization and normalization by Eq. <ref type="formula" target="#formula_6">(7)</ref> and Eq. <ref type="bibr" target="#b7">(8)</ref>. The label of a target instance is then predicted with: </p><formula xml:id="formula_8">y t = arg min y ||z t −z y || 2 , y ∈ Y t ,<label>(9)</label></formula><p>is the mean vector of the projected source data whose labels are y, δ(y, y i ) = 1 if y = y i and 0 otherwise. Note that the class meansz y are calculated using only labelled source data for unsupervised domain adaptation and labelled target data are also used for zero-shot learning problem. Following <ref type="bibr" target="#b16">[17]</ref>, we apply l2 normalization toz y before using them in Eq.(9). To this point, it is straightforward to apply the proposed approach to the zero-shot learning condition and the algorithm is summarized in Algorithm 1. For unsupervised domain adaptation, however, we only have labelled data from source domain and labelled target data are needed for domain alignment. To this end, we use pseudo-labelled target instances and the iterative learning strategy described in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Unsupervised Domain Adaptation Using CAPLS</head><p>Our domain adaptation framework based on joint subspace learning (c.f. Section IV-B) requires labelled data from both source and target domains for domain alignment. However, for unsupervised domain adaptation problem, we do not have any labelled target data. To address issue, as mentioned above, we use pseudo labelled target for domain-invariant subspace learning. Specifically, we learn a projection matrix P 0 using only labelled source data and get the pseudo labels of all the target data using Eq. <ref type="bibr" target="#b8">(9)</ref>. Once the target data are labelled, we combine the pseudo-labelled target data with the labelled source data and relearn the projection matrix P . This process is repeated for multiple times, as a result, the learned subspace becomes more domain-invariant and discriminative until convergence.</p><p>One drawback of the iterative learning strategy used in most exiting approaches is that the classification errors in the early iterations will be propagated to the later iterations thus leading the algorithm to a sub-optimal solution. To alleviate this issue, we propose a confidence-aware pseudo label selection scheme. Instead of using all the pseudo-labelled target data, we select a portion of them with high confidence to combine with labelled source data for the next iteration learning.</p><p>Specifically, we transform the distance from a given test instance z to the i-th class representationz i (i.e. d i = ||z − z i || 2 in Eq.(9) into probability q i using the following softmax function:</p><formula xml:id="formula_10">q i = e −di |Y s | i=1 e −di<label>(11)</label></formula><p>where |Y s | is the number of labels of the source data (also that of the target data). q i denotes the probability that the given test instance belongs to the i-th class. Now we use Q ∈ R n t ×|Y s | to collectively denote the predicted probability matrix of all the target data and Q ij denotes the probability of i-th target instance belong to j-th class. For each class, there is no doubt that the target instances labelled as this class with higher probabilities are more likely correctly labelled, hence they should be selected to participate in the next iteration of learning. As a result, in t-th iteration, we select top t/T percent pseudo-labelled target instances for each class as trustable ones for the next-iteration learning, where T is the total number of iteration. It is noteworthy that the selection is class-wise so that there exists pseudo-labelled target data selected for each class. The complete unsupervised domain adaptation approach using CAPLS is summarized in Algorithm 2 whose time complexity is O(T (d 3 + dn 2 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT</head><p>In this section, we describe our experiments on three commonly used domain adaptation datasets (i.e. Office+Caltech <ref type="bibr" target="#b33">[34]</ref>, Office31 <ref type="bibr" target="#b29">[30]</ref> and Office-Home <ref type="bibr" target="#b34">[35]</ref>) under unsupervised and zero-shot learning conditions. The experimental results are presented and compared with those of state-of-the-art domain adaptation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Office+Caltech dataset is one of the most commonly used datasets for unsupervised domain adaptation released by Gong et al. <ref type="bibr" target="#b33">[34]</ref>. The dataset consists of four domains: Amazon (images downloaded from online merchants), Webcam (lowresolution images by a web camera), DSLR (high-resolution images by a digital SLR camera) and Caltech-256. 10 common Algorithm 2 Unsupervised Domain Adaptation Using CAPLS Input: The labelled source data D s = {(x s i , y s i )}, i = 1, 2, ..., n s and unlabelled target data D t = {(x t i }, i = 1, 2, ..., n t , dimensionality of subspace d, number of iteration T . Output: The projection matrix P and predicted label {ŷ t }.</p><p>1: Initialize t = 0; 2: Do data pre-processing using Eq.(1); <ref type="bibr">3:</ref> Learn the projection P 0 using only D s ; 4: Assign pseudo labels for all target data using Eq.(9); 5: while t ≤ T do <ref type="bibr">6:</ref> t ← t + 1;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Select top t/T percent trustful pseudo-labelled target data for each class; <ref type="bibr">8:</ref> Learn P t using D s and selected pseudo-labelled target data; <ref type="bibr">9:</ref> Update pseudo labels for all target data using Eq.(9). 10: end while classes from all four domains are used: backpack, bike, calculator, headphone, computer-keyboard, laptop-101, computermonitor, computer-mouse, coffee-mug, and video-projector. There are 2533 images in total with 8 to 151 images per category per domain. The Decaf6 <ref type="bibr" target="#b35">[36]</ref> features (activations of the 6th fully connected layer of a convolutional neural network trained on ImageNet) are used in our experiments for a direct comparison with others.</p><p>Office31 dataset <ref type="bibr" target="#b29">[30]</ref> is also a benchmark dataset commonly used for evaluating different domain adaptation approaches. The dataset consists of three domains: Amazon, Webcam and DSLR. There are 31 common classes for all three domains containing 4,110 images in total. ResNet50 <ref type="bibr" target="#b36">[37]</ref> has been commonly used to extract features or as the backbone of deep models in literature, hence we use ResNet50 features in our experiments for this dataset.</p><p>Office-Home dataset <ref type="bibr" target="#b34">[35]</ref> is another dataset recently released for evaluation of domain adaptation algorithms. It consists of four different domains: Artistic images, Clipart, Product images and Real-World images. There are 65 object classes in each domain with a total number of 15,588 images. Again, we extracted ResNet50 features in our experiments for fair comparisons with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Unsupervised Domain Adaptation</head><p>To evaluate the effectiveness of our proposed approach on unsupervised domain adaptation, we conduct comparative experiments on all three datasets. Following the standard protocols <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we exhaustively select two different domains from one dataset as the source domain and target domain respectively, which allow us to have 12, 6 and 12 combinations for Office+Caltech, Office31 and Office-Home datasets respectively. We compare the performance of our approaches with those of typical state-of-the-art methods including both feature transformation and deep feature learning approaches. The dimension of subspace We follow <ref type="bibr" target="#b6">[7]</ref> and use per-image accuracy as the evaluation metric in all our experiments.</p><p>In addition, we also report the performance of some baseline methods to evaluate how different components of our approach contribute to the final performance. To evaluate the contribution of the CAPLS strategy, we remove it from our approach and report the performance of the baseline method dubbed SLPP. To compare the performance of different subspace learning algorithms, we replace the SLPP with LDA and report the performance of this baseline which is named as CAPLS(LDA).</p><p>Our approach consists of two hyper-parameters: the dimensionality of subspace d and the number of iterations T . We conduct an experiment to investigate how our approach is sensitive to these two hyper-parameters. Firstly, we fix T = 20 and set d = {16, 32, 64, 128, 256, 512} respectively and get results presented in <ref type="figure" target="#fig_2">Figure 2</ref> (left). It is obvious that when d is greater than 64, we can achieve stable accuracy for all three datasets. Secondly, we set d = 128 and T = {5, 10, 15, 20, 25, 30} respectively and get results shown in <ref type="figure" target="#fig_2">Figure 2</ref> (right), which indicates our approach is not sensitive to the number of iterations T . As a result, we set fixed values as d = 128 and T = 20 across all the experiments in this section. <ref type="table" target="#tab_0">Table I</ref> shows the results of comparative experiments on Of-fice+Caltech dataset under the unsupervised domain adaptation condition. For a fair comparison, the results of all methods are based on Decaf6 features. By comparing our approach with two baseline methods, i.e., SLPP and CAPLS(LDA), we can see that both the SLPP based joint subspace learning and the confidence-aware pseudo label selection scheme play important roles in achieving good performance for unsupervised domain adaptation. We compare our approach with three deep learning based methods (i.e. DCORAL <ref type="bibr" target="#b25">[26]</ref>, DDC <ref type="bibr" target="#b31">[32]</ref> and DAN <ref type="bibr" target="#b8">[9]</ref>, GTA <ref type="bibr" target="#b32">[33]</ref>). Our approach achieves superior accuracy than the deep feature learning models in most tasks and a superior average accuracy of 91.8%. When compared with other feature transformation approaches, our approach ranks the second in terms of the average accuracy over 12 tasks with slightly worse performance than MEDA <ref type="bibr" target="#b6">[7]</ref> which is a combination of manifold feature learning and dynamic distribution alignment techniques. <ref type="table" target="#tab_0">Table II</ref> shows the results of comparative experiments on Office31 dataset under the unsupervised domain adaptation condition. For a fair comparison, all the feature transformation approaches use ResNet50 features and all the deep learning methods use ResNet50 as their backbones. Our proposed approach achieves the best performance of 88.2% in terms of the average accuracy over 6 tasks, outperforming seven state-of-the-art deep feature learning methods (i.e., DAN <ref type="bibr" target="#b25">[26]</ref>, JDDA <ref type="bibr" target="#b12">[13]</ref>, RTN <ref type="bibr" target="#b10">[11]</ref>, MADA <ref type="bibr" target="#b13">[14]</ref>, iCAN <ref type="bibr" target="#b14">[15]</ref> and CDAN-M <ref type="bibr" target="#b15">[16]</ref>) and the competitive feature transformation approach MEDA <ref type="bibr" target="#b6">[7]</ref>. The comparison with two baseline methods also indicate the effectiveness of SLPP as the subspace learning algorithm and the necessity of CAPLS for unsupervised domain adaptation.   there are very few results reported on it. We compare with the results from supplementary materials of <ref type="bibr" target="#b6">[7]</ref>. Again ResNet50 is used for feature extraction or backbone networks for a fair comparison. We can see that our proposed approach outperforms others significantly with the average accuracy of 70.6%. In addition, the two baseline methods have worse results than our full model which validates the effectiveness of our framework.</p><p>Although our method achieves state-of-the-art results when deep features are employed, its performance degrades when hand-crafted features are used on Office-10 dataset for which the experimental results are not presented in this paper. One possible reason is our method favours the feature space with smaller domain shift which provides more correct pseudolabels at the beginning of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Zero-Shot Learning</head><p>To evaluate the proposed framework under a zero-shot learning condition, we conduct experiments on Office-Home dataset which consists of a sufficient number (65) of classes for ZSL. The source and target domain features are extracted by ResNet50 pre-trained on ImageNet. To simulate a ZSL scenario, we randomly select 35 classes as known classes for which there are labelled target data during subspace learning and the rest 30 classes are "unseen" for which no target data are available for learning. In our experiments, the "RealWorld" domain serves as source domain whilst "Art", "Clipart" and "Product" domains serve as target domain respectively. The reason is "RealWorld" domain data are usually easier to collect than other three domains in realistic applications. As a result, we have three tasks: R→A, R→C and R→P. In each task, all source domain ("RealWorld") data are used for training. In addition, half of the target data in each class are reserved for testing and the other half for training only if they belong to known classes.</p><p>We use fixed training/test data split for target domain data and randomly generate 5 known/unseen class splits for a thorough evaluation <ref type="bibr" target="#b0">1</ref> . Considering the unbalanced number of test images in different classes, we use per-class mean accuracy for evaluation in this experiment. Following the common way in evaluating generalized zero-shot learning algorithms <ref type="bibr" target="#b30">[31]</ref>, we report the mean accuracy on known classes Acc known and unseen classes Acc unseen respectively as well as their harmonic mean H = 2 * Acc known * Acc unseen /(Acc known +Acc unseen ).</p><p>We compare our approach with two baseline methods: 1 Nearest Neighbour (1NN) and Support Vector Machine (SVM). All source domain data and labelled target domain data are combined for training without considering any domain alignment in these two baseline methods. We also compare with state-of-the-art zero-shot learning algorithms. As mentioned in Section II, traditional zero-shot learning algorithms can only handle class-level representations in the source domain. To adapt them to our problem, we calculate class-level source domain representations by averaging the source domain samples belonging to the same class. After this adaptation, we apply two zero-shot learning algorithms, i.e., bidirectional latent embedding learning (BiDiLEL) <ref type="bibr" target="#b16">[17]</ref> and manifold regularized ridge regression (MR) <ref type="bibr" target="#b37">[38]</ref> to our problem for the comparison. We also replace the SLPP with LDA in our framework to validate the importance of preserving data structure when learning subspace of favourable properties. <ref type="table" target="#tab_0">Table IV</ref> shows the results of comparative experiments on Office-Home dataset under zero-shot learning condition. We can see that the adapted zero-shot learning algorithms fail in this problem with very low classification accuracy in terms of unseen classes though BiDiLEL achieves the best accuracy on known classes. As a result, two adapted zero-shot learning algorithms perform much worse than the baseline methods in terms of the harmonic mean H. Our proposed framework works reasonably well on all three tasks with high classification accuracy for both known and unseen classes. With the use of SLPP, our approach achieves the best classification accuracy on unseen classes and second best on known classes. The experimental results provide evidence that our proposed framework is a promising solution to domain adaptation problems under zero-shot learning conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a unified framework for visual domain adaptation under unsupervised and zero-shot learning conditions. A domain-invariant and discriminative subspace is learned by SLPP using labelled/pseudo-labelled data from both domains. A confidence-aware pseudo label selection scheme is proved to be effective for choosing proper pseudo-labelled target samples in the iterative learning. Our experimental results on unsupervised domain adaptation and zero-shot learning problem prove that the proposed approach achieves state-ofthe-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The proposed framework of domain adaptation for unsupervised (upper) and zero-shot learning (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y, y s i ) + j z lt j δ(y, y lt j ) i δ(y, y s i ) + j δ(y, y lt j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The effect of different values of d (left) and T (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>ACCURACY (%) ON OFFICE-CALTECH DATASET FOR UNSUPERVISED DOMAIN ADAPTATION. THE FEATURE TRANSFORMATION APPROACHES USE DECAF6 FEATURES. COLUMNS DISPLAY RESULTS OF SOURCE → TARGET PAIRS.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell>C→A</cell><cell>C→W</cell><cell cols="2">C→D A→C</cell><cell cols="6">A→W A→D W→C W→A W→D D→C D→A D→W Average</cell></row><row><cell></cell><cell></cell><cell cols="3">DCORAL [26]</cell><cell>92.4</cell><cell>91.1</cell><cell>91.4</cell><cell>84.7</cell><cell>-</cell><cell>-</cell><cell>79.3</cell><cell>-</cell><cell>-</cell><cell>82.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">DDC [32]</cell><cell></cell><cell>91.9</cell><cell>85.4</cell><cell>88.8</cell><cell>85.0</cell><cell>86.1</cell><cell>89.0</cell><cell>78.0</cell><cell>84.9</cell><cell>100.0</cell><cell>81.1</cell><cell>89.5</cell><cell>98.2</cell><cell>88.2</cell></row><row><cell></cell><cell></cell><cell cols="2">DAN [9]</cell><cell></cell><cell>92.0</cell><cell>90.6</cell><cell>89.3</cell><cell>84.1</cell><cell>91.8</cell><cell>91.7</cell><cell>81.2</cell><cell>92.1</cell><cell>100.0</cell><cell>80.3</cell><cell>90.0</cell><cell>98.5</cell><cell>90.1</cell></row><row><cell></cell><cell></cell><cell cols="2">CORAL [6]</cell><cell></cell><cell>92.0</cell><cell>80.0</cell><cell>84.7</cell><cell>83.2</cell><cell>74.6</cell><cell>84.1</cell><cell>75.5</cell><cell>81.2</cell><cell>100.0</cell><cell>76.8</cell><cell>85.5</cell><cell>99.3</cell><cell>84.7</cell></row><row><cell></cell><cell></cell><cell cols="2">SCA [5]</cell><cell></cell><cell>89.5</cell><cell>85.4</cell><cell>87.9</cell><cell>78.8</cell><cell>75.9</cell><cell>85.4</cell><cell>74.8</cell><cell>86.1</cell><cell>100.0</cell><cell>78.1</cell><cell>90.0</cell><cell>98.6</cell><cell>85.9</cell></row><row><cell></cell><cell></cell><cell cols="2">JDA [1]</cell><cell></cell><cell>89.6</cell><cell>85.1</cell><cell>89.8</cell><cell>83.6</cell><cell>78.3</cell><cell>80.3</cell><cell>84.8</cell><cell>90.3</cell><cell>100.0</cell><cell>85.5</cell><cell>91.7</cell><cell>99.7</cell><cell>88.2</cell></row><row><cell></cell><cell></cell><cell cols="2">JGSA [4]</cell><cell></cell><cell>91.4</cell><cell>86.8</cell><cell>93.6</cell><cell>84.9</cell><cell>81.0</cell><cell>88.5</cell><cell>85.0</cell><cell>90.7</cell><cell>100.0</cell><cell>86.2</cell><cell>92.0</cell><cell>99.7</cell><cell>90.0</cell></row><row><cell></cell><cell></cell><cell cols="2">MEDA [7]</cell><cell></cell><cell>93.4</cell><cell>95.6</cell><cell>91.1</cell><cell>87.4</cell><cell>88.1</cell><cell>88.1</cell><cell>93.2</cell><cell>99.4</cell><cell>99.4</cell><cell>87.5</cell><cell>93.2</cell><cell>97.6</cell><cell>92.8</cell></row><row><cell></cell><cell></cell><cell cols="2">SLPP</cell><cell></cell><cell>91.3</cell><cell>73.6</cell><cell>86.6</cell><cell>82.6</cell><cell>72.2</cell><cell>82.8</cell><cell>71.8</cell><cell>79.5</cell><cell>100.0</cell><cell>79.2</cell><cell>88.5</cell><cell>99.3</cell><cell>84.0</cell></row><row><cell></cell><cell></cell><cell cols="3">CAPLS(LDA)</cell><cell>91.1</cell><cell>85.4</cell><cell>94.9</cell><cell>83.5</cell><cell>86.4</cell><cell>90.4</cell><cell>87.7</cell><cell>92.5</cell><cell>100.0</cell><cell>87.8</cell><cell>92.4</cell><cell>99.7</cell><cell>91.0</cell></row><row><cell></cell><cell></cell><cell cols="2">CAPLS(Ours)</cell><cell></cell><cell>90.8</cell><cell>85.4</cell><cell>95.5</cell><cell>86.1</cell><cell>87.1</cell><cell>94.9</cell><cell>88.2</cell><cell>92.3</cell><cell>100.0</cell><cell>88.8</cell><cell>93.0</cell><cell>100.0</cell><cell>91.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">CLASSIFICATION ACCURACY (%) ON OFFICE31 DATASET FOR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">UNSUPERVISED DOMAIN ADAPTATION. THE FEATURE TRANSFORMATION</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">APPROACHES USE RESNET50 FEATURES AND DEEP MODELS USE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">RESNET50 AS BACKBONES. RESULTS ARE FROM THE ORIGINAL PAPER</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">EXCEPT THE ONES LABELLED WITH  *  FOR WHICH WE REPORT THE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">RESULTS FROM THE SUPPLEMENTARY MATERIAL OF [7].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="4">A→W D→W W→D</cell><cell cols="4">A→D D→A W→A Average</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DAN  *  [9]</cell><cell>80.5</cell><cell cols="2">97.1</cell><cell>99.6</cell><cell>78.6</cell><cell>63.6</cell><cell>62.8</cell><cell>80.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">JDDA [13]</cell><cell>82.6</cell><cell cols="2">95.2</cell><cell>99.7</cell><cell>79.8</cell><cell>57.4</cell><cell>66.7</cell><cell>80.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RTN [11]</cell><cell>84.5</cell><cell cols="2">96.8</cell><cell>99.4</cell><cell>77.5</cell><cell>66.2</cell><cell>64.8</cell><cell>81.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MADA [14]</cell><cell>90.0</cell><cell cols="2">97.4</cell><cell>99.6</cell><cell>87.8</cell><cell>70.3</cell><cell>66.4</cell><cell>85.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GTA [33]</cell><cell>89.5</cell><cell cols="2">97.9</cell><cell>99.8</cell><cell>87.7</cell><cell>72.8</cell><cell>71.4</cell><cell>86.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">iCAN [15]</cell><cell>92.5</cell><cell cols="2">98.8</cell><cell>100.0</cell><cell>90.1</cell><cell>72.1</cell><cell>69.9</cell><cell>87.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CDAN-M [16]</cell><cell>93.1</cell><cell cols="2">98.6</cell><cell>100.0</cell><cell>92.9</cell><cell>71.0</cell><cell>69.3</cell><cell>87.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MEDA [7]</cell><cell>86.2</cell><cell cols="2">97.2</cell><cell>99.4</cell><cell>85.3</cell><cell>72.4</cell><cell>74.0</cell><cell>85.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SLPP</cell><cell>77.9</cell><cell cols="2">97.4</cell><cell>99.2</cell><cell>80.1</cell><cell>68.4</cell><cell>66.2</cell><cell>81.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CAPLS(LDA)</cell><cell>77.0</cell><cell cols="2">99.1</cell><cell>99.8</cell><cell>77.9</cell><cell>61.8</cell><cell>60.8</cell><cell>79.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CAPLS(Ours)</cell><cell>90.6</cell><cell cols="2">98.6</cell><cell>99.6</cell><cell>88.6</cell><cell>75.4</cell><cell>76.3</cell><cell>88.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc>III displays the results of comparative experiments on Office-Home dataset. Since this is a relatively new dataset,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ACCURACY (%) ON OFFICE-HOME DATASET FOR UNSUPERVISED DOMAIN ADAPTATION. THE FEATURE TRANSFORMATION APPROACHES USE RESNET50 FEATURES AND DEEP MODELS USE RESNET50 AS BACKBONES. RESULTS ARE FROM THE ORIGINAL PAPER EXCEPT THE ONES LABELLED WITH * FOR WHICH WE REPORT THE RESULTS FROM THE SUPPLEMENTARY MATERIAL OF<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Method</cell><cell>A→C</cell><cell cols="12">A→P A→R C→A C→P C→R P→A P→C P→R R→A R→C R→P Average</cell></row><row><cell>DAN  *  [9]</cell><cell>43.6</cell><cell>57.0</cell><cell>67.9</cell><cell>45.8</cell><cell>56.5</cell><cell>60.4</cell><cell>44.0</cell><cell>43.6</cell><cell>67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3</cell><cell>56.3</cell></row><row><cell>JAN  *  [12]</cell><cell>45.9</cell><cell>61.2</cell><cell>68.9</cell><cell>50.4</cell><cell>59.7</cell><cell>61.0</cell><cell>45.8</cell><cell>43.4</cell><cell>70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8</cell><cell>58.3</cell></row><row><cell>CDAN-M  *  [16]</cell><cell>50.6</cell><cell>65.9</cell><cell>73.4</cell><cell>55.7</cell><cell>62.7</cell><cell>64.2</cell><cell>51.8</cell><cell>49.1</cell><cell>74.5</cell><cell>68.2</cell><cell>56.9</cell><cell>80.7</cell><cell>62.8</cell></row><row><cell>MEDA [7]</cell><cell>54.6</cell><cell>75.2</cell><cell>77.0</cell><cell>56.5</cell><cell>72.8</cell><cell>72.3</cell><cell>59.0</cell><cell>51.9</cell><cell>78.2</cell><cell>67.7</cell><cell>57.2</cell><cell>81.8</cell><cell>67.0</cell></row><row><cell>SLPP</cell><cell>49.3</cell><cell>70.5</cell><cell>74.9</cell><cell>55.7</cell><cell>68.9</cell><cell>69.7</cell><cell>57.2</cell><cell>47.3</cell><cell>75.4</cell><cell>67.5</cell><cell>53.0</cell><cell>80.5</cell><cell>64.2</cell></row><row><cell>CAPLS(LDA)</cell><cell>47.1</cell><cell>72.8</cell><cell>77.6</cell><cell>57.3</cell><cell>76.5</cell><cell>78.0</cell><cell>55.8</cell><cell>47.7</cell><cell>81.7</cell><cell>65.9</cell><cell>52.6</cell><cell>84.5</cell><cell>66.5</cell></row><row><cell>CAPLS(Ours)</cell><cell>56.2</cell><cell>78.3</cell><cell>80.2</cell><cell>66.0</cell><cell>75.4</cell><cell>78.4</cell><cell>66.4</cell><cell>53.2</cell><cell>81.1</cell><cell>71.6</cell><cell>56.1</cell><cell>84.3</cell><cell>70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY (%) ON OFFICE-HOME DATASET FOR ZERO-SHOT LEARNING CONDITION. WE REPORT THE MEAN ACCURACY OVER 5</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hellowangqian/domain-adaptation-capls.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5150" to="5158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correlation alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. JMLR. org</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition via bidirectional latent embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="383" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature space independent semi-supervised domain adaptation via kernel matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="54" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised deep domain adaptation via coupled neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5214" to="5224" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3403" to="3417" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain invariant and class discriminative feature learning for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4260" to="4273" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Alternative semantic representations for zeroshot human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot learninga comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="333" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
