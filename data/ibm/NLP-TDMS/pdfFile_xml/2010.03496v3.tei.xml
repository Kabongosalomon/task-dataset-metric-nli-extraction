<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inductive Entity Rep-resentations from Text via Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>April 19-23, 2021. 2021. April 19-23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Daza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam University of Amsterdam Discovery Lab</orgName>
								<address>
									<addrLine>Elsevier Amsterdam</addrLine>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
							<email>m.cochez@vu.nl</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Vrije Universiteit Amsterdam Discovery Lab, Elsevier Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Groth</surname></persName>
							<email>p.groth@uva.nl</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Discovery Lab, Elsevier Amsterdam</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inductive Entity Rep-resentations from Text via Link Prediction</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Reference Format</title>
						<meeting> <address><addrLine>Ljubljana, Slovenia; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">11</biblScope>
							<date type="published">April 19-23, 2021. 2021. April 19-23, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3450141</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>knowledge graphs</term>
					<term>entity representations</term>
					<term>link prediction</term>
					<term>entity classification</term>
					<term>information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graphs (KG) are of vital importance for multiple applications on the web, including information retrieval, recommender systems, and metadata annotation.</p><p>Regardless of whether they are built manually by domain experts or with automatic pipelines, KGs are often incomplete. To address this problem, there is a large amount of work that proposes using machine learning to complete these graphs by predicting new links. Recent work has begun to explore the use of textual descriptions available in knowledge graphs to learn vector representations of entities in order to preform link prediction. However, the extent to which these representations learned for link prediction generalize to other tasks is unclear. This is important given the cost of learning such representations. Ideally, we would prefer representations that do not need to be trained again when transferring to a different task, while retaining reasonable performance.</p><p>Therefore, in this work, we propose a holistic evaluation protocol for entity representations learned via a link prediction objective. We consider the inductive link prediction and entity classification tasks, which involve entities not seen during training. We also consider an information retrieval task for entity-oriented search. We evaluate an architecture based on a pretrained language model, that exhibits strong generalization to entities not observed during training, and outperforms related state-of-the-art methods (22% MRR improvement in link prediction on average). We further provide evidence that the learned representations transfer well to other tasks without fine-tuning. In the entity classification task we obtain an average improvement of 16% in accuracy compared with baselines that also employ pre-trained models. In the information retrieval task, we obtain significant improvements of up to 8.8% in NDCG@10 for natural language queries. We thus show that the learned representations are not limited KG-specific tasks, and have greater generalization properties than evaluated in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Statistical relational learning; • Information systems → Information retrieval. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs provide a structured way to represent information in the form of entities and relations between them <ref type="bibr" target="#b11">[12]</ref>. They have become central to a variety of tasks in the Web, including information retrieval <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, question answering <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">43]</ref>, and information extraction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Many of these tasks can benefit from distributed representations of entities and relations, also known as embeddings.</p><p>A large body of work in representation learning in KGs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">45]</ref> is based on the optimization of a link prediction objective, which results in embeddings that model relations in a vector space. These approaches are often touted as an alternative to logic-based systems for inference in incomplete KGs, as they can assign a score to missing links <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> They have also been proposed for implementing approximate forms of reasoning for the Semantic Web <ref type="bibr" target="#b17">[18]</ref>. However, by design, some of these methods can only compute predictions involving entities observed during training. This results in approaches that fail when applied to real-world, dynamic graphs where new entities continue to be added.</p><p>To overcome this challenge, we look to make use of the textual information within KGs. KGs like YAGO <ref type="bibr" target="#b37">[38]</ref>, DBpedia <ref type="bibr" target="#b0">[1]</ref>, and industry-deployed KGs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>, contain rich textual attributes about entities such as names, dates, and descriptions <ref type="bibr" target="#b11">[12]</ref>. Therefore, it seems reasonable to assume that for real-world applications, attribute data such as entity descriptions are readily available.</p><p>From this perspective, methods that treat KGs merely as a collection of nodes and labeled links are needlessly discarding a valuable source of information. Previous work has proposed to use textual descriptions for learning entity representations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52]</ref>, which results in a much more flexible approach, since entity representations are computed as a function of their textual description, and can thus be obtained even for entities not observed during training. Unfortunately, the evaluation protocol in these works mainly focuses on the task of link prediction, leaving other potential outcomes of such a flexible approach unexplored.</p><p>The motivation for seeking representations that generalize well is that they can be applied in a variety of settings for which they arXiv:2010.03496v3 [cs.CL] 14 Apr 2021</p><p>Earth is the third planet from the Sun. It is the ... The Sun is the star at the center of the Solar System. It ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Embedding Encoder Score Link prediction Earth is the third planet from the Sun. It is the ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Classifier location</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity classification</head><p>Intel Corporation is an American multinational...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity</head><p>Who founded Intel? Encoder Information retrieval 1 Intel 2 Avram Miller 3 Glenford Myers <ref type="figure">Figure 1</ref>: Overview of our work: using entity descriptions, an entity encoder is trained for link prediction in a knowledge graph (top). We show that the encoder can then be used without fine-tuning to obtain informative features for entity classification (middle) and information retrieval (bottom).</p><p>were not explicitly trained, while retaining reasonable performance. This avoids having to invest more resources on data collection, labelling, and fine-tuning when faced with a new task.</p><p>In this work, we are thus interested in the following research question: What are the generalization properties of entity representations learned via a link prediction objective? Our work towards answering this question results in the following contributions:</p><p>(1) We propose the use of a pretrained language model for learning representations of entities via a link prediction objective, and study its performance in combination with four different relational models. <ref type="bibr" target="#b1">(2)</ref> We propose a holistic evaluation framework for entity representations, that comprises link prediction, entity classification, and information retrieval. (3) We provide evidence that entity representations based on pretrained language models exhibit strong generalization properties across all tasks, outperforming the state-of-theart, and are thus not limited to KG-specific tasks.</p><p>The rest of this paper is organized as follows. In section 2 we discuss the related work. In section 3 we introduce the need for learning inductive entity representations, and motivate the use of a pretrained language model for the task. In section 4 we describe the experiments and results for the three tasks mentioned above. Finally, we conclude and highlight directions of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multiple methods in the literature of representation learning in KGs propose to learn an embedding for each entity and relation in a KG. Well known examples include RESCAL <ref type="bibr" target="#b28">[29]</ref>, TransE <ref type="bibr" target="#b1">[2]</ref>, and DistMult <ref type="bibr" target="#b51">[51]</ref>, among others <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">45]</ref>. While the state of the art in the task of link prediction continues to improve <ref type="bibr" target="#b34">[35]</ref>, most models essentially learn a lookup table of embeddings for entities and relations, and thus they are not applicable to the scenario where new entities are added to the graph.</p><p>A natural way to avoid this problem is to train entity encoders, that operate on a vector of entity attributes. Such encoders have been implemented using feed-forward and graph neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. While they can produce representations for new entities, they require fixing a set of attributes before training (e.g. bag-of-words, or numeric properties) restricting the domain of application. Furthermore, as recently proposed inductive methods <ref type="bibr" target="#b40">[40]</ref>, they can only produce representations for new entities, using their relations with existing entities, which is unsuitable for inductive link prediction, particularly in the challenging setting where all entities were not seen during training.</p><p>Recent work has explored using textual descriptions of entities and relations for link prediction, and proposes architectures to assign a score given the description of a relation and the entities involved in it <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">52]</ref>. However, these architectures take as input simultaneously descriptions of entities and relations and output a score. This unavoidably mixes entity and relation representations, and prevents their transfer to other tasks such as entity classification and information retrieval.</p><p>The closest methods to our work are based on the idea of training an entity encoder with a link prediction objective. DKRL <ref type="bibr" target="#b50">[50]</ref> consists of a convolutional neural network (CNN) that encodes descriptions. The performance of this method is limited as it does not take stop words into account, which discards part of the semantics in entity descriptions. Furthermore, its CNN architecture lags behind recent developments in neural networks for natural language processing, such as self-attention <ref type="bibr" target="#b44">[44]</ref>.</p><p>Pre-trained language models that use self-attention, such as BERT <ref type="bibr" target="#b7">[8]</ref> have be shown to be effective at capturing similarity between texts using distributed representations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">49]</ref>. In KE-PLER <ref type="bibr" target="#b46">[46]</ref>, the authors propose a model that uses BERT as an entity encoder, trained with an objective that combines language modeling and link prediction. The language modeling objective translates into increased training times, computing resources, and the requirement of a large corpus with long entity descriptions, of up to 512 tokens. In our work, we propose to use a pre-trained language model trained exclusively for link prediction, and obtain significant improvements at a reduced computational cost.</p><p>The evaluation protocol for both DKRL and KEPLER contains two fundamental issues that we address here. Firstly, the methods are implemented with a translational relational model <ref type="bibr" target="#b1">[2]</ref>. However, in principle this is not necessarily the best model for any description encoder, and so it remains an open question if other models, such as multiplicative interaction models <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b51">51]</ref>, are better suited. We address this by considering four different relational models in our experiments, and show that a choice of model does matter.  <ref type="bibr" target="#b1">[2]</ref>, DistMult <ref type="bibr" target="#b51">[51]</ref>, ComplEx <ref type="bibr" target="#b42">[42]</ref>, and SimplE <ref type="bibr" target="#b21">[22]</ref>. For a triple ( , , ), we denote as e , r and e the embeddings of its constituents (in SimplE these have two parts that we indicate with indices). ∥ · ∥ indicates the -norm; ⟨·, ·, ·⟩ is the generalized three-way dot product; Re(·) is the real part of a complex number; andē is the complex conjugate of a complex-valued vector e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Function</head><formula xml:id="formula_0">TransE −∥e + r − e ∥ DistMult ⟨e , r , e ⟩ ComplEx Re(⟨e , r ,ē ⟩) SimplE 1 2 ⟨e 1 , r 1 , e 1 ⟩ + ⟨e 2 , r 2 , e 2 ⟩</formula><p>Secondly, the evaluation of generalization in these works is limited. In DKRL, the entity representations are evaluated in a limited inductive link prediction setting, and in an entity classification task where entities in the test set were also used for training. In KEPLER, the authors only consider the link prediction task. In our work, we detail a more extensive evaluation framework that addresses these issues, including two different formulations of the inductive setting for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INDUCTIVE ENTITY REPRESENTATIONS</head><p>We define a knowledge graph with entity descriptions as a tuple G = (E, R, T , D), consisting of a set of entities E, relation types R, triples T , and entity descriptions D. Each triple in T has the form ( , , ), where ∈ E is the head entity of the triple, ∈ E the tail entity, and ∈ R the relation type. For each entity ∈ E, there exists a description = ( 1 , . . . , ) ∈ D, where all are words in a vocabulary V.</p><p>For an entity ∈ E, we denote its embedding as a vector e ∈ R , and similarly r ∈ R for the embedding of a relation ∈ R, where is the dimension of the embedding space. We consider the problem of optimizing the embeddings of entities and relations in the graph via link prediction, so that a scoring function (e , r , e ) assigns a high score to all observed triples ( , , ) ∈ T , and a low score to triples not in T . This can be achieved by minimizing a loss function such as a margin-based loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>,</p><formula xml:id="formula_1">∑︁ ( , , ) ∈ T max(0, 1 − (e , r , e ) + (e ′ , r , e ′ )),<label>(1)</label></formula><p>where e ′ and e ′ are embeddings for an unobserved negative triple ( ′ , , ′ ) ∉ T . Other suitable loss functions include the binary and multi-class cross-entropy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">42]</ref>. In general, for each triple in the KG, these loss functions can be written in the form L ( , ), as a function of the score for a positive triple, and for a negative triple. We list some of the scoring functions proposed in the literature in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The previous optimization objective is pervasive in transductive methods for representation learning in knowledge graphs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">45]</ref>, which are limited to learning representations for entities in a fixed set E. In these methods, the entity and relation embeddings are optimized when iterating through the set of observed triples. Therefore, by design, prediction at test time is impossible for entities not seen during training.</p><p>We can circumvent this limitation by leveraging statistical regularities present in the description of entities <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>. This can be realized by specifying a parametric entity encoder that maps the description of an entity to a vector e = ( ) ∈ R that acts as the embedding of the entity. The learning algorithm is then carried out as usual, by optimizing the parameters of the entity encoder and the relation embeddings r ∀ ∈ R, with a particular score and loss function. This process allows the encoder to learn inductive entity representations, as it can embed entities not seen during training, as long as they have an associated description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT for entity descriptions</head><p>Transformer networks <ref type="bibr" target="#b44">[44]</ref> have been shown to be powerful encoders that map text sequences to contextualized vectors, where each vector contains information about a word in context <ref type="bibr" target="#b10">[11]</ref>. Furthermore, pre-trained language models like BERT <ref type="bibr" target="#b7">[8]</ref>, which have been optimized with large amounts of text, allow fine-tuning the encoder for a different task that benefits from the pre-training step.</p><p>We select BERT for the entity encoder in our method, but other pre-trained models based on Transformers are equally applicable. Note that unlike DKRL <ref type="bibr" target="#b50">[50]</ref>, this entity encoder is well suited for inputs in natural language, rather than processed inputs where stop words have been removed. We expect accepting raw inputs helps the encoder better capture the semantics needed to learn more informative entity representations.</p><p>Given an entity description = ( 1 , . . . , ), the encoder first adds special tokens [CLS] and [SEP] to the beginning and end of the description, respectively, so that the input to BERT is the sequence^= ([CLS], 1 , . . . , , [SEP]). The output is a sequence of + 2 contextualized embeddings, including those corresponding to the added special tokens:</p><formula xml:id="formula_2">BERT(^) = (h CLS , h 1 , . . . , h , h SEP ).<label>(2)</label></formula><p>Similarly as in works that employ BERT for representations of text <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">46]</ref>, we select the contextualized vector h CLS ∈ R ℎ , where ℎ is the hidden size of BERT. This vector is then passed through a linear layer that reduces the dimension of the representation, to yield the output entity embedding e = Wh CLS , where W ∈ R ×ℎ is a parameter.</p><p>For relation embeddings, we use randomly initialized vectors r ∈ R , for each ∈ R. We then apply stochastic gradient descent to optimize the model for link prediction: for each positive triple ( , , ) ∈ T we compute a positive score . By replacing the head or the tail with a random entity, we obtain a corrupted negative triple, for which we compute a score . The loss is calculated as a function of and . This approach is quite general and admits different loss and scoring functions. The complete procedure is presented in Algorithm 1.</p><p>Note that our proposed algorithm is fundamentally different from KEPLER <ref type="bibr" target="#b46">[46]</ref>, which is trained with an additional language modeling objective that is more expensive to compute, and requires more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Learning inductive entity representations via link prediction</head><p>Input: Knowledge graph (E, R, T , D), entity encoder with parameters , learning rate , scoring function , loss function L r ← initialize randomly for each ∈ R;</p><formula xml:id="formula_3">:= { } ∪ {r | ∈ R}; for ( , , ) ∈ T do ( ′ , , ′ ) ← corrupt ( , , ); ← ( ( ), r , ( )); ← ( ( ′ ), r , ( ′ )); ← − ∇ L ( , ) end return</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational complexity</head><p>A significant portion of the cost in Algorithm 1 comes from the entity encoder. Encoding a sequence of length with BERT has a complexity of ( 2 ), thus the time complexity for training is (|T | 2 ). In practice, a fixed value of can be selected (such as 32 or 64 in our experiments), so if we consider it equal for all entities, the algorithm remains linear with respect to the number of triples in the graph, up to a constant factor.</p><p>At test time, the embeddings for all entities can be pre-computed. In this case, link prediction for a given entity and relation is linear in the number of entities in the graph, and the entity encoder is only applied to new entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical motivation</head><p>Multiple models for modeling relations in KGs have been proposed as a form of factorization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>, by representing the relations in the graph as a third-order tensor Y ∈ {0, 1} | E |× | R |×| E | , where the entry = 1 if ( , , ) ∈ T , and = 0 otherwise. For each ∈ R, ER E ⊤ is a tensor decomposition of Y, where E ∈ R | E |× and R ∈ R × , and the -th row of E contains the embedding e for ∈ E. Examples of models optimized for this kind of decomposition are RESCAL <ref type="bibr" target="#b28">[29]</ref>, DistMult <ref type="bibr" target="#b51">[51]</ref>, ComplEx <ref type="bibr" target="#b42">[42]</ref>, and SimplE <ref type="bibr" target="#b21">[22]</ref>.</p><p>For an entity description = ( 1 , . . . , ), let W ∈ R × be a matrix of word embeddings, with the embedding of word in the -th column. Approximating such a decomposition with an entity encoder thus requires correctly mapping W to the embedding of the entity e in E. In a recent result, Yun et al. <ref type="bibr" target="#b53">[53]</ref> show that Transformers are universal approximators of continuous functions with compact support 1 : R × → R . Therefore, if such a function exists so that (W ) = e , there is a Transformer that can approximate the corresponding tensor decomposition. While the existence of this function is not obvious, it further motivates an empirical study on the use of BERT for entity embeddings in a KG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATING ENTITY REPRESENTATIONS</head><p>The use of an entity encoder to obtain entity representations is a more flexible approach that would be useful not just for link prediction, but for other tasks that could benefit from a vector representation that is a function of the textual description of an entity.</p><p>To better explore the potential of such an approach, we propose an evaluation framework that comprises inductive link prediction, inductive entity classification, and information retrieval for entityoriented search. We present the results of the encoder proposed in section 3, and compare with recently proposed methods for each task. Our implementation and all the datasets that we use are made publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Link prediction</head><p>Link prediction models can be evaluated via a ranking procedure <ref type="bibr" target="#b2">[3]</ref>, using a test set of triples T ′ disjoint from the set of training triples T . For a test triple ( , , ), a prediction for the tail is evaluated by replacing with an entity^in a set of incorrect candidatesÊ, and a score is computed as (e , r ,ê ). Ideally, all the scores for incorrect candidates should be lower than the score of the correct triple. A prediction for the head is evaluated similarly, by replacing .</p><p>In the transductive setting, the entities in a test triple are assumed to be in the set of training entities. Furthermore, the set of incorrect candidates is the same as the set of training entities. In the inductive setting, we instead consider a test graph (E ′ , R ′ , T ′ , D ′ ). The sets of triples T ′ and T are disjoint, and for relations, we always assume that R ′ ⊆ R. According to the way the set of incorrect candidateŝ E is determined, we define two inductive evaluation scenarios:</p><p>Dynamic evaluation. In a test triple, a new entity may appear at the head, tail, or both positions. The set of incorrect candidatesÊ is the union of training and test entities, E ∪ E ′ . This represents a situation in which new entities are added to the KG, and is challenging as the set of incorrect candidates is larger at test time than at training.</p><p>Transfer evaluation. In a test triple, both entities at the head and tail position are new, and the set of incorrect candidatesÊ is E ′ , where E ′ is disjoint from the training set of entities E. This represents a setting where we want to perform link prediction within a subset of entities, that was not observed during training. Such a situation is of interest, for example, when transferring a trained model to a specific sub-domain of new entities.</p><p>We consider that both scenarios are highly relevant for tasks of link prediction in incomplete KGs, in contrast with previous works that only consider one or the other <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b50">50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experiments.</head><p>Datasets. We make use of FB15k-237 <ref type="bibr" target="#b41">[41]</ref> and WN18RR <ref type="bibr" target="#b6">[7]</ref>, which are datasets widely used in the link prediction literature. To obtain entity descriptions we employ the datasets made available by Yao et al. <ref type="bibr" target="#b52">[52]</ref>. FB15k-237 is a subset of Freebase, where most entities correspond to people, movies, awards, and sport teams. Descriptions were obtained from the introduction section of the Wikipedia page of each entity. In WN18RR each entity corresponds to a word sense, and descriptions are their definitions. Instead of using the conventional splits used for these datasets, we implement a dynamic evaluation scenario. We select 10% of entities and their associated triples to form a test graph, 10% for validation, and the remaining 80% for training. At test time, all entities are used as incorrect candidates. For these datasets we choose a maximum length of entity descriptions of 32 tokens. This value was chosen as using more tokens did not bring significant improvements, while increasing monotonically the time required for training (we include details for these results in Appendix C).</p><p>For the transfer evaluation, we present results on Wikidata5M, with the splits provided by Wang et al. <ref type="bibr" target="#b46">[46]</ref>. The graph is a subset of Wikidata, containing 4.6 million entities, and descriptions from the introduction section of Wikipedia. To further experiment with the scalability of our approach, we increased the description length to 64 tokens. Dataset statistics are listed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Experimental setup. Following the definition in subsection 3.1, we implement an entity encoder using the BERT-base configuration from the Transformers library <ref type="bibr" target="#b48">[48]</ref>, followed by a linear layer with 128 output units.</p><p>We study the performance of our method in combination with four relational models: TransE, DistMult, ComplEx, and SimplE. We aim to cover early translational and multiplicative models (i.e. TransE and DistMult) as well as more recent models that have been shown to result on state-of-the-art performance for link prediction <ref type="bibr" target="#b34">[35]</ref> (i.e. ComplEx and SimplE). We denote the resulting models as BERT for Link Prediction (BLP) followed by the employed relational model (e.g. BLP-TransE).</p><p>As a baseline we consider DKRL, proposed by Xie et al. <ref type="bibr" target="#b50">[50]</ref>. In our implementation of DKRL, we use GloVe embeddings <ref type="bibr" target="#b30">[31]</ref> with a dimension of 300 for the input, and an output dimension of 128. We also reproduce their Bag-Of-Words (BOW) baseline, in which an entity is encoded as the average of embeddings of words in the description. We denote these models as GloVe-DKRL and GloVe-BOW, respectively. Following recent works on the properties and applications of static embeddings from the input layer of BERT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>, we also consider variants of the baselines that use contextinsensitive BERT Embeddings (BE). We denote these as BE-DKRL and BE-BOW.</p><p>For all models, we run grid search using FB15k-237, and we select the hyperparameters with the best performance on the validation set. We reuse these hyperparameters for training with WN18RR and Wikidata5M, as we found that they performed well on these datasets.</p><p>For BLP models, in the grid search we consider the followingloss function: {margin, negative log-likelihood}, learning rate: {1e-5, 2e-5, 5e-5}, L2 regularization coefficient: {0, 1e-2, 1e-3}. We use the Adam optimizer with a learning rate decay schedule with warm-up for 20% of the total number of iterations. We train for 40 epochs with a batch size of 64 with WN18RR and FB15k-237, and 5 epochs with a batch size of 1,024 with Wikidata5M.</p><p>For the BOW and DKRL baselines the values for grid search were the following -learning rate: {1e-5, 1e-4, 1e-3}, L2 regularization coefficient: {0, 1e-2, 1e-3}. We use Adam with no learning rate schedule, and we train the models for 80 epochs with a batch size of 64 with WN18RR and FB15k-237, and 10 epochs with a batch size of 1,024 with Wikidata5M.</p><p>The number of negative samples is 64 in all experiments.</p><p>Metrics. Given the score for a correct triple, and the corresponding set of negative scores obtained by replacing the head of the triple by an incorrect candidate, we sort them to obtain a ranked list. Let ℎ be the position of the correct triple in the rank. The reciprocal rank is then 1/ ℎ . This procedure is repeated by replacing the tail, to obtain the reciprocal rank 1/ . The Mean Reciprocal Rank is the mean of these two values, averaged across all triples in the knowledge graph:</p><formula xml:id="formula_4">MRR = 1 2|T | ∑︁ ∈T 1 ℎ + 1<label>(3)</label></formula><p>The Hits at 1 metric (H@1) is obtained by counting the number of times the correct triple appears at position 1, and averaging as for the MRR. The H@3 and H@10 are computed similarly, considering the first 3 and 10 positions, respectively.</p><p>When scoring candidates for a given triple, we consider the filtered setting <ref type="bibr" target="#b1">[2]</ref> where for each triple we consider as incorrect candidates all entities in the setÊ minus those that would result in a correct triple, according to the training, validation, and test sets.</p><p>For more specific technical details on datasets and training, we refer the reader to Appendices A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results.</head><p>We report the Mean Reciprocal Rank (MRR), and Hits at 1, 3, and 10, on the test set in <ref type="table">Table 3</ref>. For reference, we also show results reported by Wang et al. <ref type="bibr" target="#b46">[46]</ref> for KEPLER. We observe that in both the dynamic evaluation (WN18RR and FB15k-237) and the transfer evaluation (Wikidata5M), BLP-TransE consistently outperforms all the baselines across all metrics.</p><p>We note that TransE results in higher link prediction performance with BLP, compared to alternatives like DistMult, ComplEx, and SimplE in WN18RR and FB15k-237. ComplEx and SimplE improve on performance in Wikidata5M, which contains around two orders of magnitude more triples for training. This suggests that more elaborate relational models might be less data efficient compared with TransE, when used with BERT for link prediction. We <ref type="table">Table 3</ref>: Results of filtered metrics for link prediction involving entities not seen during training. We use WN18RR and FB15k-237 for dynamic evaluation, and Wikidata5M for the transfer evaluation (see text for more details). Results for KEPLER are reported by Wang et al. <ref type="bibr" target="#b46">[46]</ref>. note that the gap in performance between BLP-TransE and baselines is larger in WN18RR than in FB15k-237. We hypothesize that the definitions of words in WN18RR can have subtle variations of syntax that a BERT encoder captures better, while entities in FB15k-237 can be more easily identified by keywords, so that ignoring syntax does not result in a large drop in performance. Interestingly, we observe that in Wikidata5M, KEPLER results in lower performance despite using a joint training objective that combines language modeling and link prediction. While it has been suggested that such an objective leads to increased performance <ref type="bibr" target="#b46">[46]</ref>, here we observe that this is not the case: all BLP variants outperform KEPLER when using only the link prediction objective, which also results in a reduced computational cost during training.</p><p>Despite our best efforts, we could not find a DKRL model that performed better than BOW models. This is surprising since unlike DKRL, BOW models do not take word order into account. Interestingly, for both BOW and DKRL, BE models yields consistently better results than models using GloVe embeddings, while BE models use 80% less parameters due to the use of WordPiece embeddings.</p><p>This can be attributed to differences in the data used to pretrain the embeddings, but more importantly, to the size of the embeddings: the size of BERT and GloVe embeddings is 768 and 300, respectively, which translates into a larger number of parameters in BE models. While GloVe uses one embedding per word and a vocabulary size of 400,000, BE models use wordpiece embeddings and a vocabulary size of 30,000. This means that BE models manage to reduce the number of parameters 80% in comparison with GloVe models, while resulting in better link prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of training set size. An important question in the inductive setting we consider is: what is the effect of the number of entities seen during training, on the performance on a fixed test set?</head><p>To answer this question, we use FB15k-237 and the same test set, and sample subsets for training with an increasing number of entities. We evaluate the MRR for BE-BOW and BLP-TransE. The results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We note that a reduction of 50% in the number of entities results in approximately a 27% reduction in MRR. The mismatch between these percentages suggests that the pre-trained embeddings in BE-BOW and the architecture of BLP-TransE allow them to retain performance when reducing the size of the training set. <ref type="figure" target="#fig_1">Figure 2</ref> also reveals in greater detail the constant gap between BE-BOW and BLP-TransE. Since both methods share the same pretraining regime and thus the data used during pre-training, we attribute the difference to the more powerful encoder used in BLP-TransE and the fact that it does not require dropping stopwords.</p><p>Transductive link prediction. Entity representations learned from entity descriptions can also be applied to the transductive scenario, where all entities at test time have been observed during training. This correspond to the setting where previous transductive methods for KG embeddings have been designed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b51">51]</ref>, although they cannot be applied to our experiments where textual descriptions and natural language are used. For reference, we include results in Appendix D, where we note that transductive methods significantly outperform description based encoders. We attribute this to the more challenging problem faced by description encoders: they must learn a complicated function from words to an entity representation, while transductive methods learn a lookup table with one embedding per entity and relation. Nonetheless, we stress that the applicability of description based encoders is much broader, as our work demonstrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity classification</head><p>A good description encoder must learn to extract the most informative features of an entity from its description, and compress them in the entity embedding. We test this property by using the embeddings of entities trained for link prediction, as features for a logistic regression classifier. Crucially, we maintain the inductive setting, keeping the splits from the link prediction experiments. Thus, at test time the classifier is evaluated on entities that the entity encoder did not see during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiments.</head><p>Datasets. We evaluate entity classification using WN18RR and FB15k-237. In WN18RR we use the part of speech as the class for entities, which results in a total of 4 classes. For FB15k-237 we follow a procedure similar to Xie et al. <ref type="bibr" target="#b50">[50]</ref> by selecting the 50 most common entity types as classes.</p><p>Experimental setup. Using entity embeddings as features, we train a multi-class logistic regression classifier with L2 regularization. The regularization coefficient is chosen from {1e-4, 1e-3, 1e-2, 0.1, 1, 10}, and we keep the coefficient resulting in the best accuracy on the validation set. We also train classifiers with features not explicitly trained for link prediction: in GloVe-avg and BE-avg we use the average of GlovE and context-insensitive BERT embeddings, respectively. SBERT <ref type="bibr" target="#b32">[33]</ref> is a model based on BERT that is trained to learn representations of sentences, that we apply to entity descriptions. We use their publicly available trained models 3 of the SBERT-NLI-base and SBERT-NLI-large variants.</p><p>Metrics. We report classification accuracy and its balanced version. The balanced accuracy weights each sample with the inverse prevalence of the true class, and allows us to identify when a classifier works better in average across classes, rather than performing better on the majority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>. We list the results in <ref type="table" target="#tab_2">Table 4</ref>. We observe a drastic increase in performance with all BLP models trained for link prediction, which is especially noticeable when evaluating the balanced accuracy. The marked improvements in this metric demonstrate that the embeddings are a more informative representation that allows the classifier to perform better on classes for which there is very little data, and for entities not seen during training.</p><p>Interestingly, we note that i) the baselines not trained for link prediction perform better than the BOW and DKRL baselines in most cases, and ii) SBERT models still underperform BLP models trained for link prediction. We conclude that it is the combination of a powerful BERT encoder and a link prediction fine-tuning procedure that gives rise to better entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Information retrieval</head><p>An entity can be associated with different descriptions, that can be ambiguous and not necessarily grammatical. To evaluate the robustness of an entity encoder against this variability, we test its performance in an information retrieval task: given a query about an entity, return a list of documents (entity descriptions) ranked by relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experiments.</head><p>Dataset. DBpedia-Entity v2 is a dataset for the evaluation of information retrieval (IR) systems for entity-oriented search, introduced by Hasibi et al. <ref type="bibr" target="#b16">[17]</ref>. The document index corresponds to textual descriptions of entities in DBpedia. There are 467 queries, categorized into 4 types -SemSearch ES: short and ambiguous queries, e.g. "john lennon, parents"; INEX-LD: keyword queries, e.g. "bicycle holiday nature"; List Search: queries seeking for lists, e.g. "Airports in Germany"; and QALD-2: questions in natural language, e.g. "What is the longest river?". For each query, there is a list of documents graded by relevance by crowd workers. In average, there are 104 graded documents per query.</p><p>Experimental setup. Similarly as in previous work on embeddings for information retrieval <ref type="bibr" target="#b12">[13]</ref>, we implement a re-ranking procedure, by updating a list of document scores assigned by an existing IR system (e.g. BM25). Let be query, the text in a document, and IR the score the IR system assigned to given . We use an entity encoder to compute the similarity between the embeddings of the query and the document, via their inner product:</p><formula xml:id="formula_5">new = ( ) ⊤ ( ) + (1 − ) IR .<label>(4)</label></formula><p>We select the best value of via grid search on each of 5 training folds of the data provided by Hasibi et al. <ref type="bibr" target="#b16">[17]</ref>, and report the average test fold performance. For the grid we consider 20 evenly spaced values in the interval [0, 1], and for the entity encoder we use the models trained for link prediction with Wikidata5M. As in entity classification, we do not fine-tune the entity encoders.</p><p>To obtain the base scores IR , we use BM25F-CA <ref type="bibr" target="#b33">[34]</ref>, as it is one of the best performing methods on the DBpedia-Entity v2 dataset reported by Zhiltsov et al. <ref type="bibr" target="#b54">[54]</ref>. <ref type="table">Table 5</ref>: NDCG results for the information retrieval task, across different query types. We show the results for BM25F-CA, followed by the results after re-ranking with different entity encoders. Values in bold indicate that the difference between BM25F-CA and the re-ranked results is statistically significant at p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SemSearch ES INEX-LD</head><p>ListSearch QALD-2 All Method @10 @100 @10 @100 @10 @100 @10 @100 @10 @100 Metrics. The DBpedia-Entity v2 dataset comes with a set of relevant documents for different queries. Each relevant document contains a grade of relevance, where 0 is non-relevant, 1 is relevant, and 2 is highly-relevant. This allows to compute how good the output ranking of a system is given the relevance of the documents it produces.</p><p>Assume we have a list of documents ranked by a system, and the relevance of the document at position is . The Discounted Cumulative Gain (DCG) is defined as</p><formula xml:id="formula_6">DCG@k = ∑︁ =1 2 − 1 log 2 (1 + ) .<label>(5)</label></formula><p>Let IDCG@k be the maximum DCG@k, produced by sorting all relevant documents in the corpus and computing eq. 5. The Normalized DCG (NDCG) is defined as</p><formula xml:id="formula_7">NDCG@k = DCG@k IDCG@k .<label>(6)</label></formula><p>4.3.2 Results. We report NDCG at 10 and 100 in <ref type="table">Table 5</ref>. We show the results obtained with BM25F-CA, followed by the results after re-ranking with a particular encoder. BLP-TransE yields statistically significant improvements (according to a two-tailed t-test) in both NDCG at 10 and 100, when considering all query types. Although on average the entity encoders yield higher performance across all query types, the difference with BM25F-CA is not statistically significant for SemSearch ES queries. This is expected since these queries are short and often not grammatical, differing from the entity descriptions used to train the encoders. In other query types the encoders show significant improvements, especially in QALD-2 queries, containing well formed questions.</p><p>Depending on the type of query, we observed that the optimal parameter was always between 0.1 and 0.7, indicating that the embeddings of queries and documents alone are not enough to correctly rank documents, and a fraction of the score assigned by BM25F-CA is still required to preserve retrieval quality. However, the results of this section are encouraging in the context of representation learning of entities, as they show that the entity encoders have learned a function that maps entities and queries about them to vectors that are close in the embedding space. We present a sample of an original ranking by BM25F-CA, and the reranked results by BLP-TransE in <ref type="table" target="#tab_3">Table 6</ref>. In the first example, while BM25F-CA retrieves Intel-related companies and products, BLP-TransE fixes the ranking by pushing down products and increasing the scores for persons relevant to the query. In the second query, BM25F-CA retrieves more generic topics related to "invention", plus irrelevant documents at positions 2 and 5. BLP-TransE keeps only the first document and increases the scores of other, more relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have studied the generalization properties of entity representations learned via link prediction, by proposing the use of a pretrained model fine-tuned with a link prediction, and an extensive evaluation framework that also considers tasks of entity classification and information retrieval. We find that when using the proposed model, the entity representations are inductive, as they exhibit strong generalization in link prediction for entities not seen during training.</p><p>Without requiring fine-tuning, the entity representations learned via link prediction are also transferable to the tasks of node classification and information retrieval, which demonstrates that the entity embeddings act as compressed representations of the most salient features of an entity. This provides evidence that the learned entity representations generalize to tasks beyond link prediction. This is additionally important because having generalized vector representations of knowledge graphs is useful for using them within other tasks, for example, search and automatic metadata annotation.</p><p>We consider the inductive setting where new entities appear at test time. Extending our approach to unseen relations is an interesting avenue for future research. For future work we also consider the study and improvement of the learned entity representations, for example, by enforcing independent factors of variation in the representations <ref type="bibr" target="#b24">[25]</ref>, or by learning on hyperbolic spaces better suited for hierarchical data <ref type="bibr" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TECHNICAL DETAILS</head><p>Dataset generation. The original splits of WN18RR and FB15k-237 are intended to be used in the transductive setting, where all entities in the validation and test sets are also present in the training set. We thus generate new splits for these datasets for the inductive setting. The algorithm for generating the datasets is as follows:</p><p>(1) Sample a node from the graph.</p><p>(2) Check that removing the node does not leave other nodes with zero neighbors. (3) Check that if the node is removed, the number of edges for any relation type is not lower than 100. (4) If conditions 2 and 3 are met, remove the node and add it to the set of nodes removed from the training set.</p><p>Negative sampling. In all link prediction experiments, negative samples are obtained by randomly replacing the head or tail with a random entity. We sample entities from the set of entities involved in a mini-batch of triples. This strategy allows to increase the number of negative samples while preserving efficiency, as this requires encoding less distinct entity descriptions per mini-batch, than if the negative samples were sampled from the complete set of entities in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COMPUTING RESOURCES</head><p>For the experiments with WN18RR and FB15k-237 we used a workstation with an Intel Xeon processor, 1 NVIDIA GeForce GTX 1080 Ti GPU with 11GB of memory, and 60GB of RAM. For Wikidata5M we used 4 NVIDIA TITAN RTX GPUs with 24GB of memory each.</p><p>For all the BLP models we use the BERT-base configuration <ref type="bibr" target="#b7">[8]</ref>. In particular, we use the bert-base-cased pre-trained model from the Transformers library <ref type="bibr" target="#b48">[48]</ref>. This model has 110 million parameters. For link prediction, we additionally have parameters for relation embeddings, which have a dimension of 128. The number of parameters depends on the number of relations in the dataset. This results in 1,400 relation parameters for WN18RR, 30,300 for FB15k-237, and 105,216 for Wikidata5M.</p><p>The time required for training was 7 hours for WN18RR, 14 hours for FB15k-237, and 2 days for Wikidata5M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EFFECT OF DESCRIPTION LENGTH</head><p>To determine the influence of the length of the descriptions on performance, we trained several variants of BLP-TransE with FB15k-237, varying the maximum length of the entity descriptions from 16 up to 128 tokens. The resulting link prediction performance (MRR) in the validation set, and training time are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We note that using more than 32 tokens does not bring significant improvements, while monotonically increasing the time required for training.   <ref type="bibr" target="#b39">[39]</ref>. The first group contains transductive methods, and the second corresponds to inductive methods that use entity descriptions. methods studied in our work, with previous works designed for this setting. The results are shown in <ref type="table" target="#tab_5">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TRANSDUCTIVE LINK PREDICTION</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>0.135 0.361 0.580 0.195 0.113 0.213 0.363 0.478 0.241 0.660 0.871 BLP-DistMult 0.248 0.135 0.288 0.481 0.146 0.076 0.156 0.286 0.472 0.242 0.646 0.869 BLP-ComplEx 0.261 0.156 0.297 0.472 0.148 0.081 0.154 0.283 0.489 0.262 0.664 0.877 BLP-SimplE 0.239 0.144 0.265 0.435 0.144 0.077 0.152 0.274 0.493 0.289 0.639 0.866</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Inductive link prediction performance (MRR) versus number of entities used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.721 0.449 0.544 0.432 0.518 0.368 0.460 0.462 0.554 + BE-BOW 0.629 0.721 0.458 0.546 0.431 0.522 0.377 0.469 0.460 0.552 + GloVe-DKRL 0.624 0.719 0.440 0.529 0.424 0.516 0.368 0.468 0.459 0.550 + BE-DKRL 0.627 0.720 0.436 0.530 0.435 0.525 0.374 0.466 0.459 0.553 + BLP-TransE 0.631 0.723 0.446 0.546 0.442 0.540 0.401 0.482 0.472 0.562 + BLP-DistMult 0.631 0.722 0.458 0.550 0.442 0.536 0.397 0.480 0.468 0.560 + BLP-ComplEx 0.628 0.721 0.454 0.548 0.430 0.528 0.405 0.486 0.468 0.561 + BLP-SimplE 0.628 0.721 0.454 0.552 0.439 0.527 0.399 0.477 0.464 0.557</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Entity representations learned via link prediction can also be applied to the transductive setting, where all entities at test time have been observed during training. For completeness, we compare the Inductive link prediction performance in the validation set and training time vs. maximum length of entity descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>0.144 0.431 0.679 0.235 0.150 0.253 0.411 BERT-DistMult 0.314 0.182 0.370 0.581 0.210 0.130 0.222 0.377</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of scoring functions for triples, proposed for TransE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets used in the link prediction task.</figDesc><table><row><cell></cell><cell cols="3">WN18RR FB15k-237 Wikidata5M</cell></row><row><cell>Relations</cell><cell>11</cell><cell>237</cell><cell>822</cell></row><row><cell></cell><cell></cell><cell>Training</cell><cell></cell></row><row><cell>Entities</cell><cell>32,755</cell><cell>11,633</cell><cell>4,579,609</cell></row><row><cell>Triples</cell><cell>69,585</cell><cell>215,082</cell><cell>20,496,514</cell></row><row><cell></cell><cell></cell><cell>Validation</cell><cell></cell></row><row><cell>Entities</cell><cell>4,094</cell><cell>1,454</cell><cell>7,374</cell></row><row><cell>Triples</cell><cell>11,381</cell><cell>42,164</cell><cell>6,699</cell></row><row><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>Entities</cell><cell>4,094</cell><cell>1,454</cell><cell>7,475</cell></row><row><cell>Triples</cell><cell>12,087</cell><cell>52,870</cell><cell>6,894</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Accuracy for the entity classification experiments. Raw values correspond to the regular definition of accuracy. In the balanced case (Bal.), each sample is weighted with the inverse prevalence of its true class.</figDesc><table><row><cell></cell><cell>WN18RR</cell><cell>FB15k-238</cell></row><row><cell>Method</cell><cell cols="2">Raw Bal. Raw Bal.</cell></row><row><cell>GloVe-avg</cell><cell cols="2">90.3 55.3 82.0 35.0</cell></row><row><cell>BE-avg</cell><cell cols="2">92.7 62.1 82.4 39.4</cell></row><row><cell cols="3">SBERT-NLI-base 96.3 66.5 84.5 36.6</cell></row><row><cell cols="3">SBERT-NLI-large 96.3 67.1 83.8 35.1</cell></row><row><cell>GloVe-BOW</cell><cell cols="2">91.5 56.0 82.9 34.4</cell></row><row><cell>BE-BOW</cell><cell cols="2">93.3 60.7 83.1 28.3</cell></row><row><cell>GloVe-DKRL</cell><cell cols="2">91.2 55.5 81.1 26.6</cell></row><row><cell>BE-DKRL</cell><cell cols="2">90.0 48.8 81.6 30.9</cell></row><row><cell>BLP-TransE</cell><cell cols="2">99.1 81.5 85.4 42.5</cell></row><row><cell>BLP-DistMult</cell><cell cols="2">99.5 78.5 84.3 41.0</cell></row><row><cell>BLP-ComplEx</cell><cell cols="2">99.3 78.1 85.1 38.1</cell></row><row><cell>BLP-SimplE</cell><cell cols="2">99.2 83.0 85.8 45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>First five documents retrieved by BM25F-CA, and its reranking with BLP-TransE, for two example queries.</figDesc><table><row><cell cols="2">Query: Who founded Intel?</cell><cell></cell></row><row><cell cols="2">Rank BM25F-CA</cell><cell>+ BLP-TransE</cell></row><row><cell>1</cell><cell>Intel</cell><cell>Intel</cell></row><row><cell>2</cell><cell>Intel 8253</cell><cell>Avram Miller</cell></row><row><cell>3</cell><cell>Intel 8259</cell><cell>Glenford Myers</cell></row><row><cell>4</cell><cell>Intel Play</cell><cell>Intel Play</cell></row><row><cell>5</cell><cell>Intel Ct</cell><cell>Leslie L. Vadasz</cell></row><row><cell cols="3">Query: A list of all American inventions</cell></row><row><cell cols="2">Rank BM25F-CA</cell><cell>+ BLP-TransE</cell></row><row><cell>1</cell><cell>US inventions</cell><cell>US inventions</cell></row><row><cell>2</cell><cell cols="2">The Mothers of Invention US inventions (after 1991)</cell></row><row><cell>3</cell><cell cols="2">American Heritage of Inv. US inventions (1890 -1945)</cell></row><row><cell>4</cell><cell>Invention</cell><cell>Timeline of inventions</cell></row><row><cell>5</cell><cell>Sandy Bull</cell><cell>US inventions (1946-91)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This project was funded by Elsevier's Discovery Lab. of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (Santiago, Chile) (SIGIR '15). Association for Computing Machinery, New York, NY, USA, 253-262. https://doi.org/10.1145/2766462.2767756</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results of filtered metrics for link prediction in the transductive setting.</figDesc><table /><note>*Results taken from Sun et al.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Their results are shown for functions whose range is R × , but here we state a special case where we select one column from the output.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/dfdazac/blp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/UKPLab/sentence-transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-76298-0_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-76298-0_52" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference</title>
		<editor>Karl Aberer, Key-Sun Choi, Natasha Fridman Noy, Dean Allemang, Kyung-Il Lee, Lyndon J. B. Nixon, Jennifer Golbeck, Peter Mika, Diana Maynard, Riichiro Mizoguchi, Guus Schreiber, and Philippe Cudré-Mauroux</editor>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-11-11" />
			<biblScope unit="volume">4825</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Structured Embeddings of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3659" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<editor>Wolfram Burgard and Dan Roth</editor>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011-08-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1470</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1470" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>Schuurmans and Michael P. Wellman</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA, Dale</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entity query feature expansion using knowledge base links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609628</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609628" />
	</analytic>
	<monogr>
		<title level="m">The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;14</title>
		<editor>Shlomo Geva, Andrew Trotman, Peter Bruza, Charles L. A. Clarke, and Kalervo Järvelin</editor>
		<meeting><address><addrLine>Gold Coast , QLD, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-07-06" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge vault: a web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623623</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623623" />
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<editor>Sofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08-24" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting RDF Triples in Incomplete Knowledge Bases with Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="DOI">10.1145/2245276.2245341</idno>
		<ptr target="https://doi.org/10.1145/2245276.2245341" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on Applied Computing</title>
		<meeting>the 27th Annual ACM Symposium on Applied Computing<address><addrLine>Trento, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
	<note>SAC &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Knowledge Graphs -Methodology, Tools and Selected Use Cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fensel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umutcan</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Angele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elwin</forename><surname>Huaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Kärle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandra</forename><surname>Panasiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioan</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wahler</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37439-6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37439-6" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-Embedding Empowered Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><forename type="middle">J</forename><surname>Gerritse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries ; Joemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-45439-5_7" />
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -42nd European Conference on IR Research</title>
		<editor>Mário J. Silva, and Flávio Martins</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-04-14" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CaRe: Open Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyash</forename><surname>Kenkre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1036</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1036" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embedding Logical Queries on Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems<address><addrLine>Montréal, Canada; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2030" to="2041" />
		</imprint>
	</monogr>
	<note>NIPS&apos;18)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DBpedia-Entity v2: A Test Collection for Entity Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svein</forename><forename type="middle">Erik</forename><surname>Bratsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080751</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080751" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White</editor>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-07" />
			<biblScope unit="page" from="1265" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural-symbolic integration and the Semantic Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monireh</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><forename type="middle">Kamruzzaman</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-190368</idno>
		<ptr target="https://doi.org/10.3233/SW-190368" />
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding Based Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290956</idno>
		<ptr target="https://doi.org/10.1145/3289600.3290956" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019</title>
		<editor>J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman</editor>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-02-11" />
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What Does BERT Learn about the Structure of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1356" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge Base Completion: Baselines Strike Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2609</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-2609" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Montréal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7682-simple-embedding-for-link-prediction-in-knowledge-graphs" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Nicolò Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<ptr target="http://arxiv.org/abs/1611.07308" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/locatello19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P09-1113" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poincaré Embeddings for Learning Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<ptr target="https://icml.cc/2011/papers/438_icmlpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011-06-28" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Industry-scale knowledge graphs: Lessons and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="48" to="75" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/d14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Alessandro Moschitti, Bo Pang, and Walter Daelemans</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dissecting Contextual Word Embeddings: Architecture and Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1179</idno>
		<ptr target="https://doi.org/10.18653/v1/D18-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Probabilistic Relevance Framework: BM25 and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
		<ptr target="https://doi.org/10.1561/1500000019" />
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkxSmlBFvr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<editor>Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Raphaël Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Open-World Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Carey L</publisher>
			<date type="published" when="2007-05-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Zurko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel-Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242667</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242667" />
		<editor>Prashant J. Shenoy</editor>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Inductive Relation Prediction on Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Komal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06962</idno>
		<ptr target="http://arxiv.org/abs/1911.06962" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Message Passing for Complex Question Answering over Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier David Fernandez</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maarten De Rijke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cochez</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358026</idno>
		<ptr target="https://doi.org/10.1145/3357384.3358026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
		<editor>Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu</editor>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pretrained Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06136</idno>
		<ptr target="http://arxiv.org/abs/1911.06136" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwunping</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1024</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="250" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Zero-shot Entity Linking with Dense Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03814</idno>
		<ptr target="http://arxiv.org/abs/1911.03814" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representation Learning of Knowledge Graphs with Entity Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12216" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>Schuurmans and Michael P. Wellman</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA, Dale</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6575" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<ptr target="http://arxiv.org/abs/1909.03193" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Are Transformers universal approximators of sequence-tosequence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxRM0Ntvr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Zhiltsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
