<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring Mathematical Problem Solving With the MATH Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Burns</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akul</forename><surname>Arora</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">UChicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tang</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
							<affiliation key="aff6">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
							<affiliation key="aff7">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Measuring Mathematical Problem Solving With the MATH Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12, 500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.</p><p>The MATH dataset is challenging: large language models achieved accuracies ranging from 2.9% to 6.9%. Despite these low accuracies, models clearly possess some mathematical knowledge: they achieve up to 15% accuracy on the easiest difficulty level, and they are able to generate step-by-step solutions that are coherent and on-topic even when Code and the MATH dataset can be found at github.com/hendrycks/math/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mathematics is a highly effective tool in many intellectual endeavors. It enables us to count and quantify objects, and it can be relied upon because it is consistent and based on logic. Mathematics pervades the sciences and can be used to model planetary orbits, atomic motion, signal frequencies, and much more. These phenomena can be encoded with mathematics precisely and concisely. This has even led some to describe mathematics as being "unreasonably effective" <ref type="bibr" target="#b27">(Wigner, 1960)</ref>. These observations speak to the broad reach and domain-generality of mathematics.</p><p>In machine learning, mathematics is a valuable testbed for problem-solving ability: the ability to analyze a problem, pick out good heuristics from a large set of possibilities, and chain them together to produce an answer. This contrasts with plug-and-chug calculations, a skill which ML models can already exhibit . Visual or linguistic reasoning may involve limited problem-solving ability for tasks such as image classification, but unlike math this is not the focus of these domains.</p><p>To measure the problem-solving ability of machine learning models, we introduce the MATH dataset, which consists of 12, 500 problems from high school math competitions. Given a problem from MATH, machine learning models generate a sequence, such as $\frac{2}{3}$, that encodes the final answer. These answers are unique after normalization, allowing MATH to be scored with exact match rather than with heuristic metrics such as BLEU. In addition, MATH problems are tagged by difficulty from 1 to 5, and span seven subjects including geometry, where diagrams can be specified in text with the Asymptote language. This enables a fine-grained assessment of mathematical problem-solving ability across difficulties and subjects. Finally, problems come with full step-by-step solutions. By training on these, models can learn to generate their own step-by-step solutions, which can facilitate learning and make model outputs more interpretable.</p><p>√ 2. The desired product is then −1 + cos π</p><formula xml:id="formula_0">8 4 √ 2 −1 − cos π 8 4 √ 2 = 1 − cos 2 π 8 √ 2 = 1 − (1+cos( π 4 )) 2 √ 2 = 1 − √ 2 2 .</formula><p>Figure 1: Previous work is based on formal theorem provers or straightforward plug-and-chug problems. Our dataset, MATH, has competition mathematics problems with step-by-step solutions written in L A T E X and natural language. Models are tasked with generating tokens to construct the final (boxed) answer.</p><p>incorrect. We also evaluated humans on MATH, and found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%, indicating that MATH can be challenging for humans as well.</p><p>The presence of step-by-step solutions allows models to utilize "scratch space": rather than having to generate a final answer immediately, models can first generate solutions that may contain intermediate computations. Interestingly, we found that having models generate step-by-step solutions before producing an answer actually decreased accuracy relative to immediately outputting a final answer without generating solutions, indicating the solutions are currently not useful for models at test time. In contrast, having models train on solutions increases relative accuracy by 10% compared to training on the questions and answers directly. We also find that models do better with hints in the form of partial solutions. Our results show that models can make use of actual step-by-step solutions provided to them in various ways, but that they are still unable to effectively use their own generated solutions. Bridging this gap poses an interesting direction for further research.</p><p>While MATH covers advanced problem-solving techniques, models may first need to be trained thoroughly on the fundamentals of mathematics. To address this, we create the first large-scale mathematics pretraining dataset with hundreds of thousands of step-by-step solutions in natural language and L A T E X. We call this dataset the Auxiliary Mathematics Problems and Solutions (AMPS) pretraining corpus, which consists of Khan Academy and Mathematica data. AMPS has over 100, 000 Khan Academy problems with step-by-step solutions in L A T E X; these exercises are used to teach human students concepts ranging from basic addition to Stokes' Theorem. It also contains over 5 million problems generated using Mathematica scripts, based on 100 hand-designed modules covering topics such as conic sections, div grad and curl, KL divergence, eigenvalues, polyhedra, and Diophantine equations. In total AMPS contains 23GB of problems and solutions. Domain-specific pretraining (Gururangan et al., 2020) on AMPS improves relative accuracy by around 25%, equivalent to a 15× increase in model size.</p><p>Altogether, while large Transformer models <ref type="bibr">(Vaswani et al., 2017)</ref> make some progress on the MATH dataset, such as by AMPS pretraining or by training with step-by-step solutions, accuracy nonetheless remains relatively low. While enormous Transformers pretrained on massive datasets can now solve most existing text-based tasks, this low accuracy indicates that our MATH dataset is distinctly harder. Accuracy also increases only modestly with model size: assuming a log-linear scaling trend, models would need around 10 35 parameters to achieve 40% accuracy on math, which is impractical. Instead, to make large strides on the MATH dataset with a practical amount of resources, we will need new algorithmic advancements from the broader research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural Theorem Provers. Much of the existing work on machine learning models for mathematical reasoning relies on automated theorem proving benchmarks. <ref type="bibr" target="#b10">Huang et al. (2019)</ref> use the Coq theorem proving environment to create a machine learning benchmark with 1, 602 theorems and lemmas. <ref type="bibr" target="#b1">Bansal et al. (2019)</ref> introduce the HOList benchmark for automated theorem proving, which uses a formal language to enable automatic evaluation. Rather than use HOList, <ref type="bibr" target="#b19">Polu and Sutskever (2020)</ref> use the Metamath formalization language for automated theorem proving with promising results. We show an example of Metamath in <ref type="figure">Figure 1</ref>. These benchmarks can be approached with seq2seq <ref type="bibr" target="#b25">(Sutskever et al., 2014)</ref> Transformers which have traction on the problem <ref type="bibr" target="#b19">(Polu and Sutskever, 2020;</ref><ref type="bibr" target="#b21">Rabe et al., 2020;</ref>. State-of-the-Art Accuracy on Mathematics Datasets</p><p>Figure 2: Compared to existing proof and plug-and-chug tasks, our mathematical problem solving task is considerably more challenging. HOList results are from <ref type="bibr" target="#b29">Wu et al. (2021)</ref>.</p><p>HOLStep results are from <ref type="bibr" target="#b3">Crouse et al. (2019)</ref>. DeepMind Math accuracy is the median IID accuracy from <ref type="bibr" target="#b8">Henighan et al. (2020)</ref>. Symbolic Integration accuracy is from <ref type="bibr">Lample and Charton (2020)</ref>.</p><p>Rather than prove theorems with standard pretrained Transformers, <ref type="bibr" target="#b18">McAllester (2020)</ref> proposes that the community create theorem provers that bootstrap their mathematical capabilities through open-ended self-improvement. For bootstrapping to be feasible, models will also need to understand mathematics as humans write it, as manually converting advanced mathematics to a proof generation language is extremely time-consuming. This is why Szegedy (2020) argues that working on formal theorem provers alone will be an impractical path towards world-class mathematical reasoning. We address Szegedy (2020)'s concern by creating a dataset to test understanding of mathematics written in natural language and commonplace mathematical notation. This also means that the answers in our dataset can be assessed without the need for a cumbersome theorem proving environment, which is another advantage of our evaluation framework.</p><p>Neural Calculators. Recent work shows that Transformers can sometimes perform laborious calculations around as well as calculators and computer algebra systems. <ref type="bibr">Lample and Charton (2020)</ref> use Transformers to solve algorithmically generated symbolic integration problems and achieve greater than 95% accuracy. <ref type="bibr" target="#b0">Amini et al. (2019)</ref>; <ref type="bibr" target="#b15">Ling et al. (2017)</ref> introduce plug-and-chug multiple choice mathematics problems and focus on sequenceto-program generation. <ref type="bibr" target="#b24">Saxton et al. (2019)</ref> introduce the DeepMind Mathematics dataset, which consists of algorithmically generated plug-and-chug problems such as addition, list sorting, and function evaluation, as shown in <ref type="figure">Figure 1</ref>. Recently, <ref type="bibr" target="#b8">Henighan et al. (2020)</ref> show that nearly all of the DeepMind Mathematics dataset can be straightforwardly solved with large Transformers.</p><p>Benchmarks for Enormous Transformers. There are few existing natural language benchmarks left to solve, as tasks that aggregate multiple subtasks such as SuperGLUE <ref type="bibr" target="#b26">(Wang et al., 2019)</ref> are solved by simply training enormous Transformers (He et al., 2020). <ref type="bibr" target="#b11">Kaplan et al. (2020)</ref>; <ref type="bibr" target="#b8">Henighan et al. (2020)</ref> show that the performance of Transformers predictably increases with an increase in model size and dataset size, raising the question of whether natural language processing can be solved by simply increasing compute and funding. In Appendix A.1, we even find that large GPT-3 models can perform remarkably well on a sequence completion test similar to an IQ test, the C-Test <ref type="bibr" target="#b9">(Hernández-Orallo, 1998;</ref><ref type="bibr" target="#b13">Legg and Hutter, 2007)</ref>. Even difficult logical understanding tasks such as LogiQA  will soon be straightforwardly solved by enormous Transformers should trends continue, which we also show in Appendix A.1. <ref type="bibr">Hendrycks et al. (2021)</ref> create a multiple-choice benchmark covering 57 subjects which are difficult for enormous Transformers. However, unlike our benchmark, which is a text generation task with 12, 500 mathematical reasoning questions, their benchmark is a multiple choice task that includes only a few hundred questions about mathematics. We find that our MATH benchmark is especially challenging for current models and, if trends continue, simply using bigger versions of today's Transformers will not solve our task in the foreseeable future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section, we introduce two new datasets, one for benchmarking mathematical problem-solving ability (MATH) and one for pretraining (AMPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algebra</head><p>Conic sections, polynomial GCD, De Moivre's theorem, function inverses, parametric equations, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculus</head><p>Arclength, Jacobian, Laplacian, divergence, curl, gradients, integrals, power series expansion, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head><p>Expectation, geometric mean, harmonic mean, KL divergence, variance and standard deviation, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry</head><p>Triangle area, triangle inradius, triangle orthocenter, polygon angles, polyhedron diameter, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Algebra</head><p>Characteristic polynomials, eigenvalues, point to plane distance, reduced row echelon form, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number Theory</head><p>Modular inverse, Euler's totient function, Diophantine equations, Chinese remainder theorem, ... <ref type="table">Table 1</ref>: A subset of the topics covered by our 100 hand-designed Mathematica scripts, which is part of our Auxiliary Mathematics Problems and Solutions (AMPS) pretraining dataset. Of these scripts, 37 also generate step-by-step solutions. We generated around 50, 000 exercises with each Mathematica script, or around 5 million problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The MATH Dataset</head><p>The MATH dataset consists of problems from mathematics competitions including the AMC 10, AMC 12, AIME, and more. Many of these problems can be collected from aops.com/community/c3158_usa_contests. These competitions span decades and assess the mathematical problem-solving ability of the best young mathematical talent in the United States. Unlike most prior work, most problems in MATH cannot be solved with a straightforward application of standard K-12 mathematics tools. Instead, humans often solve such problem by applying problem solving techniques and "heuristics" <ref type="bibr" target="#b20">(Pólya, 1945)</ref>.</p><p>The Mathematics Aptitude Test of Heuristics dataset, abbreviated MATH, has 12, 500 problems (7, 500 training and 5, 000 test). With this many training problems, models can learn many useful heuristics for problem solving. Each problem has a step-by-step solution and a final boxed answer. Example problems with step-by-step solutions are shown in <ref type="figure">Figure 1</ref>.</p><p>Categorizing Problems. Problems span various subjects and difficulties. The seven subjects are Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. While subjects like Prealgebra are generally easier than Precalculus, within a subject problems can take on different difficulty levels. We encode a problem's difficulty level from '1' to '5,' where a subject's easiest problems for humans are assigned a difficulty level of '1,' and a subject's hardest problems are assigned a difficulty level of '5.' Concretely, the first few problems of an AMC 8 exam are often level 1, while AIME problems are level 5. This allows us to assess performance across both different subjects and different levels of difficulty.</p><p>Formatting. Problems and solutions are consistently formatted using L A T E X and the Asymptote vector graphics language. Our usage of L A T E X allows us to flexibly encode mathematical problems while avoiding unusual symbols or cumbersome formal languages. Meanwhile, mathematical figures are encoded in the Asymptote language rather than as raster images. This enables pure language models to process figures, diagrams, and graphics, making it possible to assess these models on subjects such as geometry for the first time.</p><p>To assess models using exact match, we force the final boxed answers to follow consistent formatting rules. Specifically, probabilities are expressed as simplified fractions. Moreover, matrix entry fractions are encoded with x/y, while all other fractions are consistently encoded with the \frac{x}{y} command. Coefficients are encoded without a multiplication symbol (e.g. 5x not 5 * x). Expressions with multiple variables are entered in alphabetical order; polynomials are expressed in decreasing degree order. Different fraction encodings equivalent, such as \frac{x}{y} and \dfrac{x}{y} and x/y. Different parenthesis encodings, such as \left( and (, are treated as equivalent.</p><p>We also allow units to be included or omitted from an answer, we ignore spaces, and we treat common equivalent ways of expressing the same number (e.g., 0.5 and 1/2, or 0.1 and .1) as the same. When the answer is a factorized polynomial, we permit different orderings of the factors, so that 4(x + 1)(x − 1) is equivalent to 4(x − 1)(x + 1), and so on. These rules cover nearly all ways that different generated or actual solutions can be equivalent in practice.</p><p>Automatically Assessing Generated Answers. Due to design choices in MATH, we can assess the answers generated by a model automatically, even though the space of model outputs is combinatorially large. Automatic assessment starts by determining the beginning and end of the answer. This is possible to do even if a model generates step-by-step solutions because the final answers in MATH are wrapped and delimited with the \boxed{} command. We can consequently evaluate a model's output by parsing what is inside the \boxed{} command and comparing that with the ground truth answer, while accounting for the equivalent ways of formatting a string described above. Together, the box delimiter and formatting rules provide a unique answer in a well-defined location, which allows us to test for equivalence and use accuracy as our primary metric.</p><p>Human-Level Performance. To estimate human-level performance, we randomly sampled 20 problems from the MATH test set and gave them to humans. We artificially require that the participants have 1 hour to work on the problems and must perform calculations by hand. All participants are university students. One participant who does not like mathematics got 8/20 = 40% correct. A participant ambivalent toward mathematics got 13/20. Two participants who Model Prealgebra Algebra Number Theory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counting &amp; Probability</head><p>Geometry Intermediate Algebra Precalculus Average GPT-2 (0.1B) 5.2 5.1 5.0 2.8 5.7 6.5 7.3 5.4 (+0%) GPT-2 (0.3B) 6.7 6.6 5.5 3.8 6.9 6.0 7.1 6.2 (+15%) GPT-2 (0.7B) 6.9 6.1 5.5 5.1 8.2 5.8 7.7 6.4 (+19%) GPT-2 (1.5B) 8.3 6.2 4.8 5.4 8.7 6.1 8.8 6.9 (+28%) GPT-3 (2.7B) 2.8 2.9 3.9 3.6 2.1 2.5 2.6 2.9 (−46%) GPT-3 (175B) 7.7 6.0 4.4 4.7 3.1 4.4 4.0 5.2 (−4%) <ref type="table">Table 2</ref>: MATH accuracies across subjects for GPT-2 and few-shot GPT-3 models. The character 'B' denotes the number of parameters in billions. The gray text indicates the relative improvement over the 0.1B baseline. All GPT-2 models pretrain on AMPS, and all values are percentages. A 15× increase in model parameters increased accuracy by 1.5%, a 28% relative improvement. Model accuracy is increasing very slowly, so much future research is needed.</p><p>like mathematics got 14/20 and 15/20. A participant who got a perfect score on the AMC 10 exam and attended USAMO several times got 18/20. A three-time IMO gold medalist got 18/20 = 90%, though missed questions were exclusively due to small errors of arithmetic. Expert-level performance is theoretically 100% given enough time, though even 40% accuracy for a machine learning model would be impressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AMPS (Khan + Mathematica) Dataset</head><p>Since pretraining data can greatly influence performance <ref type="bibr">(Hernandez et al., 2021;</ref><ref type="bibr">Gururangan et al., 2020)</ref> and since mathematics is a small fraction of online text, we introduce a large and diverse mathematics pretraining corpus. Our pretraining dataset, the Auxiliary Mathematics Problems and Solutions (AMPS) dataset, has problems and step-by-step solutions typeset in L A T E X. AMPS contains over 100, 000 problems pulled from Khan Academy and approximately 5 million problems generated from manually designed Mathematica scripts.</p><p>Khan Academy. The Khan Academy subset of AMPS has 693 exercise types with over 100, 000 problems and full solutions. Problem types range from elementary mathematics (e.g. addition) to multivariable calculus (e.g. Stokes' theorem), and are used to teach actual K-12 students. Many of the exercises can be regenerated using code from github.com/Khan/khan-exercises/. We show the full list of problem types in the <ref type="figure" target="#fig_1">Figures 13 to 16</ref>.</p><p>Mathematica. To make AMPS larger, we also contribute our own Mathematica scripts to generate approximately 50× more problems than our Khan Academy dataset. With Mathematica, we designed 100 scripts that test distinct mathematics concepts, 37 of which include full step-by-step L A T E X solutions in addition to final answers. We generated around 50, 000 exercises from each of our scripts, or around 5 million problems in total. This results in over 23 GB of mathematics problems, making it larger than the 16 GB of natural language used to train BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>.</p><p>Problems include various aspects of algebra, calculus, counting and statistics, geometry, linear algebra, and number theory (see <ref type="table">Table 1</ref> for a sampling of topics). Unlike prior approaches to algorithmically generating mathematics problems, we use Mathematica's computer algebra system so that we can manipulate fractions, transcendental numbers, and analytic functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform experiments to investigate performance on the MATH dataset. We find that accuracy remains low even for the best models. Furthermore, unlike for most other text-based datasets, we find that accuracy is increasingly very slowly with model size. If trends continue, then we will need algorithmic improvements, rather than just scale, to make substantial progress on MATH. Nevertheless, we show that making progress is also possible today. We find that pretraining on AMPS increases relative accuracy by 25%, which is comparable to the improvement due to a 15× increase in model size.</p><p>We also experiment with using step-by-step solutions. We find that having models generate their own step-by-step solutions before producing an answer actually degrades accuracy. We qualitatively assess these generated solutions and find that while many steps remain illogical, they are often related to the question. Finally, we show that step-by-step solutions can still provide benefits today. We find that providing partial ground truth step-by-step solutions can improve performance, and that providing models with step-by-step solutions at training time also increases accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Models and Hyperparameters. Because MATH answers must be generated, we use autoregressive language models, namely GPT-2 <ref type="bibr" target="#b22">(Radford et al., 2016)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, which are decoder models pretrained on natural language text. Our GPT-2 models tokenize numbers so that each digit is its own token . We were Problem: Suppose a and b are positive real numbers with a &gt; b and ab = 8. Find the minimum value of a 2 +b 2 a−b . Generated solution:</p><formula xml:id="formula_1">Let s = a + b. By QM-AM, a 2 +b 2 a−b ≥ a+b 2 = s 2 . Then a 2 +b 2 a−b ≥ s 2 4 , so a 2 + b 2 ≥ s 2 2 . Hence, a 2 +b 2 a−b ≥ s 2 2 + 2s 2 a−b = s 2 2 + 2s 2 = 8 . Ground truth solution: We can write a 2 +b 2 a−b = a 2 +b 2 −2ab+16 a−b = (a−b) 2 +16 a−b = a − b + 16 a−b . By AM-GM, a − b + 16 a−b ≥ 2 (a − b) · 16</formula><p>a−b = 8. Equality occurs when a − b = 4 and ab = 8. We can solve these equations to find a = 2 √ 3 + 2</p><formula xml:id="formula_2">and b = 2 √ 3 − 2.</formula><p>Thus, the minimum value is 8 .</p><p>Problem: Right ∆ABC has legs measuring 8 cm and 15 cm. The triangle is rotated about one of its legs. What is the number of cubic centimeters in the maximum possible volume of the resulting solid? Express your answer in terms of π. Generated solution:</p><p>Rotating the triangle about one leg gives us the following triangle: 8</p><formula xml:id="formula_3">15 (8)(15) (8)(−1) + (15)(−1) = 120 7 = 120π .</formula><p>Ground truth solution: If the triangle is rotated about the shorter leg, then the radius is the longer leg and the height is the shorter leg, and the volume is 1 3 · (15 2 π)(8) = 600π cubic centimeters. If the triangle is rotated about the longer leg, then the radius is the shorter leg and the height is the longer leg, and the volume is 1 3 (8 2 π)(15), which is 8 15 of the volume we found earlier. So, the maximum possible volume is 600π cubic centimeters. <ref type="figure">Figure 3</ref>: Problems, step-by-step solutions generated by our GPT-2 1.5B model, and ground truth solutions. Observe that models can provide the right answer yet generate a misleading and wrong explanation. The second generated solution demonstrates that models are capable of generating Asymptote commands to create figures and graphics.</p><p>unable to get T5 <ref type="bibr" target="#b23">(Raffel et al., 2020)</ref>, which has a tokenizer that removes many L A T E X symbols, to have competitive accuracy after a broad hyperparameter sweep.</p><p>Before fine-tuning on MATH, models pretrain on AMPS. We pretrain for one epoch, using AdamW <ref type="bibr" target="#b17">(Loshchilov and Hutter, 2019)</ref>, using a batch size of 128, and using a weight decay of 0.05. We use the standard autoregressive language modeling objective. During pretraining, we upsample Khan Academy data by a factor of 5 and we downsample Mathematica by a factor of 2 to account for the large difference in dataset sizes.</p><p>During fine-tuning, models predict final answers and solutions. Concretely, if P is the problem statement, we train with an equal mix of " P Final Answer: &lt;Answer&gt;" and " P Full Solution: &lt;Step-by-Step Solution&gt;" sequences. This makes it possible for the model to both generate full solutions and also to output just the final answer. For fine-tuning we use the same batch size and weight decay as in pretraining.</p><p>Unless otherwise specified, for GPT-2 we use the default HuggingFace <ref type="bibr" target="#b28">(Wolf et al., 2020)</ref> generation parameters, except that we use beam search. Our beam search has a beam size of 20 when only generating the final answer, and a beam size of 10 when generating full step-by-step solutions. By default, we evaluate models by prompting them with " P Final Answer:" so that they directly generate the final answer to each problem, not the step-by-step solution.</p><p>We also evaluate GPT-3 in a few-shot setting (no fine-tuning) using the OpenAI API. We use the 'Ada' GPT-3 model which has approximately 2.7 billion parameters, and the 'Davinci' model which has approximately 175 billion parameters. Since we are performing few-shot evaluation, we construct our prompt by prepending 8 problems with correct answers (but not step-by-step solutions due to space). Using temperature 0, models output up to 20 tokens for the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyzing Model Performance</head><p>Model Size. While increasing model parameters often automatically solves many tasks , we find that MATH is unusually challenging for enormous Transformers. <ref type="table">Table 2</ref> shows that the average accuracy across subjects for the smallest model, GPT-2 with 0.1 billion parameters, is 5.4%. Meanwhile, a GPT-2 model with 15× the number of parameters attains 6.9% accuracy, a 28% relative improvement. This indicates that while having more parameters helps, absolute accuracy remains far from the ceiling and is only increasing slowly, quite unlike most other text-based tasks.</p><p>Problem Difficulty. We also analyze model accuracy while controlling for problem difficulty. Higher levels of difficulty correspond to lower accuracy, as expected. These results are visualized in <ref type="figure">Figure 10</ref> of Appendix A.3. The accuracy of Problem: The three-digit number "ab5" is divisible by 3. How many different three-digit numbers can "ab5" represent? Generated solution: A number is divisible by 3 if and only if the sum of its digits is divisible by 3. So ab5 is divisible by 3 if and only if the two-digit number ab leaves a remainder of 1 when divided by 3. There are 90 two-digit numbers, of which 90/3 = 30 leave a remainder of 1 when divided by 3. Ground truth solution: If the number is divisible by 3, the sum of the digits must be divisible by 3. So a + b must be one more than a multiple of 3, since 5 is one less than a multiple of 3. We have several options: If a + b = 1, it must be (1, 0), one possibility. For a + b = 4, a can be 1 to 4, for four possibilities. For a + b = 7, a can be 1 to 7, so seven possibilities. For a + b = 10, anything from (1, 9) to (9, 1) works, so nine possibilities. For a + b = 13, the pairs range from (4, 9) to (9, 4), for six possibilities. If a + b = 16, we can have (7, 9), (8, 8), or (9, 7), so three possibilities. a + b = 19 and up is not possible. So the total number is 1 + 4 + 7 + 9 + 6 + 3 = 30</p><p>Problem: Beginning at point A in the diagram, Dora selects one of the four possible directions with equal probability. Each time she comes to an intersection, she again randomly selects one of the possible directions. What is the probability that, in her first four steps, she will walk completely around the gray square? Express your answer as a common fraction.</p><p>A Generated solution: The total probability that Dora chooses a direction completely around the gray square is 5 10 = 1 2 .</p><p>Ground truth solution: The only way for the Dora to end up at her starting point in four steps is for her to traverse the four sides of the gray square. She can do this in two ways: clockwise and counterclockwise. The probability of each of these two paths is 1 4 4 = 1 256 . Therefore, the probability that she ends up where she started is</p><formula xml:id="formula_4">1 256 + 1 256 = 1 128</formula><p>. <ref type="figure">Figure 4</ref>: Additional example problems, generated solutions, and ground truth solutions from our MATH dataset. The first problem's generated solution has the right answer with a correct and simple explanation. The second problem is a combinatorics problem specified with a figure, which the model gets wrong.</p><p>GPT-2 (1.5B) is around 15% for level 1 (easy) and around 4% for level 5 (hard). Even our benchmark's easiest problems are more challenging than previous benchmarks that focused on straightforward plug-and-chug problems.</p><p>AMPS Pretraining. As an ablation, we test how models with AMPS pretraining compare with models that were not pretrained on AMPS. Without pretraining on AMPS, a GPT-2 (1.5B) model fine-tuned on MATH attains 5.5% accuracy.</p><p>In contrast, a GPT-2 (1.5B) model both pretrained on AMPS and fine-tuned on MATH attains 6.9%, a 25% relative improvement in accuracy. Consequently AMPS increases accuracy about as much as a 15× increase in parameters, indicating its value as a pretraining dataset.</p><p>We tried additionally pretraining on StackExchange, a real-world but less curated source of mathematics text. A GPT-2 (0.3B) model pretrained on both AMPS and questions and answers from Math StackExchange (∼ 3 GB) had 6.0% accuracy, which is actually less than the 6.2% accuracy attained by pretraining on AMPS alone. Thus our dataset is more useful for pretraining even than diverse real-world mathematics data.</p><p>Error Detection. To determine whether we can trust the answers from a model, we analyze model confidence to see whether confidence tends to be higher for correct answers. We define confidence as the average prediction probability of the tokens that make up a generated answer. We histogram confidences for correct and incorrect answers in <ref type="figure" target="#fig_0">Figure 5</ref>. GPT-2 (1.5B) is highly overconfident, with confidences that are typically around 100%, and there is substantial overlap between correct and incorrect answers. Following <ref type="bibr" target="#b6">Hendrycks and Gimpel (2017)</ref>, we computed the probability that a correct answer has higher confidence than an incorrect answer. To do this, we compute the Area Under the Receiver Operating Characteristic curve (AUROC). An AUROC of 100% corresponds to being able to perfectly detect correct and incorrect answers, while 50% corresponds to random chance. We find that with GPT-2 (1.5B), the AUROC is quite low at 68.8%. This suggests there is substantial room for improvement in detecting model errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analyzing Step-by-Step Solutions</head><p>Scratch Space. Our MATH dataset and AMPS pretraining dataset provide full step-by-step solutions, an important and rare type of side information (Murty et al., 2020) that can in principle teach models how to derive answers and use scratch space. By training a language model on these solutions, we can have models generate full step-by-step solutions. This may be especially useful for difficult problems, for which outputting the correct answer after just a few forward passes may be insufficient. By allowing the model to use several steps of processing before outputting a final answer, the model could adaptively use computation and have higher peformance, in addition to making its reasoning more interpretable.</p><p>We test this by prompting models with " P Full Solution:" to generate a full solution along with a final boxed answer, rather than the boxed answer alone. We evaluated this for GPT-2 (1.5B) and found that this actually makes  . The incorrect answer histogram is translucent and overlays the correct answer histogram. GPT-2 1.5B is overconfident and has an AUROC of 68.8%, indicating that it is not good at detecting errors from confidence alone. Confidences below 0.75 are omitted for ease of visualization. performance worse, dropping accuracy to 5.3%. We hypothesize that the drop in accuracy from using scratch space arises from a snowballing effect, in which partially generated "solutions" with mistakes can derail subsequent generated text. Nevertheless, when generation becomes more reliable and models no longer confuse themselves by their own generations, our dataset's solutions could in principle teach models to use scratch space and attain higher accuracy.</p><p>Examples. We can also qualitatively assess the step-by-step solutions that the model generates. We show examples of generated solutions in <ref type="figure">Figures 3 and 4</ref>. We find that the model can consistently generate correct L A T E X and often performs steps that appear related to the question at hand, but still makes many logical mistakes, both in terms of what the question seems to be asking and in individual steps that are part of a larger derivation.</p><p>The Benefits of MATH Solutions. We find that giving models partial step-by-step MATH solutions during inference can improve accuracy. We test performance when we allow models to predict the final answer given a "hint" in the form of a portion of the ground truth step-by-step solution. To do so, for this experiment we prompt models with " P &lt;Partial Step-by-Step Solution without Final Answer&gt; Final Answer:" during both fine-tuning and evaluation for different partial fractions of the step-by-step solution. This is the same as the default setting when we let models see 0% of the step-by-step solution. When models see "99%" of the solution, they are given the whole step-by-step solution except for the final answer. We show results with GPT-2 (0.7B) for different fractions of the solution in <ref type="figure" target="#fig_1">Figure 6</ref>. Observe that the model still only attains approximately 40% when given 99% of the solution, indicating room for improvement.</p><p>Finally, we also find that providing models with step-by-step during training can further improve performance. We run an ablation by fine-tuning models on MATH with the same setup as before, except that we only show examples with the final answer and no step-by-step solution. If we fine-tune with only the final answer, the GPT-2 (1.5B) accuracy decreases by 0.6% to 6.3%, a 10% relative reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we laid groundwork for future research in machine learning for mathematical problem solving. We introduced the MATH benchmark, which enables the community to measure mathematical problem-solving ability. In addition to having answers, all MATH problems also include answer explanations, which models can learn from to generate their own step-by-step solutions. We also introduce AMPS, a diverse pretraining corpus that can enable future models to learn virtually all of K-12 mathematics. While most other text-based tasks are already nearly solved by enormous Transformers, MATH is notably different. We showed that accuracy is slowly increasing and, if trends continue, the community will need to discover conceptual and algorithmic breakthroughs to attain strong performance on MATH. Given the broad reach and applicability of mathematics, solving the MATH dataset with machine learning would be of profound practical and intellectual significance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Logic and Intelligence Tests</head><p>While enormous Transformers perform poorly on MATH, they do well on other logic and intelligence tests.</p><p>We analyze Transformers on LogiQA , a task with logical reasoning questions such as "David knows Mr. Zhang's friend Jack, and Jack knows David's friend Ms. Lin. Everyone of them who knows Jack has a master's degree, and everyone of them who knows Ms. Lin is from Shanghai. Who is from Shanghai and has a master's degree?" As shown in <ref type="figure" target="#fig_2">Figure 7</ref>, Transformers are improving on LogiQA, so much so that they will attain human-level performance relatively soon, should trends continue.</p><p>We also find that Transformers also do well on the C-Test, a pattern completion test that has a 77% correlation with human IQ <ref type="bibr">(Hernández-Orallo, 2000)</ref>. An example of a problem from C-Test is the sequence "a, a, z, c, y, e, x, _" which has the answer "g." We regenerated hundreds of C-Test examples to test GPT-3 (175B) in a 5-shot setting. While GPT-3 had abysmal performance when the sequences were letters, converting letters to numbers helped. After changing 'a' to 0, 'b' to 1, . . ., and 'z' to 25, accuracy became approximately 40% on the hardest examples (C-Test questions with complexity "13"). For comparison, on these same examples, average humans attained around 20% accuracy <ref type="bibr">(Hernández-Orallo, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Further Dataset Information</head><p>Rendering Graphics. For the first time, our dataset makes it possible for text-based models to process graphical mathematical figures by expressing figures in asymptote code. For example, <ref type="figure" target="#fig_4">Figure 8</ref> shows asymptote code and the figure it produces. In short, it is possible to concisely specify many visual mathematics problems with code, sidestepping the complexity of multi-modal models. Contrasting AMPS and DeepMind Mathematics. AMPS has several hundred exercise types or modules (Khan Academy has 693 modules and Mathematica has 100), while DeepMind mathematics (DM) has only a few dozen. We show all Khan Academy modules in <ref type="figure" target="#fig_1">Figures 13 to 16</ref>. Most DM exercises increase the diversity of problems by simply having a wide range of coefficients and constants. For example, its derivatives module exclusively covers polynomial derivatives with wide-ranging coefficients, while ours covers mixtures of dozens of major analytic functions. DM opts not to cover concepts and subjects such as logarithms and geometry, unlike AMPS. While DM is formatted in plaintext, AMPS is formatted in L A T E X. Finally, while DM solely has final answers, all 693 Khan Academy modules and 37 of our Mathematica modules have full step-by-step solutions. size(40); draw(shift(1.38,0) * yscale(0.3) * Circle((0,0), .38)); draw((1,0)--(1,-2)); draw((1.76,0)--(1.76,-2)); draw((1,-2).. <ref type="figure" target="#fig_1">(1.38,-2.114)..(1.76,-2)</ref>); path p =(1.38,-2.114).. <ref type="figure" target="#fig_0">(1.74,-1.5)..(1,-0.5)..(1.38,-.114)</ref>; pair a=(1.38,-2.114), b=(1.76,-1.5); path q =subpath(p, 1, 2); path r=subpath(p,0,1); path s=subpath(p,2,3); draw(r); draw(s); draw(q, dashed); label("$5$",midpoint((1.76,0)--(1.76,-2)),E); Example from a Khan Academy module: Problem: In history class, the girl to boy ratio is 9 to 6. If there are a total of 60 students, how many boys are there?</p><p>Solution: A ratio of 9 girls to 6 boys means that a set of 15 students will have 9 girls and 6 boys. A class of 60 students has 4 sets of 15 students. Because we know that there are 6 boys in each set of 15 students, the class must have 4 groups of 6 boys each. There is a total of 24 boys in history class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Mathematica code that generates practice problems:</head><p>In[1]:= For[i=0,i&lt;50000,i++, roundbasis = RandomChoice <ref type="figure" target="#fig_0">[{0.8,0.1,0.05,0.05}-&gt;{1,1/2,1/3,1/5}</ref>  <ref type="figure">For[j=0,j&lt;d2,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Difficulty Analysis</head><p>We break down MATH accuracy by difficulty levels. In <ref type="figure">Figure 10</ref>, we observe that human difficulty and machine difficulty track each other. In <ref type="figure">Figure 11</ref>, we find that accuracy can vary by level and subject substantially. Finally, in <ref type="figure">Figure 12a</ref> and <ref type="figure">Figure 12b</ref>, we analyze the relation between accuracy and problem and solution length, and find that problems with long questions or ground truth solutions indeed tend to be more difficult than problems with short questions or solutions. Khan Academy Modules (1/4): 2 step equations; 2-step addition word problems within 100; 2-step subtraction word problems within 100; 2-step word problems; absolute minima and maxima (closed intervals); absolute minima and maxima (entire domain); absolute value equations; absolute value of complex numbers; add and subtract complex numbers; add and subtract matrices; add and subtract polynomials; add and subtract rational expressions; add and subtract rational expressions: factored denominators; add and subtract rational expressions: like denominators; add and subtract rational expressions: unlike denominators; add and subtract vectors; add 1 or 10; add 1s or 10s (no regrouping); add 3 numbers; add and subtract fractions; add and subtract fractions word problems; add and subtract within 20 word problems; add fractions with unlike denominators; add within 10; add within 1000; add within 20; add within 5; adding and subtracting decimals word problems; adding and subtracting in scientific notation; adding and subtracting negative fractions; adding and subtracting negative numbers; adding and subtracting rational numbers; adding and subtracting decimals word problems; adding and subtracting fractions; adding and subtracting mixed numbers 0.5; adding and subtracting mixed numbers 1; adding and subtracting polynomials; adding and subtracting radicals; adding and subtracting rational expressions 0.5; adding and subtracting rational expressions 1; adding and subtracting rational expressions 1.5; adding and subtracting rational expressions 2; adding and subtracting rational expressions 3; adding and subtracting rational numbers; adding and subtracting with unlike denominators 5; adding and subtracting with unlike denominators 6; adding decimals (hundredths); adding decimals (tenths); adding decimals and whole numbers (hundredths); adding decimals and whole numbers (tenths); adding decimals: thousandths; adding fractions; adding fractions 0.5; adding up to four 2-digit numbers; adding vectors; addition and subtraction word problems; addition and subtraction word problems 2; addition word problems within 100; age word problems; amplitude of sinusoidal functions from equation; analyze concavity; angle addition postulate; angle of complex numbers; approximation with local linearity; arc length; area and perimeter of rectangles word problems; area between two curves; area between two curves given end points; area between two polar curves; area bounded by polar curves; area bounded by polar curves intro; area of a circle; area of parallelograms; area problems; areas of circles and sectors; arithmetic sequences 1; arithmetic sequences 2; arithmetic series; average value of a function; average word problems; basic division; basic multiplication; basic partial derivatives; basic set notation; binomial probability formula; calculating binomial probability; center and radii of ellipses from equation; chain rule capstone; chain rule intro; change of variables: bound; change of variables: factor; circles and arcs; circulation form of green's theorem; classifying critical points; combinations; combined vector operations; combining like terms; combining like terms with distribution; combining like terms with negative coefficients; combining like terms with rational coefficients; complementary and supplementary angles; complete solutions to 2-variable equations; completing the square; completing the square (intermediate); completing the square (intro); complex numbers from absolute value and angle; complex plane operations; composite exponential function differentiation; composite numbers; conditional statements and truth value; construct exponential models; construct sinusoidal functions; continuity at a point (algebraic); converting between point slope and slope intercept form; converting between slope intercept and standard form; converting decimals to fractions 1; converting decimals to fractions 2; converting decimals to percents; converting fractions to decimals; converting mixed numbers and improper fractions; converting multi digit repeating decimals to fractions; converting multi-digit repeating decimals to fractions; converting percents to decimals; converting recursive and explicit forms of arithmetic sequences; converting recursive and explicit forms of geometric sequences; counting 1; counting 2; cube roots; cube roots 2; cumulative geometric probability; defined and undefined matrix operations; definite integral as the limit of a riemann sum; definite integrals of piecewise functions; definite integrals: common functions; definite integrals: reverse power rule; degrees to radians; density word problems; dependent probability; derivatives 1; derivatives of a x and log a x; derivatives of sin(x) and cos(x); derivatives of tan(x), cot(x), sec(x), and csc(x); derivatives of e x and ln(x); determinant of a 2x2 matrix; determinant of a 3x3 matrix; difference of squares; differentiability at a point: algebraic; differential equations: exponential model equations; differentiate integer powers (mixed positive and negative); differentiate polynomials; differentiate products; differentiate quotients; differentiate rational functions; differentiate related functions; differentiating using multiple rules; direct comparison test; direct substitution with limits that don't exist; direction of vectors; disc method: revolving around other axes; disc method: revolving around x-or y-axis; discount, markup, and commission word problems; discount, tax, and tip word problems; disguised derivatives; distance between point and line; distance formula; distributive property with variables; divide by 1; divide by 10; divide by 2; divide by 3; divide by 4; divide by 5; divide by 6; divide by 7; divide by 8; divide by 9; divide complex numbers; divide decimals by whole numbers; divide fractions by whole numbers; divide mixed numbers; divide polynomials by linear expressions; divide polynomials by monomials (with remainders); divide polynomials by x (no remainders); divide polynomials by x (with remainders); divide polynomials with remainders; ... Khan Academy Modules (4/4): represent linear systems with matrices; represent linear systems with matrix equations; reverse power rule; reverse power rule: negative and fractional powers; reverse power rule: rewriting before integrating; reverse power rule: sums and multiples; rewriting decimals as fractions challenge; right triangle trigonometry word problems; roots of decimals and fractions; sample and population standard deviation; scalar matrix multiplication; scalar multiplication; scientific notation; secant lines and average rate of change; secant lines and average rate of change with arbitrary points; secant lines and average rate of change with arbitrary points (with simplification); second derivative test; second derivatives (implicit equations); second derivatives (parametric functions); second derivatives (vector-valued functions); segment addition; separable differential equations; significant figures; simplify roots of negative numbers; simplify square roots (variables); simplify square-root expressions; simplifying expressions with exponents; simplifying fractions; simplifying radicals; simplifying radicals 2; simplifying rational expression with exponent properties; simplifying rational expressions 2; simplifying rational expressions 3; simplifying rational expressions 4; sinusoidal models word problems; slope-intercept from two points; solid geometry; solutions to quadratic equations; solutions to systems of equations; solve equations using structure; solve exponential equations using exponent properties; solve exponential equations using exponent properties (advanced); solve exponential equations using logarithms: base-10 and base-e; solving equations in terms of a variable; solving for the x intercept; solving for the y intercept; solving proportions; solving quadratics by completing the square 1; solving quadratics by completing the square 2; solving quadratics by factoring; solving quadratics by factoring 2; solving quadratics by taking the square root; solving rational equations 1; solving rational equations 2; special right triangles; square and cube challenge; square roots of perfect squares; standard deviation; standard deviation of a population; stokes' theorem; substitution with negative numbers; subtract decimals (hundredths); subtract decimals and whole numbers (hundredths); subtract within 10; subtract within 1000; subtract within 20; subtract within 5; subtracting decimals (tenths); subtracting decimals and whole numbers (tenths); subtracting decimals: thousandths; subtracting fractions; subtracting fractions with common denominators; subtracting fractions with unlike denominators; subtraction word problems within 100; summation notation intro; sums of consecutive integers; surface integrals to find surface area; switching bounds on double integrals; symbols practice: the gradient; systems of equations; systems of equations with elimination; systems of equations with simple elimination; systems of equations with substitution; systems of equations word problems; tangents to polar curves; taylor and maclaurin polynomials; the derivative and tangent line equations; the divergence theorem; the fundamental theorem of calculus and definite integrals; the hessian matrix; translate one-step equations and solve; trigonometry 0.5; trigonometry 1; trigonometry 1.5; trigonometry 2; triple integrals; two-step equations; two-step equations with decimals and fractions; two-step equations word problems; u-substitution: definite integrals; u-substitution: indefinite integrals; unit circle; unit vectors; use arithmetic sequence formulas; use geometric sequence formulas; use the properties of logarithms; use the pythagorean identity; using the mean value theorem; using the quadratic formula; using units to solve problems; variance; vector word problems; vector-valued functions differentiation; verify solutions to differential equations; vertex of a parabola; volume word problems; volumes with cross sections: squares and rectangles; volumes with cross sections: triangles and semicircles; washer method: revolving around other axes; washer method: revolving around x-or y-axis; word problems with "more" and "fewer" 2; write common decimals as fractions; write common fractions as decimals; write decimals as fractions; write differential equations; write equations of parallel and perpendicular lines; writing basic expressions with variables; writing basic expressions word problems; writing expressions; writing expressions 2; writing expressions with variables; writing expressions word problems; writing functions with exponential decay; writing linear functions word problems; writing proportional equations; writing proportions; wrong statements in triangle proofs; z scores 1; z scores 2; z scores 3; zero product property. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Histogram of the answer confidences of GPT-2 1.5B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Models conditioned on most of a problem's stepby-step solution can often understand the solution to predict the final answer. '99%' of a solution is all the solution text before the final answer. Not all solutions have an answer that is immediate from from the preceding solution text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Difficult natural language tasks such as LogiQA will soon be solved just by making models larger, assuming trends continue. The Transformers in this figure are UnifiedQA<ref type="bibr" target="#b12">(Khashabi et al., 2020)</ref> models of various sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>AMPS Examples. We show concrete examples from AMPS inFigure 9. AMPS is a mixture of examples from Khan Academy and our 100 Mathematica modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5Figure 8 :</head><label>8</label><figDesc>Example of asymptote code and the figure it produces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>\n$"&lt;&gt;ToString[TeXForm[PolynomialGCD[p,q]//TraditionalForm]]&lt;&gt;"$"}] ] Figure 9: A Khan Academy problem and solution, followed by the code for a simple Mathematica module used to generate polynomials GCD problems. These problems are available in AMPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Problems that are more difficult for humans are also more difficult for GPT-2. Accuracy per subject per difficulty level. Subject accuracy vs problem length. Each point represents a subject at a specific difficulty level. We exclude problems with asymptote figures. Results are from GPT-2 (1.5B). Subject accuracy vs solution length. Each point represents a subject at a specific difficulty level. We exclude problems with asymptote figures. Results are from GPT-2 (1.5B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Khan Academy modules in our AMPS pretraining dataset (Part 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 16 :</head><label>16</label><figDesc>Khan Academy modules in AMPS (Part 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>]; d1 = RandomInteger[{1,6}]; d2 = RandomInteger[{1,3}]; q=0; p=0; While[q==0, For[j=0,j&lt;d1,j++, q += Round[RandomReal[{-5,5}], roundbasis] * x^j</head><label></label><figDesc></figDesc><table><row><cell>;</cell></row><row><cell>];</cell></row><row><cell>];</cell></row><row><cell>While[p==0,</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jose Hernández-Orallo, Alex Gunning, and Neeraj Kapoor. DH is supported by the NSF GRFP Fellowship and an Open Philanthropy Project Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">MathQA: Towards interpretable math word problem solving with operation-based formalisms. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Holist: An environment for machine learning of higher-order theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilcox</surname></persName>
		</author>
		<idno>abs/1904.03241</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>extended version</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjamin Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Language models are few-shot learners. ArXiv, abs/2005.14165, 2020</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving graph neural network representations of logical formulae with subgraph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<idno>abs/1911.06904</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv, abs/2004.10964, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint/>
	</monogr>
	<note>ArXiv, abs/2006.03654, 2020</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling laws for transfer. arXiv, 2021. J. Hernández-Orallo. Beyond the turing test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heewoo Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic, Language and Information</title>
		<editor>Danny Hernandez, J. Kaplan, T. Henighan, and Sam McCandlish</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="447" to="466" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Scaling laws for autoregressive generative modeling. ArXiv, abs/</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A formal definition of intelligence based on an intensional variant of algorithmic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>EIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gamepad: A learning environment for theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1806.00608</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamin Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno>abs/2001.08361</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Universal intelligence: A definition of machine intelligence. Minds and Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franccois</forename><surname>Charton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="391" to="444" />
		</imprint>
	</monogr>
	<note>Deep learning for symbolic mathematics. ArXiv, abs/1912.01412, 2020</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modelling high-level mathematical reasoning in mechanised declarative proofs. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LogiQA: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expbert: Representation engineering with natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<idno>abs/2005.05512</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Shikhar Murty, Pang Wei Koh, and Percy Liang</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>MathZero, the classification problem, and set-theoretic type theory</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How to Solve It</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Pólya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1945" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mathematical reasoning via self-supervised skip-tree training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Analysing mathematical reasoning abilities of neural models. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Christian Szegedy. A promising path towards autoformalization and general artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>CICM, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, L. Kaiser, and Illia Polosukhin</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Sequence to sequence learning with neural networks. Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of mathematics in the natural sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Pure and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lime: Learning inductive bias for primitives of mathematical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/2101.06223</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">/4): divide powers; divide quadratics by linear expressions (no remainders)(2-digit divisors); divide with remainders (2-digit by 1-digit); dividing complex numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Academy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>dividing decimals 1</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">dividing decimals 2; dividing decimals: hundredths; dividing decimals: thousandths; dividing fractions; dividing fractions word problems; dividing fractions word problems 2; dividing mixed numbers with negatives</title>
		<imprint/>
	</monogr>
	<note>dividing negative numbers</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">dividing polynomials by binomials 1; dividing polynomials by binomials 2; dividing polynomials by binomials 3; dividing positive and negative fractions; dividing positive fractions; dividing rational numbers; dividing whole numbers by fractions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">dividing whole numbers by unit fractions; dividing whole numbers like 56/35 to get a decimal; divisibility 0.5; divisibility tests; domain of a function; double integrals with variable bounds; empirical rule; equation of a circle in factored form; equation of a circle in non factored form; equation of a hyperbola</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">); evaluate logarithms: change of base rule; evaluate piecewise functions; evaluate radical expressions challenge; evaluate sequences in recursive form</title>
	</analytic>
	<monogr>
		<title level="m">ellipse; equation of an ellipse from features; equations and inequalities word problems; equations of parallel and perpendicular lines; equations with parentheses; equations with parentheses: decimals and fractions; equations with variables on both sides; equations with variables on both sides: decimals and fractions</title>
		<imprint/>
	</monogr>
	<note>equivalent fractions; estimating square roots; evaluate composite functions; evaluate function expressions; evaluate functions; evaluate logarithms; evaluate logarithms (advanced. evaluating composite functions; evaluating expressions in 2 variables</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">evaluating expressions in one variable; evaluating expressions with multiple variables; evaluating expressions with multiple variables: fractions and decimals; evaluating expressions with one variable; evaluating expressions with variables word problems; evaluating logarithms; evaluating logarithms 2</title>
		<imprint/>
	</monogr>
	<note>expected value; explicit formulas for arithmetic sequences</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">explicit formulas for geometric sequences; exponent rules; exponential expressions word problems (algebraic); exponential model word problems; exponential vs. linear growth over time; exponents with integer bases; exponents with negative fractional bases; expressing ratios as fractions; expressions with unknown variables; expressions with unknown variables 2; extend arithmetic sequences; extend geometric sequences</title>
		<imprint/>
	</monogr>
	<note>extend geometric sequences: negatives and fractions</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">factoring difference of squares 1; factoring difference of squares 2; factoring difference of squares 3; factoring linear binomials; factoring polynomials by grouping; factoring polynomials with two variables; factoring quadratics 1; factoring quadratics with a common factor; features of a circle from its expanded equation; features of a circle from its standard equation; features of quadratic functions; find area elements; find composite functions; find critical points; find critical points of multivariable functions; find inflection points; find inverses of rational functions</title>
	</analytic>
	<monogr>
		<title level="m">extraneous solutions to rational equations; factor higher degree polynomials; factor polynomials using structure; factor quadratics by grouping; factor using polynomial division; factor with distributive property (variables)</title>
		<imprint/>
	</monogr>
	<note>find missing divisors and dividends (1-digit division</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">finding absolute values; finding curl in 2d; finding curl in 3d; finding derivative with fundamental theorem of calculus; finding derivative with fundamental theorem of calculus: chain rule; finding directional derivatives; finding divergence; finding gradients; finding inverses of linear functions; finding partial derivatives; finding percents; finding perimeter; finding tangent planes; finding the laplacian; finite geometric series; finite geometric series word problems; foci of an ellipse from equation; fraction word problems 1; fractional exponents; fractional exponents 2; fractions as division by a multiple of 10; function as a geometric series; function inputs and outputs: equation; function rules from equations; gcf and lcm word problems; general triangle word problems; geometric probability; geometric sequences 1; geometric sequences 2; geometric series formula; graphing points and naming quadrants; graphing systems of equations; greatest common factor; greatest common factor of monomials; higher order partial derivatives; identify composite functions; identify separable equations; identifying numerators and denominators; identifying slope of a line; imaginary unit powers; implicit differentiation; improper integrals; increasing and decreasing intervals</title>
	</analytic>
	<monogr>
		<title level="m">find missing factors (1-digit multiplication); find missing number (add and subtract within 20); find the inverse of a 2x2 matrix; find the missing number (add and subtract within 1000); find trig values using angle addition identities</title>
		<imprint/>
	</monogr>
	<note>indefinite integrals: e x and 1/x; indefinite integrals: sin and cos. independent probability; inequalities word problems; infinite geometric series. integer sums. integral test</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">integrals and derivatives of functions with known power series; integrals in spherical and cylindrical coordinates; integrate and differentiate power series; integrating trig functions; integration by parts</title>
		<imprint/>
	</monogr>
	<note>integration by parts: definite integrals</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">integration using completing the square; integration using long division; integration using trigonometric identities</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">integration with partial fractions; intercepts from an equation; interpret quadratic models; interval of convergence; inverse of a 3x3 matrix; inverses of functions</title>
		<imprint/>
	</monogr>
	<note>iterated integrals; jacobian determinant; l&apos;hopital&apos;s rule (composite exponential functions</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">hopital&apos;s rule: 0/0; l&apos;hopital&apos;s rule: ∞/∞; lagrange error bound</title>
		<imprint/>
	</monogr>
	<note>least common multiple; ..</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Khan Academy modules in AMPS (Part</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">substitution; limits by factoring; limits of piecewise functions; limits of trigonometric functions; limits using conjugates; limits using trig identities; line integrals in vector fields; linear equation and inequality word problems; linear equations with unknown coefficients; linear equations word problems; linear models word problems; logical arguments and deductive reasoning; maclaurin series of sin(x), cos(x), and e x ; make 10; manipulate formulas; markup and commission word problems; matrix addition and subtraction; matrix dimensions; matrix elements; matrix equations: addition and subtraction; matrix equations: scalar multiplication; matrix row operations; matrix transpose; mean, median, and mode</title>
	</analytic>
	<monogr>
		<title level="m">Khan Academy Modules (3/4): limits at infinity of quotients; limits at infinity of quotients with square roots; limits at infinity of quotients with trig; limits by direct</title>
		<imprint/>
	</monogr>
	<note>midline of sinusoidal functions from equation. midpoint of a segment; miscellaneous</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">model with one-step equations and solve; modeling with multiple variables; modeling with sinusoidal functions; modeling with sinusoidal functions: phase shift; motion along a curve (differential calc); motion problems (differential calc); motion problems (with integrals); multi-digit addition; multi-digit division; multi-digit multiplication</title>
		<imprint/>
	</monogr>
	<note>multi-digit subtraction</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">multi-step linear inequalities; multi-step word problems with whole numbers; multiplication and division word problems; multiplication and division word problems (within 100); multiply and divide complex numbers in polar form; multiply and divide powers (integer exponents); multiply and divide rational expressions (advanced); multiply binomials; multiply binomials by polynomials; multiply binomials intro; multiply by 0 or 1; multiply by 2 and 4</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">and 2-digit factors); multiply decimals (up to 4-digit factors); multiply difference of squares; multiply matrices; multiply matrices by scalars; multiply mixed numbers; multiply monomials; multiply monomials by polynomials; multiply powers; multiply unit fractions and whole numbers; multiply whole numbers and decimals; multiplying and dividing in scientific notation; multiplying a matrix by a matrix; multiplying a matrix by a vector; multiplying and dividing complex numbers in polar form; multiplying and dividing negative numbers; multiplying and dividing rational expressions 1; multiplying and dividing rational expressions 2; multiplying and dividing rational expressions 3; multiplying and dividing rational expressions 4; multiplying and dividing rational expressions 5; multiplying and dividing scientific notation; multiplying by multiples of 10</title>
	</analytic>
	<monogr>
		<title level="m">multiply by tens word problems; multiply complex numbers; multiply decimals</title>
		<imprint/>
	</monogr>
	<note>multiplying complex numbers; multiplying decimals like 0.847x3.54 (standard algorithm); positive and negative fractions; multiplying rational numbers</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2; normal form of green&apos;s theorem; number of solutions of quadratic equations; one step equations; one step equations with multiplication; one-step addition and subtraction equations; one-step addition and subtraction equations: fractions and decimals; one-step equations with negatives (add and subtract); one-step equations with negatives (multiply and divide); one-step inequalities; one-step multiplication and division equations; one-step multiplication and division equations: fractions and decimals; operations with logarithms</title>
	</analytic>
	<monogr>
		<title level="m">multivariable chain rule; multivariable chain rule intro; negative exponents; new operator definitions 1; new operator definitions</title>
		<imprint/>
	</monogr>
	<note>order of operations; order of operations (no exponents</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">p-series; parametric curve arc length; parametric equations differentiation; parametric velocity and speed; partial derivatives of vector valued functions; partial fraction expansion; partial sums intro; particular solutions to differential equations; particular solutions to separable differential equations; parts of complex numbers; percent problems; perfect squares; period of sinusoidal functions from equation; permutations; permutations and combinations; planar motion (differential calc); planar motion (with integrals); polar and rectangular forms of complex numbers; polynomial special products: difference of squares; polynomial special products: perfect square; positive and zero exponents; positive exponents with positive and negative bases</title>
	</analytic>
	<monogr>
		<title level="m">order of operations challenge; order of operations with negative numbers; ordered pair solutions to linear equations</title>
		<imprint/>
	</monogr>
	<note>potential functions; power rule. power rule (positive integer powers. power rule (with rewriting the expression); powers of complex numbers; powers of fractions</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">at least one&quot; success; probability with permutations and combinations; problems involving definite integrals (algebraic); properties of exponents (rational exponents); proportion word problems; pythagorean identities; pythagorean theorem; quadratic word problems (factored form); quadratic word problems (standard form); quadratic word problems (vertex form); quadratics by factoring; quadratics by taking square roots; radians and degrees; radians to degrees</title>
		<imprint/>
	</monogr>
	<note>prime numbers; probabilities of compound events; probability 1; probability in normal density curves. probability of. radical equations; radius, diameter, and circumference; range of a function; rate conversion. rate problems</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">rate problems 2; rates of change in other applied contexts (non-motion problems); rates with fractions; ratio test; ratio word problems; reciprocal trig functions; recursive formulas for arithmetic sequences; recursive formulas for geometric sequences; regroup when adding 1-digit numbers; relate addition and subtraction; related rates (advanced); related rates (multiple rates)</title>
		<imprint/>
	</monogr>
	<note>related rates (pythagorean theorem. related rates intro; relationship between exponentials and logarithms</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">relative minima and maxima; remainder theorem; remainder theorem and factors</title>
		<imprint/>
	</monogr>
	<note>removable discontinuities; ..</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Khan Academy modules in AMPS (Part</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
