<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoru</forename><surname>Wang</surname></persName>
							<email>wangshaoru2018@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
							<email>chang.huang@horizon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<email>wmhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection and instance segmentation are two fundamental computer vision tasks. They are closely correlated but their relations have not yet been fully explored in most previous work. This paper presents RDSNet, a novel deep architecture for reciprocal object detection and instance segmentation. To reciprocate these two tasks, we design a two-stream structure to learn features on both the object level (i.e., bounding boxes) and the pixel level (i.e., instance masks) jointly. Within this structure, information from the two streams is fused alternately, namely information on the object level introduces the awareness of instance and translation variance to the pixel level, and information on the pixel level refines the localization accuracy of objects on the object level in return. Specifically, a correlation module and a cropping module are proposed to yield instance masks, as well as a mask based boundary refinement module for more accurate bounding boxes. Extensive experimental analyses and comparisons on the COCO dataset demonstrate the effectiveness and efficiency of RDSNet. The source code is available at https://github.com/wangsr126/RDSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection and instance segmentation are two fundamental and closely related tasks in computer vision, focusing on progressive image understanding on the object level and the pixel level respectively. Due to the application of deep neural networks, recent years have witnessed significant advances of these two tasks. However, their relations have not yet been fully explored in most previous work. Therefore, it remains meaningful and challenging to improve the performance of these two tasks by leveraging the interaction between the object-level and pixel-level information.</p><p>The goal of object detection is to localize each object with a rectangular bounding box and classify it into a specific category. In this task, one of the most critical challenges lies in object localization, namely the specification of an inclusive and tight bounding box. As can be commonly observed in many state-of-the-art methods, localization error can easily degrade their performance, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Localization error mainly results from the mechanism of using regression method to obtain the bounding boxes, as pointwise regression is not directly aware of the whole object. For this reason, it is more rational to cast object localization into a pixel-level task, which is consistent with the definition of bounding box, i.e., the minimum enclosing rectangle of the object mask. Therefore, if the object masks are provided, it will be more straightforward and accurate to obtain the bounding boxes according to the masks. Instance segmentation aims to further predict the perpixel binary mask of each object besides category. The core idea of instance segmentation is to introduce instance-aware pixel category. Currently, most existing approaches follow a two-stage paradigm (e.g. Mask R-CNN <ref type="bibr" target="#b13">(He et al. 2017)</ref>), that is, masks are generated separately for each detection proposal. In this way, the masks are aware of individual object instances naturally. However, such step-by-step process makes the masks heavily depend on the bounding boxes obtained by the detector and vulnerable to their localization errors. Moreover, the utilization of the operator as ROI pooling <ref type="bibr" target="#b11">(Girshick 2015)</ref> largely restricts the mask resolution for large objects. The FCIS model <ref type="bibr" target="#b18">(Li et al. 2017</ref>) introduces position-sensitive map for instance-aware segmentation, but the resulting masks are still restricted to the detection re-sults. Some other methods get rid of detectors <ref type="bibr" target="#b10">(Fathi et al. 2017</ref>) , but they are inferior in accuracy. The origin of these drawbacks mainly lies in the insufficient and inadequate utilization of the object-level information.</p><p>According to the above analyses, object detection and instance segmentation have non-negligible potentials to benefit from each other. Unfortunately, few existing works focus on the relations between them. HTC <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>) is a representative work that adopts cascade architecture for progressive refinement of the two tasks and achieves promising results. However, such multi-stage design brings relatively high computation cost.</p><p>In this work, we present a Reciprocal Object Detection and Instance Segmentation Network (RDSNet) to leverage the relation between these two tasks. RDSNet adopts a twostream structure, i.e., object stream and pixel stream. Features from such two streams are extracted jointly and simultaneously, and then fused alternately across each other. Specifically, the object stream focuses on the object-level features and is formed by a regression-based detector, while the pixel stream focuses on the pixel-level features and follows the FCN <ref type="bibr" target="#b25">(Long, Shelhamer, and Darrell 2015)</ref> architecture to ensure high-resolution outputs. To leverage objectlevel cues from the object stream, a correlation module and a cropping module are proposed, which introduce the awareness of instances and the translation-variance property to the pixel stream, and yield instance-aware segmentation masks. In turn, a mask based boundary refinement module is proposed to alleviate the localization error with the help of the pixel stream, i.e., produce more accurate bounding boxes based on the instance masks.</p><p>RDSNet takes a sufficient consideration of reciprocal relations between object detection and instance segmentation tasks. Compared with previous approaches, it has the following three advantages: 1) Masks generated by RDSNet have consistent high resolution for objects in various scales; 2) Masks are less dependent on detection results thanks to the ingenious cropping module; 3) More accurate and tighter bounding boxes are obtained with a novel pixel-level formulation of object bounding box locations.</p><p>Our main contribution is that we explore the reciprocal relations between the object detection and instance segmentation tasks. And an end-to-end unified architecture RDSNet is proposed to leverage such object-level and pixel-level tasks from each other, which demonstrates the potentials of the multi-tasks fusion concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Object Detection. Most modern CNN-based detectors rely on the regression methods to obtain the bounding boxes of objects. One typical approach is the anchor based methods <ref type="bibr">(2016; 2016; 2017a; 2017b)</ref>, which is first used in the Faster R-CNN model <ref type="bibr" target="#b29">(Ren et al. 2015)</ref>. Dense anchors of multiple scales and aspect ratios are put at each slidingwindow location and serve as regression references. Detectors classify such anchor boxes and regress the offsets from the anchor boxes to the bounding boxes. Another branch of regression-based detectors eliminates the anchor boxes, i.e., anchor-free, which directly predict the center of objects and regress boundary 1 at each location <ref type="bibr" target="#b14">(Huang et al. 2015;</ref><ref type="bibr" target="#b32">Yang et al. 2019;</ref><ref type="bibr" target="#b31">Tian et al. 2019)</ref>. In this work, we propose a simple but effective method, extending above regressionbased detectors to the instance segmentation task and the localization accuracy will be improved.</p><p>Recently, some newly proposed approaches detect objects as keypoints related to the bounding box <ref type="bibr" target="#b17">(Law and Deng 2018;</ref><ref type="bibr" target="#b34">Zhou, Zhuo, and Krahenbuhl 2019;</ref><ref type="bibr" target="#b9">Duan et al. 2019</ref>), but complicated post-processing is required to group such points belonging to the same instance.</p><p>Instance Segmentation. Existing instance segmentation approaches can be grouped as two-stage and one-stage ones. Two-stage approaches follows the top-down process, i.e., detect-then-segment <ref type="bibr" target="#b13">(He et al. 2017)</ref>, in which the object is firstly detected as a bounding box and then a binary mask is generated for each object. Approaches built on Mask R-CNN (e.g. <ref type="bibr" target="#b24">(Liu et al. 2018)</ref>) have dominated several popular benchmarks <ref type="bibr" target="#b19">(Lin et al. 2014;</ref><ref type="bibr" target="#b6">Cordts et al. 2016)</ref>. However, such step-by-step process makes mask quality heavily depend on the box accuracy.</p><p>One-stage approaches are also known as single-shot ones, since objects are directly classified, located and segmented without generating candidate region proposals. A branch of one-stage approaches <ref type="bibr">(2017; 2017; 2017; 2017; 2019)</ref> follows the bottom-up process, i.e., label-pixels-then-cluster, in which pixels are firstly labeled with a category or embedded to a feature space, and then grouped into each object. These approaches derive from methods developed for semantic segmentation, and higher-resolution masks are obtained naturally. However, the unawareness of objects status (numbers, locations, etc.) beforehand complicates the design of predefined categories or embedded space, resulting in inferior results. We consider the origin of predicament lies in the lack of object-level information. Another branch of onestage approaches <ref type="bibr" target="#b18">(Li et al. 2017;</ref><ref type="bibr" target="#b1">Bolya et al. 2019</ref>) is proposed to leverage the top-down and bottom-up approaches jointly. These methods follow the label-pixels-then-cluster process roughly while the grouping method relies on detection results, directly or indirectly (e.g., cropping the masks with bounding boxes predicted by the detector). Our approach follows this process in general, but the object-level information are introduced to simplify the embedded space design with a correlation module, and a modified cropping module is proposed to lower the dependencies of the instance masks on the bounding boxes.</p><p>Boundary Refinement Cascade R-CNN (Cai and Vasconcelos 2018) adopts a cascade architecture to refine the detection results by multi-stage iterative localization. HTC <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>) further improves the information flow. But these methods are designed for two-stage approaches. Instead, our method refines the boundary localization based on a novel formulation, with the compatibility to one-stage approaches and less computation.  <ref type="figure">Figure 2</ref>: The architecture of the proposed RDSNet, which follows a two-stream structure, i.e., object stream and pixel stream. Information from these two streams are mutually interacted by several well-designed modules: the correlation module and cropping module introduce the awareness of instance and the translation variance to pixel stream, assisting in generating the instance masks (see Sec. 3.2). In turn, the instance masks assist the object stream in obtaining more accurate bounding boxes (see Sec. 3.3). c denotes the class number, k denotes the anchors number at one location, d denotes the representation dimensions, and denotes the convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RDSNet</head><p>In this section, we first introduce the overall architecture of RDSNet, where the core is a two-stream structure, consisting of the object stream and the pixel stream, as shown in <ref type="figure">Fig. 2</ref>. Then the bidirectional interaction across the two streams are presented, i.e., leveraging the object-level information to facilitate instance segmentation and the pixel-level information to facilitate object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-stream structure</head><p>The core of RDSNet is the two-stream structure, namely the object stream and the pixel stream. These two streams share the same FPN <ref type="bibr" target="#b20">(Lin et al. 2017a</ref>) backbone and are then separated for each corresponding task. Such parallel structure enables the decoupling of object-level and pixel-level information and alterable resolutions for different tasks.</p><p>Object Stream. The object stream focuses on object-level information, including object categories, locations, etc. It can be formed by various regression-based detectors <ref type="bibr" target="#b22">(Liu et al. 2016;</ref><ref type="bibr" target="#b28">Redmon and Farhadi 2018;</ref><ref type="bibr" target="#b21">Lin et al. 2017b</ref>). In addition, we add a new branch in parallel with the classification and regression branches to extract the object feature for each anchor (or location). This stream is responsible for producing detection results that will later be refined by pixel-level information (see Sec. 3.3).</p><p>Pixel Stream. The pixel stream focuses on pixel-level information, and follows the FCN (2015) design for highresolution outputs. Specifically, per-pixel features are extracted in this stream, and then used to generate instance masks by utilizing object-level information (see Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Assisted Instance Segmentation</head><p>This subsection introduces a novel way to yield instance masks by leveraging the object-level information with new designed correlation and cropping modules.</p><p>From Instance-agnostic to Instance-aware. Instance segmentation aims to assign an instance-aware category to each pixel, but it often suffers from the ambiguity that no predefined categories for pixels are available due to the uncertain numbers and locations of objects in 2D image plane. A proper solution is to leverage the object-level information to introduce the awareness of instances. To this end, a correlation module is designed to link each pixel to its corresponding instance according to the similarity between their representations, which are learned from the object stream and the pixel stream, respectively. Given an object o, we denote its representation by φ(v o ) ∈ R 2×d×1×1 , where v o represents the feature of that object from object stream, and d is the dimension of the representation. The 2 dimension of φ(v o ) indicates that we take both foreground and background into consideration. Similarly, we denote the pixel representations of the entire image as Ψ(U ) ∈ R 1×d×h f ×w f , where U represents the feature map from pixel stream, h f and w f are the spatial dimensions of Ψ(U ).</p><p>The purpose of the correlation module is to measure the similarity between φ(v o ) and Ψ(U ). The correlation operation is defined by</p><formula xml:id="formula_0">M o = softmax(Ψ(U ) φ(v o )) ,<label>(1)</label></formula><p>where stands for the convolution operator. The two channels of similarity map M o ∈ R 2×1×h f ×w f can be viewed as the foreground and background probabilities of each pixel corresponding to object o. Pixel-wise cross entropy loss is appended on M o in training stage. For all objects in an image, the correlation operation is repeated respectively and synchronously. The correlation module enables the mask generator to be trained end-to-end. In a sense, the training process of our approach with correlation is similar to metric learning <ref type="bibr" target="#b10">(Fathi et al. 2017)</ref>, that is, pulling the representations of foreground pixels towards their corresponding object representation in feature space, and pushing those of background pixels away, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>From Translation-invariant to Translation-variant. Unlike most two-stage instance segmentation approaches <ref type="bibr" target="#b13">(He et al. 2017)</ref>, mask generated by the above correlation module for each object covers the whole image, regardless of the object size and location. Such characteristic guarantees high-resolution results, but noise is easily involved. This drawback is largely attributed to the translationinvariant property of convolution: any two pixels with similar appearances tend to have similar representations, although they might actually belong to different instances or background. The property makes it difficult to exclude the noise directly due to the absence of spatial information in the pixel representations. Fortunately, we can overcome this drawback simply by using the bounding boxes produced by the object stream as they can provide adequate spatial restrictions. Specifically, for each object, pixels outside its bounding box are directly set as background and ignored during training. Such cropping strategy makes the instance mask restricted to the internal area of the bounding box and the pixels faraway are not involved in the instance mask even though they have similar appearances. However, simply cropping with such bounding boxes makes the instance masks suffer from the localization errors of detection results (as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)(b)</ref>) and unexpectedly leads to a strong coupling relation between detection and segmentation results.</p><p>To address this issue, a compromise is made by cropping the masks with expanded bounding boxes. During inference, such strategy guarantees relatively low dependen-2 Only foreground channel is taken. cies of masks on bounding boxes, and the pixels far enough are not involved in the masks. Moreover, cropping with the expanded bounding boxes enables a reasonable diversity of negative pixels during training. Two extreme cases, i.e., no cropping and cropping without expanding, are both harmful for our task because too much diversity causes convergence difficulty while insufficient diversity results in deficient feature space, respectively.</p><p>It should be noted that cropping with expanded bounding boxes makes more background pixels involved for each object during training, making the background pixels easily dominate the training procedure. To maintain a manageable balance between the foreground and background (1:1 in our experiments), online hard example mining (OHEM) (Shrivastava, Gupta, and Girshick 2016) for background pixels is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mask Assisted Object Detection</head><p>In this section, we introduce how to enhance the detection results by utilizing the pixel-level information. According to the aforementioned analyses, pixel-level information has the potential to benefit the detection task, especially for object boundary localization. To this end, we develop a new formulation for boundary localization based on the Bayes' theorem. In this formulation, we comprehensively utilize the bounding box and instance mask obtained from the object stream and pixel stream to get a more accurate bounding box of each object. Based on this formulation, a mask based boundary refinement module (MBRM) is proposed.</p><p>Mask Based Boundary Refinement Module. Bounding box is originally defined as the minimum enclosing rectangle of an object, indicating that it absolutely depends on the region covered by the instance mask. In this sense, it seems indirect to obtain bounding boxes by regression methods that are commonly adopted in existing object detection approaches. Instead, if an instance mask is provided, a quite straightforward solution is to use its minimum enclosing rectangle as the detection result. This is exactly our baseline named direct. In this case, the regressed bounding box is only used for mask generation in the pixel stream.</p><p>Although the regressed bounding boxes might contain localization errors, we think they still provide a reasonable prior for the object boundary location to some extent. Therefore, our formulation leverages the detection and segmentation results jointly. Specifically, we view the coordinate of a boundary as a discrete random variable. From the probabilistic perspective, an object boundary location is the argmax of the probability of a coordinate where the boundary locates, namely</p><formula xml:id="formula_1">x = argmax i P (X = i|M ) ,<label>(2)</label></formula><p>where X is the discrete random variable for the horizontal coordinate of the left boundary, M ∈ R h×w is the foreground channel of M in Eq. (1) up-sampled to the input image size, h × w, with all the dimensions of size 1 removed, and P (X = i|M ) denotes the posterior probability given the corresponding instance mask M .   <ref type="bibr" target="#b28">(Redmon and Farhadi 2018)</ref> and Hourglass <ref type="bibr" target="#b27">(Newell, Yang, and Deng 2016)</ref>, respectively.</p><p>In the following, we only take the derivation for the left boundary as an example, and it can be easily extended to the other boundaries.</p><p>Following the Bayes' theorem, we have</p><formula xml:id="formula_2">P (X = i|M ) = P (X = i)P (M |X = i) w t=1 P (X = t)P (M |X = t) ,<label>(3)</label></formula><p>where P (X = i) and P (M |X = i) are the corresponding prior and likelihood probabilities. Assuming that the boundary is only related to the maximum of each row in M , and it only affects its neighboring pixels, the likelihood probability can be defined as</p><formula xml:id="formula_3">P (M |X = i) = P (m x |X = i) (4) = P (m x i−s,...,i+s |X = i) ,<label>(5)</label></formula><p>where m x i = max 1≤j≤h M ij , and s is a hyper-parameter, describing the influence scope of the boundary on its neighboring pixels. Ideally, a pixel on the boundary only affects its two nearest neighboring pixels, i.e., the one outside the bounding box has probability 0 and the other inside has probability 1. In this case, s = 1. However, the instance mask is not so sharp, making it difficult to provide a proper formulation for P (m x i−s,...,i+s |X = i). Therefore, we approximate it with a one-dimensional convolution with kernel size 2s + 1, followed by a sigmoid function for normalization, and the parameters are learned by back-propagation.</p><p>For P (X = i), we simply adopt a discrete Gaussian distribution</p><formula xml:id="formula_4">P (X = i) = αe −(i−µ) 2 /2σ 2 x ,<label>(6)</label></formula><p>where α is the normalization coefficient. Obviously, the distribution of the boundary location is related to the instance scale, thus we set</p><formula xml:id="formula_5">µ = x r , σ x = γw b ,<label>(7)</label></formula><p>where w b denotes the width of the bounding box, x r denotes the horizontal coordinate of the regressed left boundary, and γ specifies the weighting of the regressed boundary. It can be seen that a smaller γ indicates a higher weighting of the regressed boundary, and vice versa. During training, the ground-truth boundary is transformed to one-hot format along the width or height directions of the image, and cross entropy loss is used to train the above coordinate classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>Our model is trained with the following multi-task loss:</p><formula xml:id="formula_6">L = L cls + λ r L reg + λ m L mask ,<label>(8)</label></formula><p>where L cls and L reg are the commonly used classification and regression losses in detection tasks <ref type="bibr" target="#b29">(Ren et al. 2015;</ref><ref type="bibr" target="#b21">Lin et al. 2017b)</ref>, and L mask is the pixel-wise cross entropy loss described in Sec. 3.2. Only the representations of positive anchors (matched with ground-truth boxes) are fed into the correlation module to generate instance masks, which are then cropped with the expanded ground-truth boxes and used to calculate L mask . In other words, pixels outside the expanded boxes are ignored in L mask . L ref ine is the cross entropy loss defined in Sec. 3.3. λ r and λ m are hyperparameters for loss re-weighting. The parameters in MBRM is trained individually with L ref ine after all others parameters are trained to convergence with L. The reason is that MBRM only requires relatively good regression boxes and instance masks. During inference, the object categories and bounding boxes are first obtained by the detector in the object stream, along with the representation of each instance. Meanwhile, the pixel representations are generated in the pixel stream. Next, only proposals after NMS are processed in the correlation module to generate instance masks, which are then cropped with the expanded boxes obtained by the detector. In order to get the exact coordinates, such instance masks are up-sampled to the input image size and then fed into MBRM. The masks are binarized with threshold 0.4 at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, experimental analyses and comparisons are performed to demonstrate the reciprocal relations between the object detection and instance segmentation tasks. We report the results on COCO dataset <ref type="bibr" target="#b19">(Lin et al. 2014)</ref> and use the commonly-used metrics for both object detection (AP bb ) and instance segmentation (AP m ). We train on train2017, and evaluate on val2017 and test-dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We implement RDSNet based on mmdetection <ref type="bibr" target="#b4">(Chen et al. 2019b)</ref>. We use ResNet-101  with FPN <ref type="bibr" target="#b20">(Lin et al. 2017a</ref>) as our backbone. For the object stream, we choose a strong one-stage detector, RetinaNet <ref type="bibr" target="#b21">(Lin et al. 2017b</ref>) as our detector unless otherwise indicated, as well as our baseline, to validate the effectiveness of our method.</p><p>For the pixel stream, we adopt the architecture of semantic segmentation branch in PanopticFPN <ref type="bibr" target="#b16">(Kirillov et al. 2019)</ref> to merge the FPN pyramid into a single output, i.e.pixel representations, except that the number of channels is modified to 256 for richer representations.</p><p>Dimensions of the instance and pixel representations are 32. We use different expanding ratios of bounding boxes for cropping masks during training and inference. During training, we use ground-truth bounding boxes and expand both the heights and the widths of them by 1.5 times with center point retaining. During inference, the expanding ratio is set to 1.2. All λs are set to 1.</p><p>We train our models on 4 GPUs (2 images per GPU) and adopt 1× training strategy <ref type="bibr" target="#b4">(Chen et al. 2019b</ref>) along with all other settings same as RetinaNet, and then parameters in MBRM are trained individually for another 1k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Assisted Instance Segmentation</head><p>In this section, we first validate the effectiveness of our correlation and cropping module. We compare RDSNet with YOLACT <ref type="bibr" target="#b1">(Bolya et al. 2019)</ref>, another one-stage approach for instance segmentation. We adopt the backbone and detection head of YOLACT and apply the correlation module with the expanded cropping strategy (denoted as RDSNet s ), compared with the linear combining method with simply cropping in YOLACT. As shown in Tab. 3, 31.0 mAP (+1.1 mAP) is achieved for instance segmentation with correlation method compared to 29.9 mAP of YOLACT. What's more, the fast speed is maintained. Compared to only foreground coefficients with additional restriction in YOLACT, modeling both foreground and background representations for each object possibly contributes to easier convergence and thus better results.</p><p>Additional ablation experiments in Tab. 3 shows the effectiveness of the cropping module. If we simply crop the masks with expanded regressed bounding boxes during inference, performance degradation is observed (row 2 v.s. row 3), which indicates that the model is unable to handle the large diversity of background pixels unless expanding strategy is applied during training (row 3 v.s. row 5). Once OHEM for negative pixels is adopted, 1.9 mAP improvement over YOLACT is observed (row 7).   Then, we compare RDSNet with the state-of-the-art methods for instance segmentation. As shown in Tab. 1, our method achieves better balance between speed and accuracy among one-stage methods. With small input size (550 or 600), we achieve 32.1 mAP with a real-time speed (32 fps). With 800 input size, RDSNet outperforms most one-stage methods except TensorMask <ref type="bibr" target="#b5">(Chen et al. 2019c)</ref>, which is however nearly 3 times slower. Compared to the two-stage methods, it is worth noting that RDSNet overcomes the inherent drawbacks of Mask R-CNN <ref type="bibr" target="#b13">(He et al. 2017</ref>) to a large extent, such as the low resolution of masks, strong dependencies of masks on bounding boxes, etc., as demonstrated in <ref type="figure" target="#fig_0">Fig. 1 and Fig. 4</ref> . Besides, we argue that the speed of RDSNet is restricted to the speed of our detector <ref type="bibr" target="#b21">(Lin et al. 2017b</ref>) (10.9 fps). As shown in Tab. 2, only slight latency is brought to the original detector in RDSNet. As a consequence, further speeding up is achievable by switching to other faster detectors, which is beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mask Assisted Object Detection</head><p>For detection task, the key novelty of RDSNet is to refine bounding boxes with instance mask in a one-stage process. As shown in Tab. 2, we find multi-task training with an extra mask generator does bring a certain amount of improvement on our baseline (RetinaNet <ref type="bibr" target="#b21">(Lin et al. 2017b)</ref>), but further  <ref type="table">Table 4</ref>: Demonstration of the effectiveness of MBRM on COCO val2017. Simply regarding the minimum enclosing rectangle of the instance mask as detection results (row 2) does not work well on small objects. However, our MBRM (row 3) works better by introducing the regression bounding box as prior. consistent improvement is achieved by MBRM with negligible computational cost. Note that the gain all comes from the more accurate localization of boundary, instead of all other aspects. For fair comparison, only single model results without test-time augmentations are shown in the table.</p><p>We further analyze the sensitivity of hyper-parameter in MBRM on COCO val2017, i.e., s and γ, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. When γ = 0, the refinement module is not activated. We observe that different γ results in variant improvement. γ around 0.05 works stably so γ = 0.05 is used in all experiments. s indicates how faraway a pixel from the boundary is still affected. Larger s leads to more accurate results within a certain range, while further increasing s does not bring much improvement. We use s = 4 for all experiments.</p><p>Then, we compare our MBRM with the direct method, as shown in Tab. 4. We find direct method works badly on small objects, which indicates that the prior of regression bounding box is necessary. Our MBRM works better especially for large objects, while slight decline on small objects is negligible, which would be fixed if more precise masks for small objects are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a unified architecture for object detection and instance segmentation, and experimental analyses demonstrate the reciprocal relations between these two tasks. The drawbacks of previous works like the low resolution of instance masks, heavy dependencies of masks on boxes and localization errors of bounding boxes are largely overcome in this work. We argue that object detection and instance segmentation tasks should not be studied separately and hope future work focus on the co-relation between different image perception tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Localization errors in object detection. (a)(b) Boxes do not fully enclose objects. (c)(d) Boxes do not enclose objects tightly. Most of these errors can be easily corrected if we fully leverage the reciprocal relations between the object detection and instance segmentation tasks. Results are obtained by Mask R-CNN (He et al. 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration for the representations of the objects 2 and pixels, both of which are embedded into d-dimension feature space in the object and pixel streams respectively. Pixel representations are close to corresponding instance representation in feature space and different objects have distinct representations. Dimension reduction (from d to 3) and L2 normalization are performed to representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons of some results on COCO val2017. The top, middle and bottom rows are obtained by Mask R-CNN, RDSNet w/o expanded cropping or MBRM, and full version of RDSNet. RDSNet gives sharper masks compared to Mask R-CNN. The circled regions highlight the advantage of MBRM in alleviating localization errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Hyper-parameter sensitivity of MBRM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: Instance segmentation results on COCO test-dev. P means Titan XP or 1080Ti, and V means Tesla V100. 'aug' means data augmentation during training: • is trained with only horizontal flipping augmentation and√ is trained further with scale augmentation. † means this entry is obtained by models provided by mmdetection<ref type="bibr" target="#b4">(Chen et al. 2019b</ref>).</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Scale epoch aug</cell><cell>FPS</cell><cell cols="2">AP m AP m 50</cell><cell>AP m 75</cell><cell>AP m S</cell><cell>AP m M</cell><cell>AP m L</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mask R-CNN (He et al. 2017)  †</cell><cell>800</cell><cell>12</cell><cell>•</cell><cell>9.5/V</cell><cell>36.2</cell><cell>58.3</cell><cell>38.6</cell><cell>16.7</cell><cell>38.8</cell><cell>51.5</cell></row><row><cell cols="2">MS R-CNN (Huang et al. 2019)  †</cell><cell>800</cell><cell>12</cell><cell>•</cell><cell>9.1/V</cell><cell>37.4</cell><cell>57.9</cell><cell>40.4</cell><cell>17.3</cell><cell>39.5</cell><cell>53.0</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FCIS (Li et al. 2017) YOLACT (Bolya et al. 2019) RDSNet s (ours) TensorMask (Chen et al. 2019c)</cell><cell>600 550 550 800</cell><cell>12 48 48 72</cell><cell>• √ √ √</cell><cell cols="2">6.6/P 33.0/P 29.8 29.2 32.0/P 32.1 2.6/V 37.3</cell><cell>49.5 48.5 53.0 59.5</cell><cell>-31.2 33.4 39.5</cell><cell>7.1 9.9 11.0 17.5</cell><cell>31.3 31.3 33.8 39.3</cell><cell>50.0 47.7 51.0 51.6</cell></row><row><cell cols="2">RDSNet (ours) RDSNet (ours)</cell><cell>800 800</cell><cell>12 24</cell><cell>• √</cell><cell>8.8/V 8.8/V</cell><cell>34.6 36.4</cell><cell>55.8 57.9</cell><cell>36.7 39.0</cell><cell>14.9 16.4</cell><cell>37.4 39.5</cell><cell>50.3 51.6</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">Scale Backbone</cell><cell>FPS</cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell>AP bb S</cell><cell>AP bb M</cell><cell>AP bb L</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mask R-CNN (He et al. 2017) †</cell><cell>800</cell><cell></cell><cell>R-101</cell><cell>9.5/V</cell><cell>39.7</cell><cell>61.6</cell><cell>43.2</cell><cell>23.0</cell><cell>43.2</cell><cell>49.7</cell></row><row><cell cols="2">Cascade Mask R-CNN (2018) †</cell><cell>800</cell><cell></cell><cell>R-101</cell><cell>6.8/V</cell><cell>43.1</cell><cell>61.5</cell><cell>46.9</cell><cell>24.0</cell><cell>45.9</cell><cell>55.4</cell></row><row><cell cols="2">HTC (Chen et al. 2019a) †</cell><cell>800</cell><cell></cell><cell>R-101</cell><cell>4.1/V</cell><cell>45.1</cell><cell>64.3</cell><cell>49.0</cell><cell>25.2</cell><cell>48.0</cell><cell>58.2</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">YOLOv3(Redmon and Farhadi 2018)</cell><cell>608</cell><cell></cell><cell>D-53</cell><cell cols="2">19.8/P 33.0</cell><cell>57.9</cell><cell>34.3</cell><cell>18.3</cell><cell>35.4</cell><cell>41.9</cell></row><row><cell cols="2">RefineDet (Zhang et al. 2018)</cell><cell>512</cell><cell></cell><cell>R-101</cell><cell>9.1/P</cell><cell>36.4</cell><cell>57.5</cell><cell>39.5</cell><cell>16.6</cell><cell>39.9</cell><cell>51.4</cell></row><row><cell cols="2">CornerNet (Law and Deng 2018)</cell><cell>512</cell><cell></cell><cell>H-104</cell><cell>4.4/P</cell><cell>40.5</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell>56.7</cell></row><row><cell></cell><cell cols="2">RetinaNet (Lin et al. 2017b)</cell><cell></cell><cell></cell><cell cols="2">16.8/V 36.0</cell><cell>55.2</cell><cell>38.7</cell><cell>17.4</cell><cell>39.6</cell><cell>49.7</cell></row><row><cell>RDSNet</cell><cell>+ mask</cell><cell>600</cell><cell></cell><cell>R-101</cell><cell cols="2">15.5/V 36.1</cell><cell>56.7</cell><cell>38.5</cell><cell>17.3</cell><cell>38.9</cell><cell>51.3</cell></row><row><cell></cell><cell>+ MBRM</cell><cell></cell><cell></cell><cell></cell><cell cols="2">14.5/V 37.3</cell><cell>56.7</cell><cell>39.3</cell><cell>17.0</cell><cell>40.0</cell><cell>54.0</cell></row><row><cell></cell><cell cols="2">RetinaNet (Lin et al. 2017b)</cell><cell></cell><cell></cell><cell cols="2">10.9/V 38.1</cell><cell>58.5</cell><cell>40.8</cell><cell>21.2</cell><cell>41.5</cell><cell>48.2</cell></row><row><cell cols="2">RDSNet + mask</cell><cell>800</cell><cell></cell><cell>R-101</cell><cell>8.8/V</cell><cell>39.4</cell><cell>60.1</cell><cell>42.5</cell><cell>22.1</cell><cell>42.6</cell><cell>49.9</cell></row><row><cell></cell><cell>+ MBRM</cell><cell></cell><cell></cell><cell></cell><cell>8.5/V</cell><cell>40.3</cell><cell>60.1</cell><cell>43.0</cell><cell>22.1</cell><cell>43.5</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Object detection results on COCO test-dev. We denote the backbone by network-depth, where R, D and H refer to ResNet, DarkNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Demonstration of the effectiveness of the cropping module on COCO val2017.</figDesc><table><row><cell>LC: Linear Combina-</cell></row><row><cell>tion, Corr: Correlation, TE: Expand during training, IE: Ex-</cell></row><row><cell>pand during inference. Our finally adopted choice (last row)</cell></row><row><cell>yields the highest mAP. It should be noted that by using Corr</cell></row><row><cell>instead of LC, RDSNet already outperforms YOLACT by</cell></row><row><cell>1.1 in mAP.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>−1.7 11.8 −5.3 37.7 −2.0 55.1 +1.8 MBRM 37.2 +1.3 16.9 −0.2 40.8 +1.1 56.5 +3.2</figDesc><table><row><cell>Method</cell><cell>AP bb</cell><cell>AP bb S</cell><cell>AP bb M</cell><cell>AP bb L</cell></row><row><cell cols="2">baseline 35.9</cell><cell>17.1</cell><cell>39.7</cell><cell>53.3</cell></row><row><cell>direct</cell><cell>34.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Unless otherwise specified, we use boundary to refer to the box boundary, not object boundary.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Realtime instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<title level="m">Semantic instance segmentation via deep metric learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Singleshot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
