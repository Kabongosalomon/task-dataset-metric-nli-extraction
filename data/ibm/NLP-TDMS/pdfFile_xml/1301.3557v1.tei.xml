<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
							<email>zeiler@cs.nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Courant Institute</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
							<email>fergus@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Courant Institute</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.</p><p>1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models are prone to over-fitting due to their high capacity. A range of regularization techniques are used to prevent this, such as weight decay, weight tying and the augmentation of the training set with transformed copies <ref type="bibr" target="#b8">[9]</ref>. These allow the training of larger capacity models than would otherwise be possible, which yield superior test performance compared to smaller unregularized models.</p><p>Dropout, recently proposed by Hinton et al. <ref type="bibr" target="#b1">[2]</ref>, is another regularization approach that stochastically sets half the activations within a layer to zero for each training sample during training. It has been shown to deliver significant gains in performance across a wide range of problems, although the reasons for its efficacy are not yet fully understood.</p><p>A drawback to dropout is that it does not seem to have the same benefits for convolutional layers, which are common in many networks designed for vision tasks. In this paper, we propose a novel type of regularization for convolutional layers that enables the training of larger models without over-fitting, and produces superior performance on recognition tasks.</p><p>The key idea is to make the pooling that occurs in each convolutional layer a stochastic process. Conventional forms of pooling such as average and max are deterministic, the latter selecting the largest activation in each pooling region. In our stochastic pooling, the selected activation is drawn from a multinomial distribution formed by the activations within the pooling region.</p><p>An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images <ref type="bibr" target="#b12">[13]</ref>, which delivers excellent MNIST performance. Other types of data augmentation, such as flipping and cropping differ in that they are global image transformations. Furthermore, using stochastic pooling in a multi-layer model gives an exponential number of deformations since the selections in higher layers are independent of those below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Convolutional Networks</head><p>Our stochastic pooling scheme is designed for use in a standard convolutional neural network architecture. We first review this model, along with conventional pooling schemes, before introducing our novel stochastic pooling approach.</p><p>A classical convolutional network is composed of alternating layers of convolution and pooling (i.e. subsampling). The aim of the first convolutional layer is to extract patterns found within local regions of the input images that are common throughout the dataset. This is done by convolving a template or filter over the input image pixels, computing the inner product of the template at every location in the image and outputting this as a feature map c, for each filter in the layer. This output is a measure of how well the template matches each portion of the image. A non-linear function f () is then applied element-wise to each feature map c: a = f (c). The resulting activations a are then passed to the pooling layer. This aggregates the information within a set of small local regions, R, producing a pooled feature map s (of smaller size) as output. Denoting the aggregation function as pool(), for each feature map c we have:</p><formula xml:id="formula_0">s j = pool(f (c i )) ∀i ∈ R j<label>(1)</label></formula><p>where R j is pooling region j in feature map c and i is the index of each element within it.</p><p>The motivation behind pooling is that the activations in the pooled map s are less sensitive to the precise locations of structures within the image than the original feature map c. In a multi-layer model, the convolutional layers, which take the pooled maps as input, can thus extract features that are increasingly invariant to local transformations of the input image. This is important for classification tasks, since these transformations obfuscate the object identity.</p><p>A range of functions can be used for f (), with tanh() and logistic functions being popular choices.</p><p>In this is paper we use a linear rectification function f (c) = max(0, c) as the non-linearity. In general, this has been shown <ref type="bibr" target="#b9">[10]</ref> to have significant benefits over tanh() or logistic functions. However, it is especially suited to our pooling mechanism since: (i) our formulation involves the non-negativity of elements in the pooling regions and (ii) the clipping of negative responses introduces zeros into the pooling regions, ensuring that the stochastic sampling is selecting from a few specific locations (those with strong responses), rather than all possible locations in the region.</p><p>There are two conventional choices for pool(): average and max. The former takes the arithmetic mean of the elements in each pooling region:</p><formula xml:id="formula_1">s j = 1 |R j | i∈Rj a i<label>(2)</label></formula><p>while the max operation selects the largest element:</p><formula xml:id="formula_2">s j = max i∈Rj a i<label>(3)</label></formula><p>Both types of pooling have drawbacks when training deep convolutional networks. In average pooling, all elements in a pooling region are considered, even if many have low magnitude. When combined with linear rectification non-linearities, this has the effect of down-weighting strong activations since many zero elements are included in the average. Even worse, with tanh() non-linearities, strong positive and negative activations can cancel each other out, leading to small pooled responses.</p><p>While max pooling does not suffer from these drawbacks, we find it easily overfits the training set in practice, making it hard to generalize well to test examples. Our proposed pooling scheme has the advantages of max pooling but its stochastic nature helps prevent over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stochastic Pooling</head><p>In stochastic pooling, we select the pooled map response by sampling from a multinomial distribution formed from the activations of each pooling region. More precisely, we first compute the probabilities p for each region j by normalizing the activations within the region:</p><formula xml:id="formula_3">p i = a i k∈Rj a k<label>(4)</label></formula><p>We then sample from the multinomial distribution based on p to pick a location l within the region. The pooled activation is then simply a l :</p><formula xml:id="formula_4">s j = a l where l ∼ P (p 1 , . . . , p |Rj | )<label>(5)</label></formula><p>The procedure is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The samples for each pooling region in each layer for each training example are drawn independently to one another. When back-propagating through the network this same selected location l is used to direct the gradient back through the pooling region, analogous to back-propagation with max pooling.</p><p>Max pooling only captures the strongest activation of the filter template with the input for each region. However, there may be additional activations in the same pooling region that should be taken into account when passing information up the network and stochastic pooling ensures that these non-maximal activations will also be utilized. </p><formula xml:id="formula_5">★! a)!Image! b)!Filter! c)!Rec0fied!Linear! e)!Probabili0es,!p i 0! 0! 0! 0! 0! 0! 0! 1.6! 2.4! 0! 0! 0! 0! 0! 0! 0! 0.4! 0.6! d)!Ac0va0ons,!a i 1.6! f)!Sampled!! !!!!Ac0va0on,!s! Sample!a!loca0on! from!P():!e.g.!!l = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic Weighting at Test Time</head><p>Using stochastic pooling at test time introduces noise into the network's predictions which we found to degrade performance (see Section 4.7). Instead, we use a probabilistic form of averaging. In this, the activations in each region are weighted by the probability p i (see Eqn. 4) and summed:</p><formula xml:id="formula_6">s j = i∈Rj p i a i<label>(6)</label></formula><p>This differs from standard average pooling because each element has a potentially different weighting and the denominator is the sum of activations i∈Rj a i , rather than the pooling region size |R j |. In practice, using conventional average (or sum) pooling results in a huge performance drop (see Section 4.7).</p><p>Our probabilistic weighting can be viewed as a form of model averaging in which each setting of the locations l in the pooling regions defines a new model. At training time, sampling to get new locations produces a new model since the connection structure throughout the network is modified. At test time, using the probabilities instead of sampling, we effectively get an estimate of averaging over all of these possible models without having to instantiate them. Given a network architecture with d different pooling regions, each of size n, the number of possible models is n d where d can be in the 10 4 -10 6 range and n is typically 4,9, or 16 for example (corresponding to 2 × 2, 3 × 3 or 4 × 4 pooling regions). This is a significantly larger number than the model averaging that occurs in dropout <ref type="bibr" target="#b1">[2]</ref>, where n = 2 always (since an activation is either present or not). In Section 4.7 we confirm that using this probability weighting achieves similar performance compared to using a large number of model instantiations, while requiring only one pass through the network.</p><p>Using the probabilities for sampling at training time and for weighting the activations at test time leads to state-of-the-art performance on many common benchmarks, as we now demonstrate. </p><formula xml:id="formula_7">CIFAR&amp;100) CIFAR&amp;10) SVHN) MNIST) mean) Local)CN) mean) mean) a)) d)) e)) g)) h)) f)) c)) b)) !" !" !"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We compare our method to average and max pooling on a variety of image classification tasks. In all experiments we use mini-batch gradient descent with momentum to optimize the cross entropy between our network's prediction of the class and the ground truth labels. For a given parameter x at time t the weight updates added to the parameters, ∆x t are ∆x t = 0.9∆x t−1 − g t where g t is the gradient of the cost function with respect to that parameter at time t averaged over the batch and is a learning rate set by hand.</p><p>All experiments were conducted using an extremely efficient C++ GPU convolution library <ref type="bibr" target="#b5">[6]</ref> wrapped in MATLAB using GPUmat <ref type="bibr">[14]</ref>, which allowed for rapid development and experimentation. We begin with the same network layout as in Hinton et al. 's dropout work <ref type="bibr" target="#b1">[2]</ref>, which has 3 convolutional layers with 5x5 filters and 64 feature maps per layer with rectified linear units as their outputs. We use this same model and train for 280 epochs in all experiments aside from one additional model in Section 4.5 that has 128 feature maps in layer 3 and is trained for 500 epochs. Unless otherwise specified we use 3 × 3 pooling with stride 2 (i.e. neighboring pooling regions overlap by 1 element along the borders) for each of the 3 pooling layers. Additionally, after each pooling layer there is a response normalization layer (as in <ref type="bibr" target="#b1">[2]</ref>), which normalizes the pooling outputs at each location over a subset of neighboring feature maps. This typically helps training by suppressing extremely large outputs allowed by the rectified linear units as well as helps neighboring features communicate. Finally, we use a single fully-connected layer with soft-max outputs to produce the network's class predictions. We applied this model to four different datasets: MNIST, CIFAR-10, CIFAR-100 and Street View House Numbers (SVHN), see <ref type="figure" target="#fig_1">Fig. 2</ref> for examples images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR-10</head><p>We begin our experiments with the CIFAR-10 dataset where convolutional networks and methods such as dropout are known to work well <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. This dataset is composed of 10 classes of natural images with 50,000 training examples in total, 5,000 per class. Each image is an RGB image of size 32x32 taken from the tiny images dataset and labeled by hand. For this dataset we scale to [0,1] and follow Hinton et al. 's <ref type="bibr" target="#b1">[2]</ref> approach of subtracting the per-pixel mean computed over the dataset from each image as shown in <ref type="figure" target="#fig_1">Fig. 2(f)</ref>. Cross-validating with a set of 5,000 CIFAR-10 training images, we found a good value for the learning rate to be 10 −2 for convolutional layers and 1 for the final softmax output layer. These rates were annealed linearly throughout training to 1/100th of their original values. Additionally, we found a small weight decay of 0.001 to be optimal and was applied to all layers. These hyperparameter settings found through cross-validation were used for all other datasets in our experiments.</p><p>Using the same network architecture described above, we trained three models using average, max and stochastic pooling respectively and compare their performance. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the progression of train and test errors over 280 training epochs. Stochastic pooling avoids over-fitting, unlike average and max pooling, and produces less test errors. <ref type="table">Table 1</ref> compares the test performance of the three pooling approaches to the current state-of-the-art result on CIFAR-10 which uses no data augmentation but adds dropout on an additional locally connected layer <ref type="bibr" target="#b1">[2]</ref>. Stochastic pooling surpasses this result by 0.47% using the same architecture but without requiring the locally connected layer.  <ref type="table">Table 1</ref>: CIFAR-10 Classification performance for various pooling methods in our model compared to the state-of-the-art performance <ref type="bibr" target="#b1">[2]</ref> with and without dropout.</p><p>To determine the effect of the pooling region size on the behavior of the system with stochastic pooling, we compare the CIFAR-10 train and test set performance for 5x5, 4x4, 3x3, and 2x2 pooling sizes throughout the network in <ref type="figure" target="#fig_3">Fig. 4</ref>. The optimal size appears to be 3x3, with smaller regions overfitting and larger regions possibly being too noisy during training. At all sizes the stochastic pooling outperforms both max and average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MNIST</head><p>The MNIST digit classification task is composed of 28x28 images of the 10 handwritten digits <ref type="bibr" target="#b7">[8]</ref>.</p><p>There are 60,000 training images with 10,000 test images in this benchmark. The images are scaled to [0,1] and we do not perform any other pre-processing.</p><p>During training, the error using both stochastic pooling and max pooling dropped quickly, but the latter completely overfit the training data. Weight decay prevented average pooling from over-fitting, but had an inferior performance to the other two methods.  tic pooling outperforms all other methods that do not use data augmentation methods such as jittering or elastic distortions <ref type="bibr" target="#b6">[7]</ref>. The current state-of-the-art single model approach by Ciresan et al. <ref type="bibr" target="#b0">[1]</ref> uses elastic distortions to augment the original training set. As stochastic pooling is a different type of regularization, it could be combined with data augmentation to further improve performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CIFAR-100</head><p>The CIFAR-100 dataset is another subset of the tiny images dataset, but with 100 classes <ref type="bibr" target="#b4">[5]</ref>. There are 50,000 training examples in total (500 per class) and 10,000 test examples. As with the CIFAR-10, we scale to [0,1] and subtract the per-pixel mean from each image as shown in <ref type="figure" target="#fig_1">Fig. 2(h</ref>  <ref type="table">Table 3</ref>: CIFAR-100 Classification performance for various pooling methods compared to the stateof-the-art method based on receptive field learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Street View House Numbers</head><p>The Street View House Numbers (SVHN) dataset is composed of 604,388 images (using both the difficult training set and simpler extra set) and 26,032 test images <ref type="bibr" target="#b10">[11]</ref>. The goal of this task is to classify the digit in the center of each cropped 32x32 color image. This is a difficult real world problem since multiple digits may be visible within each image. The practical application of this is to classify house numbers throughout Google's street view database of images.</p><p>We found that subtracting the per-pixel mean from each image did not really modify the statistics of the images (see <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) and left large variations of brightness and color that could make clas-sification more difficult. Instead, we utilized local contrast normalization (as in <ref type="bibr" target="#b11">[12]</ref>) on each of the three RGB channels to pre-process the images <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. This normalized the brightness and color variations and helped training proceed quickly on this relatively large dataset.</p><p>Despite having significant amounts of training data, a large convolutional network can still overfit. For this dataset, we train an additional model for 500 epochs with 64, 64 and 128 feature maps in layers 1, 2 and 3 respectively. Our stochastic pooling helps to prevent overfitting even in this large model (denoted 64-64-128 in <ref type="table">Table 4</ref>), despite training for a long time. The existing state-of-theart on this dataset is the multi-stage convolutional network of Sermanet et al. <ref type="bibr" target="#b11">[12]</ref>, but stochastic pooling beats this by 2.10% (relative gain of 43%).  <ref type="table">Table 4</ref>: SVHN Classification performance for various pooling methods in our model with 64 or 128 layer 3 feature maps compared to state-of-the-art results with and without data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Reduced Training Set Size</head><p>To further illustrate the ability of stochastic pooling to prevent over-fitting, we reduced the training set size on MINST and CIFAR-10 datasets. <ref type="figure" target="#fig_5">Fig. 5</ref> shows test performance when training on a random selection of only 1000, 2000, 3000, 5000, 10000, half, or the full training set. In most cases, stochastic pooling overfits less than the other pooling approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Importance of Model Averaging</head><p>To analyze the importance of stochastic sampling at training time and probability weighting at test time, we use different methods of pooling when training and testing on CIFAR-10 (see <ref type="table" target="#tab_7">Table 5</ref>). Choosing the locations stochastically at test time degrades performance slightly as could be expected, however it still outperforms models where max or average pooling are used at test time. To confirm that probability weighting is a valid approximation to averaging many models, we draw N samples of the pooling locations throughout the network and average the output probabilities from those N models (denoted Stochastic-N in <ref type="table" target="#tab_7">Table 5</ref>). As N increases, the results approach the probability weighting method, but have the obvious downside of an N -fold increase in computations.</p><p>Using a model trained with max or average pooling and using stochastic pooling at test time performs poorly. This suggests that training with stochastic pooling, which incorporates non-maximal elements and sampling noise, makes the model more robust at test time. Furthermore, if these nonmaximal elements are not utilized correctly or the scale produced by the pooling function is not correct, such as if average pooling is used at test time, a drastic performance hit is seen. When using probability weighting during training, the network easily over-fits and performs suboptimally at test time using any of the pooling methods. However, the benefits of probability weighting at test time are seen when the model has specifically been trained to utilize it through either probability weighting or stochastic pooling at training time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Visualizations</head><p>Some insight into the mechanism of stochastic pooling can be gained by using a deconvolutional network of Zeiler et al. <ref type="bibr" target="#b13">[15]</ref> to provide a novel visualization of our trained convolutional network.</p><p>The deconvolutional network has the same components (pooling, filtering) as a convolutional network but are inverted to act as a top-down decoder that maps the top-layer feature maps back to the input pixels. The unpooling operation uses the stochastically chosen locations selected during the forward pass. The deconvolution network filters (now applied to the feature maps, rather than the input) are the transpose of the feed-forward filters, as in an auto-encoder with tied encoder/decoder weights. We repeat this top-down process until the input pixel level is reached, producing the visualizations in <ref type="figure" target="#fig_6">Fig. 6</ref>. With max pooling, many of the input image edges are present, but average pooling produces a reconstruction with no discernible structure. <ref type="figure" target="#fig_6">Fig. 6(a)</ref> shows 16 examples of pixel-space reconstructions for different location samples throughout the network. The reconstructions are similar to the max pooling case, but as the pooling locations change they result in small local deformations of the visualized image.</p><p>Despite the stochastic nature of the model, the multinomial distributions effectively capture the regularities of the data. To demonstrate this, we compare the outputs produced by a deconvolutional network when sampling using the feedforward (FF) proabilities versus sampling from uniform (UN) distributions. In contrast to <ref type="figure" target="#fig_6">Fig. 6</ref>(a) which uses only feedforward proabilities, <ref type="figure" target="#fig_6">Fig. 6</ref>(b-h) replace one or more of the pooling layers' distributions with uniform distributions. The feed forward probabilities encode significant structural information, especially in the lower layers of the model. Additional visualizations and videos of the sampling process are provided as supplementary material at www.matthewzeiler.com/pubs/iclr2013/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We propose a simple and effective stochastic pooling strategy that can be combined with any other forms of regularization such as weight decay, dropout, data augmentation, etc. to prevent overfitting when training deep convolutional networks. The method is also intuitive, selecting from information the network is already providing, as opposed to methods such as dropout which throw information away. We show state-of-the-art performance on numerous datasets, when comparing to other approaches that do not employ data augmentation. Furthermore, our method has negligible computational overhead and no hyper-parameters to tune, thus can be swapped into to any existing convolutional network architecture. Each image in a 4x4 block is one instantiation of the pooling locations using stochastic pooling. For sampling the locations, each layer (indicated in parenthesis) can either use: (i) the multinomial distribution over a pooling region derived from the feed-forward (FF) activations as in Eqn. 4, or (ii) a uniform (UN) distribution. We can see that the feed-forward probabilities encode much of the structure in the image, as almost all of it is lost when uniform sampling is used, especially in the lower layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Toy example illustrating stochastic pooling. a) Input image. b) Convolutional filter. c) Rectified linear function. d) Resulting activations within a given pooling region. e) Probabilities based on the activations. f) Sampled activation. Note that the selected element for the pooling region may not be the largest element. Stochastic pooling can thus represent multi-modal distributions of activations within a region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A selection of images from each of the datasets we evaluated. The top row shows the raw images while the bottom row are the preprocessed versions of the images we used for training. The CIFAR datasets (f,h) show slight changes by subtracting the per pixel mean, whereas SVHN (b) is almost indistinguishable from the original images. This prompted the use of local contrast normalization (c) to normalize the extreme brightness variations and color changes for SVHN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR-10 train and test error rates throughout training for average, max, and stochastic pooling. Max and average pooling test errors plateau as those methods overfit. With stochastic pooling, training error remains higher while test errors continue to decrease. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>CIFAR-10 train and test error rates for various pooling region sizes with each method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Test error when training with reduced dataset sizes on MNIST (left) and CIFAR-10 (right). Stochastic pooling generally overfits the least.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Top down visualizations from the third layer feature map activations for the horse image (far left). Max and average pooling visualizations are also shown on the left. (a)-(h):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.08</cell><cell></cell><cell></cell></row><row><cell></cell><cell>5x5</cell><cell>0</cell><cell></cell><cell></cell><cell>6.4</cell><cell></cell><cell></cell><cell>19.25 19.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15.86</cell><cell></cell></row><row><cell>Pooling Region Size</cell><cell>3x3 4x4</cell><cell>0 0</cell><cell>1.8</cell><cell>3.4</cell><cell>4.38 4.88</cell><cell></cell><cell>15.13 15.71</cell><cell>19.53 18.59 19.52 18.83</cell><cell>Avg Train Avg Test Max Train Max Test Stochas&gt;c Train</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stochas&gt;c Test</cell></row><row><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2x2</cell><cell>0</cell><cell></cell><cell>3.18</cell><cell></cell><cell></cell><cell></cell><cell>21.11 20.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16.55</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>% Error</cell><cell></cell><cell></cell></row></table><note>compares the three pooling ap- proaches to state-of-the-art methods on MNIST, which also utilize convolutional networks. Stochas-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>MNIST Classification performance for various pooling methods. Rows 1 &amp; 2 show the current state-of-the-art approaches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). Due to the limited number of training examples per class, typical pooling methods used in convolutional networks do not perform well, as shown inTable 3. Stochastic pooling outperforms these methods by preventing over-fitting and surpasses what we believe to be the state-of-the-art method by 2.66%.</figDesc><table><row><cell></cell><cell cols="2">Train Error % Test Error %</cell></row><row><cell>Receptive Field Learning [4]</cell><cell>-</cell><cell>45.17</cell></row><row><cell>Avg Pooling</cell><cell>11.20</cell><cell>47.77</cell></row><row><cell>Max Pooling</cell><cell>0.17</cell><cell>50.90</cell></row><row><cell>Stochastic Pooling</cell><cell>21.22</cell><cell>42.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>CIFAR-10 Classification performance for various train and test combinations of pooling methods. The best performance is obtained by using stochastic pooling when training (to prevent over-fitting), while using the probability weighting at test time.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Weight decay prevented training errors from reaching 0 with average and stochastic pooling methods and required the high number of epochs for training. All methods performed slightly better with weight decay.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning multiple layers of featurs from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>TR- 2009</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/cuda-convnet/" />
		<title level="m">cuda-convnet</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The MNIST database</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
