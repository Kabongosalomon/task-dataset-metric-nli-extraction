<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WSOD with PSNet and Box Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WSOD with PSNet and Box Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>WSOD</term>
					<term>Proposal scoring</term>
					<term>Box regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection(WSOD) task uses only image-level annotations to train object detection task. WSOD does not require time-consuming instance-level annotations, so the study of this task has attracted more and more attention. Previous weakly supervised object detection methods iteratively update detectors and pseudo-labels, or use feature-based mask-out methods. Most of these methods do not generate complete and accurate proposals, often only the most discriminative parts of the object, or too many background areas. To solve this problem, we added the box regression module to the weakly supervised object detection network and proposed a proposal scoring network (PSNet) to supervise it. The box regression module modifies proposal to improve the IoU of proposal and ground truth. PSNet scores the proposal output from the box regression network and utilize the score to improve the box regression module. In addition, we take advantage of the PRS algorithm for generating a more accurate pseudo label to train the box regression module. Using these methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The object detection task is to find the objects belonging to specified classes and their locations in the images. Benefiting from the rapid development of deep learning in recent years, the fully supervised object detection task has made significant progress. However, the fully supervised task requires instance-level annotation for training, which costs a lot of time and resources. In fact, unlabeled or image labeled datasets cannot be effectively used by the fully supervised method. On the other hand, image-level annotated datasets are easy to generate, and can even be automatically generated by web search engines. In order to effectively utilize these readily available datasets, we focus on weakly supervised object detection(WSOD) task. The WSOD task only takes the image-level annotations to train the instance-level object detection network, which is different from the fully supervised object detection task.</p><p>There are three main methods for weakly supervised object detection: The first is to iteratively update the detector and pseudo labels from inaccurate pseudo labels; The second is to construct an end-to-end network that can take image-level annotations as supervision to train this object detection network. The third two-stage method is that taking algorithm to optimize pseudo la-bels from other WSOD network and training a fully supervised object detection network. In addition, according to different methods of proposing proposals, each of the above methods can be divided into two classes: one is to propose proposals based on feature map that predicts the probability of each pixel belonging to each classes, and then get the possible instances and their locations in the image; The second is detector-based method that uses a trained detector to identify multiple proposals and determine whether each proposal belongs to a specific object class or not. Comparing the effects of these methods, the end-to-end detector-based approach perform well, and our work follows this series of methods.</p><p>The earliest end-to-end detector-based WSOD network is WSDDN <ref type="bibr" target="#b3">[1]</ref>, which trains a two-streams network to predict the classification accuracy of each proposal and their contributions to each class. The results of the two streams are combined to get the image classification score, so the WSDDN can take advantage of the image-label annotations for training. Subsequent other work aims to improve the performance of this network, like adding more classification streams, using clustering method, adding fully supervised module, and so on. The end-to-end detector-based approach has two drawbacks: one is that context information cannot be fully used to classify the proposal; The second is that the most discriminative parts of the object may be detected instead of the entire object.</p><p>In order to make full use of the context information of proposal and avoid finding only the most discriminative part, we design a new network structure that add a box regression branch to a traditional WSOD network. In the previous WSOD network, there is usually no box regression part, while this branch plays an important role in fully supervised object detection networks. The box regression network can adjust the position and scale of the proposal, make it closer to the ground truth. In the fully supervised object detection task, we can use instance-level label as supervision to train the box regression network; but in the WSOD task, the network cannot obtain the instance-level annotation, and thus cannot train this branch. In order to obtain reliable instance annotations to train the regression network, we designed a proposal scoring network named PSNet that can detect whether the proposal completely covers the object. The PSNet is a specially trained multi-label classification network. Even if an object in the image is occluded or incomplete, the PSNet can detect the presence of the object. The PSNet can be used to evaluate image without the proposal area. If the proposal completely covers the whole object, the rest of the image will not contain information about it. We use the PSNet to evaluate the output of the WSOD network, and then select the appropriate proposals as pseudo labels to train box regression network. Examples of the output of PSNet are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To verify the effectiveness of our approach, we conducted experiments on the VOC2007 dataset. The end result is xx.</p><p>The contributions of the paper are as followings: 1.Introduce box regression branch to WSOD network, reduce the difference between proposal and ground truth.</p><p>2.Propose the PSNet to evaluate proposals, as a supervision training box regression branch.</p><p>3.Our work is simple to apply and does not require modifications to the backbone. Most end-to-end WSOD network can be used as our backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Weakly supervised object detection task use only image-level annotations, which is an effective way to utilize datasets without instance-level labels. In recent years, there have been a lot of work for this task, which can be roughly divided into three approaches: the alternating approach, end-to-end approach, and two stage approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Alternating approach</head><p>The early weakly supervised object detection work uses an alternate approach, in which iteratively train the detector and update pseudo labels, so that an effective detector could be trained from the initial rough annotations. A typical alternative approach is <ref type="bibr" target="#b4">[2]</ref>, in which song et al. assumed that the object exists at the center of the image and trained a detector with such unreliable annotations. Obviously, such a hypothesis is unfounded, and the detectors trained with these unreliable annotations has a poor performance. Song utilized the detector to generate inference in the training dataset, obtained new instance labels. Unreliable annotations can be updated with these instance labels. Repeating the processes that training the detector and update the instance annotations resulted in a stable result.</p><p>Some other alternate methods are: Cinbis et al. <ref type="bibr" target="#b5">[3]</ref> proposed a multi-fold learning method to solve the problem that alternating approaches are easily trapped in local optima. Li et al. <ref type="bibr" target="#b6">[4]</ref> did not iteratively update detector and pseudo labels, but trained a classifier with the entire image, and then used the mask-out strategy to select the most confident proposal from the feature map of the classifier. Jie et al. <ref type="bibr" target="#b7">[5]</ref> proposed a selftaught learning method to select reliable seed proposals. However, the current series of alternative methods In the yellow box is box regression network. In the blue box is our PSNet. In training process: (1) The image is first tested with the PCL network to generate 4096-dimension feature of proposals extracted by the convolution module and the corresponding detection scores. (2) Then the box regression network makes a regression correction to the proposals. (3) The output proposal is given to the PSNet to predict the proposal completeness score. The PSNet network then uses the PRS algorithm to find the optimal proposal, which is used as a pseudo label training box regression network. In inference process, the PSNet does not work.</p><p>are not performing well, because alternating approaches are time-consuming and easily trapped in local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">End-to-end approach</head><p>The end-to-end approach does not iteratively update the detector and pseudo-annotations, but instead uses an image level annotation to train an end-to-end network. Early end-to-end networks skillfully used proposals to generate the classification outputs of images and trained the network with classification losses. Bilen et al. <ref type="bibr" target="#b3">[1]</ref> proposed an end-to-end method called weakly supervised deep detection network (WSDDN). The WSDDN has two streams, a classification stream and a detection stream. The results of these two streams are combined to determine the detection score of each proposal and the classification confidence of the image. Kantorov et al. <ref type="bibr" target="#b8">[6]</ref> extended WSDDN to utilize contextual information. Diba et al. <ref type="bibr" target="#b9">[7]</ref> and Wei et al. <ref type="bibr" target="#b10">[8]</ref> take advantage of semantic segmentation network and CAM <ref type="bibr" target="#b11">[9]</ref> to select region proposals that tightly cover the instance. Kosugi et al. <ref type="bibr" target="#b12">[10]</ref> developed a context classification network to select the proposal that cover exactly the whole instance in image, and proposed CAP Labeling and SRN Labeling methods to improve the performance of OICR. Tang et Al. <ref type="bibr" target="#b13">[11]</ref> developed high-accuracy region proposals by exploiting the low-level information in CNN.</p><p>Tang also proposed two other end-to-end networks, OICR <ref type="bibr" target="#b14">[12]</ref> and PCL <ref type="bibr" target="#b15">[13]</ref>. OICR added iterative instance classifier stream to the WSDDN structure. The instance classifier stream takes the output of previous instance classifier network as supervision to train the next instance classifier. OICR incorporates the idea of the alternate method into an end-to-end approach, generates a more accurate proposal by combining the results of multiple streams. The PCL network is also an end-to-end network based on OICR that take advantage of cluster method. In PCL the spatially overlapping proposals are grouped into one set. The proposals in the same set are more likely to be different parts of the same instance, and the information of multiple proposals can be combined to find the most appropriate proposal. Because of the good performance of the PCL network, our work takes it as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Two stage approach</head><p>The main feature of the two-stage method is to use other methods to generate pseudo-labels, and then take a full-supervised method to train an object detection network. The first phase of the two-stage approach is to generate pseudo labels by other methods, so it often incorporates other end-to-end methods or alternate methods.</p><p>Zhang et al. <ref type="bibr" target="#b16">[14]</ref> took advantage of WSDDN to propose inaccurate proposals in the image, and took the PGA and PGE two proposal fusion algorithm to get a more accurate proposal to train the faster R-CNN <ref type="bibr" target="#b17">[15]</ref> model. Zhang et al. <ref type="bibr" target="#b18">[16]</ref> proposed an algorithm named mEAS(mean Energy Accumulated Scores) to calculate the complexity of image content. The training dataset is divided into different groups according to different content complexity, and the fully supervised object detection network is trained with different groups by turns according to the difficulty on the basis of the original two-stage method. In this case, the model can start training from a simpler task and prepare for later difficult training goals, so as to obtain better detection results. Zeng et al. <ref type="bibr" target="#b19">[17]</ref> revised the shape of the proposals using the low-level features such as superpixel segmentation, boundary, texture color, etc. The two-stage approach can make full use of the end-to-end network results and facilitate the integration of various rule-based proposal update methods, so there have been many new work in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Bounding box regression</head><p>Bounding box regression can reduce localization errors of predicted boxes, which is first introduced in <ref type="bibr" target="#b20">[18]</ref>. Fully supervised object detection network takes it to get more accurate results, while the box regression module is rare in WSOD network due to the lack of supervision. Gao et al. <ref type="bibr" target="#b21">[19]</ref> introduced bounding box regressors into OICR network to help selecting pseudo ground truths. Zeng et al. <ref type="bibr" target="#b19">[17]</ref> took advantage of the fusion of bottomup feature and top-down feature as supervision to train box regressors. In our paper, we propose a proposal scoring network(PSNet) to evaluate the appearance of proposals, which can be a supervision of box regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The backbone of our work is PCL <ref type="bibr" target="#b15">[13]</ref>, which is an end-to-end weak supervised object detection network. The network structure of PCL is similar to that of OICR, except that PCL owns a proposal cluster method.</p><p>The PCL network consists of two modules: Basic MIL network and instance refinement module. The basic MIL network is WSDDN <ref type="bibr" target="#b3">[1]</ref>, which is a WSOD network of two streams. The instance refinement module is composed of multi instance classifier refinement networks. Every instance classifier refinement network contains a fully connected layer and a softmax layer.</p><p>The PCL network take advantage of the WSDDN to generate the initial object detection results. The basic MIL network WSDDN consists of two streams, a classification stream and a detection stream, that calculate region-wise scores in a different way based on CNN features pooled by Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b22">[20]</ref>. The classification stream applies softmax operation on proposals to get classification output. The detection stream applies the softmax operation on the classes to get the contribution of different proposals to different classes. The final object detection results are elementwise product of outputs of the two streams.</p><p>The object detection results initialized by WSDDN can be viewed as pseudo labels to train instance refinement module. The instance refinement module contains multiple instance classifier refinement networks with the same structure. These streams are connected one by one, which means that the output of the previous instance classifier refinement network is taken as the pseudo labels to train the next instance classifier refinement network, and the first instance classifier refinement network take outputs from the WSDDN as pseudo labels. Compared with OICR, PCL apply the proposal cluster method to the process of training the instance refinement module, and enhance the performance of pseudo label by clustering the proposals that may belong to the same instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">PSNet</head><p>In the previous WSOD network, the output proposals of the network are not always close to the ground truth. In many cases, the proposal with the highest score contains only the most discriminative area of the object, or contains too many unnecessary background areas. The most important reason for this is that the detector-based method does not use the context information of the proposal, and only determines whether the proposal contains the special object. This will cause some of the proposals that are much smaller than the ground truth to be higher scored than that closer to the ground truth; and some proposals that contain too many background areas will have high scores because they contain some area of the object. Obviously we need to solve the above two problem.</p><p>In fact, a good proposal with high score should have the following two features:</p><p>Completeness: the proposal should completely contain all the pixels of an instance.</p><p>Compactness: the proposal should not contain unnecessary background areas.</p><p>In order to judge whether a proposal is good, we propose a completeness detection network named PSNet. The network detects whether the image contains an area of an object. We fill the proposal area in the image with the mean pixel, and then put the whole image into the PSNet. The more object areas contained in the processed image, the higher the output score of the network. We can judge the completeness and compactness of the proposal based on the PSNet output score.</p><p>Our proposed PSNet is a multi-label classification network with 21 outputs. An effective multi-label classification network can determine whether there is an object of specific category in an image. But there are two problems if we directly using the multi-label classification network as a proposal scoring network: 1. The network can't distinguish between the object and its environment, especially those with a fixed background. For example, even if there is no train in the image, as long as the railway appears, the network will judge that there is a train, because the two are related to each other. 2.</p><p>The network can't work well when there are multiple objects of the same category in the image. When there are multiple instances of the same class, even if a proposal completely covers one of those, the network will find the remaining instances, and the classification accuracy will not be significantly reduced.</p><p>To solve these two problems, we use a class-agnostic saliency detection network. The saliency detection network can detect areas of strong saliency in the image. The strong saliency areas are usually objects in the image, and the weak saliency areas are usually the background. One simple way to use the saliency images is to remove all background areas in the datasets and only use the images with foreground area to train the PSNet. However, the effect of this method is not good, and the trained PSNet has a significant drop in classification accuracy on the test set. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the labeled instances in the image are not salient objects, and the foreground area detected by the saliency detection network does not include these instances, which generates noisy labels. Because of these noisy labels, the network accuracy will drop dramatically.</p><p>We take advantage of the VOC2007 train dataset and saliency detection network to create three datasets: V1, V2, and V3, and use the three datasets to train the PSNet in order to avoid the effects of noisy annotations. The V1 dataset is the original VOC2007 train dataset. The V2 dataset is the foreground of the V1 dataset segmented by the saliency detection network, and the V3 dataset is the background of the V1 dataset segmented by the saliency detection network. The V1 dataset ensures that the classification network has reliable training labels. The V2 and V3 datasets enable the network to decouple objects from their background, avoiding the misidentification of the environment as objects. The PSNet trained by the above method can effectively solve the first problem.</p><p>The saliency detection network can also solve the second problem: when there are multiple instances of the same category in an image, hiding an instance has little effect on the classification accuracy. If the instances of the same category are not close together, their saliency segmentation areas are non-connected regions. So when we use PSNet to test the completeness of a proposal, we only keep the foreground area with the highest IoU and fill the other foreground areas with the mean pixels. This method can reduce the influence of different instances in the picture where the spatial location is far away, but there is no effect on the influence between similar instances in the picture where the spatial location is very close.</p><p>In order to separate different instances whose salient  areas connected, we used a seed region method. First, we apply different thresholds into the results of the saliency detection network, then obtain different segmented images S1(with high threshold) and S2(with low threshold). The area connected in S2 is dispersed into several non-connected areas in S1, and these nonconnected areas are named seed areas. These seed areas in S1 can grow and expand outward until it eventually fills up the saliency area in S2. The above operation can obtain independent saliency region of different instances, separate different instances that are close to but not overlapped, effectively solving the problem of interference of multiple instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Box regression</head><p>In our work, we add the box regression branch to the existing WSOD network PCL. The role of this branch is to revise the proposal that differs from ground truth, making it more accurate. The input of this branch is features extracted from the proposals through the VGG16 <ref type="bibr" target="#b23">[21]</ref> conv module, and the output is a prediction of the difference between the proposals and the closest ground truth. In our paper, the input is a feature vector of N*4096??? dimension, and the output is a predicted vector of N*4 dimension, where N is the number of proposals.</p><p>Training a box regression network requires instancelevel annotation, which is difficult to obtain in weakly supervised object detection task. Thus in the previous WSOD work, the proposal is mostly unchanged. The mainly task of these network is to select the proposal that is most likely to contain instance from a series of proposals. To change this condition, we propose the PSNet network that can score the accuracy of the proposal. If the difference between the proposal and the ground truth is small, the output of the PSNet is very close to 0, and vice versa. Therefore, the output of PSNet can be used as a loss to train box regression network.</p><p>However, the process in which the outputs of the box regression network are converted to the input of PSNet is a non-mathematical process, as showing in <ref type="figure" target="#fig_1">Figure 2</ref>. Because the operation that fill the inside of the proposal with the mean pixels cannot be expressed by a mathematical formula, and it cannot be derived. This means the loss backward will be truncated at the input of the PSNet, so the loss cannot be passed to the parameters of the box regression module. Therefore, we take advantage of an iterative optimization approach to utilize PSNet to train the regression network. The detailed steps are: 1. Apply random offset to the output of the box regression network to get 15 different proposals. 2. Send them together with the original proposal into the PSNet network. 3. Select the highest-scored proposal and then apply random offset to it to get the new 15 proposals. 4. Repeat steps 2 and 3 until the highest scored proposal stable. 5.The final proposal obtained in step 4 is used as a pseudo-label for training the regression network.</p><p>We take this formula to score proposals in the second step:</p><p>S proposal = S PS Net + 0.001 * S area</p><p>The higher the S proposal , the better the proposal. S PS Net and S area in (1) are defined as:</p><formula xml:id="formula_1">S PS Net = PS Net oral PS Net new (2)</formula><p>PS Net oral is the classification output of PSNet for the original proposal, and PS Net new is the classification output of PSNet for the new proposal.</p><formula xml:id="formula_2">S area =        − log 2 ( Area2 Area1 ) Area2 &gt; Area1, 1 − Area2 Area1 Area2 &lt; Area1.<label>(3)</label></formula><p>Area1 is the area of the original proposal, and Area2 is the area of the new proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We experimented on PASCAL VOC dataset to verify the performance of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and parameters</head><p>We conducted experiments on the PASCAL VOC2007 and 2012 datasets. The VOC2007 dataset contains 20 classes and 9,962 images (5011 for training and 4951 for testing). The VOC2007+2012 dataset has 20 classes, including 22,531 images (11540 for training and 10991 for testing). The images in the PASCOL VOC dataset are divided into three groups: train, test, and trainval for training, testing, and cross-validation. We employ mAP and CorLoc to measure the performance of our network. mAP measures the performance of the network on the test set. We take an IoU threshold of 0.5. CorLoc measures the localization accuracy on the trainval dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>Our PSNet take VGG16 <ref type="bibr" target="#b23">[21]</ref> as the backbone. The complete network structure: remove the VGG16 classifier module and add a average pooling layer and two fully connected layers. The last fully connected layer has 21 outputs (20 classes + background). We use the convolutional layer parameters of the pre-trained VGG16 on the imagenet as feature extractors. During training process, we keep the convolution module parameters of the network unchanged. The Loss function is BCELoss, using the SGD optimizer, a total of 15 epoch training. The first ten epochs have a lr of 1e-2 and a momentum of 0.9; the last five epochs have a lr of 1e-3 and a momentum of 0.9.</p><p>The box regression module is composed of three fully connected layers fc1, fc2, and fc3. The fc1 and fc2 are the same as fc6 and fc7 in VGG16 network. The input of the fc3 layer is 4096, and output is 4dimensional(tx,ty,tw,th). We initialized fc1 and fc2 with VGG16 pretrained parameters on imagenet, and initialized fc3 randomly. During training, the Loss function is SmoothL1Loss, using the SGD method, a total of 8 epoch. The initial lr is 1e-3, and after every two epochs, the lr is reduced to 10 percent of previous. The momentum is 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Reference</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of PSNet outputs: (a) a dog without proposal occlusion, (b) a dog whose head is occluded by the proposal box, (c) a dog that proposal covers part of the body, and (d) proposal completely cover the entire dog. If proposal does not completely cover the entire dog, PSNet gives a high score. If proposal completely cover the entire dog, PSNet gives a low score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The figure above is architecture of our work: In the red box is the backbone WSOD network, and in our work it is the PCL network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The PSNet training process is schematically illustrated, and V1, V2, and V3 are shown in the figure. Train with each dataset by turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Some results of PRS algorithm: the green box in the figure represents the proposal found by the PCL<ref type="bibr" target="#b15">[13]</ref> network, and the red box represents the proposal obtained after the PRS algorithm. Obviously our approach can get a proposal that closer to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average precision (%) on PASCAL VOC 2007. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</figDesc><table><row><cell>-VOC 2007</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosugi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CorLoc (%) on PASCAL VOC 2007 datasets. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean -VOC</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosugi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">85</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1611" to="1619" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object-aware instance labeling for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6064" to="6072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">W2f: A weaklysupervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4262" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wsod2: Learning bottom-up and top-down objectness distillation for weaklysupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8292" to="8300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">C-wsl: Countguided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1920" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Saliency guided end-to-end learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06768</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
