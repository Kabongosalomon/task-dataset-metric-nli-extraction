<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Editing Text in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-21">2019. October 21-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
							<email>zhangchengquan@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
							<email>liujiaming03@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<email>hanjunyu@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<email>liujingtuo@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<address>
									<settlement>Baidu Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Editing Text in the Wild</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM &apos;19)</title>
						<meeting>the 27th ACM International Conference on Multimedia (MM &apos;19)						</meeting>
						<imprint>
							<date type="published" when="2019-10-21">2019. October 21-25</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3350929</idno>
					<note>* Equal contribution. This work was mainly done when Liang Wu was an intern at Baidu Inc. † Corresponding author. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). MM &apos;19, October 21-25, 2019, Nice, France 2019, Nice, France. ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/3343031.3350929</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Editing</term>
					<term>Text Synthesis</term>
					<term>Text Erasure</term>
					<term>GAN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image. Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Text in images/videos, or known as scene text, contains rich semantic information that is very useful in many multi-media applications. In the past decade, scene text reading and its application have witnessed significant progresses <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. In this paper, we focus on a new task related to scene text: scene text editing. Given a text image, our goal is to replace the text instance in it without damaging its realistic look. As illustrated in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, the proposed scene text editor produces realistic text images by editing each word in the source image, retaining the styles of both the text and background. Editing scene text has drawn increasing attention from both academia and industry, driven by practical applications such as text image synthesis <ref type="bibr" target="#b32">[33]</ref>, advertising photo editing, text image correction, augmented reality translation <ref type="bibr" target="#b4">[5]</ref>.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, there are two major challenges for scene text editing: text style transfer and background texture retention. Specially, the text style consists of diverse factors such as language, font, color, orientation, stroke size and spatial perspective, which makes it hard to precisely capture the complete text style in source image and transfer them to the target text. Meanwhile, it is also difficult to maintain the consistency of the edited background, especially when text appears on some complex scenes, such as menu and street store sign. Moreover, if the target text is shorter than the original text, the exceeding region of characters should be erased and filled with appropriate texture.</p><p>Considering these challenges, we propose a style retention network (SRNet) for scene text editing which learns from pairs of images. The core idea of SRNet is to decompose the complex task into several simpler, modular and joint-trainable sub networks: text conversion module, background inpainting module and fusion module, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Firstly, the text conversion module (TCM) transfers the text style of the source image to the target text, including font, color, position, and scale. In order to keep the semantics of the target text, we introduce a skeleton-guided learning mechanism to the TCM, whose effectiveness has been verified in Exp. 4.4. At the same time, the background inpainting module (BIM) erases the original text stroke pixels and fills them with appropriate texture in a bottom-up feature fusion manner, following the general architecture of a "U-Net" <ref type="bibr" target="#b22">[23]</ref>. Finally, the fusion module automatically learns how to fuse foreground information and background texture information effectively, so as to synthesize edited text image. Generative Adversarial Networks (GAN) models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref> have achieved great progress in some tasks, such as image-to-image translation, style transfer, these methods typically apply the encoderdecoder architecture that embeds the input into a subspace then decodes it to generate desired images. Instead of choosing such a single branch structure, the proposed SRNet decomposes the network into modular sub networks, while decomposes the complex task into several easy-to-learn tasks. This strategy of network decomposing has been proven useful in recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Besides, the experiment results of SRNet are better than pix2pix <ref type="bibr" target="#b10">[11]</ref>, a successful method used in image-to-image translation, which further confirms the effectiveness and robustness of SRNet. Compared with the work of character replacement <ref type="bibr" target="#b23">[24]</ref>, our methods works in a more efficient word-level editing way. In addition to the ability to edit scene text image in the same language (such as the English words on ICDAR 2013), SRNet also shows very encouraging results in cross-language text editing and information hiding tasks, as exhibited in <ref type="figure" target="#fig_5">Fig. 7, 8</ref>.</p><p>The major contribution of this paper is the style retention network (SRNet) proposed to edit scene text image. SRNet possesses obvious advantages over existing methods in several folds:</p><p>• To our knowledge, this work is the first to address the problem of word or text-line level scene text editing by an endto-end trainable network; • We decompose SRNet into several simple, modular and learnable modules, including a text conversion module, a background inpainting module and the final fusion module, which enables SRNet to generate more realistic results than most image-to-image translation GAN models;</p><p>• Under the guidance of stroke skeleton, the proposed network can keep the semantic information as much as possible; • The proposed method exhibits superior performance on several scene text editing tasks like intra-language text image editing, AR translation (cross-language), information hiding (e.g. word-level text erasure), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 GAN</head><p>Recently, GANs <ref type="bibr" target="#b6">[7]</ref> have attracted increasing attention and made great progress in many fields ,including generating images from noise <ref type="bibr" target="#b18">[19]</ref>, image-to-image translation <ref type="bibr" target="#b10">[11]</ref>, style transfer <ref type="bibr" target="#b39">[40]</ref>, pose transfer <ref type="bibr" target="#b40">[41]</ref>, etc. The framework of GANs consists of two modules: generator and discriminator, where the former aims to generate data close to the realistic distribution while the latter strives to learn how to distinguish between real and fake data. DCGAN <ref type="bibr" target="#b21">[22]</ref> firstly used convolutional neural networks (CNN) as the structures of generator and discriminator, improved training stability of GAN. Conditional-GAN <ref type="bibr" target="#b18">[19]</ref> generated the required images under the constraints of given conditions, and achieved significant results in pixel-level alignment image generation task. Pix2pix <ref type="bibr" target="#b10">[11]</ref> implemented the mapping task from image to image, which was able to learn the mapping relationship between input domain and output domain. Cycle-GAN <ref type="bibr" target="#b39">[40]</ref> accomplished the cross-domain conversion task under the unpaired style images while achieving excellent performance. However, existing GANs are difficult applied in text editing task directly, because the text content changes while the shape of text needs change greatly, and the complex background texture information also need to be preserved well when editing a scene text image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Style Transfer</head><p>Maintaining the scene text style consistency before and after editing is extremely challenging. There are some efforts attempting to migrate or copy text style information from a given image or stylized text sample. Some methods focus on character-level style transfer, for example, Lyu et al . <ref type="bibr" target="#b16">[17]</ref> proposed an auto-encoder guided GAN to synthesize calligraphy images with specified style from standard Chinese font images. Sun et al . <ref type="bibr" target="#b28">[29]</ref> used a VAE structure to implement a stylized Chinese character generator. Zhang et al . <ref type="bibr" target="#b36">[37]</ref> tried to learn the style transfer ability between Chinese characters at the stroke level. Other methods focus on text effects transfer, which can learn visual effects from any given scene image and bring huge commercial value in some specific applications like generating special-effects typography library. Yang et al . <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>proposed a patch-based texture synthesis algorithm that can map the subeffect patterns to the corresponding positions of the text skeleton to generate image blocks. It is worth noting that this method is based on the analysis of statistical information, which may be sensitive to glyph difference and thus induce a heavy computational burden. Recently, TET-GAN <ref type="bibr" target="#b31">[32]</ref> used the GAN to design a lightweight framework that can simultaneously support stylization and destylization on a variety of text effects. Meanwhile, MC-GAN <ref type="bibr" target="#b1">[2]</ref> used two sub-networks to solve English alphabet glyph transfer and effect transfer respectively, which accomplished the few-shot font style transfer task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style image</head><p>Conv-BN-ReLU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resblock</head><p>Text conversion module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion module</head><p>Background inpainting module Different from these existing methods, the proposed framework in this paper is trying to solve the migration problem of arbitrary text styles and special effects at a word or text-line level, rather than at the character level. In practice, word-level annotations are much easier to obtain than character-level annotations, and editing word is more efficient than editing characters. Besides, word-level editors favor word-level layout consistency. When dealing with words of different lengths, our word-level editor can adjust the placement of foreground characters adaptively, while character-level methods ignore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Erasure and Editing</head><p>Background texture needs to be consistent with that before editing for scene text editing. There are some related works of text erasure, trying to erase the scene text stroke pixels while completing image inpainting on corresponding positions. Nakamura et al . <ref type="bibr" target="#b20">[21]</ref> proposed an image-patch based framework for text erasure, but large computational cost is induced due to the sliding window based processing mechanism. EnsNet <ref type="bibr" target="#b34">[35]</ref> firstly introduced the generative adversarial network to text erasing, which can erase the scene text on the whole image in an end-to-end manner. With the help of refined loss, the visualization results looks better than those of pix2pix <ref type="bibr" target="#b10">[11]</ref>. Our background inpainting module is also inspired by generative adversarial networks. In the process of text editing, we only pay attention to background erasure at word-level, therefore, the background inpainting module in SRNet can be designed more light and still have good erasure performance which is illustrated in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>We noticed that a recent paper <ref type="bibr" target="#b23">[24]</ref> try to study the issue of scene text editing, but it can only transfer the color and font of a single character in one process while ignoring the consistency of background texture. Our method integrates the advantages of the approaches of text style transfer and text erasing. We propose a style retention network which can not only transfer text style by an efficient manner (word or text-line level processing mechanism) but also retain or inpaint the complete background regions to make the result of scene text editing more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We present a style retention network (SRNet) for scene text editing. During training, the SRNet takes as input a pair of images (I s , I t ) where I s is the source style image and I t is the target text image. The outputs ((T sk ,T t ),T b ,T f ) where T sk is the target text skeleton, T t is the foreground image which has the same text style as I s . T b is the background of I s and T f is the final target text image. In order to effectively tackle the two major challenges mentioned in Sec. 1, we decompose the SRNet into three simpler and learnable sub networks: 1) text conversion module, 2) background inpainting module and 3) fusion module, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, the text style from source image I s is transferred to the target text with the help of a skeleton-guided learning mechanism aiming to retain text semantics(Sec. 3.1). Meanwhile the background information is filled by learning an erasure or inpainting task (Sec. 3.2). Lastly, the transferred target image and completed background are fused by the text fusion network, generating the edited image (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Conversion Module</head><p>We render the target text into a standard image with a fixed font and background pixel value setting to 127, and the rendered image is denoted as target text image I t . The text conversion module (blue part in <ref type="figure" target="#fig_1">Fig. 2</ref>) takes the source image I s and the target text image I t as inputs, and aims to extract the foreground style from the source image I s and transfers it to the target text image I t . In particular, the foreground style contains text style, including font, color, geometric deformation, and so on. Thus, the text conversion module outputs an image O t which has the semantics of the target text and the text style of the source image. An encoder-decoder FCN is adopted in this work. For encoding, the source image I s is encoded by 3 down-sampling convolutional layers and 4 residual blocks <ref type="bibr" target="#b8">[9]</ref>, the input text image I t is also encoded by the same architecture, then two features are concatenated along their depth axis. For decoding, there are 3 up-sampling transposed convolutional layers and 1 Convolution-BatchNorm-LeakyReLU blocks to generate the output O t . Moreover, we introduce a skeleton-guided learning mechanism to generate more robust text. We use G T to denote the text conversion module and the output can be represented as:</p><formula xml:id="formula_0">O t = G T (I t , I s ).</formula><p>(1)</p><p>Skeleton-guided Learning Mechanism. Different from other natural objects, humans distinguish different texts mostly according to the skeleton or glyph of text. It is necessary to maintain the text skeleton in I t after transferring the text style from source style image I s . To achieve this, we introduce a skeleton-guided learning mechanism. Specifically, we add a skeleton response block which is composed of 3 up-sampling layers and 1 convolutional layer followed by a sigmoid activation function to predict a single channel skeleton map, and then concatenate the skeleton heatmap and decoder output along depth axis. We use the dice loss <ref type="bibr" target="#b17">[18]</ref> instead of the cross-entropy loss to measure the reconstruction quality of the skeleton response map since it is found to yield more accurate results. Mathematically, the skeleton loss is defined as:</p><formula xml:id="formula_1">L sk = 1 − 2 N i (T sk ) i (O sk ) i N i (T sk ) i + N i (O sk ) i ,<label>(2)</label></formula><p>where N is the number of pixell; T sk is the skeleton ground truth map; O sk is output map of the skeleton module. We further adopt the L1 loss to supervise the output of text conversion module. Combing with the skeleton loss, the text conversion loss is:</p><formula xml:id="formula_2">L T = ∥T t − O t ∥ 1 + α L sk ,<label>(3)</label></formula><p>where T t is the ground truth of text conversion module, and α is regularization parameter, which is set to 1.0 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Background Inpainting Module</head><p>In this module, our main goal is to obtain the background via a word-level erasure task. As depicted in the green part in <ref type="figure" target="#fig_1">Fig. 2</ref>, this module takes only the source image I s as its input, and outputs a background image O b , in which all text stroke pixels are erased and filled with proper texture. The input image is encoded by 3 down-sampling convolutional layers with stride 2 and follows with 4 residual blocks, then the decoder generates the output image with original size via 3 up-sampling convolutional layers. We use the leaky ReLU activation function after each layer while tanh function for the output layer. We denote the background generator as G B . In order to make the visual effects more realistic, we need to restore the texture of background as much as possible. U-Net <ref type="bibr" target="#b22">[23]</ref>, which proposes to add skip connections between mirrored layers, proven remarkably effective and robust at solving object segmentation and image-to-image translation tasks. Here, we adopt this mechanism in the up-sampling process, where previous encoding feature maps with the same size are concatenated to reserve richer texture. This helps to restore the lost background information during the downsampling process. Different from other full text image erasure methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, our method aims at word-level image inpainting task. Text appearing in word-level image tends to be relatively standard in scale, so our network structure has possesses simple and neat design. Inspired by the work of Zhang et al . <ref type="bibr" target="#b34">[35]</ref>, the adversarial learning is added to learn more realistic appearance. The detailed architecture of the background image discriminator D B is described in Sec. 3.4. The whole loss function of background inpainting module is formulated as:</p><formula xml:id="formula_3">L B = E (T b , I s ) [log D B (T b , I s )] + E I s log[1 − D B (O b , I s )]+ β ∥T b − O b ∥ 1 ,<label>(4)</label></formula><p>where T b is the ground truth of background. The formula is combined by adversarial loss and L1 loss, and β is set to 10 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusion Module</head><p>The fusion module is designed to fuse the target text image and background texture information harmoniously, so as to synthesize edited scene text image. As the orange part illustrates in <ref type="figure" target="#fig_1">Fig. 2</ref>, the fusion model also follows the encoder-decoder FCN framework. We feed the foreground image, generated by text conversion module, to the encoder, which consists of three down-sampling convolutional layers and residual blocks. Next, a decoder with three upsampling transposed convolutional layers and Convolution-Batch-Norm-LeakyReLU blocks to generates the final edited image. It is noteworthy that we connect the decoding feature maps of the background inpainting module to the corresponding feature maps with the same resolution in the up-sampling phase of the fusion decoder. In this way, the fusion network outputs the images whose background details are substantially restored; text object and background are fused well while achieving synthesis realism in the appearance. We use G F and O f to denote the fusion generator and its outputs respectively. Besides, the adversarial loss is added here, and the detailed structure of the corresponding discriminator D F will be introduced in Sec. 3.4. In summary, we can formulate the optimization objectives of the fusion module as the following:</p><formula xml:id="formula_4">L ′ F = E (T f , I t ) [log D F (T f , I t )] + E I t log[1 − D F (O f , I t )]+ θ 1 ∥T f − O f ∥ 1 ,<label>(5)</label></formula><p>where T f is the ground truth of edited scene images. We choose θ 1 = 10 to keep balance between adversarial loss and L1 loss. VGG-Loss. In order to reduce distortions and make more realistic images, we introduce the VGG-loss to the fusion module that includes perceptual loss <ref type="bibr" target="#b12">[13]</ref> and style loss <ref type="bibr" target="#b5">[6]</ref>. As the name suggests, the perceptual loss L per penalizes results that are not perceptually similar to labels by defining a distance measure between activation maps of a pre-trained network (we adopt the VGG-19 model <ref type="bibr" target="#b27">[28]</ref> pretrained on ImageNet <ref type="bibr" target="#b24">[25]</ref>). Meanwhile, the style loss L styl e computes the differences in style. The VGG-loss L vдд can <ref type="figure">Figure 3</ref>: Some results on ICDAR2013 dataset. Images from left to right: input images and edited results. It should be noted that on the third row we replaced the words whose lengths is different from the original text; the last row shows some cases with long text. be represented by:</p><formula xml:id="formula_5">L vдд = θ 2 L per + θ 3 L styl e ,<label>(6)</label></formula><formula xml:id="formula_6">L per = E[ i 1 M i ∥ϕ i (T f ) − ϕ i (O f )∥ 1 ],<label>(7)</label></formula><formula xml:id="formula_7">L styl e = E j [∥G ϕ j (T f ) − G ϕ j (O f )∥ 1 ],<label>(8)</label></formula><p>where ϕ i is the activation map from relu1_1, relu2_1, relu3_1, relu4_1 and relu5_1 layer of VGG-19 model; M i is the element size of the feature map obtained by the i − th layer; G is Gram matrix G(F) = FF T ∈ R n×n ; the weights θ 2 and θ 3 set to 1 and 500 respectively. The whole training objectives of the fusion model is:</p><formula xml:id="formula_8">L F = L ′ F + L vдд .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discriminators</head><p>Two discriminators sharing the same structure as PatchGAN <ref type="bibr" target="#b10">[11]</ref> are applied in our network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Inference</head><p>In the training stage, the whole network is trained in an end-to-end manner, and the overall loss of the model is: Following the training procedures of GAN, we alternately train the generator and discriminators. We synthesize the image pairs with similar style except text as our training data. Besides, the foreground, text skeleton and background images can be obtained with the help of text stroke segmentation masks. The generator takes In the inference phase, given the standard text image and the style image, the generator can output the erased result of style image and edited image. For the whole image, we crop out the target patches according to the bounding box annotations and feed them to our network, then we paste the results to original locations to get the visualization of whole image.</p><formula xml:id="formula_9">L G = arg min G max D B , D F (L T + L B + L F ),<label>(10)</label></formula><p>In this section, we present some results in <ref type="figure">Fig. 3</ref> to verify that our model has a strong ability of scene text editing, and we compare our method with other neural network based methods to prove the effectiveness of our approach. An ablation study is also conducted to evaluate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The datasets used for the experiments in this paper are introduced as following:</p><p>Synthetic Data We improve the text synthesis technology <ref type="bibr" target="#b7">[8]</ref> to synthesize data in a pair of style but with different text, the main idea is to select fonts, color, parameters of deformation randomly to generate styled text, then render it on the background image, and, at the same time, we can get the corresponding background, foreground text and text skeleton after image skeletonization <ref type="bibr" target="#b35">[36]</ref> as ground truth <ref type="figure" target="#fig_2">(Fig. 4)</ref>. In our experiments, we resize the text image height to 64 and keep the same aspect ratio. The training set consists of a total of 50000 images and the test set contains 500 images.</p><p>Real-world Dataset The ICDAR 2013 <ref type="bibr" target="#b13">[14]</ref> is a natural scene text data set organized by the 2013 International Conference on Document Analysis and Recognition for competition. This dataset focuses on the detection and recognition of horizontal English text in natural scenes, containing 229 training pictures and 233 test pictures. The text in each image has a detailed label and all text is annotated by horizontal rectangles. Every image has one or more text boxes. We crop the text regions according to the bounding box and input the cropped images to our network, then paste the results back to their original location. Noted that we only train our model on synthetic data, and all real-world data is used for testing only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implemented our network architecture based on pix2pix <ref type="bibr" target="#b10">[11]</ref>. Adam <ref type="bibr" target="#b14">[15]</ref> optimizer is adopted to train the model with β 1 = 0.5, β 2 = 0.999 until the output tends to be stable in training phase. Learning rate is initially set to 2 × 10 −4 and gradually decayed to 2 × 10 −6 after 30 epochs. We chose α = θ 2 = 1, β = θ 1 = 10, θ 3 = 500 to make the loss gradient norms of each part close in back propagation. We apply spectral normalization <ref type="bibr" target="#b19">[20]</ref> to both generator and discriminator and use batch normalization <ref type="bibr" target="#b9">[10]</ref> in generator only. The batch size is set to 8 and the input images is resized to w × 64 with the aspect ration unchanged. In training, we get the batch data randomly and the image width is resized to the average width, when testing we can input images with variable width to get desired results. The model takes about 8 hours to train with a single NVIDIA TITAN Xp graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We adopt the commonly used metrics in image generation to evaluate our method, which includes the following: 1) MSE, also known as ℓ 2 error; 2) PSNR, which computes the the ratio of peak signal to noise; 3) SSIM <ref type="bibr" target="#b29">[30]</ref>, which computes the mean structural similarity index between two images. A lower ℓ 2 error or higher SSIM and PSNR mean the results are similar to ground truth. We only calculate the above mentioned metrics on the synthetic test data, because the real dataset does not have paired data. On the real data, we calculate the recognition accuracy to evaluate the quality of the generated result. Since the input of our network is cropped image, we only compute those metrics on the cropped regions. Additionally, visual assessment is also used in real dataset to qualitatively compare the performance of various methods. The adopted text recognition model is an attention-based text recognizer <ref type="bibr" target="#b26">[27]</ref> whose backbone is replaced with a VGG-like model. It is trained on Jaderberg-8M synth data <ref type="bibr" target="#b11">[12]</ref> and ICDAR 2013 training data, and them are augmented by random rotation and random resize in x-axis. Each text editing model renders 1000 word images based on ICDAR 2013 testing data as their respective test sets. Recognition accuracy is defined as Equ. 11, where y refers to the ground truth of n −th sample, and y ′ refers to its corresponding predicted result; N refers to the number of samples in the whole test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>seq_acc =</head><p>n ∈N t e s t (I(y == y ′ )) N t est .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we study the effects of various components of the proposed network with qualitative and quantitative results. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the results of different settings such as: removing the skeleton guided module, without decomposition strategy, and removing the vgg loss L vдд (perceptual loss and style loss). Skeleton-guided Module. After the removal of skeleton module, due to the lack of supervision information of the text skeleton during training, the text structure after transfer is prone to yield local bending even breakage, which is easy to affect the quality of the generated images. In contrast, the full-module method maintains the transfer text structure well and learns the deformation of the original text correctly. From Tab. 1, we can see that the results are worse than full model on all metrics, especially a significant decline appeared in SSIM. This shows skeleton-guided module has a positive effect on the overall structure.</p><p>Benefits from Decomposition. A major contribution of our work is to decompose the foreground text and background to different modules. We also conduct experiments on models that did Source Image Pix2pix Ours <ref type="figure">Figure 6</ref>: A comparison of our model with pix2pix. not decompose the foreground text from background. In short, we removed the background inpainting branch, so the foreground text feature and background feature are processed by the foreground module simultaneously. From the <ref type="figure" target="#fig_4">Fig. 5</ref>, we can find the results are not satisfactory. The original text still remains in the synthetic image, and the text and the background are very vague. From Tab. 1, we can find the metrics of no-decomposition are generally the worst,which verifies that the mechanism of decomposition is helpful to learn clear strokes and reduce learning complexity. Discussion of VGG Loss. As can be seen from these examples in <ref type="figure" target="#fig_4">Fig. 5</ref>, the results look unrealistic in appearance without the VGG loss. In this setting, we can find some details like characters in same word has different scales, the structure of text is not maintained well, etc. The results on all metrics are worse than full model, which also illustrates the importance of this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Previous Work</head><p>Note that there was no work focusing on word-level text editing task before, so we choose pix2pix <ref type="bibr" target="#b10">[11]</ref> network, which can complete the image translation task to compare with our method. In order to make pix2pix network implement multiple style translation, we concatenate the style image and the target text in depth as input of the network. Both methods maintain the same configurations 17.78 EAST <ref type="bibr" target="#b38">[39]</ref> Scene text eraser <ref type="bibr" target="#b20">[21]</ref> 16.03 Ensnet <ref type="bibr" target="#b34">[35]</ref> 10.51 SRNet 4.64</p><p>during training. As can be seen from the <ref type="figure">Fig. 6</ref>, our method completes the foreground text transfer and retention of the background texture correctly; the structure of the edited text is regular; the font is consistent as before and the texture of background is more reasonable, while the results are similar to the real picture in the overall sense. Quantitative comparison with pix2pix can be found in Tab. 1. It indicates that our method is superior to the pix2pix method in all of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Cross-Language Editing</head><p>In this section, we conduct an experiment on cross-language text editing task to check the generalization ability of our model. The application can be used in visual translation and AR translation to improve visual experience. Considering that the relation of Latin fonts and non-Latin fonts are not mapped well, for convenience, we only complete translation tasks from English to Chinese. In the training phase, we adopt the same text image synthesis method mentioned in Sec. 4.1 to generate large amounts of training data. It is worth noting that we map all English fonts to several common Chinese fonts manally by analyzing the stroke similarity from the size, thickness, inclination, etc. We evaluate it on the ICDAR2013 test set and use the translation results as input text to check the generalization of our model. The results are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, from which we can see that even if the output is Chinese characters, the color, geometric deformation and background texture can be kept very well, and the structure of characters is the same as the input text. These realistic results show the superior synthesis performance of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Text Information Hiding</head><p>The subtask that extracts the background information can also output the erased image. Different from the two text erasing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, in many cases, the entire image is not required to remove all text, it is more practical to erase part of the text in an image. We are aiming at the word-level text erasure which can select text area freely in the picture needed to be erased. As the erasure examples shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, we can see that the locations of original text are filled with appropriate textures. Tab. 2 shows the detection results on erased images. Due to the particularity of our method, we erased the cropped images and pasted them back to compare with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Failure Cases</head><p>Although our method is capable of most scene images, there are still some limitations. Our methods may fail when the text have  very complex structures or rare font shapes. <ref type="figure" target="#fig_7">Fig. 9</ref> shows some failed cases of our method. In the top row, although the foreground text has been transferred successfully, it can be found that the shadow of the original text still remains in the output image. In the middle row of images, our model fails to extract the style of text with such a complicated spatial structure, and the result of the background erasure is also sub-optimal. In the bottom row of images, the boundaries surrounding the text are not transfered with text. We attribute these failure cases to the inadequacy of these samples in training data, so we assume they could be alleviated by augmenting the training set with more font effects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>This paper proposes an end-to-end network for text editing task, which can replace the text from scene text image while maintaining the original style. We mainly divide it into three steps to achieve this function: (1) extract foreground text style and transfer to input text with the help of skeleton; (2) erase the style image with appropriate texture to get background image; (3) merge the transferred text with the erased background. To our best knowledge, this paper is the first work to edit text image in the word-level. Our method has achieved outstanding results in both subjective visual realness and objective quantitative scores on ICDAR13 dataset. At the same time, the network also have the ability to erase text and edit on cross-language situation, and the effectiveness of our network has been verified through the comprehensive ablation studies.</p><p>In the future, we hope to solve text editing in more complex scenarios while making the model easier to use. We will edit text between more language pairs to fully exploit the ability of the proposed model. We will try to propose new evaluation metrics to evaluate the quality of text editing properly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) The process of scene text editing. (b) Two challenges of text editing: rich text style and complex background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall structure of SRNet. The network consists of a skeleton-guided text conversion module, a background inpainting module and a fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of synthetic data. From top to bottom: style image, target image, foreground text, text skeleton, background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>I t , I s as input with the supervision of T sk , T t , T b , T f and outputs the text replaced image O t . For the adversarial training, (I s ,O b ) and (I s ,T b ) are fed into D B to chase for background consistency; (I t ,O f ) and (I t ,T f ) are fed into D F to ensure accurate results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample results of ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The translation examples. Left: input images, right: translation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The erasure examples. Left: input images, right: erasure results. We erase the text randomly in every image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>The failure cases. Left: source images; right: edited results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>They are composed of five convolution layers to reduce the scale to 1/16 of the original size. The discriminator D B in background inpainting module concatenate I s with O b or T b as input to judge whether the erased result O b and the target background T b is similar, while the discriminator D F in fusion module concatenate I t and O f or T f to measure the consistence between the final output O f and the ground truth image T f .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation results.</figDesc><table><row><cell>method</cell><cell cols="2">ℓ 2 error PSNR SSIM seq_acc</cell></row><row><cell>pix2pix [11]</cell><cell>0.092 16.54 0.63</cell><cell>0.717</cell></row><row><cell>without skeleton</cell><cell>0.025 20.08 0.64</cell><cell>0.798</cell></row><row><cell cols="2">without decomposition 0.064 18.56 0.66</cell><cell>0.786</cell></row><row><cell>without vgg loss</cell><cell>0.022 20.39 0.74</cell><cell>0.778</cell></row><row><cell>SRNet</cell><cell cols="2">0.014 21.12 0.79 0.827</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison SRNet with previous methods on IC-DAR2013, lower value means better effect. Note that our method erased text according to the word-level annotations.</figDesc><table><row><cell>Detection</cell><cell>Erasure Methods</cell><cell>F-measure(%)</cell></row><row><cell></cell><cell>Original image</cell><cell>75.37</cell></row><row><cell></cell><cell>Pix2Pix [11]</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by NSFC 61733007, to Dr. Xiang Bai by the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team 2017QYTD08. We sincerely thank Zhen Zhu and Tengteng Huang for their valuable discussions and continuous help to this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to Compose Neural Networks for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-content gan for few-shot font style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7564" to="7573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8340" to="8348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TranslatAR: A mobile augmented reality translator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fragoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Gauglitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Zamora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Kleban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Fernandez</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mota</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition</title>
		<editor>ICDAR. IEEE</editor>
		<meeting><address><addrLine>Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>ICLR. 13</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04256</idno>
		<title level="m">Scene Text Detection and Recognition: The Deep Learning Era</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoder guided gan for chinese calligraphy synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IC3DV. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene text eraser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="832" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subhankar Ghosh, and Umapada Pal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01192</idno>
	</analytic>
	<monogr>
		<title level="m">STEFANN: Scene Text Editor using Font Adaptive Neural Network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to write stylized chinese characters by reading a handful of examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Awesome typography: Statistics-based text effects transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tet-gan: Text effects transfer via stylization and destylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1238" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context-Aware Unsupervised Text Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1688" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10552" to="10561" />
		</imprint>
	</monogr>
	<note>Errui Ding, and Xinghao Ding</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensnet: Ensconce text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songxuan</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="801" to="808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast parallel algorithm for thinning digital patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="236" to="239" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Separating style and content for generalized style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8447" to="8455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Progressive Pose Attention Transfer for Person Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2347" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
