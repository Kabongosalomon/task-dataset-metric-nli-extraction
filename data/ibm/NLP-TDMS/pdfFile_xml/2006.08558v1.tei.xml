<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaobing</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Berkeley ‡ Tsinghua-Berkeley Shenzhen Institute</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To learn intrinsic low-dimensional structures from high-dimensional data that most discriminate between classes, we propose the principle of Maximal Coding Rate Reduction (MCR 2 ), an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class. We clarify its relationships with most existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees for learning diverse and discriminative features. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions and can learn intrinsic representations in supervised, self-supervised, and unsupervised settings in a unified manner. Empirically, the representations learned using this principle alone are significantly more robust to label corruptions in classification than those using cross-entropy, and can lead to state-of-the-art results in clustering mixed data from self-learned invariant features. * The first two authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Context and Motivation</head><p>Given a random vector x ∈ R D which is drawn from a mixture of, say k, distributions D = {D j } k j=1 , one of the most fundamental problems in machine learning is how to effectively and efficiently learn the distribution from a finite set of i.i.d samples, say X = [x 1 , x 2 , . . . , x m ] ∈ R D×m . To this end, we seek a good representation through a continuous mapping, f (x, θ) : R D → R d , that captures intrinsic structures of x and best facilitates subsequent tasks such as classification or clustering.</p><p>Supervised learning of discriminative representations. To ease the task of learning D, in the popular supervised setting, a true class label, represented as a one-hot vector y i ∈ R k , is given for each sample x i . Extensive studies have shown that for many practical datasets (images, audios, and natural languages, etc.), the mapping from the data x to its class label y can be effectively modeled by training a deep network <ref type="bibr" target="#b3">[GBC16]</ref>, here denoted as f (x, θ) : x → y with network parameters θ ∈ Θ. This is typically done by minimizing the cross-entropy loss over a training set {(x i , y i )} m i=1 , through backpropagation over the network parameters θ:</p><formula xml:id="formula_0">min θ∈Θ CE(θ, x, y) . = −E[ y, log[f (x, θ)] ] ≈ − 1 m m i=1 y i , log[f (x i , θ)] .<label>(1)</label></formula><p>Despite its effectiveness and enormous popularity, there are two serious limitations with this approach: 1) It aims only to predict the labels y even if they might be mislabeled. Empirical studies show that deep networks, used as a "black box," can even fit random labels [ZBH + 17]. 2) With such an end-to-end data fitting, it is not clear to what extent the intermediate features learned by the network capture the intrinsic structures of the data that make meaningful classification possible in the first place. <ref type="bibr">1</ref> The precise geometric and statistical properties of the learned features are also often obscured, which leads to the lack of interpretability and subsequent performance guarantees (e.g., generalizability, transferability, and robustness, etc.) in deep learning. Therefore, the goal of this paper is to address such limitations of current learning frameworks by reformulating the objective towards learning explicitly meaningful representations for the data x.</p><p>Minimal discriminative features via information bottleneck. One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features z = f (x, θ) ∈ R d of the data that are discriminative among multiple classes. Learned representations z then facilitate the subsequent classification task for predicting the class label y by optimizing a classifier g(z):</p><formula xml:id="formula_1">x f (x,θ) − −−−−− → z(θ) g(z)</formula><p>− −−−− → y. The information bottleneck (IB) formulation [TZ15] further hypothesizes that the role of the network is to learn z as the minimal sufficient statistics for predicting y. Formally, it seeks to maximize the mutual information I(z, y) 2 between z and y while minimizing I(x, z) between x and z: max θ∈Θ IB(x, y, z(θ)) . = I(z(θ), y) − βI(x, z(θ)), β &gt; 0.</p><p>(2)</p><p>This framework has been successful in describing certain behaviors of deep networks. 3 But by being task-dependent (depending on the label y) and seeking a minimal set of most informative features for the task at hand (for predicting the label y only), the network sacrifices generalizability, robustness, or transferability. <ref type="bibr">4</ref> To address this, our framework uses label y only as side information to assist learning discriminative features, hence making learned features more robust to mislabeled data.</p><p>Contractive learning of generative representations. Complementary to the above supervised discriminative approach, auto-encoding [BH89, <ref type="bibr" target="#b13">Kra91]</ref> is another popular unsupervised (labelfree) framework used to learn good latent representations. The idea is to learn a compact latent representation z ∈ R d that adequately regenerates the original data x to certain extent, say through optimizing some decoder or generator g(z, η) 5 :</p><formula xml:id="formula_2">x f (x,θ) − −−−−− → z(θ) g(z,η) − −−−−− → x(θ, η).<label>(3)</label></formula><p>Typically, such representations are learned in an end-to-end fashion by imposing certain heuristics on geometric or statistical "compactness" of z, such as its dimension, energy, or volume. For example, the contractive autoencoder [RVM + 11] penalizes local volume expansion of learned features approximated by the Jacobian ∂z ∂θ . Another key design factor of this approach is the choice of a proper, but often elusive, metric that can measure the desired similarity between x and the decoded x, either between sample pairs x i and x i 6 or between the two distributions D x and D x . 7</p><p>Representations learned through this framework can be arguably rich enough to regenerate the data to a certain extent. But depending on the choice of the regularizing heuristics on z and similarity metrics on x (or D x ), the objective is typically task-dependent and often grossly approximated [RVM + 11, GPAM + 14]. When the data contain complicated multi-modal structures, naive heuristics or inaccurate metrics may fail to capture all internal subclass structures 8 or to explicitly discriminate among them for classification or clustering purposes. To address this, we propose a principled measure (on z) to learn representations that promotes multi-class discriminative property from data of mixed structures, which works in both supervised and unsupervised settings.</p><p>This work: Learning diverse and discriminative representations. Whether the given data X of a mixed distribution D can be effectively classified depends on how separable (or discriminative)</p><p>f (x, θ) the component distributions D j are (or can be made). One popular working assumption is that the distribution of each class has relatively low-dimensional intrinsic structures. <ref type="bibr">9</ref> Hence we may assume the distribution D j of each class has a support on a low-dimensional submanifold, say M j with dimension d j D, and the distribution D of x is supported on the mixture of those submanifolds, M = ∪ k j=1 M j , in the high-dimensional ambient space R D , as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> left. With the manifold assumption in mind, we want to learn a mapping z = f (x, θ) that maps each of the submanifolds M j ⊂ R D to a linear subspace S j ⊂ R d (see <ref type="figure" target="#fig_0">Figure 1</ref> middle). To do so, we require our learned representation to have the following properties: Although the above properties are all highly desirable for the latent representation z, they are by no means easy to obtain: Are these properties compatible so that we can expect to achieve them all at <ref type="bibr">9</ref> There are many reasons why this assumption is plausible: 1. high dimensional data are highly redundant; 2. data that belong to the same class should be similar and correlated to each other; 3. typically we only care about equivalent structures of x that are invariant to certain classes of deformation and augmentations.</p><formula xml:id="formula_3">R D R d M M1 M2 Mj x i S 1 S 2 S j z i</formula><p>10 So x ∈ M iff τ (x) ∈ M for all τ ∈ T . once? If so, is there a simple but principled objective that can measure the goodness of the resulting representations in terms of all these properties? The key to these questions is to find a principled "measure of compactness" for the distribution of a random variable z or from its finite samples Z. Such a measure should directly and accurately characterize intrinsic geometric or statistical properties of the distribution, in terms of its intrinsic dimension or volume. Unlike cross-entropy (1) or information bottleneck (2), such a measure should not depend explicitly on class labels so that it can work in all supervised, self-supervised, semi-supervised, and unsupervised settings.</p><p>Low-dimensional degenerate distributions. In information theory [CT06], the notion of entropy H(z) is designed to be such a measure. 11 However, entropy is not well-defined for continuous random variables with degenerate distributions. 12 This is unfortunately the case here. To alleviate this difficulty, another related concept in information theory, more specifically in lossy data compression, that measures the "compactness" of a random distribution is the so-called rate distortion [CT06]: Given a random variable z and a prescribed precision &gt; 0, the rate distortion R(z, ) is the minimal number of binary bits needed to encode z such that the expected decoding error 13 is less than . Although this framework has been successful in explaining feature selection in deep networks <ref type="bibr" target="#b22">[MWHK19]</ref>, the rate distortion of a random variable is difficult, if not impossible to compute, except for simple distributions such as discrete and Gaussian.</p><p>Nonasymptotic rate distortion for finite samples. When evaluating the lossy coding rate R, one practical difficulty is that we normally do not know the distribution of z. Instead, we have a finite number of samples as learned representations where z i = f (x i , θ) ∈ R d , i = 1, . . . , m, for the given data samples X = [x 1 , . . . , x m ]. Fortunately, <ref type="bibr" target="#b21">[MDHW07]</ref> provides a precise estimate on the number of binary bits needed to encoded finite samples from a subspace-like distribution. In order to encode the learned representation Z = [z 1 , . . . , z m ] up to a precision , the total number of bits needed is given by the following expression 14 : L(Z, ) . = m+d 2 log det I + d m 2 ZZ . Therefore, the compactness of learned features as a whole can be measured in terms of the average coding length per sample (as the sample size m is large), a.k.a. the coding rate subject to the distortion :</p><formula xml:id="formula_4">R(Z, ) . = 1 2 log det I + d m 2 ZZ . (4)</formula><p>Rate distortion of data with a mixed distribution. In general, the features Z of multi-class data may belong to multiple low-dimensional subspaces. To evaluate the rate distortion of such mixed data more accurately, we may partition the data Z into multiple subsets: Z = Z 1 ∪ · · · ∪ Z k , with each in one low-dim subspace. So the above coding rate (4) is accurate for each subset. For convenience, let Π = {Π j ∈ R m×m } k j=1 be a set of diagonal matrices whose diagonal entries encode the membership of the m samples in the k classes. <ref type="bibr">15</ref> Then, according to <ref type="bibr" target="#b21">[MDHW07]</ref>, with respect to this partition, the average number of bits per sample (the coding rate) is</p><formula xml:id="formula_5">R c (Z, | Π) . = k j=1 tr(Π j ) 2m log det I + d tr(Π j ) 2 ZΠ j Z .<label>(5)</label></formula><p>Notice that when Z is given, R c (Z, | Π) is a concave function of Π. The function log det(·) in the above expressions has been long known as an effective heuristic for rank minimization problems, with guaranteed convergence to local minimum <ref type="bibr" target="#b2">[FHB03]</ref>. As it nicely characterizes the rate distortion of Gaussian or subspace-like distributions, log det(·) can be very effective in clustering or classification of mixed data [MDHW07, WTL + 08, KPCC15]. We will soon reveal more desired properties of this function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Principle of Maximal Coding Rate Reduction</head><p>On one hand, for learned features to be discriminative, features of different classes/clusters are preferred to be maximally incoherent to each other. Hence they together should span a space of the 11 given the probability density p(z) of a random variable, H(z) . = − p(z) log p(z) dz. 12 The same difficulty resides with evaluating mutual information I(x, z) for degenerate distributions. <ref type="bibr">13</ref> Say in terms of the 2 -norm, we have E[ z − z 2] ≤ for the decoded z. 14 This formula can be derived either by packing -balls into the space spanned by Z or by computing the number of bits needed to quantize the SVD of Z subject to the precision, see <ref type="bibr" target="#b21">[MDHW07]</ref> for proofs. <ref type="bibr">15</ref> That is, the diagonal entry Πj(i, i) of Πj indicates the probability of sample i belonging to subset j. Therefore Π lies in a simplex:</p><formula xml:id="formula_6">Ω . = {Π | Πj ≥ 0, Π1 + · · · + Π k = I}.</formula><p>largest possible volume (or dimension) and the coding rate of the whole set Z should be as large as possible. On the other hand, learned features of the same class/cluster should be highly correlated and coherent. Hence, each class/cluster should only span a space (or subspace) of a very small volume and the coding rate should be as small as possible. Therefore, a good representation Z of X is one such that, given a partition Π of Z, achieves a large difference between the coding rate for the whole and that for all the subsets:</p><formula xml:id="formula_7">∆R(Z, Π, ) . = R(Z, ) − R c (Z, | Π).<label>(6)</label></formula><p>If we choose our feature mapping z = f (x, θ) to be a deep neural network, the overall process of the feature representation and the resulting rate reduction w.r.t. certain partition Π can be illustrated by the following diagram:</p><formula xml:id="formula_8">X f (x,θ) − −−−−− → Z(θ) Π, − −−− → ∆R(Z(θ), Π, ).<label>(7)</label></formula><p>Note that ∆R is monotonic in the scale of the features Z. So to make the amount of reduction comparable between different representations, <ref type="bibr">16</ref> we need to normalize the scale of the learned features, either by imposing the Frobenius norm of each class Z j to scale with the number of features in Z j ∈ R d×mj : Z j 2 F = m j or by normalizing each feature to be on the unit sphere: z i ∈ S d−1 . This formulation offers a natural justification for the need of "batch normalization" in the practice of training deep neural networks <ref type="bibr" target="#b10">[IS15]</ref>. An alternative, arguably simpler, way to normalize the scale of learned representations is to ensure that the mapping of each layer of the network is approximately isometric [QYW + 20].</p><p>Once the representations are comparable, our goal becomes to learn a set of features Z(θ) = f (X, θ) and their partition Π (if not given in advance) such that they maximize the reduction between the coding rate of all features and that of the sum of features w.r.t. their classes:</p><formula xml:id="formula_9">max θ,Π ∆R Z(θ), Π, = R(Z(θ), ) − R c (Z(θ), | Π), s.t. Z j (θ) 2 F = m j , Π ∈ Ω. (8)</formula><p>We refer to this as the principle of maximal coding rate reduction (MCR 2 ), an embodiment of Aristotle's famous quote: "the whole is greater than the sum of the parts." Note that for the clustering purpose alone, one may only care about the sign of ∆R for deciding whether to partition the data or not, which leads to the greedy algorithm in <ref type="bibr" target="#b21">[MDHW07]</ref>. <ref type="bibr">17</ref> Here to seek or learn the best representation, we further desire the whole is maximally greater than its parts.</p><p>Relationship to information gain. The maximal coding rate reduction can be viewed as a generalization to Information Gain (IG), which aims to maximize the reduction of entropy of a random variable, say z, with respect to an observed attribute, say π: max π IG(z, π) . = H(z) − H(z | π), i.e., the mutual information between z and π [CT06]. Maximal information gain has been widely used in areas such as decision trees <ref type="bibr" target="#b28">[Qui86]</ref>. However, MCR 2 is used differently in several ways: 1) One typical setting of MCR 2 is when the data class labels are given, i.e. Π is known, MCR 2 focuses on learning representations z(θ) rather than fitting labels. 2) In traditional settings of IG, the number of attributes in z cannot be so large and their values are discrete (typically binary). Here the "attributes" Π represent the probability of a multi-class partition for all samples and their values can even be continuous. 3) As mentioned before, entropy H(z) or mutual information I(z, π) [HFLM + 18] is not well-defined for degenerate continuous distributions whereas the rate distortion R(z, ) is and can be accurately and efficiently computed for (mixed) subspaces, at least.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Properties of the Rate Reduction Function</head><p>In theory, the MCR 2 principle (8) benefits from great generalizability and can be applied to representations Z of any distributions with any attributes Π as long as the rates R and R c for the distributions can be accurately and efficiently evaluated. The optimal representation Z * and partition Π * should have some interesting geometric and statistical properties. We here reveal nice properties of the optimal representation with the special case of subspaces, which have many important use cases in machine learning. When the desired representation for Z is multiple subspaces, the rates R and R c in (8) are given by (4) and (5), respectively. At the maximal rate reduction, MCR 2 achieves its optimal representations, denoted as Z * = Z * 1 ∪ · · · ∪ Z * k ⊂ R d with rank(Z * j ) ≤ d j . One can show that Z * has the following desired properties (see Appendix A for a formal statement and detailed proofs).</p><p>Theorem 2.1 (Informal Statement). Suppose Z * = Z * 1 ∪ · · · ∪ Z * k is the optimal solution that maximizes the rate reduction (8). We have:</p><p>• Between-class Discriminative: As long as the ambient space is adequately large (d ≥ k j=1 d j ), the subspaces are all orthogonal to each other, i.e. (Z * i ) Z * j = 0 for i = j.</p><p>• Maximally Diverse Representation: As long as the coding precision is adequately high, i.e., 4 &lt; min j mj m</p><formula xml:id="formula_10">d 2 d 2 j</formula><p>, each subspace achieves its maximal dimension, i.e. rank(Z * j ) = d j . In addition, the largest d j − 1 singular values of Z * j are equal.</p><p>In other words, in the case of subspaces, the MCR 2 principle promotes embedding of data into multiple independent subspaces, with features distributed isotropically in each subspace (except for possibly one dimension). In addition, among all such discriminative representations, it prefers the one with the highest dimensions in the ambient space. This is substantially different from the objective of information bottleneck (2).</p><p>Comparison to the geometric OLE loss. To encourage the learned features to be uncorrelated between classes, the work of <ref type="bibr" target="#b20">[LQMS18]</ref> has proposed to maximize the difference between the nuclear norm of the whole Z and its subsets Z j , called the orthogonal low-rank embedding (OLE) loss:</p><formula xml:id="formula_11">max θ OLE(Z(θ), Π) . = Z(θ) * − k j=1 Z j (θ) * ,</formula><p>added as a regularizer to the cross-entropy loss (1). The nuclear norm · * is a nonsmooth convex 18 surrogate for low-rankness, whereas log det(·) is smooth concave instead. Unlike the rate reduction ∆R, OLE is always negative and achieves the maximal value 0 when the subspaces are orthogonal, regardless of their dimensions. So in contrast to ∆R, this loss serves as a geometric heuristic and does not promote diverse representations. In fact, OLE typically promotes learning one-dim representations per class, whereas MCR 2 encourages learning subspaces with maximal dimensions <ref type="figure" target="#fig_18">(Figure 7</ref> of [LQMS18] versus our <ref type="figure" target="#fig_17">Figure 6</ref>).</p><p>Relation to contrastive learning. If samples are evenly drawn from k classes, a randomly chosen pair (x i , x j ) is of high probability belonging to difference classes if k is large. <ref type="bibr">19</ref> We may view the learned features of two samples together with their their augmentations Z i and Z j as two classes. Then the rate reduction</p><formula xml:id="formula_12">∆R ij = R(Z i ∪ Z j , ) − 1 2 (R(Z i , ) + R(Z j , ))</formula><p>gives a "distance" measure for how far the two sample sets are. We may try to further "expand" pairs that likely belong to different classes. From Theorem 2.1, the (averaged) rate reduction ∆R ij is maximized when features from different samples are uncorrelated Z i Z j = 0 (see <ref type="figure" target="#fig_1">Figure 2</ref>) and features Z i from the same sample are highly correlated. Hence, when applied to sample pairs, MCR 2 naturally conducts the 18 Nonsmoothness poses additional difficulties in using this loss to learn features via gradient descent. <ref type="bibr">19</ref> For example, when k ≥ 100, a random pair is of probability 99% belonging to different classes. </p><formula xml:id="formula_13">∆R (test) R (train) R (test) R c (train) R c (test) (b)</formula><p>Training loss versus testing loss.   so-called contrastive learning [HCL06, OLV18, HFW + 19]. But MCR 2 is not limited to expand (or compress) pairs of samples and can uniformly conduct "contrastive learning" for a subset with any number of samples as long as we know they likely belong to different (or the same) classes, say by randomly sampling subsets from a large number of classes or with a good clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments with Instantiations of MCR 2</head><p>Our theoretical analysis above shows how the maximal coding rate reduction (MCR 2 ) is a principled measure for learning discriminative and diverse representations for mixed data. In this section, we demonstrate experimentally how this principle alone, without any other heuristics, is adequate to learning good representations in the supervised, self-supervised, and unsupervised learning settings in a unified fashion. Due to limited space and time, instead of trying to exhaust all its potential and practical implications with extensive engineering, our goal here is only to validate effectiveness of this principle through its most basic usage and fair comparison with existing frameworks. More implementation details and experiments are given in Appendix B. The code can be found in https: //github.com/ryanchankh/mcr2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supervised Learning of Robust Discriminative Features</head><p>Supervised learning via rate reduction. When class labels are provided during training, we assign the membership (diagonal) matrix Π = {Π j } k j=1 as follows: for each sample x i with label j, set Π j (i, i) = 1 and Π l (i, i) = 0, ∀l = j. Then the mapping f (·, θ) can be learned by optimizing (8), where Π remains constant. We apply stochastic gradient descent to optimize MCR 2 , and for each iteration we use mini-batch data {(x i , y i )} m i=1 to approximate the MCR 2 loss. Evaluation via classification. As we will see, in the supervised setting, the learned representation has very clear subspace structures. So to evaluate the learned representations, we consider a natural nearest subspace classifier. For each class of learned features Z j , let µ j ∈ R p be its mean and U j ∈ R p×rj be the first r j principal components for Z j , where r j is the estimated dimension of class j. The predicted label of a test data x is given by j = arg min j∈{1,...,</p><formula xml:id="formula_14">k} (I − U j U j )(f (x , θ) − µ j ) 2 2 .</formula><p>Experiments on real data. We consider CIFAR10 dataset <ref type="bibr" target="#b15">[Kri09]</ref> and ResNet-18 <ref type="bibr" target="#b9">[HZRS16]</ref> for f (·, θ). We replace the last linear layer of ResNet-18 by a two-layer fully connected network with ReLU activation function such that the output dimension is 128. We set the mini-batch size as m = 1, 000 and the precision parameter 2 = 0.5. More results can be found in Appendix B.3.2. , our features are not only orthogonal but also of much higher dimension. We compare the singular values of representations, both overall data and individual classes, learned by using cross-entropy and MCR 2 in <ref type="figure" target="#fig_17">Figure 6</ref> and <ref type="figure" target="#fig_18">Figure 7</ref> in Appendix B.3.1. We find that the representations learned by using MCR 2 loss are much more diverse than the ones learned by using cross-entropy loss. In addition, we find that we are able to select diverse images from the same class according to the "principal" components of the learned features (see <ref type="figure" target="#fig_19">Figure 8</ref> and <ref type="figure" target="#fig_21">Figure 9</ref> in Appendix B.3.1).</p><p>Robustness to corrupted labels. Because MCR 2 by design encourages richer representations that preserves intrinsic structures from the data X, training relies less on class labels than traditional loss such as cross-entropy (CE). To verify this, we train the same network 20 using both CE and MCR 2 with certain ratios of randomly corrupted training labels. <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the learning process: for different levels of corruption, while the rate for the whole set always converges to the same value, the rates for the classes are inversely proportional to the ratio of corruption, indicating our method only compress samples with valid labels. The classification results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. By applying exact the same training parameters, MCR 2 is significantly more robust than CE, especially with higher ratio of corrupted labels. This can be an advantage in the settings of self-supervised learning or constrastive learning when the grouping information can be very noisy. , we use the MCR 2 principle to learn representations that are invariant to certain class of transformations/augmentations, say T with a distribution P T . Given a mini-batch of data {x j } k j=1 , we augment each sample x j with n transformations/augmentations {τ i (·)} n i=1 randomly drawn from P T . We simply label all the augmented samples X j = [τ 1 (x j ), . . . , τ n (x j )] of x j as the j-th class, and Z j the corresponding learned features. Using this self-labeled data, we train our feature mapping f (·, θ) the same way as the supervised setting above. For every mini-batch, the total number of samples for training is m = kn.</p><p>Evaluation via clustering. To learn invariant features, our formulation itself does not require the original samples x j come from a fixed number of classes. For evaluation, we may train on a few classes and observe how the learned features facilitate classification or clustering of the data. A common method to evaluate learned features is to train an additional linear classifier [OLV18, HFW + 19], with ground truth labels. But for our purpose, because we explicitly verify whether the so-learned invariant features have good subspace structures when the samples come from k classes, we use an off-the-shelf subspace clustering algorithm EnSC [YLRV16], which is computationally efficient and is provably correct for data with well-structured subspaces. We also use K-Means on the original data X as our baseline for comparison. We use normalized mutual information (NMI), clustering accuracy (ACC), and adjusted rand index (ARI) for our evaluation metrics, see Appendix B.3.4 for their detailed definitions.</p><p>Controlling dynamics of expansion and compression. By directly optimizing the rate reduction ∆R = R − R c , we achieve 0.570 clustering accuracy on CIFAR10 dataset, which is the second best 20 Both CE and MCR 2 can have better performance by choosing larger models for our mapping.    Experiments on real data. Similar to the supervised learning setting, we train exactly the same ResNet-18 network on the CIFAR10, CIFAR100, and STL10 [CNL11] datasets. We set the minibatch size as k = 20, number of augmentations for each sample as n = 50 and the precision parameter as 2 = 0.5. Nevertheless, compared to the representations learned in the supervised setting where the optimal partition Π in (8) is initialized by correct class information, the representations here learned with self-supervised classes are far from being optimal 21 -they at best correspond to local maxima of the MCR 2 objective (8) when θ and Π are jointly optimized. It remains wide open how to design better optimization strategies and dynamics to learn from unlabelled or partially-labelled data better representations (and the associated partitions) close to the global maxima of the MCR 2 objective (8).</p><formula xml:id="formula_15">. = 1 2γ1 log det(I + γ2d m 2 ZZ ). With γ 1 = γ 2 = k,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>This work provides rigorous theoretical justifications and clear empirical evidences for why the maximal coding rate reduction (MCR 2 ) is a fundamental principle for learning discriminative low-dim representations in almost all learning settings. It unifies and explains existing effective frameworks and heuristics widely practiced in the (deep) learning literature. It remains open why MCR 2 is robust to label noises in the supervised setting, why self-learned features with MCR 2 alone are effective for clustering, and how in future practice instantiations of this principle can be systematically harnessed to further improve clustering or classification tasks.</p><p>We believe that MCR 2 gives a principled and practical objective for (deep) learning and can potentially lead to better design operators and architectures of a deep network. A potential direction is to monitor quantitatively the amount of rate reduction ∆R gained through every layer of the deep network. By optimizing the rate reduction through the network layers, it is no longer engineered as a "black box."</p><p>On the learning theoretical aspect, although this work has demonstrated only with mixed subspaces, this principle applies to any mixed distributions or structures, for which configurations that achieve maximal rate reduction are of independent theoretical interest. Another interesting note is that the MCR 2 formulation goes beyond the supervised multi-class learning setting often studied through empirical risk minimization (ERM) <ref type="bibr" target="#b1">[DSBDSS15]</ref>. It is more related to the expectation maximization (EMX) framework [BDHM + 17], in which the notion of "compression" plays a crucial role for purely theoretical analysis. We hope this work provides a good connection between machine learning theory and its practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Yi would like to thank Professor Yann LeCun of New York University for having a stimulating discussion in his NYU office last November about the search for a proper "energy" function for features to be learned by a deep network [LCH + 06], during the preparation of a joint proposal. Professor John Wright of Columbia University, who was the leading author of the first two papers on the lossy coding approach to clustering and classification [MDHW07, WTL + 08], has provided valuable insights and suggestions during germination of this work. We would like to thank Professor Emmanuel Candés of Stanford University for having an online discussion with Yi, during the pandemic, about the rate distortion function for low-dimensional structures. Yi also likes to thank Professor Zhi Ding of UC Davis for discussing the role of rate distortion and lossy coding in communications and information theory and for providing us some pertinent references.</p><p>Professor Shankar Sastry of UC Berkeley has always encouraged us to look into fundamental connections between low-dimensional subspaces and deep learning from the perspective of Generalized PCA [VMS16]. Coincidentally, this work was partly motivated to address an inquiry from Professor Ruzena Bajcsy of UC Berkeley earlier this year on how to clarify the role of "latent features" learned in a network in a principled manner. We would also like to thank Professor Jiantao Jiao and Professor Jacob Steinhardt of UC Berkeley for extensive discussions about how to make deep learning robust. During the preparation of this manuscript, Dr. Harry Shum, who collaborated with Yi on lossy coding during his visit to Microsoft Research Asia in 2007 [WTL + 08], has given excellent suggestions on how to better visualize the learned features, leading to some of the interesting illustrations in this work.</p><p>Yaodong would like to thank Zitong Yang and Xili Dai for helpful discussions on the log det(·) function. Ryan would like to thank Yuexiang Zhai for helpful discussions on learning subspace structures. Last but not the least, we are very grateful for Xili Dai and Professor Xiaojun Yuan of UESTC and Professor Hao Chen of UC Davis who have generously provided us their GPU clusters to help us conduct the extensive experiments reported in this paper.</p><p>[BDHM + 17] Shai Ben-David, Pavel Hrubes, Shay Moran, Amir Shpilka, and Amir Yehudayoff. A learning problem that is independent of the set theory ZFC axioms, 2017.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Properties of the Rate Reduction Function</head><p>This section is organized as follows. We present background and preliminary results for the log det(·) function and the coding rate function in Section A.1. Then, Section A.2 and A.3 provide technical lemmas for bounding the coding rate and coding rate reduction functions, respectively. Such lemmas are key results for proving our main theoretical results, which are stated informally in Theorem 2.1 and formally in Section A.4. Finally, proof of our main theoretical results is provided in Section A.5.</p><p>Notations Throughout this section, we use S d ++ , R + and Z ++ to denote the set of symmetric positive definite matrices of size d × d, nonnegative real numbers and positive integers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preliminaries</head><p>Properties of the log det(·) function.</p><formula xml:id="formula_16">Lemma A.1. The function log det(·) : S d ++ → R is strictly concave. That is, log det((1 − α)Z 1 + αZ 2 )) ≥ (1 − α) log det(Z 1 ) + α log det(Z 2 )</formula><p>for any α ∈ (0, 1) and {Z 1 , Z 2 } ⊆ S d ++ , with equality holds if and only if Z 1 = Z 2 .</p><p>Proof. Consider an arbitrary line given by Z = Z 0 + t∆Z where Z 0 and ∆Z = 0 are symmetric matrices of size d × d. Let f (t) . = log det(Z 0 + t∆Z) be a function defined on an interval of values of t for which Z 0 + t∆Z ∈ S d ++ . Following the same argument as in [BV04], we may assume Z 0 ∈ S d ++ and get</p><formula xml:id="formula_17">f (t) = log det Z 0 + d i=1 log(1 + tλ i ), where {λ i } d i=1 are eigenvalues of Z − 1 2 0 ∆ZZ − 1 2 0 .</formula><p>The second order derivative of f (t) is given by</p><formula xml:id="formula_18">f (t) = − d i=1 λ 2 i (1 + tλ i ) 2 &lt; 0.</formula><p>Therefore, f (t) is strictly concave along the line Z = Z 0 + t∆Z. By definition, we conclude that log det(·) is strictly concave.</p><p>Properties of the coding rate function. The following properties, also known as the Sylvester's determinant theorem, for the coding rate function are known in the paper <ref type="bibr" target="#b21">[MDHW07]</ref>. Lemma A.2 (Commutative property <ref type="bibr" target="#b21">[MDHW07]</ref>). For any Z ∈ R d×m we have </p><formula xml:id="formula_19">R(Z, ) . = 1 2 log det I + d m 2 ZZ = 1 2 log det I + d m 2 Z Z .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Lower and Upper Bounds for Coding Rate</head><p>The following result provides an upper and a lower bound on the coding rate of Z as a function of the coding rate for its components {Z j } k j=1 . The lower bound is tight when all the components {Z j } k j=1 have the same covariance (assuming that they have zero mean). The upper bound is tight when the components {Z j } k j=1 are pair-wise orthogonal.</p><p>Lemma A.4. For any {Z j ∈ R d×mj } k j=1 and any &gt; 0, let</p><formula xml:id="formula_20">Z = [Z 1 , · · · , Z k ] ∈ R d×m with m = k j=1 m j . We have k j=1 m j 2 log det I + d m j 2 Z j Z j ≤ m 2 log det I + d m 2 ZZ ≤ k j=1 m 2 log det I + d m 2 Z j Z j ,<label>(9)</label></formula><p>where the first equality holds if and only if</p><formula xml:id="formula_21">Z 1 Z 1 m 1 = Z 2 Z 2 m 2 = · · · = Z k Z k m k ,</formula><p>and the second equality holds if and only if Z j1 Z j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k.</p><p>Proof. By Lemma A.1, log det(·) is strictly concave. Therefore,</p><formula xml:id="formula_22">log det k j=1 α j S j ≥ k j=1 α j log det(S j ), for all {α j &gt; 0} k j=1 , k j=1 α j = 1 and {S j ∈ S d ++ } k j=1 ,</formula><p>where equality holds if and only if S 1 = S 2 = · · · = S k . Take α j = mj m and S j = I + d mj 2 Z j Z j , we get </p><formula xml:id="formula_23">m 2 log det I + d m 2 ZZ ≥ k j=1 m j 2 log det I + d m j 2 Z j Z j ,</formula><p>We now take</p><formula xml:id="formula_25">Q = I + d m 2 Z Z = I + d m 2      Z 1 Z 1 Z 1 Z 2 · · · Z 1 Z k Z 2 Z 1 Z 2 Z 2 · · · Z 2 Z 2 . . . . . . . . . . . . Z k Z 1 Z k Z 2 · · · Z k Z k     </formula><p>, and (11)</p><formula xml:id="formula_26">S = I + d m 2      Z 1 Z 1 0 · · · 0 0 Z 2 Z 2 · · · 0 . . . . . . . . . . . . 0 0 · · · Z k Z k      .</formula><p>From the property of determinant for block diagonal matrix, we have</p><formula xml:id="formula_27">log det(S) = k j=1 log det I + d m 2 Z j Z j .<label>(12)</label></formula><p>Also, note that tr(S −1 Q)</p><formula xml:id="formula_28">= tr    (I + d m 2 Z 1 Z 1 ) −1 (I + d m 2 Z 1 Z 1 ) · · · (I + d m 2 Z 1 Z 1 ) −1 (I + d m 2 Z 1 Z k ) . . .</formula><p>. . . . . .</p><formula xml:id="formula_29">(I + d m 2 Z k Z k ) −1 (I + d m 2 Z k Z 1 ) · · · (I + d m 2 Z k Z k ) −1 (I + d m 2 Z k Z k )    = tr    I · · · * . . . . . . . . . * · · · I    = m,<label>(13)</label></formula><p>where "*" denotes nonzero quantities that are irrelevant for the purpose of computing the trace. Plugging (12) and (13) back in (10) gives</p><formula xml:id="formula_30">m 2 log det I + d m 2 Z Z ≤ k j=1 m 2 log det I + d m 2 Z j Z j ,</formula><p>where the equality holds if and only if Q = S, which by the formulation in (11), holds if and only if Z j1 Z j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k. Further using the result in Lemma A.2 gives</p><formula xml:id="formula_31">m 2 log det I + d m 2 ZZ ≤ k j=1 m 2 log det I + d m 2 Z j Z j ,</formula><p>which produces the upper bound in (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 An Upper Bound on Coding Rate Reduction</head><p>We may now provide an upper bound on the coding rate reduction ∆R(Z, Π, ) (defined in (8)) in terms of its individual components {Z j } k j=1 . Lemma A.5. For any Z ∈ R d×m , Π ∈ Ω and &gt; 0, let Z j ∈ R d×mj be ZΠ j with zero columns removed. We have</p><formula xml:id="formula_32">∆R(Z, Π, ) ≤ k j=1 1 2m log   det m I + d m 2 Z j Z j det mj I + d mj 2 Z j Z j   ,<label>(14)</label></formula><p>with equality holds if and only if Z j1 Z j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k.</p><p>Proof. From (4), (5) and (6), we have</p><formula xml:id="formula_33">∆R(Z, Π, ) = R(Z, ) − R c (Z, | Π) = 1 2 log det I + d m 2 ZZ − k j=1 tr(Π j ) 2m log det I + d ZΠ j Z tr(Π j ) 2 = 1 2 log det I + d m 2 ZZ − k j=1 m j 2m log det I + d Z j Z j m j 2 ≤ k j=1 1 2 log det I + d m 2 Z j Z j − k j=1 m j 2m log det I + d Z j Z j m j 2 = k j=1 1 2m log det m I + d m 2 Z j Z j − k j=1 1 2m log det mj I + d Z j Z j m j 2 = k j=1 1 2m log   det m I + d m 2 Z j Z j det mj I + d mj 2 Z j Z j   ,</formula><p>where the inequality follows from the upper bound in Lemma A.4, and that the equality holds if and only if Z j1 Z j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Main Results: Properties of Maximal Coding Rate Reduction</head><p>We now present our main theoretical results. The following theorem states that for any fixed encoding of the partition Π, the coding rate reduction is maximized by data Z that is maximally discriminative between different classes and is diverse within each of the classes. This result holds provided that the sum of rank for different classes is small relative to the ambient dimension, and that is small. Theorem A.6. Let Π = {Π j ∈ R m×m } k j=1 with {Π j ≥ 0} k j=1 and Π 1 + · · · + Π k = I be a given set of diagonal matrices whose diagonal entries encode the membership of the m samples in the k classes. Given any &gt; 0, d &gt; 0 and {d ≥ d j &gt; 0} k j=1 , consider the optimization problem Z * ∈ arg max where Z * j ∈ R d×tr(Π j ) denotes Z * Π j with zero columns removed.</p><formula xml:id="formula_34">Z∈R d×m ∆R(Z, Π, ) s.t. ZΠ j 2 F = tr(Π j ), rank(ZΠ j ) ≤ d j , ∀j ∈ {1, . . . , k}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Main Results</head><p>We start with presenting a lemma that will be used in the proof to Theorem A.6. Lemma A.7. Given any twice differentiable f : R + → R, integer r ∈ Z ++ and c ∈ R + , consider the optimization problem</p><formula xml:id="formula_35">max x r p=1 f (x p ) s.t. x = [x 1 , . . . , x r ] ∈ R r + , x 1 ≥ x 2 ≥ · · · ≥ x r , and r p=1</formula><p>x p = c. Proof. The result holds trivially if r = 1. Throughout the proof we consider the case where r &gt; 1.</p><p>We consider the optimization problem with the inequality constraint x 1 ≥ · · · ≥ x r in <ref type="formula" target="#formula_0">(16)</ref>  x p = c.</p><p>(17)</p><p>We need to show that any global solution x * = [x * 1 , . . . , x * r ] to (17) is either x * = [ c r , . . . , c r ] or x * = [x H , . . . , x H , x L ] · P for some x H &gt; c r , x L &gt; 0 and permutation matrix P ∈ R r×r . Let x * q = c, (18)</p><formula xml:id="formula_36">L(x, λ) = r p=1 f (x p ) − λ 0 · r p=1 x p − c − r p=1 λ p x</formula><formula xml:id="formula_37">x * q ≥ 0, ∀q ∈ {1, . . . , r}, (19) λ * q ≥ 0, ∀q ∈ {1, . . . , r}, (20) λ * q · x * q = 0, ∀q ∈ {1, . . . , r}, and (21) [f (x * 1 ), . . . , f (x * r )] = [λ * 0 , . . . , λ * 0 ] + [λ * 1 , . . . , λ * r ].<label>(22)</label></formula><p>By using the KKT conditions, we first show that all entries of x * are strictly positive. To prove by contradiction, suppose that x * has r 0 nonzero entries and r − r 0 zero entries for some 1 ≤ r 0 &lt; r. Note that r 0 ≥ 1 since an all zero vector x * does not satisfy the equality constraint (18).</p><p>Without loss of generality, we may assume that x * p &gt; 0 for p ≤ r 0 and x * p = 0 otherwise. By (21), we have λ * 1 = · · · = λ * r0 = 0. Plugging it into (22), we get f (x * 1 ) = · · · = f (x * r0 ) = λ * 0 . From (22) and noting that x r0+1 = 0 we get</p><formula xml:id="formula_38">f (0) = f (x r0+1 ) = λ * 0 + λ * r0+1 .</formula><p>Finally, from (20), we have λ * r0+1 ≥ 0. Combining the last three equations above gives f (0) − f (x * 1 ) ≥ 0, contradicting the assumption that f (0) &lt; f (x) for all x &gt; 0. This shows that r 0 = r, i.e., all entries of x * are strictly positive. Using this fact and (21) gives λ * p = 0 for all p ∈ {1, . . . , r}. Combining this with (22) gives</p><formula xml:id="formula_39">f (x * 1 ) = · · · = f (x * r ) = λ * 0 .<label>(23)</label></formula><p>It follows from the fact that f (x) is strictly unimodal that</p><formula xml:id="formula_40">∃ x H ≥ x L &gt; 0 s.t. {x * p } r p=1 ⊆ {x L , x H }.<label>(24)</label></formula><p>That is, the set {x * p } r p=1 may contain no more than two values. To see why this is true, suppose that there exists three distinct values for {x * p } r p=1 . Without loss of generality we may assume that 0 &lt; x * 1 &lt; x * 2 &lt; x * 3 . If x * 2 ≤ x T (recall x T := arg max x≥0 f (x)), then by using the fact that f (x) is strictly increasing in [0, x T ], we must have f (x * 1 ) &lt; f (x * 2 ) which contradicts (23). A similar contradiction is arrived by considering f (x *</p><p>2 ) and f (x * 3 ) for the case where x * 2 &gt; x T . There are two possible cases as a consequence of (24). First, if x L = x H , then we have x * 1 = · · · = x * r . By further using (18) we get</p><p>x * 1 = · · · = x * r = c r .</p><p>It remains to consider the case where x L &lt; x H . First, by the unimodality of f (x), we must have</p><formula xml:id="formula_41">x L &lt; x T &lt; x H , therefore f (x L ) &gt; 0 and f (x H ) &lt; 0.</formula><p>(25) Let := |{p : x p = x L }| be the number of entries of x * that are equal to x L and h := r − . We show that it is necessary to have = 1 and h = r − 1. To prove by contradiction, assume that &gt; 1 and h &lt; r − 1. Without loss of generality we may assume {x * p = x H } h p=1 and {x * p = x L } r p=h+1 . By (25), we have f (x * p ) &gt; 0 for all p &gt; h. In particular, by using h &lt; r − 1 we have f (x * r−1 ) &gt; 0 and f (x * r ) &gt; 0.</p><p>On the other hand, by using the second order necessary conditions for constraint optimization (see, e.g., [NW06, Theorem 12.5]), the following result holds</p><formula xml:id="formula_43">v ∇ xx L(x * , λ * )v ≤ 0, for all v : ∇ x r p=1 x * p − c , v = 0 ⇐⇒ r p=1 f (x * p ) · v 2 p ≤ 0, for all v = [v 1 , . . . , v r ] : r p=1 v p = 0 .<label>(27)</label></formula><p>Take v to be such that v 1 = · · · = v r−2 = 0 and v r−1 = −v r = 0. Plugging it into <ref type="formula" target="#formula_8">(27)</ref> gives f (x * r−1 ) + f (x * r ) ≤ 0, which contradicts (26). Therefore, we may conclude that = 1. That is, x * is given by</p><formula xml:id="formula_44">x * = [x H , . . . , x H , x L ], where x H &gt; x L &gt; 0.</formula><p>By using the condition in (18), we may further show that</p><formula xml:id="formula_45">(r − 1)x H + x L = c =⇒ x H = c r − 1 − c x L &lt; x L r − 1 , (r − 1)x H + x L = c =⇒ (r − 1)x H + x H &gt; c =⇒ x H &gt; c r ,</formula><p>which completes our proof.</p><p>Proof of Theorem A.6. Without loss of generality, let Z * = [Z * 1 , . . . , Z * k ] be the optimal solution of problem (15).</p><p>To show that Z * j , j ∈ {1, . . . , k} are pairwise orthogonal, suppose for the purpose of arriving at a contradiction that (Z * j1 ) Z * j2 = 0 for some 1 ≤ j 1 &lt; j 2 ≤ k. By using Lemma A.5, the strict inequality in (14) holds for the optimal solution Z * . That is,</p><formula xml:id="formula_46">∆R(Z * , Π, ) &lt; k j=1 1 2m log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   .<label>(28)</label></formula><p>On the other hand, since</p><formula xml:id="formula_47">k j=1 d j ≤ d, there exists {U j ∈ R d×dj } k j=1 such that the columns of the matrix [U 1 , . . . , U k ] are orthonormal. Denote Z * j = U * j Σ * j (V * j ) the compact SVD of Z * j , and let Z = [Z 1 , . . . , Z k ], where Z j = U j Σ * j (V * j ) . It follows that (Z j1 ) Z j2 = V * j1 Σ * j1 (U j1 ) U j2 Σ * j2 (V * j2 ) = V * j1 Σ * j1 0Σ * j2 (V * j2 ) = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k.</formula><p>That is, the matrices Z 1 , . . . , Z k are pairwise orthogonal. Applying Lemma A.5 for Z gives</p><formula xml:id="formula_48">∆R(Z , Π, ) = k j=1 1 2m log   det m I + d m 2 Z j (Z j ) det mj I + d mj 2 Z j (Z j )   = k j=1 1 2m log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   ,<label>(29)</label></formula><p>where the second equality follows from Lemma A.3. Comparing (28) and (29) gives ∆R(Z , Π, ) &gt; ∆R(Z * , Π, ), which contradicts the optimality of Z * . Therefore, we must have</p><formula xml:id="formula_49">(Z * j1 ) Z * j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k. Moreover, from Lemma A.3 we have ∆R(Z * , Π, ) = k j=1 1 2m log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   .<label>(30)</label></formula><p>We now prove the result concerning the singular values of Z * j . To start with, we claim that the following result holds:</p><formula xml:id="formula_50">Z * j ∈ arg max Zj log   det m I + d m 2 Z j Z j det mj I + d mj 2 Z j Z j   s.t. Z j 2 F = m j , rank(Z j ) ≤ d j .<label>(31)</label></formula><p>To see why (31) holds, suppose that there exists Z j such that Z j</p><formula xml:id="formula_51">2 F = m j , rank( Z j ) ≤ d j and log   det m I + d m 2 Z j Z j det mj I + d mj 2 Z j Z j   &gt; log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   .<label>(32)</label></formula><p>Denote Z j = U j Σ j V j the compact SVD of Z j and let</p><formula xml:id="formula_52">Z = [Z * 1 , . . . , Z * j−1 , Z j , Z * j+1 , . . . , Z * k ], where Z j := U * j Σ j V j . Note that Z j 2 F = m j , rank(Z j ) ≤ d j and (Z j ) Z * j = 0 for all j = j.</formula><p>It follows that Z is a feasible solution to (15) and that the components of Z are pairwise orthogonal. By using Lemma A.5, Lemma A.3 and (32) we have</p><formula xml:id="formula_53">∆R(Z , Π, ) = 1 2m log   det m I + d m 2 Z j (Z j ) det mj I + d mj 2 Z j (Z j )   + j =j 1 2m log   det m I + d m 2 Z * j (Z * j ) det m j I + d m j 2 Z * j (Z * j )   = 1 2m log   det m I + d m 2 Z j ( Z j ) det mj I + d mj 2 Z j ( Z j )   + j =j 1 2m log   det m I + d m 2 Z * j (Z * j ) det m j I + d m j 2 Z * j (Z * j )   &gt; 1 2m log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   + j =j 1 2m log   det m I + d m 2 Z * j (Z * j ) det m j I + d m j 2 Z * j (Z * j )   = k j=1 1 2m log   det m I + d m 2 Z * j (Z * j ) det mj I + d mj 2 Z * j (Z * j )   .</formula><p>Combining it with (30) shows ∆R(Z , Π, ) &gt; ∆R(Z * , Π, ), contradicting the optimality of Z * . Therefore, the result in (31) holds.</p><p>Observe that the optimization problem in (31) depends on Z j only through its singular values. That is, by letting σ j := [σ 1,j , . . . , σ min(mj ,d),j ] be the singular values of Z j , we have</p><formula xml:id="formula_54">log   det m I + d m 2 Z j Z j det mj I + d mj 2 Z j Z j   = min{mj ,d} p=1 log (1 + d m 2 σ 2 p,j ) m (1 + d mj 2 σ 2 p,j ) mj ,</formula><p>also, we have</p><formula xml:id="formula_55">Z j 2 F = min{mj ,d} p=1</formula><p>σ 2 p,j and rank(Z j ) = σ j 0 .</p><p>Using these relations, (31) is equivalent to</p><formula xml:id="formula_56">max σj ∈R min{m j ,d} + min{mj ,d} p=1 log (1 + d m 2 σ 2 p,j ) m (1 + d mj 2 σ 2 p,j ) mj s.t.</formula><p>min{mj ,d} p=1 σ 2 p,j = m j , and rank(Z j ) = σ j 0 (33)</p><p>Let σ * j = [σ * 1,j , . . . , σ * min{mj ,d},j ] be an optimal solution to (33). Without loss of generality we assume that the entries of σ * j are sorted in descending order. It follows that σ * p,j = 0 for all p &gt; d j , and</p><formula xml:id="formula_57">[σ * 1,j , . . . , σ * dj ,j ] = arg max [σ1,j ,...,σ d j ,j ]∈R d j + σ1,j ≥···≥σ d j ,j dj p=1 log (1 + d m 2 σ 2 p,j ) m (1 + d mj 2 σ 2 p,j ) mj s.t. dj p=1 σ 2 p,j = m j . (34)</formula><p>Then we define As proved in Theorem A.6, the proposed MCR 2 objective promotes within-class diversity. In this section, we use simulated data to verify the diversity promoting property of MCR 2 . As shown in <ref type="table">Table 3</ref>, we calculate our proposed MCR 2 objective on simulated data. We observe that orthogonal subspaces with higher dimension achieve higher MCR 2 value, which is consistent with our theoretical analysis in Theorem A.6. <ref type="table">Table 3</ref>: MCR 2 objective on simulated data. We evaluate the proposed MCR 2 objective defined in (8),</p><formula xml:id="formula_58">f (x; d, , m j , m) = log (1 + d m 2 x) m (1 + d mj 2 x) mj ,</formula><p>including R, R c , and ∆R, on simulated data. The output dimension d is set as 512, 256, and 128. We set the batch size as m = 1000 and random assign the label of each sample from 0 to 9, i.e., 10 classes. We generate two types of data: 1) (RANDOM GAUSSIAN) For comparison with data without structures, for each class we generate random vectors sampled from Gaussian distribution (the dimension is set as the output dimension d) and normalize each vector to be on the unit sphere. 2) (SUBSPACE) For each class, we generate vectors sampled from its corresponding subspace with dimension dj and normalize each vector to be on the unit sphere. We consider the subspaces from different classes are orthogonal/nonorthogonal to each other.  <ref type="bibr">22</ref> For data augmentation in the supervised setting, we apply the RandomCrop and RandomHorizontalFlip. For the supervised setting, we train the models for 500 epochs and use stage-wise learning rate decay every 200 epochs (decay by a factor of 10). For the supervised setting, we train the models for 100 epochs and use stage-wise learning rate decay at 20-th epoch and 40-th epoch (decay by a factor of 10).</p><p>Evaluation Details. For the supervised setting, we set the number of principal components for nearest subspace classifier r j = 30. We also study the effect of r j in Section B.3.2. For the CIFAR100 dataset, we consider 20 superclasses and set the cluster number as 20, which is the same setting as in [CWM + 17, WXYL18].</p><p>Datasets. We apply the default datasets in PyTorch, including CIFAR10, CIFAR100, and STL10.</p><p>Augmentations T used for the self-supervised setting. We apply the same data augmentation for CIFAR10 dataset and CIFAR100 dataset and the pseudo-code is as follows. Cross-entropy training details. For CE models presented in <ref type="table" target="#tab_1">Table 1</ref>, <ref type="figure" target="#fig_17">Figure 6</ref>(d)-6(f), and <ref type="figure" target="#fig_18">Figure 7</ref>, we use the same network architecture, ResNet-18 <ref type="bibr" target="#b9">[HZRS16]</ref>, for cross-entropy training on CIFAR10, and set the output dimension as 10 for the last layer. We apply SGD, and set learning rate lr=0.1, momentum momentum=0.9, and weight decay wd= 5e-4. We set the total number of training epoch as 400, and use stage-wise learning rate decay every 150 epochs (decay by a factor of 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 PCA Results of MCR 2 Training versus Cross-Entropy Training</head><p>For comparison, similar to <ref type="figure" target="#fig_4">Figure 3</ref>(c), we calculate the principle components of representations learned by MCR 2 training and cross-entropy training. For cross-entropy training, we take the output of the second last layer as the learned representation. The results are summarized in <ref type="figure" target="#fig_17">Figure 6</ref>. We also compare the cosine similarity between learned representations for both MCR 2 training and cross-entropy training, and the results are presented in <ref type="figure" target="#fig_18">Figure 7</ref>.</p><p>As shown in <ref type="figure" target="#fig_17">Figure 6</ref>, we observe that representations learned by MCR 2 are much more diverse, the dimension of learned features (each class) is around a dozen, and the dimension of the overall features is nearly 120, and the output dimension is 128. In contrast, the dimension of the overall features learned using entropy is slightly greater than 10, which is much smaller than that learned by MCR 2 . From <ref type="figure" target="#fig_18">Figure 7</ref>, for MCR 2 training, we find that the features of different class are almost orthogonal.</p><p>Visualize representative images selected from CIFAR10 dataset by using MCR 2 . As mentioned in Section 1, obtaining the properties of desired representation in the proposed MCR 2 principle is equivalent to performing nonlinear generalized principle components on the given dataset. As shown in <ref type="figure" target="#fig_17">Figure 6</ref>(a)-6(c), MCR 2 can indeed learn such diverse and discriminative representations. In order to better interpret the representations learned by MCR 2 , we select images according to their "principal" components (singular vectors using SVD) of the learned features. In <ref type="figure" target="#fig_19">Figure 8</ref>, we visualize images selected from class-'Bird' and class-'Ship'. For each class, we first compute top-10 singular     vectors of the SVD of the learned features and then for each of the top singular vectors, we display in each row the top-10 images whose corresponding features are closest to the singular vector. As shown in <ref type="figure" target="#fig_19">Figure 8</ref>, we observe that images in the same row share many common characteristics such as shapes, textures, patterns, and styles, whereas images in different rows are significantly different from each other -suggesting our method captures all the different "modes" of the data even within the same class. Notice that top rows are associated with components with larger singular values, hence they are images that show up more frequently in the dataset.</p><p>In <ref type="figure" target="#fig_21">Figure 9</ref>(a), we visualize the 10 "principal" images selected from CIFAR10 for each of the 10 classes. That is, for each class, we display the 10 images whose corresponding features are most coherent with the top-10 singular vectors. We observe that the selected images are much more diverse and representative than those selected randomly from the dataset (displayed on the CIFAR official website), indicating such principal images can be used as a good "summary" of the dataset. we first compute the top-10 singular vectors of the SVD of the learned features Zj. Then for the l-th singular vector of class j, u l j , and for the feature of the i-th image of class j, z i j , we calculate the absolute value of inner product, | z i j , u l j |, then we select the top-10 images according to | z i j , u l j | for each singular vector. In the above two figures, each row corresponds to one singular vector (component C l ). The rows are sorted based on the magnitude of the associated singular values, from large to small.  we first compute the top-10 singular vectors of the SVD of the learned features Zj. Then for the l-th singular vector of class j, u l j , and for the feature of the i-th image of class j, z i j , we calculate the absolute value of inner product, | z i j , u l j |, then we select the largest one for each singular vector within class j. Each row corresponds to one class, and each image corresponds to one singular vector, ordered by the value of the associated singular value. Training details for mainline experiment. For the model presented in <ref type="figure" target="#fig_0">Figure 1 (Right)</ref> and <ref type="figure" target="#fig_4">Figure 3</ref>, we use ResNet-18 to parameterize f (·, θ), and we set the output dimension d = 128, precision 2 = 0.5, mini-batch size m = 1, 000. We use SGD in Pytorch [PGM + 19] as the optimizer, and set the learning rate lr=0.01, weight decay wd=5e-4, and momentum=0.9.</p><p>Experiments for studying the effect of hyperparameters and architectures. We present the experimental results of MCR 2 training in the supervised setting by using various training hyperparameters and different network architectures. The results are summarized in <ref type="table" target="#tab_10">Table 4</ref>. Besides the ResNet architecture, we also consider VGG architecture [SZ15] and ResNext achitecture [XGD + 17]. From <ref type="table" target="#tab_10">Table 4</ref>, we find that larger batch size m can lead to better performance. Also, models with higher output dimension d require larger training batch size m. Effect of r j on classification. Unless otherwise stated, we set the number of components r j = 30 for nearest subspace classification. We study the effect of r j when used for classification, and the results are summarized in <ref type="table" target="#tab_11">Table 5</ref>. We observe that the nearest subspace classification works for a wide range of r j . Effect of 2 on learning from corrupted labels. To further study the proposed MCR 2 on learning from corrupted labels, we use different precision parameters, 2 = 0.75, 1.0, in addition to the one shown in <ref type="table" target="#tab_1">Table 1</ref>. Except for the precision parameter 2 , all the other parameters are the same as the mainline experiment (the first row in <ref type="table" target="#tab_10">Table 4</ref>). The first row ( 2 = 0.5) in <ref type="table" target="#tab_12">Table 6</ref> is identical to the MCR 2 TRAINING in <ref type="table" target="#tab_2">Table 2</ref>. Notice that with slightly different choices in 2 , one might even see slightly improved performance over the ones reported in the main body. Training details of MCR 2 -CTRL. For three datasets (CIFAR10, CIFAR100, and STL10), we use ResNet-18 as in the supervised setting, and we set the output dimension d = 128, precision 2 = 0.5, mini-batch size k = 20, number of augmentations n = 50, γ 1 = γ 2 = 20. We observe that MCR 2 -CTRL can achieve better clustering performance by using smaller γ 2 , i.e., γ 2 = 15, on CIFAR10 and CIFAR100 datasets. We use SGD in Pytorch [PGM + 19] as the optimizer, and set the learning rate lr=0.1, weight decay wd=5e-4, and momentum=0.9.</p><p>Training dynamic comparison between MCR 2 and MCR 2 -CTRL . In the self-supervised setting, we compare the training process for MCR 2 and MCR 2 -CTRL in terms of R, R, R c , and ∆R. For MCR 2 training, the features first expand (for both R and R c ) then compress (for ). For MCR 2 -CTRL, both R and R c first compress then R expands quickly and R c remains small, as we have seen in <ref type="figure" target="#fig_8">Figure 5</ref> in the main body.</p><p>Clustering results comparison. We compare the clustering performance between MCR 2 and MCR 2 -CTRL in terms of NMI, ACC, and ARI. The clustering results are summarized in <ref type="table" target="#tab_13">Table 7</ref>. We find that MCR 2 -CTRL can achieve better performance for clustering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.4 Clustering Metrics and More Results</head><p>We first introduce the definitions of normalized mutual information (NMI) [SG02], clustering accuracy (ACC), and adjusted rand index (ARI) <ref type="bibr" target="#b5">[HA85]</ref>.</p><p>Normalized mutual information (NMI). Suppose Y is the ground truth partition and C is the prediction partition. The NMI metric is defined as</p><formula xml:id="formula_59">NMI(Y, C) = k i=1 s j=1 |Y i ∩ C j | log m|Yi∩Cj | |Yi||Cj | k i=1 |Y i | log |Yi| m s j=1 |C j | log |Cj | m ,</formula><p>where Y i is the i-th cluster in Y and C j is the j-th cluster in C, and m is the total number of samples.</p><p>Clustering accuracy (ACC). Given m samples, {(x i , y i )} m i=1 . For the i-th sample x i , let y i be its ground truth label, and let c i be its cluster label. The ACC metric is defined as</p><formula xml:id="formula_60">ACC(Y , C) = max σ∈S m i=1 1{y i = σ(c i )} m ,</formula><p>where S is the set includes all the one-to-one mappings from cluster to label, and Y = [y 1 , . . . , y m ], C = [c 1 , . . . , c m ].</p><p>Adjusted rand index (ARI). Suppose there are m samples, and let Y and C be two clustering of these samples, where Y = {Y 1 , . . . , Y r } and C = {C 1 , . . . , C s }. Let m ij denote the number of the intersection between Y i and C j , i.e., m ij = |Y i ∩ C j |. The ARI metric is defined as</p><formula xml:id="formula_61">ARI = ij mij 2</formula><p>More experiments on the effect of hyperparameters of MCR 2 -CTRL. We provide more experimental results of MCR 2 -CTRL training in the self-supervised setting by varying training hyperparameters on the STL10 dataset. The results are summarized in <ref type="table" target="#tab_14">Table 8</ref>. Notice that the choice of hyperparameters only has small effect on the performance with the MCR 2 -CTRL objective. We may hypothesize that, in order to further improve the performance, one has to seek other, potentially better, control of optimization dynamics or strategies. We leave those for future investigation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left and Middle: The distribution D of high-dim data x ∈ R D is supported on a manifold M and its classes on low-dim submanifolds Mj, we learn a map f (x, θ) such that zi = f (xi, θ) are on a union of maximally uncorrelated subspaces {Sj}. Right: Cosine similarity between learned features by our method for the CIFAR10 training dataset. Each class has 5,000 samples and their features span a subspace of over 10 dimensions (see Figure 3(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of two learned representations Z and Z via reduced rates: R is the number of -balls packed in the joint distribution and R c is the sum of the numbers for all the subspaces (the green balls). ∆R is their difference (the number of blue balls). The MCR 2 principle prefers Z (the left one).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Evolution of R, R c , ∆R during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>PCA: (red) overall data; (blue) individual classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Evolution of the rates of MCR 2 in the training process and principal components of learned features. ∆R Z(θ), Π, . R c (Z(θ), | Π).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Evolution of rates R, R c , ∆R of MCR 2 during training with corrupted labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>(a) illustrates how the two rates and their difference (for both training and test data) evolves over epochs of training: After an initial phase, R gradually increases while R c decreases, indicating that features Z are expanding as a whole while each class Z j is being compressed.Figure 3(c)shows the distribution of singular values per Z j andFigure 1 (right)shows the angles of features sorted by class. Compared to the geometric loss<ref type="bibr" target="#b20">[LQMS18]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>MCR 2 -CTRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Evolution of the rates of (left) MCR 2 and (right) MCR 2 -CTRL in the training process in the selfsupervised setting on CIFAR10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the learning dynamics change from Fig 5(a) to Fig 5(b): All features are first compressed then gradually expand. We denote the controlled MCR 2 training by MCR 2 -CTRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma A. 3 (</head><label>3</label><figDesc>Invariant property [MDHW07]). For any Z ∈ R d×m and any orthogonal matrices U ∈ R d×d and V ∈ R m×m we have R(Z, ) = R(U ZV , ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>with equality holds if and only if Z1Z 1 m1=</head><label>1</label><figDesc>· · · = Z k Z k m k . This proves the lower bound in (9). We now prove the upper bound. By the strict concavity of log det(·), we havelog det(Q) ≤ log det(S) + ∇ log det(S), Q − S , for all {Q, S} ⊆ S m ++ , where equality holds if and only if Q = S. Plugging in ∇ log det(S) = S −1 (see e.g., [BV04]) and S −1 = (S −1 ) gives log det(Q) ≤ log det(S) + tr(S −1 Q) − m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Under the conditions• (Large ambient dimension) d ≥ k j=1 d j , and• (High coding precision) 4 &lt; min j∈{1,...,k}tr(Πj ) m d 2 d 2 j , the optimal solution Z * satisfies • (Between-class discriminative) (Z * j1 ) Z * j2 = 0 for all 1 ≤ j 1 &lt; j 2 ≤ k, i.e.,Z * j1 and Z * j2 lie in orthogonal subspaces, and • (Within-class diverse) For each j ∈ {1, . . . , k}, the rank of Z * j is equal to d j and either all singular values of Z * j are equal to tr(Πj ) dj , or the d j − 1 largest singular values of Z * j are equal and have value larger than tr(Πj ) dj ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>•</head><label></label><figDesc>Let x * be an arbitrary global solution to (16). If the conditions• f (0) &lt; f (x) for all x &gt; 0, There exists x T &gt; 0 such that f (x) is strictly increasing in [0, x T ] and strictly decreasing in [x T , ∞), • f ( c r ) &lt; 0 (equivalently, c r &gt; x T ),are satisfied, then we have either• x * = [ c r , . . . , c r ], or • x * = [x H , . . . , x H , x L ]for some x H ∈ ( c r , c r−1 ) and x L &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>PCA: MCR 2 training learned features for overall data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>PCA: cross-entropy training learned features for overall data (first 30 components). PCA: cross-entropy training learned features for overall data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>PCA: cross-entropy training learned features for every class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 :</head><label>6</label><figDesc>Principal component analysis (PCA) of learned representations for the MCR 2 trained model (first row) and the cross-entropy trained model (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 7 :</head><label>7</label><figDesc>Cosine similarity between learned features by using the MCR 2 objective (left) and CE loss (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of principal components learned for class 2-'Bird' and class 8-'Ship'. For each class j,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>10 representative images from each class based on top-10 principal components of the SVD of learned representations by MCR 2 . Randomly selected 10 images from each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of top-10 "principal" images for each class in the CIFAR10 dataset. (a) For each class-j,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>(b) For each class, 10 images are randomly selected in the dataset. These images are the ones displayed in the CIFAR dataset website [Kri09]. B.3.2 Experimental Results of MCR 2 in the Supervised Learning Setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification results with features learned with labels corrupted at different levels. Self-supervised Learning of Invariant Features Learning invariant features via rate reduction. Motivated by self-supervised learning algorithms [LHB04, KRFL09, OLV18, HFW + 19, WXYL18]</figDesc><table><row><cell></cell><cell cols="5">RATIO=0.1 RATIO=0.2 RATIO=0.3 RATIO=0.4 RATIO=0.5</cell></row><row><cell>CE TRAINING</cell><cell>90.91%</cell><cell>86.12%</cell><cell>79.15%</cell><cell>72.45%</cell><cell>60.37%</cell></row><row><cell>MCR 2 TRAINING</cell><cell>91.16%</cell><cell>89.70%</cell><cell>88.18%</cell><cell>86.66%</cell><cell>84.30%</cell></row><row><cell>3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Clustering results on CIFAR10, CIFAR100, and STL10 datasets. result compared with previous methods. More details can be found in Appendix B.3.3. Empirically, we observe that, without class labels, the overall coding rate R expands quickly and the MCR 2 loss saturates (at a local maximum), seeFig 5(a). Our experience suggests that learning a good representation from unlabeled data might be too ambitious when directly optimizing the original ∆R. Nonetheless, from the geometric meaning of R and R c , one can design a different learning strategy by controlling the dynamics of expansion and compression differently during training. For instance, we may re-scale the rate by replacing R(Z, ) with R(Z, )</figDesc><table><row><cell>DATASET</cell><cell cols="3">METRIC K-MEANS JULE</cell><cell>RTM</cell><cell>DEC</cell><cell>DAC</cell><cell cols="2">DCCM MCR 2 -CTRL</cell></row><row><cell></cell><cell>NMI</cell><cell>0.087</cell><cell cols="4">0.192 0.197 0.257 0.395</cell><cell>0.496</cell><cell>0.630</cell></row><row><cell>CIFAR10</cell><cell>ACC</cell><cell>0.229</cell><cell cols="4">0.272 0.309 0.301 0.521</cell><cell>0.623</cell><cell>0.684</cell></row><row><cell></cell><cell>ARI</cell><cell>0.049</cell><cell cols="4">0.138 0.115 0.161 0.305</cell><cell>0.408</cell><cell>0.508</cell></row><row><cell></cell><cell>NMI</cell><cell>0.084</cell><cell>0.103</cell><cell>-</cell><cell cols="2">0.136 0.185</cell><cell>0.285</cell><cell>0.362</cell></row><row><cell>CIFAR100</cell><cell>ACC</cell><cell>0.130</cell><cell>0.137</cell><cell>-</cell><cell cols="2">0.185 0.237</cell><cell>0.327</cell><cell>0.347</cell></row><row><cell></cell><cell>ARI</cell><cell>0.028</cell><cell>0.033</cell><cell>-</cell><cell cols="2">0.050 0.087</cell><cell>0.173</cell><cell>0.167</cell></row><row><cell></cell><cell>NMI</cell><cell>0.124</cell><cell>0.182</cell><cell>-</cell><cell cols="2">0.276 0.365</cell><cell>0.376</cell><cell>0.446</cell></row><row><cell>STL10</cell><cell>ACC</cell><cell>0.192</cell><cell>0.182</cell><cell>-</cell><cell cols="2">0.359 0.470</cell><cell>0.482</cell><cell>0.491</cell></row><row><cell></cell><cell>ARI</cell><cell>0.061</cell><cell>0.164</cell><cell>-</cell><cell cols="2">0.186 0.256</cell><cell>0.262</cell><cell>0.290</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows the results of the proposed MCR 2 -CTRL in comparison with methods JULE [YPB16], RTM [NMM19], DEC [XGF16], DAC [CWM + 17], and DCCM [WLW + 19] that have achieved the best results on these datasets. Surprisingly, without utilizing any inter-class or inter-sample information and heuristics on the data, the invariant features learned by our method with augmentations alone achieves a better performance over other highly engineered clustering methods. More ablation studies can be found in Appendix B.3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>BH89] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53-58, 1989. Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous spaces. In Advances in Neural Information Processing Systems, pages 9142-9153, 2019. Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, pages 215-223, 2011. Alexander Strehl and Joydeep Ghosh. Cluster ensembles-a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 3(Dec):583-617, 2002. [SZ15] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for largescale image recognition. In International Conference on Learning Representations, 2015. [TZ15] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pages 1-5. IEEE, 2015. [VMS16] Rene Vidal, Yi Ma, and S. S. Sastry. Generalized Principal Component Analysis.</figDesc><table><row><cell cols="2">[BV04] [CGW19] Taco S Cohen, [CNL11] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. Adam Coates, [SG02] Springer Publishing Company, Incorporated, 1st edition, 2016.</cell></row><row><cell cols="2">[WDCB05] Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. The</cell></row><row><cell></cell><cell>multiscale structure of non-differentiable image manifolds. In Proceedings of SPIE,</cell></row><row><cell></cell><cell>the International Society for Optical Engineering, pages 59141B-1, 2005.</cell></row><row><cell cols="2">[WLW + 19] Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and Hongbin</cell></row><row><cell></cell><cell>Zha. Deep comprehensive correlation mining for image clustering. In Proceedings of</cell></row><row><cell></cell><cell>the IEEE International Conference on Computer Vision, pages 8150-8159, 2019.</cell></row><row><cell>[ZF14]</cell><cell>Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional</cell></row><row><cell></cell><cell>networks. In European Conference on Computer Vision, pages 818-833. Springer,</cell></row><row><cell></cell><cell>2014.</cell></row><row><cell>[ZHF18]</cell><cell>Pan Zhou, Yunqing Hou, and Jiashi Feng. Deep adversarial subspace clustering. In</cell></row><row><cell></cell><cell>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</cell></row><row><cell></cell><cell>pages 1596-1604, 2018.</cell></row><row><cell>[ZJH + 18]</cell><cell>Tong Zhang, Pan Ji, Mehrtash Harandi, Richard Hartley, and Ian Reid. Scalable deep</cell></row><row><cell></cell><cell>k-subspace clustering. In Asian Conference on Computer Vision, pages 466-481.</cell></row><row><cell></cell><cell>Springer, 2018.</cell></row><row><cell>[ZJH + 19]</cell><cell>Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, and Hongdong Li. Neural</cell></row><row><cell></cell><cell>collaborative subspace clustering. arXiv preprint arXiv:1904.10596, 2019.</cell></row></table><note>[CT06] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006.[CW16] Taco Cohen and Max Welling. Group equivariant convolutional networks. In Interna- tional Conference on Machine Learning, pages 2990-2999, 2016.[CWM + 17] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep adaptive image clustering. In Proceedings of the IEEE International Conference on Computer Vision, pages 5879-5887, 2017.[WTL + 08] John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, and Heung-Yeung Shum. Clas- sification via minimum incremental coding length (micl). In Advances in Neural Information Processing Systems, pages 1633-1640, 2008.[WXYL18] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733-3742, 2018.[XGD + 17] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.[XGF16] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In International Conference on Machine Learning, pages 478-487, 2016.[YLRV16] Chong You, Chun-Guang Li, Daniel P Robinson, and René Vidal. Oracle based active set algorithm for scalable elastic net subspace clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3928-3937, 2016.[YPB16] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep rep- resentations and image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5147-5156, 2016.[ZBH + 17] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.[ZLY + 19] Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, and Zhouchen Lin. Self-supervised convolutional subspace clustering network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5473-5482, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>removed: max x=[x1,...,xr]∈R r</figDesc><table><row><cell></cell><cell>r</cell></row><row><cell></cell><cell>f (x p ) s.t.</cell></row><row><cell>+</cell><cell>p=1</cell></row></table><note>r p=1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>be the Lagragian function for (17) where λ = [λ 0 , λ 1 , . . . , λ r ] is the Lagragian multiplier.</figDesc><table><row><cell>By the</cell></row><row><cell>first order optimality conditions (i.e., the Karush-Kuhn-Tucker (KKT) conditions, see, e.g., [NW06,</cell></row><row><cell>Theorem 12.1]), there exists λ  *  = [λ  *  0 , λ  *  1 , . . . , λ  *  r ] such that</cell></row><row><cell>r</cell></row><row><cell>p=1</cell></row></table><note>p</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Simulations -Verifying Diversity Promoting Properties of MCR 2</figDesc><table><row><cell>B Additional Simulations and Experiments</cell></row><row><cell>B.1</cell></row><row><cell>and rewrite (34) as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The implementation of network architectures used in this paper are mainly based on this github repo.</figDesc><table><row><cell></cell><cell>R</cell><cell>R c</cell><cell>∆R</cell><cell>ORTHOGONAL? OUTPUT DIMENSION</cell></row><row><cell cols="4">RANDOM GAUSSIAN 552.70 193.29 360.41</cell><cell>512</cell></row><row><cell cols="4">SUBSPACE (dj = 50) 545.63 108.46 437.17</cell><cell>512</cell></row><row><cell cols="2">SUBSPACE (dj = 40) 487.07</cell><cell>92.71</cell><cell>394.36</cell><cell>512</cell></row><row><cell cols="2">SUBSPACE (dj = 30) 413.08</cell><cell>74.84</cell><cell>338.24</cell><cell>512</cell></row><row><cell cols="2">SUBSPACE (dj = 20) 318.52</cell><cell>54.48</cell><cell>264.04</cell><cell>512</cell></row><row><cell cols="2">SUBSPACE (dj = 10) 195.46</cell><cell>30.97</cell><cell>164.49</cell><cell>512</cell></row><row><cell>SUBSPACE (dj = 1)</cell><cell>31.18</cell><cell>4.27</cell><cell>26.91</cell><cell>512</cell></row><row><cell cols="4">RANDOM GAUSSIAN 292.71 154.13 138.57</cell><cell>256</cell></row><row><cell cols="2">SUBSPACE (dj = 25) 288.65</cell><cell>56.34</cell><cell>232.31</cell><cell>256</cell></row><row><cell cols="2">SUBSPACE (dj = 20) 253.51</cell><cell>47.58</cell><cell>205.92</cell><cell>256</cell></row><row><cell cols="2">SUBSPACE (dj = 15) 211.97</cell><cell>38.04</cell><cell>173.93</cell><cell>256</cell></row><row><cell cols="2">SUBSPACE (dj = 10) 161.87</cell><cell>27.52</cell><cell>134.35</cell><cell>256</cell></row><row><cell>SUBSPACE (dj = 5)</cell><cell>98.35</cell><cell>15.55</cell><cell>82.79</cell><cell>256</cell></row><row><cell>SUBSPACE (dj = 1)</cell><cell>27.73</cell><cell>3.92</cell><cell>23.80</cell><cell>256</cell></row><row><cell cols="3">RANDOM GAUSSIAN 150.05 110.85</cell><cell>39.19</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 12) 144.36</cell><cell>27.72</cell><cell>116.63</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 10) 129.12</cell><cell>24.06</cell><cell>105.05</cell><cell>128</cell></row><row><cell>SUBSPACE (dj = 8)</cell><cell>112.01</cell><cell>20.18</cell><cell>91.83</cell><cell>128</cell></row><row><cell>SUBSPACE (dj = 6)</cell><cell>92.55</cell><cell>16.04</cell><cell>76.51</cell><cell>128</cell></row><row><cell>SUBSPACE (dj = 4)</cell><cell>69.57</cell><cell>11.51</cell><cell>58.06</cell><cell>128</cell></row><row><cell>SUBSPACE (dj = 2)</cell><cell>41.68</cell><cell>6.45</cell><cell>35.23</cell><cell>128</cell></row><row><cell>SUBSPACE (dj = 1)</cell><cell>24.28</cell><cell>3.57</cell><cell>20.70</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 50) 145.60</cell><cell>75.31</cell><cell>70.29</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 40) 142.69</cell><cell>65.68</cell><cell>77.01</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 30) 135.42</cell><cell>54.27</cell><cell>81.15</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 20) 120.98</cell><cell>40.71</cell><cell>80.27</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 15) 111.10</cell><cell>32.89</cell><cell>78.21</cell><cell>128</cell></row><row><cell cols="2">SUBSPACE (dj = 12) 101.94</cell><cell>27.73</cell><cell>74.21</cell><cell>128</cell></row><row><cell cols="2">B.2 Implementation Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Training Setting. We mainly use ResNet-18 [HZRS16] in our experiments, where we use 4 residual</cell></row><row><cell cols="3">blocks with layer widths [64, 128, 256, 512].</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The augmentations we use for STL10 dataset and the pseudo-code is as follows.</figDesc><table><row><cell>import torchvision.transforms as transforms</cell></row><row><cell>TRANSFORM = transforms.Compose([</cell></row><row><cell>transforms.RandomResizedCrop(32),</cell></row><row><cell>transforms.RandomHorizontalFlip(),</cell></row><row><cell>transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),</cell></row><row><cell>transforms.RandomGrayscale(p=0.2),</cell></row><row><cell>transforms.ToTensor()])</cell></row><row><cell>import torchvision.transforms as transforms</cell></row><row><cell>TRANSFORM = transforms.Compose([</cell></row><row><cell>transforms.RandomResizedCrop(96),</cell></row><row><cell>transforms.RandomHorizontalFlip(),</cell></row><row><cell>transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),</cell></row><row><cell>transforms.RandomGrayscale(p=0.2),</cell></row><row><cell>GaussianBlur(kernel_size=9),</cell></row><row><cell>transforms.ToTensor()])</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Experiments of MCR 2 in the supervised setting on the CIFAR10 dataset.</figDesc><table><row><cell>ARCH</cell><cell cols="3">DIM d PRECISION 2 BATCHSIZE m</cell><cell>lr</cell><cell>ACC</cell><cell>COMMENT</cell></row><row><cell>RESNET-18</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell cols="2">92.20% MAINLINE, FIG 3</cell></row><row><cell>RESNEXT-29</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell cols="2">92.55% DIFFERENT</cell></row><row><cell>VGG-11</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell>90.76%</cell><cell>ARCHITECTURE</cell></row><row><cell>RESNET-18</cell><cell>512</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell cols="2">88.60% EFFECT OF</cell></row><row><cell>RESNET-18</cell><cell>256</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell>92.10%</cell><cell>OUTPUT</cell></row><row><cell>RESNET-18</cell><cell>64</cell><cell>0.5</cell><cell>1,000</cell><cell>0.01</cell><cell>92.21%</cell><cell>DIMENSION</cell></row><row><cell>RESNET-18 RESNET-18 RESNET-18</cell><cell>128 128 128</cell><cell>1.0 0.4 0.2</cell><cell>1,000 1,000 1,000</cell><cell>0.01 0.01 0.01</cell><cell cols="2">93.06% EFFECT OF 91.93% PRECISION 90.06%</cell></row><row><cell>RESNET-18</cell><cell>128</cell><cell>0.5</cell><cell>500</cell><cell>0.01</cell><cell>82.33%</cell><cell></cell></row><row><cell>RESNET-18 RESNET-18 RESNET-18</cell><cell>128 128 512</cell><cell>0.5 0.5 0.5</cell><cell>2,000 4,000 2,000</cell><cell>0.01 0.01 0.01</cell><cell>93.02% 92.59% 92.47%</cell><cell>EFFECT OF BATCH SIZE</cell></row><row><cell>RESNET-18</cell><cell>512</cell><cell>0.5</cell><cell>4,000</cell><cell>0.01</cell><cell>92.17%</cell><cell></cell></row><row><cell>RESNET-18</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell>0.05</cell><cell>86.02%</cell><cell></cell></row><row><cell>RESNET-18</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell cols="2">0.005 92.39%</cell><cell>EFFECT OF lr</cell></row><row><cell>RESNET-18</cell><cell>128</cell><cell>0.5</cell><cell>1,000</cell><cell cols="2">0.001 92.23%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Effect of number of components rj for nearest subspace classification in the supervised setting. NUMBER OF COMPONENTS rj = 10 rj = 20 rj = 30 rj = 40 rj = 50 MAINLINE (LABEL NOISE RATIO=0.0) 92.68% 92.53% 92.20% 92.32% 92.17% LABEL NOISE RATIO=0.1 91.71% 91.73% 91.16% 91.83% 91.78% LABEL NOISE RATIO=0.2 90.68% 90.61% 89.70% 90.62% 90.54% LABEL NOISE RATIO=0.3 88.24% 87.97% 88.18% 88.15% 88.10% LABEL NOISE RATIO=0.4 86.49% 86.67% 86.66% 86.71% 86.44% LABEL NOISE RATIO=0.5 83.90% 84.18% 84.30% 84.18% 83.76%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Effect of Precision 2 on classification results with features learned with labels corrupted at different levels by using MCR 2 training. Experimental Results of MCR 2 in the Self-supervised Learning Setting</figDesc><table><row><cell cols="6">PRECISION RATIO=0.1 RATIO=0.2 RATIO=0.3 RATIO=0.4 RATIO=0.5</cell></row><row><cell>2 = 0.5</cell><cell>91.16%</cell><cell>89.70%</cell><cell>88.18%</cell><cell>86.66%</cell><cell>84.30%</cell></row><row><cell>2 = 0.75</cell><cell>92.37%</cell><cell>90.82%</cell><cell>89.91%</cell><cell>87.67%</cell><cell>83.69%</cell></row><row><cell>2 = 1.0</cell><cell>91.93%</cell><cell>91.11%</cell><cell>89.60%</cell><cell>87.09%</cell><cell>84.53%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Clustering comparison between MCR 2 and MCR 2 -CTRL on CIFAR10 dataset.</figDesc><table><row><cell></cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>MCR 2</cell><cell cols="3">0.544 0.570 0.399</cell></row><row><cell cols="4">MCR 2 -CTRL 0.630 0.684 0.508</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Experiments of MCR 2 -CTRL in the self-supervised setting on STL10 dataset.</figDesc><table><row><cell>ARCH</cell><cell cols="2">PRECISION 2 LEARNING RATE lr</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>RESNET-18</cell><cell>0.5</cell><cell>0.1</cell><cell cols="3">0.446 0.491 0.290</cell></row><row><cell>RESNET-18</cell><cell>0.75</cell><cell>0.1</cell><cell cols="3">0.450 0.484 0.288</cell></row><row><cell>RESNET-18</cell><cell>0.25</cell><cell>0.1</cell><cell cols="3">0.447 0.489 0.293</cell></row><row><cell>RESNET-18</cell><cell>0.5</cell><cell>0.2</cell><cell cols="3">0.477 0.473 0.295</cell></row><row><cell>RESNET-18</cell><cell>0.5</cell><cell>0.05</cell><cell cols="3">0.444 0.496 0.293</cell></row><row><cell>RESNET-18</cell><cell>0.25</cell><cell>0.05</cell><cell cols="3">0.454 0.489 0.294</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Here different representations can be either representations associated with different network parameters or representations learned after different layers of the same deep network.17  Strictly speaking, in the context of clustering finite samples, one needs to use the more precise measure of the coding length mentioned earlier, see<ref type="bibr" target="#b21">[MDHW07]</ref> for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">We find that the supervised learned representation on CIFAR10 in Section 3.1 can easily achieve a clustering accuracy over 99% on the entire training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">https://github.com/kuangliu/pytorch-cifar</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>x p = m j .</p><p>We compute the first and second derivative for f with respect to x, which are given by</p><p>Note that</p><p>• by using the condition 4 &lt; mj m</p><p>Therefore, we may apply Lemma A.7 and conclude that the unique optimal solution to (35) is either </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiclass learnability and the ERM principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2377" to="2404" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><surname>Hindi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 American Control Conference</title>
		<meeting>the 2003 American Control Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2156" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>GPAM + 14</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hflm + 18] R Devon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Hfw + 19] Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian T Jolliffe ; Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
	<note>Principal Component Analysis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Logdet rank minimization with application to subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE Journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning invariant features through topographic filter maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1605" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artemy</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Kuyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07593</idno>
		<title level="m">Caveats for information bottleneck in deterministic scenarios</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Lch + 06] Yann Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal image synthesis with conditional implicit maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OLE: Orthogonal lowrank embedding-a plug and play geometric loss for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Musé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8109" to="8118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segmentation of multivariate mixed data via lossy data coding and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>Derksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1546" to="1562" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A ratedistortion framework for explaining neural network decisions. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Hauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gitta</forename><surname>Kutyniok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A decoder-free approach for unsupervised clustering and manifold learning with random triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Nina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamison</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarissa</forename><surname>Milligan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>PFX + 17</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>PGM + 19</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep isometric learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on International Conference on Machine Learning</title>
		<meeting>the International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>QYW + 20</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
	<note>RVM + 11</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
