<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noisy Concurrent Training for Efficient Learning under Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Sarfraz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Arani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noisy Concurrent Training for Efficient Learning under Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) fail to learn effectively under label noise and have been shown to memorize random labels which affect their generalization performance. We consider learning in isolation, using one-hot encoded labels as the sole source of supervision, and a lack of regularization to discourage memorization as the major shortcomings of the standard training procedure. Thus, we propose Noisy Concurrent Training (NCT) which leverages collaborative learning to use the consensus between two models as an additional source of supervision. Furthermore, inspired by trial-to-trial variability in the brain, we propose a counter-intuitive regularization technique, target variability, which entails randomly changing the labels of a percentage of training samples in each batch as a deterrent to memorization and over-generalization in DNNs. Target variability is applied independently to each model to keep them diverged and avoid the confirmation bias. As DNNs tend to prioritize learning simple patterns first before memorizing the noisy labels, we employ a dynamic learning scheme whereby as the training progresses, the two models increasingly rely more on their consensus. NCT also progressively increases the target variability to avoid memorization in later stages. We demonstrate the effectiveness of our approach on both synthetic and real-world noisy benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Much of the recent advances in deep learning can be attributed to supervised learning algorithms which require huge amounts of annotated data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. However, manually annotating the data is laborious and usually expensive task <ref type="bibr" target="#b24">[25]</ref> which can be prone to error when not verified by multiple annotators. Furthermore, to utilize the widespread open-source data, various techniques were proposed for automatically annotating the data using user tags and keywords <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> and scaling up crowd-sourced datasets <ref type="bibr" target="#b23">[24]</ref>. * Equal contribution.Accepted as a conference paper at WACV 2021. Our proposed method, noisy concurrent training (NCT) effectively prevents the models from memorizing the noisy labels even though no distinction is made between them during training.</p><p>While these approaches allow the creation of large datasets for training, they lead to noisy annotations. A number of studies have shown that label noise has an adverse effect on the performance of the models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>. It is therefore pertinent to adapt the training procedure to leverage these datasets.</p><p>Deep neural networks (DNNs) have been shown to easily fit random labels <ref type="bibr" target="#b1">[2]</ref> which makes it challenging to train the models efficiently. The majority of the existing methods for training under label noise can be broadly categorized into two approaches: i) correcting the labels by estimating the noise transition matrix <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, ii) identifying the noisy labels to either filter out <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> or down-weight those samples <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. However, the former approach depends on accurately estimating the noise transition matrix which is difficult especially for a high number of classes, and the latter approach requires an efficient method for identifying noisy labels and/or an estimate of the percentage of noisy instances. Amongst these, there has been more focus on separating the noisy and clean instances where a common criterion is to consider low-loss instances as a proxy for clean labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. However, harder instances can be perceived as noisy and hence the model can be biased towards easy instances. Both approaches consider the annotations quality as the primary reason for the decrease in model's performance and hence the proposed solutions rely on accurately relabeling, filtering out or down-weighting instances with incorrect labels.</p><p>Here we provide an alternative viewpoint on the issue of learning with noisy labels and attempt to improve the robustness of the underlying training framework. We focus on the insufficiency of the standard training method. The cross-entropy loss maximizes a bound on the mutual information between one-hot encoded labels and the learned representation. The model receives no information about the similarity of a data point among the classes and hence when the provided label is incorrect, it has no source of useful information about the instance or extra supervision to mitigate the adverse effect of the noisy label. There is also a lack of regularization to discourage the model from memorizing the training labels.</p><p>To overcome these issues, we propose noisy concurrent training (NCT) which introduces variability in supervision signal in a collaborative learning framework and takes advantage of building consensus among two different models. Each model, in addition to a supervised learning loss, is trained with a mimicry loss that aligns the posterior distributions of the two models for building consensus on the secondary class probabilities as well as the primary class prediction. To discourage memorization, we derive inspiration from neuroscience where the role of noise in the nervous system has been extensively studied. Based on trial-to-trial response variation in the brain <ref type="bibr" target="#b27">[28]</ref> and the constructive role noise plays in forcing the biological neural networks to be more robust and explore more states <ref type="bibr" target="#b6">[7]</ref>, we propose to use a counter-intuitive regularization technique we refer to as target variability as a deterrent to memorization and overgeneralization in DNNs.</p><p>Specifically, target variability entails randomly changing the labels of a percentage of training samples in a batch, independently for each model. In addition to discouraging memorization, this keeps the two models sufficiently diverged and therefore retains the benefits of mutual learning, i.e. filtering different types of errors and avoiding confirmation bias in self-training. Furthermore, since DNNs tend to learn simple patterns first and memorize the noisy labels in the later epochs <ref type="bibr" target="#b1">[2]</ref>, NCT employs a dynamic learning scheme whereby as training progresses, the contribution of the supervised learning loss diminishes and the models focus more on building consensus. NCT also progressively increases the target variability to counter the higher tendency of DNNs to memorize the noisy labels at the later stages. We show the efficacy of our proposed approach on noisy versions of CIFAR10, CIFAR100 <ref type="bibr" target="#b13">[14]</ref>, and Tiny-ImageNet <ref type="bibr" target="#b16">[17]</ref> as well as two real-world noisy datasets Clothing1M <ref type="bibr" target="#b34">[35]</ref> and WebVision-v1 <ref type="bibr" target="#b18">[19]</ref>. Empirical results show the versatility and effectiveness of NCT under different noise types and noise levels. In addition to improving the performance of the model on noisy datasets, NCT also improves the performance on clean datasets which demonstrates its utility as a general-purpose robust learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The pervasiveness of label noise in real-world datasets has led to a number of approaches for training DNNs under noisy labels. One approach is to implicitly or explicitly relabel the training labels. F-correction <ref type="bibr" target="#b25">[26]</ref> estimates the noise transition matrix to correct the noisy labels. However, estimating the noise transition matrix is particularly challenging for a higher number of classes. Joint-Optim <ref type="bibr" target="#b31">[32]</ref> uses a joint optimization framework of learning the model parameters and estimating true labels using the running average of the model's predictions. P-correction <ref type="bibr" target="#b35">[36]</ref> models the labels as probability distributions over the classes and updates these distributions through back-propagation in an end-to-end manner.</p><p>Another approach involves correcting the loss function by reweighting the training samples. Bootstrap <ref type="bibr" target="#b26">[27]</ref> introduces a perceptual consistency term in the learning and uses a weighted combination of predicted and original labels as the correct labels. Instead of using a static weight for all samples, M-correction <ref type="bibr" target="#b0">[1]</ref> models sample loss with a beta mixture model to individually weigh each sample dynamically and adapts the Mixup <ref type="bibr" target="#b38">[39]</ref> augmentation. Mixup encourages the model to have linear behavior between samples and exhibits strong robustness to label noise. D2L <ref type="bibr" target="#b20">[21]</ref> uses a similar combination of the original labels and network predictions depending on the dimensionality of the latent feature subspace.</p><p>A variant of the loss correction approach focuses on separating the clean and noisy instances. MentorNet <ref type="bibr" target="#b12">[13]</ref> uses a predefined curriculum for selecting the clean instances but it is difficult to design a reliable criterion. Decoupling <ref type="bibr" target="#b22">[23]</ref> trains two networks simultaneously and at each epoch only uses the training instances where the two models disagree for updating the models. Disagreement amongst the two models, however, is not an optimal criterion for filtering out noisy labels and the disagreement region contains a number of noisy labels. Co-teaching <ref type="bibr" target="#b9">[10]</ref> and Co-teaching+ <ref type="bibr" target="#b36">[37]</ref> use low loss training instances as a proxy for clean instances and use cross-update between the two models whereby each model selects the low-loss samples for the other model. These methods require an accurate estimate of the noise level which is difficult to obtain especially in the absence of a clean validation dataset. Iterative-CV <ref type="bibr" target="#b3">[4]</ref> randomly divides noisy datasets and utilizes cross-validation to identify clean samples before applying the co-teaching method on selected samples. However, these approaches do not utilize noisy training instances for representation learning. Also using low-loss instances for identifying clean instances biases the model towards easy instances as hard instances are usually perceived as noisy.</p><p>There are a few other approaches such as Meta-Cleaner <ref type="bibr" target="#b39">[40]</ref> and Meta-Learning <ref type="bibr" target="#b17">[18]</ref>. The former hallucinates clean representations of an object category according to a subset from the same category to identify noisy labels. The latter proposes a gradient-based method to find model parameters that are more noise-tolerant.</p><p>The aforementioned approaches, in general, focus on accurately relabeling, filtering out, or down-weighting instances with incorrect labels as they consider the quality of the annotations as the primary reason for the model's failure to learn efficiently. Our proposed method, instead, focuses on improving the robustness of the underlying training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we first provide the motivation and intuition behind our method, Noisy Concurrent Training, and then formally present the different components of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our approach is loosely inspired by Boyd et al. <ref type="bibr" target="#b2">[3]</ref> study on cultural niche where they posit that the uniquely developed ability of humans to learn from others is absolutely crucial for human ecological success. The authors suggest that cultural learning can increase the average fitness of the population only if it increases the ability of the population to create adaptive information. A possible mechanism through which cultural learning can benefit the individual, as well as the population, is that it allows individuals to learn selectively -using environmental cues when they provide clear guidance and learning from others when they do not. This ability to learn or imitate selectively is advantageous because opportunities to learn from experience or by observation of the world vary. Furthermore, some psychological models assume that our learning psychology has a genetically heritable information quality threshold that governs whether an individual relies on inferences from environmental cues or learns from others. Individuals with a low information quality threshold rely on even poor cues whereas individuals with a high threshold usually imitate. As the mean information quality threshold in the population increases, the fitness of learners increases because they are more likely to make accurate or low-cost inferences. At the same time, the frequency of imitators also increases.</p><p>Our proposed approach attempts to simulate the mechanism of cultural learning in neural networks. NCT involves training models concurrently whereby each model is trained with a convex combination of a supervised learning loss and a mimicry loss. Supervision from supervised loss can be considered as learning from the environmental cues whereas supervision from the mimicry loss can be viewed as imitation in cultural learning. Even though the groundtruth labels (environmental cues) can be noisy, DNNs tend to prioritize learning simple patterns first before memorizing noisy labels, therefore in the initial phase of learning, the models can learn more from the supervised loss, gradually increasing the fitness of the two models (population).</p><p>As training progresses, the information quality threshold can be increased and the model can rely more on imitating each other and building consensus. This is simulated using a dynamic balancing scheme which progressively increases Sample a mini-batch:</p><formula xml:id="formula_0">(x (1) , y (1) ), ..., (x (b) , y (b) ) ∼ D 3:</formula><p>Compute the dynamic balancing factor α d based on Eq. 4 <ref type="bibr">4:</ref> Compute the target variability rate r d based on Eq. 5 <ref type="bibr">5:</ref> Get the new targets:ŷ 1 ,ŷ 2 = TARGET_VARIABILITY_FUNCTION({y <ref type="bibr" target="#b0">(1)</ref> , ..., y (b) }, r d , C) (Algorithm 2) <ref type="bibr">6:</ref> Compute the loss functions for both M1 and M2 models:</p><formula xml:id="formula_1">L θ1 = (1 − α d )L CE (σ(z θ2 ),ŷ 1 ) + α d τ 2 D KL ( σ(z θ 1 ) τ || σ(z θ 2 ) τ ) L θ2 = (1 − α d )L CE (σ(z θ2 ),ŷ 2 ) + α d τ 2 D KL ( σ(z θ 1 ) τ || σ(z θ 2 ) τ ) 7:</formula><p>Compute stochastic gradients and update the parameters:</p><formula xml:id="formula_2">θ * 1 ← θ 1 − η ∂L θ 1 ∂θ1 θ * 2 ← θ 2 − η ∂L θ 2 ∂θ2</formula><p>return θ * 1 and θ * 2 the weight of the mimicry loss while reducing the weight of the supervised learning loss. This shifts the priority of the two models towards consensus building on their accumulated knowledge (model prediction) and aligning their posterior probability distributions. The mimicry loss provides an extra supervision signal for training the models in addition to the one-hot labels which can enable the models to learn useful information even from training samples with incorrect labels.</p><p>Furthermore, inspired by trial-to-trial variability in the brain, NCT employs a simple yet counter-intuitive regularization technique hereby referred to as Target Variability whereby during training, the target labels of a fraction of samples are randomly changed for each batch independently for the two models. Target variability serves multiple purposes: it implicitly increases the information quality threshold by indicating to the model that it cannot rely too much on the noisy labels, acts as a strong deterrent to memorizing the training labels and also keeps the two models sufficiently diverged to avoid the confirmation bias arising from the method reducing to self-training. <ref type="figure" target="#fig_1">Figure 2</ref> delineates the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation</head><formula xml:id="formula_3">Given a dataset of N samples, D = {(x (i) , y (i) )} N i=1</formula><p>, where x (i) is the input image and y (i) ∈ {0, 1} C is the one-hot ground-truth label over C classes which can be noisy, we formulate our proposed method, NCT, as dynamic collaboration learning between a cohort of two networks parametrized by θ 1 and θ 2 . Each network is trained with a supervised loss (standard cross-entropy, L CE ) and a mimicry loss (KullbackâȂŞLeibler divergence, D KL ). The overall loss for each model is as follows:</p><formula xml:id="formula_4">L θ1 = (1−α)L CE (σ(z θ1 ), y)+ατ 2 D KL ( σ(z θ2 ) τ || σ(z θ1 ) τ ) (1) L θ2 = (1−α)L CE (σ(z θ2 ), y)+ατ 2 D KL ( σ(z θ1 ) τ || σ(z θ2 ) τ ) (2)</formula><p>where σ is the softmax function, z θ are the output logits and τ is the temperature which is usually set to 1. Using a higher τ value produces a softer probability distribution over classes. The balancing parameter α ∈ [0, 1] controls the relative weightage between the two losses.</p><p>For inference, we use the average ensemble of the two models,</p><formula xml:id="formula_5">y pred = σ( z θ1 + z θ2 2 )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Balancing</head><p>Given a mixture of clean and noisy labels, DNNs tend to prioritize learning simple patterns first and fit the clean data before memorizing the noisy labels <ref type="bibr" target="#b1">[2]</ref>. NCT employs a dynamic balancing scheme whereby initially the two networks learn more from the supervision loss, i.e. smaller α d value, and as the training progresses, the networks focus more on building consensus and aligning their posterior distribution through D KL , i.e α d → 1. To simulate this behavior, we use a sigmoid ramp-up function following <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_6">α d = α max exp (−β(1 − e e r ) 2 )<label>(4)</label></formula><p>where α max is the maximum alpha value, e is the current epoch, e r is the ramp-up length (the epoch at which α d reaches the maximum value) and β controls the shape of the function. <ref type="figure" target="#fig_1">Figure 2</ref> shows the dynamic balancing functions for different values of β. Create the noise masks:</p><formula xml:id="formula_7">m = [m j ∼ U(0, 1)] b &lt; r d 3:</formula><p>Sample the random targets:</p><formula xml:id="formula_8">y i = [l j ∼ U(0, C − 1)|l j = y j ] b 4:</formula><p>Apply target variability and create the new targets:</p><formula xml:id="formula_9">y i = m y i + (1 − m) y 5: returnŷ 1 andŷ 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Target Variability</head><p>To mimic the trial-to-trial variability in the brain, variations in neural responses to the same stimuli, NCT uses target variability whereby for each sample in the training batch, with probability r, the one-hot labels are changed to a random class sampled from a uniform distribution over the number of classes C. Target variability acts as a regularizer and discourages the model from memorizing the labels. Target variability is applied independently to each model so that the two networks remain sufficiently diverged so that collectively they can filter different types of errors. As the networks tend to memorize the noisy labels in later stages of training, NCT employs dynamic target variability whereby the target variability rate r d is lower for initial epochs and increases progressively during the training <ref type="figure" target="#fig_1">(Figure 2</ref>). NCT uses a logarithmic ramp-up function, , otherwise (5) where r min and r max are the minimum and maximum target variability rates, e is the current epoch, e max is the total number of epochs and e w is the warmup length. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the effectiveness of dynamic target variability in regularizing the model against memorizing the noise labels. The details of the proposed method are summarized in Algorithms 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>For our empirical analysis, we benchmark the performance of our approach on noisy versions of three different datasets CIFAR-10, CIFAR-100 <ref type="bibr" target="#b13">[14]</ref> and Tiny-ImageNet <ref type="bibr" target="#b16">[17]</ref> which represents classifications tasks of increasing complexity and are commonly used in literature to evaluate performance under noisy supervision <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b8">9]</ref>. We follow previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> on CIFAR-10 and CIFAR-100 where noise labels are generated by replacing a percentage of true labels with corrupted labels sampled uniformly from all the classes (i.e., the true label can be randomly maintained). For Tiny-ImageNet, we follow the experimental setup in <ref type="bibr" target="#b36">[37]</ref> and test the performance of our model on two different types of label corruption: symmetry flipping and pair flipping. Here, symmetric noise is generated by replacing a percentage of true labels with corrupted labels sampled uniformly from the other classes (i.e., the true label cannot be maintained) whereas pair flipping simulates the scenario where annotators confuse between a pair of classes.</p><p>It is important to note that the interplay of the hyperparameters of NCT is complementary in nature and therefore the desired effect can be achieved by keeping the majority of the parameters fixed and tuning only a few. For dynamic balancing, we fix τ = 4 and α max = 0.9 as they are commonly used in knowledge distillation literature. In order to avoid overfitting to noisy labels in the initial training stage, we use β = −0.65 and e r is set to 90% of the total epochs so that the transition of weight from supervised loss to mimicry is not too slow <ref type="figure" target="#fig_1">(Figure 2</ref>, β = −5 vs β = −0.65). For dynamic target variability, we fix r min = 0 and e w = 1 while r max ∈ {0, 0.1, 0.3, 0.5, 0.7, 0.9} is selected using a small validation set. Hence, only the r max value is tuned for each experiment while the rest of the parameters remain constant. Following <ref type="bibr" target="#b0">[1]</ref>, we train our method on PreActResNet-18 <ref type="bibr" target="#b10">[11]</ref> and perform random crop and random horizontal flip followed by standard normalization. We train our models for 200 epochs using SGD with 0.9 momentum, weight decay of 1e-5 and batch size 128. The initial learning rate of 0.02 is decayed by a factor of 10 after 180 epochs for CIFAR-10 and CIFAR-100 and 140 for Tiny-ImageNet. For CIFAR-10 we use r max values 0.1, 0.3, 0.5 for clean, symmetric-20 and symmetric-50, respectively. For CIFAR-100 we use r max = 0.1 for clean and r max = 0.7 for symmetric-20 and symmetric-50. For Tiny-ImageNet, we use r max = 0.1 for all the experiments.</p><p>We further test the versatility of our method on two real-world noisy datasets Clothing1M <ref type="bibr" target="#b34">[35]</ref> and WebVision-v1 <ref type="bibr" target="#b18">[19]</ref>. Clothing1M consists of 14 classes with one million training images collected from online shopping websites with auto-generated labels from surrounding text. Following previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, we use ResNet-50 with Im-ageNet pretrained weights. We train the models for 200 epochs with an initial learning rate of 0.002 decayed by a factor of 10 at 180 epoch and r max = 0.5. For each epoch, we sample 1000 mini-batches of size 32 from the training data while ensuring the labels are balanced. WebVision contains 2.4 million images crawled from the Internet by using queries generated from the 1,000 semantic concepts of the benchmark ILSVRC 2012 dataset <ref type="bibr" target="#b4">[5]</ref>. Following Chen et al. <ref type="bibr" target="#b3">[4]</ref>, we train Inception-ResNet-v2 <ref type="bibr" target="#b30">[31]</ref> models on the first 50 classes of the Google image subset. We train the models for 100 epochs with an initial learning rate of 0.01 decayed by a factor of 10 at 90 epoch and r max = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Here we first compare NCT with the priors works on both simulated noisy datasets and real-world noisy datasets, and then analyze the effect of the different components of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with Prior Works</head><p>We compare NCT with multiple baseline methods under similar experimental setup. Since the quality of the dataset is not known a priori, the learning method should be general to work in both noisy as well as clean datasets. For this reason, we compare our method on both clean and various levels of label noise. <ref type="table" target="#tab_2">Table 1</ref> shows consistent improvement for lower noise levels. On clean CIFAR-100, the gap between M-Correction and NCT is considerable. However, our method does not perform well compared to M-Correction for very high levels of symmetric noise (50%). <ref type="table">Table 2</ref> shows that the effectiveness of our approach generalizes beyond CIFAR datasets to the complicated Tiny-ImageNet classification task. On symmetric noise, we see a similar pattern as on CIFAR datasets. For asymmetric noise, which perhaps better simulates real-world noise, NCT provides a significant improvement in generalization. M-Correction shows an unstable behavior on asymmetric noise, indicated by the high standard deviation in performance. Notably, there is considerable performance gap on clean dataset between NCT and other methods on the more challenging CIFAR-100 and Tiny-ImageNet datasets. This can be attributed to the fact that NCT does not make strong assumptions about label noise distribution or attempt to identify noisy labels. In the absence of an ideal separation criterion, clean samples particularly hard ones, can be wrongly identified as noisy samples and subsequently removing them or diminishing their influence can adversely affect performance. This effect is more pronounced as the number of classes increases.</p><p>To verify the practical usage of NCT, we also evaluate the method on two real-world noisy datasets. <ref type="table">Table 3  Table 3</ref>. Comparison with prior methods trained on WebVision dataset. The results for baselines are copied from Chen et al. <ref type="bibr" target="#b3">[4]</ref> and following them, we report the final accuracy (%) on the Web-Vision and ImageNet ILSVRC12 validation sets. For our method, we report the mean and 1 STD of three different seed values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alg./Dataset</head><p>WebVision ILSVRC12 top1 top5 top1 top5 F-correction <ref type="bibr" target="#b25">[26]</ref>   <ref type="table">Table 4</ref>. Comparison with prior methods on Clothing1M. The results for baselines are copied from original papers and following them, we report the best test accuracy (%). For our method, we report the mean and 1 STD of three different seed values.</p><p>Alg. Test Accuracy Standard 68.94 F-correction <ref type="bibr" target="#b25">[26]</ref> 69.84 Joint-Optim <ref type="bibr" target="#b31">[32]</ref> 72.16 M-correction <ref type="bibr" target="#b0">[1]</ref> 71.00 Meta-Cleaner <ref type="bibr" target="#b39">[40]</ref> 72.50 Meta-Learning <ref type="bibr" target="#b17">[18]</ref> 73.47 P-correction <ref type="bibr" target="#b35">[36]</ref> 73.49 NCT 74.02±0.08</p><p>shows that NCT provides a considerable performance gain (∼10% increase in top1 accuracy) over the prior methods on the WebVision dataset. For Clothing1M, <ref type="table">Table 4</ref> provides marginal gain over P-correction. The empirical results on both clean and noisy versions of benchmark datasets as well as consistent improvement on real-world noisy datasets demonstrate the effectiveness of NCT as a general-purpose learning framework that is robust to label noise. Our method does not perform well on very high levels of noise, as it does not involve identifying clean and noisy samples and treating them differently as the goal of the study is to improve the noise tolerance of the underlying training framework. However, we argue that perhaps very high levels of symmetric noise, e.g. 50% or 90%, do not truly represent the nature of label noise in real-world datasets. While we can expect a considerable amount of label noise, greater than or close to 50% would be implausible. Also, real-world datasets mostly contain structured (asymmetrical noise) with confusion between visually similar classes.</p><p>Furthermore, harder samples, where the orientation of the object, size, position, or the background makes it less indistinguishable from other classes, are more likely to be  misclassified rather than all data points within an object class having an equal chance of being incorrectly labeled. Though still not truly representative, perhaps asymmetric pair flip noise is closer to noise distributions in the real world. Therefore, while these synthetic noisy datasets provide us with key insights and help in comparing the utility of various approaches, overemphasis on high levels of synthetic noise can potentially bias our methods towards noise distributions that are not representative of the real-world noisy datasets. This is particularly applicable to methods which focus on identifying the noisy labels where the noise distribution plays a more crucial role. We hope to bring into attention the need for a uniform set of synthetic noise distributions which are more representative of real-world label noise distributions to better study the characteristics of these datasets and benchmark the utility of different methods. In addition to these, real-world noisy datasets can provide a better estimate of the utility of the proposed approaches in the practical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Compression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE NCT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Information Compression</head><p>To study the effect of our proposed method on the learned representations of the models, we follow the analysis in <ref type="bibr" target="#b15">[16]</ref> to do a comparative study on the effectiveness of NCT to compress information in learned representations relative to standard training under noisy labels. A number of studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref> have shown that there is a relationship between the compression of information in the features learned by DNNs and their generalization. They relate the degree of information compression in the network's hidden states to bounds on generalization whereby stronger compression leads to strong generalization bounds. To this end, we freeze the learned representations of the model and study how well the frozen representations can fit random binary  <ref type="figure" target="#fig_3">Figure 3</ref> shows that NCT is able to consistently learn more compressed features compared to standard training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of Target Variability</head><p>Here, we analyze the sensitivity of our method to the target variability parameters. We use the CIFAR-10 dataset with the same experimental setup as for our previous experiments and show the effect of changing the r max value while keeping all other parameters fixed. <ref type="table" target="#tab_5">Table 5</ref> shows that target variability provides significant performance gain compared to the baseline NCT method without target variability (r max = 0). Generally, for a wide range of target variability rates, 0.3 ≤ r max ≤ 0.7, NCT is not very sensitive to the choice of r max value. The method is more sensitive to the r max value for higher noise levels (50%) compared to the lower noise levels (20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>To analyze the effect of individual components of NCT, we sequentially remove components from the final method and see how the performance is affected. We use the Tiny-ImageNet dataset with the same experimental setup as our previous experiments. (a) The performance drop with NCT <ref type="table">Table 6</ref>. Ablation study on the Tiny-ImageNet dataset. we report the highest (Best) and the average (Avg.) test accuracy (%) over the last 10 epochs. The mean and 1 STD of three different seed values are reported. EN, TV and DB stand for ensemble inference, target variability, and dynamic balancing, respectively.  <ref type="bibr" target="#b40">[41]</ref> which replaces the one-way knowledge transfer from a large pretrained model in traditional knowledge distillation <ref type="bibr" target="#b11">[12]</ref> with knowledge sharing between a cohort of compact models trained collaboratively. The significant drop suggests that progressively shifting the focus of learning from the training labels to building consensus increases the effectiveness of the method to learn under label noise. (d) Finally, the gap between Standard and DML across all noise variations show the effectiveness of collaborative learning under label noise. This shows that all the components contribute to the robustness of NCT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed Noisy Concurrent Training which involves training a cohort of two models in conjunction and building consensus among the two models in addition to the supervised learning loss. The method dynamically shifts the focus of learning from fitting the training labels in the initial learning phases towards building consensus in the later stages. The method also employs target variability as deterrent to memorization and progressively increases the variability during training. We showed the effectiveness of our method on multiple synthetic noisy datasets with varying degrees and types of label noise as well as realworld noisy datasets. Our study shows that increasing the robustness of the underlying training framework as an alternative to filtering and down-weighting noisy labels is a promising direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Average cross-entropy loss and accuracy on CIFAR-10 with 50% symmetric label noise for the training samples with clean and noisy labels across the training epoch. Left: As training progresses standard model with cross-entropy loss (CE) memorizes the noisy labels. Right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>NCT involves training two models concurrently in a collaborative manner whereby each model is trained with a mimicry loss in addition to a supervised learning loss. The two models build consensus by aligning the posterior probabilities through the KL divergence loss. On each epoch, the (noisy) ground-truth labels for the two models are randomly flipped to a different class by a target variability function with rate r d . Target variability is applied independently for each model to prevent memorization and keep the two models diverged. NCT employs a dynamic learning scheme: (i)dynamic balancing function progressively increases the weight of the mimicry loss α d and (ii) target variability function increases the target variability rate r d as training progresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r d = r min , if e ≤ e w r min + (r max − r min ) log[e−ew] log[emax−ew]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparative analysis on the degree to which standard training (CE) and Noisy Concurrent Training (NCT) with frozen learned representations (under varying levels of label noise) can fit binary random labels. Lower training error indicates higher information compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Noisy Concurrent Training Algorithm Input: Dataset D, Number of classes C, Temperature τ , Learning rate η, Batch size b, Total epochs e max , Maximum target variability rate r max , Warmup length e w , Maximum alpha value α max , Ramp-up length e r , Phase shift β Initialize: M1 and M2 parameterized by θ 1 and θ 2 1: while Not Converged do</figDesc><table><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 TARGET_VARIABILITY_FUNCTION Input: Labels y, mini-batch size b, Number of classes C, Target variability rate r d 1: for i ∈ [1, 2] do</figDesc><table><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison with prior methods on CIFAR-10 and CIFAR-100 datasets with symmetric noise. The results for baselines are copied from Arazo et al.<ref type="bibr" target="#b0">[1]</ref> and following them, we report the highest test accuracy (%) across all epochs (Best) and the final epoch accuracy (Last). For our method, we report the average and 1 STD of three different seed values.NCTBest95.6±0.1 94.4±0.1 90.7±0.3 80.1±0.1 74.4±0.2 53.4±0.3 Last 95.5±0.1 94.3±0.0 89.7±0.3 80.0±0.2 74.1±0.1 52.3±0.7</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell cols="2">Alg./Noise (%)</cell><cell></cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>0</cell><cell>20</cell><cell>50</cell></row><row><cell>Standard</cell><cell></cell><cell>Best Last</cell><cell>93.8 93.7</cell><cell>89.7 81.8</cell><cell>84.8 55.9</cell><cell>75.2 75.1</cell><cell>62.8 62.7</cell><cell>48.0 40.8</cell></row><row><cell cols="2">Bootstrap [27]</cell><cell>Best Last</cell><cell>94.7 94.6</cell><cell>86.8 82.9</cell><cell>79.8 58.4</cell><cell>76.1 75.9</cell><cell>62.1 62.0</cell><cell>46.6 37.9</cell></row><row><cell cols="2">F-correction [26]</cell><cell>Best Last</cell><cell>94.7 94.6</cell><cell>86.8 83.1</cell><cell>79.8 59.4</cell><cell>75.4 75.2</cell><cell>61.5 61.4</cell><cell>46.6 37.3</cell></row><row><cell cols="2">Mixup [39]</cell><cell>Best Last</cell><cell>95.3 95.2</cell><cell>95.6 92.3</cell><cell>87.1 77.6</cell><cell>74.8 74.4</cell><cell>67.8 66.0</cell><cell>57.3 46.6</cell></row><row><cell cols="2">M-correction [1]</cell><cell>Best Last</cell><cell>93.6 93.4</cell><cell>94.0 93.8</cell><cell>92.0 91.9</cell><cell>73.3 71.3</cell><cell>73.9 73.4</cell><cell>66.1 65.4</cell></row><row><cell>Noise Type</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Symmetric</cell><cell></cell><cell></cell><cell cols="2">Asymmetric</cell></row><row><cell>Noise (%)</cell><cell></cell><cell>0</cell><cell></cell><cell>20</cell><cell></cell><cell>50</cell><cell></cell><cell>45</cell></row><row><cell>Alg.</cell><cell>Best</cell><cell cols="2">Avg.</cell><cell>Best</cell><cell>Avg.</cell><cell>Best</cell><cell>Avg.</cell><cell>Best</cell><cell>Avg.</cell></row><row><cell>Standard</cell><cell cols="3">57.4±0.5 56.7±0.5</cell><cell>35.8</cell><cell>35.6</cell><cell>19.8</cell><cell>19.6</cell><cell>26.32</cell><cell>26.2</cell></row><row><cell>Decoupling [23]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>37.0</cell><cell>36.3</cell><cell>22.8</cell><cell>22.6</cell><cell>26.61</cell><cell>26.1</cell></row><row><cell>F-correction [26]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>44.5</cell><cell>44.4</cell><cell>33.1</cell><cell>32.8</cell><cell>0.67</cell><cell>0.6</cell></row><row><cell>MentorNet [13]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>45.7</cell><cell>45.5</cell><cell>35.8</cell><cell>35.5</cell><cell>26.61</cell><cell>26.2</cell></row><row><cell cols="4">Co-teaching+ [37] 52.4±0.2 52.1±0.2</cell><cell>48.2</cell><cell>47.7</cell><cell>41.8</cell><cell>41.2</cell><cell>26.87</cell><cell>26.5</cell></row><row><cell>M-correction [1]</cell><cell cols="9">57.7±0.3 57.2±0.4 57.2±0.5 56.6±0.4 51.6±0.3 51.3±0.3 24.8±10.0 24.1±10.3</cell></row><row><cell>NCT</cell><cell cols="7">62.4±0.5 61.5±0.2 58.0±0.2 57.2±0.3 47.8±0.1 47.4±0.2</cell><cell>43.0±0.2</cell><cell>42.4±0.1</cell></row></table><note>Table 2. Comparison with prior methods on Tiny-ImageNet dataset with symmetric and asymmetric pair flip noise. The results for baselines are copied from Yu et al. [37] and following them, we report the highest (Best) and the average (Avg.) test accuracy (%) over the last 10 epochs. For a fair comparison, we run M-Correction on the noise simulation in [37] using their public code and hyperparameters mentioned in their paper. We also run Standard and Co-teaching+ on clean dataset. For all these experiments performed by us, we report the mean and 1 STD of three different seed values.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of target variability rate parameter, rmax, on CIFAR-10. We report the highest test accuracy (%) across all epochs (Best) and the final epoch accuracy (Last). The mean and 1 STD of three different seed values are reported. neurons on top of the frozen representations of PreActResNet-18 model trained on CIFAR-10 under varying degrees of symmetric label noise and fit them on random binary labels. For all experiments, we select the first two classes and assign random labels i.e. the model fits 10000 random labels. The difficulty in fitting the random variables show how well the model compresses information in the learned representations. Therefore, lower training accuracy shows better information compression.</figDesc><table><row><cell></cell><cell cols="2">Symmetric (%)</cell></row><row><cell>r max</cell><cell>20</cell><cell>50</cell></row><row><cell>0.0</cell><cell cols="2">Best 94.25±0.12 85.37±0.27 Last 93.94±0.15 79.60±0.17</cell></row><row><cell>0.1</cell><cell cols="2">Best 94.26±0.09 86.56±0.20 Last 94.08±0.08 81.00±0.23</cell></row><row><cell>0.3</cell><cell cols="2">Best 94.40±0.07 89.35±0.29 Last 94.25±0.03 86.83±0.32</cell></row><row><cell>0.5</cell><cell cols="2">Best 94.25±0.12 90.70±0.28 Last 94.19±0.09 89.74±0.29</cell></row><row><cell>0.7</cell><cell cols="2">Best 93.33±0.08 89.69±0.07 Last 93.21±0.02 89.48±0.25</cell></row><row><cell>0.9</cell><cell cols="2">Best 88.20±0.24 82.88±0.36 Last 87.05±0.13 72.23±0.27</cell></row><row><cell cols="3">labels. For NCT, we pick one of the two trained models. We</cell></row><row><cell cols="3">add a 2-layer multi-layer perceptron (MLP) network with</cell></row><row><cell>400 and 200</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>EN, where only one model, θ 1 is used for inference at test time while the training process remains unchanged, show that the ensemble of two diverged models in NCT consistently provides improvement in performance. (b) The effect of removing target variability from NCT, NCT w/o TV, is more pronounced for higher noise level. For Symmetric 50 and Asymmetric 45, target variability provides considerable gain while it marginally reduces for Symmetric 20. (c) Removing the target variability and dynamic balancing (NCT w/o (TV + DB)) reduces NCT to Deep Mutual Learning (DML)</figDesc><table><row><cell>Noise Type</cell><cell cols="2">Symmetric</cell><cell>Asymmetric</cell></row><row><cell>Noise (%)</cell><cell>20</cell><cell>50</cell><cell>45</cell></row><row><cell>NCT</cell><cell cols="2">Best 58.0±0.2 47.8±0.1 Avg. 57.2±0.3 47.4±0.2</cell><cell>43.0±0.2 42.4±0.1</cell></row><row><cell>NCT w/o EN</cell><cell cols="2">Best 57.0±0.4 46.8±0.1 Avg. 56.2±0.2 46.3±0.2</cell><cell>42.5±0.3 41.6±0.1</cell></row><row><cell>NCT w/o TV</cell><cell cols="2">Best 58.1±0.3 47.0±0.2 Avg. 57.6±0.3 46.4±0.2</cell><cell>42.2±0.3 41.5±0.3</cell></row><row><cell>NCT w/o (TV + DB)</cell><cell cols="2">Best 54.0±0.4 40.0±0.3 Avg. 53.1±0.4 39.2±0.3</cell><cell>39.2±0.4 38.3±0.4</cell></row><row><cell>Standard</cell><cell cols="2">Best 42.1±0.3 24.1±0.3 Avg. 41.1±0.1 23.2±0.2</cell><cell>31.4±0.5 30.2±0.2</cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cultural niche: Why social learning is essential for human adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Richerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="10918" to="10925" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Supplement 2</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05040</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale visual recognition challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ILSVRC 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Noise in the nervous system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldo</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Selen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="303" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 12th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS 231N</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02612</idno>
		<title level="m">Dimensionality-driven learning with noisy labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new baseline for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="316" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling up crowd-sourcing to very large datasets: a case for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Trial-to-trial variability in the responses of neurons carries information about stimulus location in the rat whisker thalamus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Scaglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">A</forename><surname>Moxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guglielmo</forename><surname>Foffani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="14956" to="14961" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatically annotating images with keywords: A review of image annotation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Fong</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihli</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recent Patents on Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
