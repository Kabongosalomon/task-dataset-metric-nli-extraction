<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fare Garnot</roleName><forename type="first">Vivien</forename><surname>Sainte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ENSG, IGN</orgName>
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution">Univ Gustave Eiffel</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mande</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ENSG, IGN</orgName>
								<orgName type="laboratory">LASTIG</orgName>
								<orgName type="institution">Univ Gustave Eiffel</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mande</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Time Sequence · Self-Attention · Multi-Headed Attention · Sentinel Satellite</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing accessibility and precision of Earth observation satellite data offers considerable opportunities for industrial and state actors alike. This calls however for efficient methods able to process time-series on a global scale. Building on recent work employing multi-headed self-attention mechanisms to classify remote sensing time sequences, we propose a modification of the Temporal Attention Encoder of Garnot et al. <ref type="bibr" target="#b4">[5]</ref>. In our network, the channels of the temporal inputs are distributed among several compact attention heads operating in parallel. Each head extracts highly-specialized temporal features which are in turn concatenated into a single representation. Our approach outperforms other state-of-the-art time series classification algorithms on an open-access satellite image dataset, while using significantly fewer parameters and with a reduced computational complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series of remote sensing data, such as satellites images taken at regular intervals, provide a wealth of useful information for Earth monitoring. However, they are also typically very large, and their analysis is resource-intensive. For example, the Sentinel satellites gather over 25 Tb of data every year in the EU. While exploiting the spatial structure of the data poses a challenge on its own, we focus in this paper on the efficient extraction of discriminative temporal features from sequences of spatial descriptors.</p><p>Among the many possible approaches to handling time-series of remote sensing data, one can concatenate observations in the temporal dimension <ref type="bibr" target="#b6">[7]</ref>, use temporal statistics <ref type="bibr" target="#b7">[8]</ref>, histograms <ref type="bibr" target="#b0">[1]</ref>, time-kernels <ref type="bibr" target="#b11">[12]</ref>, or shapelets <ref type="bibr" target="#b15">[16]</ref>. Probabilistic graphical models such as Conditional Random Fields can also be used to exploit the temporal structure of the data <ref type="bibr" target="#b1">[2]</ref>.</p><p>Deep learning-based methods are particularly well-suited for dealing with the large amount of data collected by satellite sensors. Neural networks can either model the temporal dimension independently of the spatial dimensions with arXiv:2007.00586v3 [cs.CV] 8 Jul 2020 recurrent Neural Networks <ref type="bibr" target="#b3">[4]</ref> or one-dimensional convolutions <ref type="bibr" target="#b8">[9]</ref>, or jointly with convolutional recurrent networks <ref type="bibr" target="#b9">[10]</ref> or 3D convolutions <ref type="bibr" target="#b5">[6]</ref>.</p><p>More recently, the self-attention mechanism introduced by Vaswani et al. <ref type="bibr" target="#b12">[13]</ref>, initially developed for Natural Language Processing (NLP), has been successfully used and adapted to remote sensing tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5]</ref>. In Section 2.1, we present these approaches and their differences in greater details.</p><p>In this paper, we introduce the Lightweight Temporal Attention Encoder (L-TAE), a novel attention-based network focusing on memory and computational efficiency. Our approach is based on the Temporal Attention Encoder (TAE) of Garnot et al. <ref type="bibr" target="#b4">[5]</ref>, with several modifications meant to avoid redundant computations and parameters, while retaining a high degree of expressiveness and adaptability. We evaluate the performance of our approach on the openaccess dataset Sentinel2-Agri <ref type="bibr" target="#b4">[5]</ref>. With an equal parameter count, our algorithm outperforms all state-of-the-art competing methods in terms of precision and computational efficiency. Our method allows for efficient parameters usage, as our L-TAE outperforms TAEs with close to 10 times the parameter count, as well as recurrent units over 300 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Throughout this section, we consider a generic input time series of length T comprised of E-dimensional feature vectors e = [e <ref type="bibr" target="#b0">(1)</ref> , · · · , e (T ) ] ∈ R E×T . For example, such vectors can be a sequence of learned embeddings of super-spectral satellite images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Headed Self-Attention</head><p>In its original iteration <ref type="bibr" target="#b12">[13]</ref>, self-attention-initially designed for text translationconsists of the following steps:</p><p>(i) compute a triplet of key-query-value k (t) , q (t) , v (t) for each position t of the input sequence with a shared linear layer applied to e (t) , (ii) compute attention masks representing the compatibility (dot-product) between the queries at each position and the keys corresponding to previous elements in the sequence, (iii) associate to each position of the sequence an output defined as the sum of the previous values weighted by the corresponding attention mask.</p><p>This process is done in parallel for H different sets of independent parametersor heads-whose outputs are then concatenated. This scheme allows each head to specialize in detecting certain characteristics of the feature vectors.</p><p>Rußwurm et al. <ref type="bibr" target="#b10">[11]</ref> propose to apply this architecture to embed sequences of satellite observations by max-pooling the resulting sequence of outputs in the temporal dimension. Garnot et al. <ref type="bibr" target="#b4">[5]</ref> introduce the TAE, a modified selfattention scheme. First, they propose to directly use the input embeddings as values (v (t) = e (t) ), taking advantage of the end-to-end training of the image embedding functions alongside the TAE. Additionally, they define a single master queryq for each sequence, computed from the temporal average of the queries. This master query is compared to the sequence of keys to produce a single attention mask of dimension T used to weight the temporal mean of values into a single feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lightweight Attention</head><p>We build on this effort to adapt multi-headed self-attention to the task of sequence embedding. Our focus is on efficiency, both in terms of parameter count and computational load.</p><p>Channel Grouping: we propose to split the E channels of the input elements into H groups of size E = E/H with H being the number of heads 1 , in the manner of Wu et al. <ref type="bibr" target="#b13">[14]</ref>. We denote by e (t) h the groups of input channels for the h-th group of the t-th element of the input sequence <ref type="bibr" target="#b0">(1)</ref>.</p><p>We encode the number of days elapsed since the beginning of the sequence into an E -dimensional positional vector p of characteristic scale τ = 1000 (2). In order for each head to access this information, p is duplicated and added to each channel group. Each head operates in parallel on its corresponding group of channels, thus accelerating the costly computation of keys and queries. This also allows for each head to specialize alongside its channel group, and avoid redundant operations between heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-as-Parameter:</head><p>We define the K-dimensional master query q h of each head h as a model parameter instead of the results of a linear layer. The immediate benefit is a further reduction of the number of parameters, while the lack of flexibility is compensated by the larger number of available heads.</p><p>Attention Masks: As a result, only the keys are obtained with a learned linear layer <ref type="formula">(3)</ref>, while values are bypassed (v (t) = e (t) ), and the queries are model parameters. The attention masks a h ∈ [0, 1] T of each head h are defined as the scaled softmax of the dot-product between the keys and the master query (4). The outputs o h of each heads are defined as the sum in the temporal dimension of the corresponding inputs weighted by the attention mask a h <ref type="bibr" target="#b4">(5)</ref>. Finally, the heads outputs are concatenated into a vector of size E and processed by a multi-layer perceptron MLP to the desired size <ref type="bibr" target="#b5">(6)</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref>, we represent a schematic representation of our network. The different steps of the L-TAE can also be condensed by the following operations, for h = 1 · · · H and t = 1 · · · T :</p><formula xml:id="formula_0">e (t) h = e (t) [(h − 1)E + i] E i=1 (1) p (t) = sin day(t)/τ i E E i=1 (2) k (t) h = FC h (e (t) h + p (t) ) (3) a h = softmax 1 √ K q h · k (t) h T t=1 (4) o h = T t=1 a h [t] e (t) h + p (t) (5) o = MLP([o 1 , · · · , o H ]) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatio-temporal classifier</head><p>Our proposed L-TAE temporal encoder is meant to be learned alongside a spatial encoding module and a decoder module in an end-to-end fashion <ref type="bibr" target="#b6">(7)</ref>. The spatial encoder S maps a sequence of raw inputs X (t) to a sequence of learned features e (t) , computed independently at each position of the sequence. The decoder D maps the output o of the L-TAE to a target vector y, such as class logits in the case of a classification task.</p><formula xml:id="formula_1">X (t) T t=1 S −−−−→ e (t) T t=1 L-TAE − −−−−− → o D − −−−− → y .<label>(7)</label></formula><p>3 Numerical Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our proposed method with the public dataset Sentinel2-Agri <ref type="bibr" target="#b4">[5]</ref>, comprised of 191 703 sequences of 24 superspectral images of agricultural parcels from January to October. The acquisitions have a spatial resolution of 10m per pixel and 10 spectral bands. Each parcel is annotated within a 20 class nomenclature of agricultural crops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metric and Protocol</head><p>We use two classification metrics to assess the performance of predictions: the Overall Accuracy (OA) and the mean Intersection-over-Union (mIoU). The former accounts for the precision of the prediction regardless of the class distribution, while the latter computes the IoU for each class and averages the results over the class set. Given that the dataset is unbalanced (4 classes represent 90% of the samples) the mIoU gives a more faithful assessment of the performance. We propose two evaluation protocols to assess the efficiency of our proposed light-weight temporal attention encoder:</p><p>• We assess the performance of our method and several state-of-the-art parcel classification algorithms on the dataset Sentinel2-Agri. In order to perform a fair comparison, we chose configurations corresponding to around 150k parameters for all methods. We report the results in <ref type="table" target="#tab_0">Table 1</ref> alongside the theoretical number of floating point operations (in FLOPs) required for the sequence embedding modules to process a single sequence at inference time. • We complement this first experiment by comparing the performance of different configurations of sequence embedding algorithms, and plot the performance with respect to the number of parameters. In order to remove the effects of the different spatial encoders, we use the same spatial encoder (a pixel set encoder <ref type="bibr" target="#b4">[5]</ref>) in all experiments. We only adapt the last linear layer of the spatial encoder to produce embeddings of the desired dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluated Methods</head><p>We evaluate the performance of recent algorithms operating on satellite image time series in order to assess the relative improvement offered by our proposed method.</p><p>-PSE+TAE The approach proposed by Garnot et al. <ref type="bibr" target="#b4">[5]</ref>. They use a Pixel-Set Encoder (PSE) module to encode each image independently, and process the resulting sequence of embeddings with a TAE module. The decoder D is a 2-layer MLP. -PSE+L-TAE Our proposed method. We keep the same architecture as the PSE+TAE, and replace the TAE by our L-TAE network. -CNN+GRU A similar approach <ref type="bibr" target="#b3">[4]</ref> to PSE+TAE, with a CNN instead of the PSE and a Gated Recurrent Unit <ref type="bibr" target="#b2">[3]</ref> instead of the TAE. -CNN+TempCNN Another variation of this architecture, with a twodimensional CNN to encode the images and a one-dimensional CNN processing the temporal dimension independently. This architecture is based on the work of Pelletier et al. <ref type="bibr" target="#b8">[9]</ref>.</p><p>-Transformer Rußwurm et al. were the first to introduce self-attention methods to the classification of remote sensing images. In their work <ref type="bibr" target="#b10">[11]</ref>, the statistics of images is simply averaged over the parcels' pixels, while the resulting sequence is processed by a Transformer network <ref type="bibr" target="#b12">[13]</ref>. The output sequence of embeddings is max-pooled along the temporal dimension to produce a single embedding for the input sequence. -ConvLSTM Rußwurm et al. <ref type="bibr" target="#b9">[10]</ref> combine the embedding of the spatial and temporal dimensions by using a ConvLSTM network <ref type="bibr" target="#b14">[15]</ref>. This work has been adapted to process parcels instead of pixels <ref type="bibr" target="#b4">[5]</ref>. -Random Forest We use the temporal concatenation scheme of Bailly et al. to train a random forest of 100 trees using the parcel-wise mean and standard deviation of the spectral bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we report the performances of competing methods (taken from <ref type="bibr" target="#b4">[5]</ref>) and the L-TAE architecture, all obtained with a 5-fold cross-validation scheme. Our proposed L-TAE architecture outperforms other methods on this dataset both in overall accuracy and mIoU. While the OA is essentially unchanged compared to the TAE, the increase of 0.8 mIoU points is noteworthy since our model is not only simpler but also less computationally demanding by almost an order of magnitude.</p><p>We would like to emphasize that FLOP counts do not necessarily reflect the computational speed of the model in practice. In our non-distributed implementation, the total inference times are dominated by loading times and the spatial embedding module. However, this metric serves to illustrate the simplicity and efficicency of our network. Furthermore, our network maintains a high precision even with a drastic decrease in the parameter count, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We evaluate the four best performing sequence embedding modules (L-TAE, TAE, GRU, TempCNN) in the previous experiment with different configurations, ranging from 9k to 3M parameters. These algorithms all operate with the same decoder and spatial module: a PSE and decoder layer totaling 31k parameters. The smallest L-TAE configuration, with only 9k parameters, achieves a better mIoU score than a TAE with almost 110k parameters, a TempCNN with over 700k parameters, and a GRU with 3M parameters. See <ref type="table">Table 4</ref> in the Appendix for the detailed configurations corresponding to each points. In <ref type="figure" target="#fig_2">Figure 3</ref>, we represent the average attention masks of a 16-head L-TAE for two different classes. We observe that the masks of the different heads focus on narrow and distinct time-extents, i.e. display a high degree of specialization. We also note that the masks are adaptive to the parcels crop types. This suggests that the attention heads are able to cater the learned features to the plant types considered. We argue that our channel grouping strategy, in which each head processes distinct time-stamped features, allows for this specialization and leads to an efficient use of the trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study and Robustness Assessment</head><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report the performance of our proposed L-TAE architecture with different configurations of the following hyper-parameters: number of heads H, dimension of keys K, and number of channels E in the input sequence. We note that our model retains a consistent performance throughout all configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of heads:</head><p>The number of heads seems to only have a limited effect on the performance. We hypothesize that while a higher number of heads H is beneficial, a smaller group size E is however detrimental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Dimension:</head><p>Our experiments show that smaller key dimensions than the typical values used in NLP or for the TAE (K = 32) perform better on our problem. Even 2-dimensional keys allow for the L-TAE to achieve performances similar to the TAE.</p><p>Input Dimension: The variation in performance observed with larger input embeddings is expected: it corresponds to a richer representation. However, the returns are decreasing on the considered dataset with respect to the number of incurred parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-as-Parameter</head><p>In order to evaluate the impact of our different design choices, we train a variation of our network with the same master-query scheme than the TAE. The larger resulting linear layer increases the size of the model for a total of 170k parameters, resulting in a mIoU of only 49.7. This indicates that the query-as-parameter scheme is not only beneficial in terms of compactness but also performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Computational Complexity</head><p>In <ref type="table">Table 3</ref>, we report the asymptotic complexity of different sequence embedding algorithms. For the L-TAE, the channel grouping strategy removes the influence of H in the computation of keys and outputs compared to a TAE or a Transformer. The complexity of the L-TAE is also lower than the GRU's as M , the size of the hidden state, is typically larger than K (130 vs 8 in the experiments presented in <ref type="table" target="#tab_0">Table 1</ref>). <ref type="table">Table 3</ref>. Asymptotic complexity of different temporal extraction modules for the computation of keys, attention masks, and output vectors. For the GRU, the complexity of the memory update is given in the Keys and Mask columns. X is the size of the output vector. M is the size of the hidden state of the GRU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a new lightweight network for embedding sequences of observations such as satellite time-series. Thanks to a channel grouping strategy and the definition of the master query as a trainable parameter, our proposed approach is more compact and computationally efficient than other attention-based architectures. Evaluated on an open-access satellite dataset, the L-TAE performs better than state-of-the-art approaches, with significantly fewer parameters and a reduced computational load, opening the way for continent-scale automated analysis of Earth observation. Our implementation of the L-TAE can be accessed in the open-source repository: github.com/VSainteuf/lightweight-temporal-attention-pytorch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The proposed L-TAE module processing an input sequence e of T vectors of size E, with H = 3 heads and keys of size K. The channels of the input embeddings are distributed among heads. Each head uses a learnt queryq h , while a linear layer FC h maps inputs to keys. The outputs of all heads are concatenated into a vector with the same size as the input embeddings, regardless of the number of heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Performance (in mIoU) of different approaches plotted with respect to the number of parameters in the sequence embedding module. The number of parameters is given on a logarithmic scale. The shaded areas depict the observed standard deviation of mIoU across the five cross-validation folds. The L-TAE outperforms other models across all model sizes, and the smallest 9k-parameter L-TAE instance yields better mIoU than the 100k-parameter TAE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Average attention masks of the L-TAE for parcels of classes Spring Cereal (left) and Summer Cereal (right), for a model with 16 heads (from top to bottom). The masks illustrate how each head focuses on short temporal intervals which depend on crop type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>EK) O(HT K) O(HEX) Transf. O(HT EK) O(HT 2 K) O(HEX) GRU O (M T (E + M )) O(M X)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of our proposed models and competing approaches parameterized to all have 150k parameters approximately. MFLOPs is the number of floating points operations (in 10 6 FLOPs) in the temporal feature extraction module and for one sequence. This only applies to networks which have a clearly separated temporal module.</figDesc><table><row><cell></cell><cell>OA</cell><cell>mIoU</cell><cell>MFLOPs</cell></row><row><cell>PSE+L-TAE (ours)</cell><cell>94.3 ±0.2</cell><cell>51.7 ±0.4</cell><cell>0.18</cell></row><row><cell>PSE+TAE [5]</cell><cell>94.2 ±0.1</cell><cell>50.9 ±0.8</cell><cell>1.7</cell></row><row><cell>CNN+GRU [4]</cell><cell>93.8 ±0.3</cell><cell>48.1 ±0.6</cell><cell>3.6</cell></row><row><cell>CNN+TempCNN [9]</cell><cell>93.3 ±0.2</cell><cell>47.5 ±1.0</cell><cell>0.81</cell></row><row><cell>Transformer [11]</cell><cell>92.2 ±0.3</cell><cell>42.8 ±1.1</cell><cell>1.1</cell></row><row><cell>ConvLSTM [10]</cell><cell>92.5 ±0.5</cell><cell>42.1 ±1.2</cell><cell>-</cell></row><row><cell>Random Forest [2]</cell><cell>91.6 ±1.7</cell><cell>32.5 ±1.4</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Impact of several hyper-parameters on the performance of our method. Underlined, the default parameters values in this study; in bold, the best performance.</figDesc><table><row><cell>H Params. mIoU</cell><cell>K Params. mIoU</cell><cell cols="3">E Params. mIoU</cell></row><row><cell>2 114k 51.6</cell><cell>2 118k 50.7</cell><cell>32</cell><cell>46k</cell><cell>49.6</cell></row><row><cell>4 118k 51.0</cell><cell>4 127k 51.3</cell><cell>64</cell><cell>59k</cell><cell>49.6</cell></row><row><cell>8 127k 51.2</cell><cell>8 143k 51.7</cell><cell cols="2">128 65k</cell><cell>51.1</cell></row><row><cell>16 143k 51.7</cell><cell>16 176k 50.8</cell><cell cols="3">256 143k 51.7</cell></row><row><cell>32 176k 51.2</cell><cell>32 242k 51.2</cell><cell cols="3">512 254k 51.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">E and H are typically powers of 2 and E &gt; H, ensuring that E remains integer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In <ref type="table">Table 4</ref>, we give the exact configurations used to obtain the values in <ref type="figure">Figure 2</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense bag-oftemporal-sift-words for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guyet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Learning on Temporal Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Workshop on Ad</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crop-rotation structured classification using multi-source Sentinel images and LPIS for crop type mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IGARSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Time-space tradeoff in deep learning models for crop classification on satellite multi-spectral image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S F</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IGARSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Satellite image time series classification with pixel-set encoders and temporal self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S F</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for crop classification with multi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parcel-based crop classification in ukraine using Landsat-8 data and Sentinel-1A data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lavreniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Shelestov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Assessing the robustness of random forests to map land cover with high resolution satellite image time series over large areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dedieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Temporal convolutional neural network for the classification of satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional LSTMs for cloud-robust segmentation of remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-attention for raw optical satellite time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Körner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10536</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient temporal kernels between feature sets for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Group normalization. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
