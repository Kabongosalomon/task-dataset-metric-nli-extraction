<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Texture Bias for Few-Shot CNN Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Azad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdur</forename><forename type="middle">R</forename><surname>Fayjie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CRCHUM Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Kauffmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CRCHUM Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CRCHUM Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">1É TS Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Texture Bias for Few-Shot CNN Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the initial belief that Convolutional Neural Networks (CNNs) are driven by shapes to perform visual recognition tasks, recent evidence suggests that texture bias in CNNs provides higher performing models when learning on large labeled training datasets. This contrasts with the perceptual bias in the human visual cortex, which has a stronger preference towards shape components. Perceptual differences may explain why CNNs achieve human-level performance when large labeled datasets are available, but their performance significantly degrades in low-labeled data scenarios, such as few-shot semantic segmentation. To remove the texture bias in the context of few-shot learning, we propose a novel architecture that integrates a set of Difference of Gaussians (DoG) to attenuate high-frequency local components in the feature space. This produces a set of modified feature maps, whose high-frequency components are diminished at different standard deviation values of the Gaussian distribution in the spatial domain. As this results in multiple feature maps for a single image, we employ a bi-directional convolutional long-short-term-memory to efficiently merge the multi scale-space representations. We perform extensive experiments on three well-known fewshot segmentation benchmarks -Pascal i5, COCO-20i and FSS-1000-and demonstrate that our method outperforms state-of-the-art approaches in two datasets under the same conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep models, and particularly convolutional neural networks (CNNs), have shown an impressive performance in many visual recognition tasks, including semantic segmentation <ref type="bibr" target="#b0">[1]</ref>. However, their extreme hunger for labeled training data strongly limits their scalability to novel classes and reduces their applicability to rare categories. Few-shot learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> has appeared as an appealing alternative to train deep models in a low-labeled data scenario. In this setting, the model is trained to accommodate for novel categories with only a handful of labeled images, typically known as support images. In few-shot segmentation approaches, the learned knowledge from the support images is typically fed into a parametric module to guide the segmentation of the unseen images, referred to as queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2003.04052v3 [cs.CV] 23 Dec 2020</head><p>Recent works have demonstrated that the CNN bias towards recognizing textures rather than shapes introduces several benefits under the standard learning paradigm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, which contrasts with the inductive bias found in the human visual cortex, that is driven by shapes <ref type="bibr" target="#b5">[6]</ref>. This does not represent a problem when training and testing classes are drawn from the same distribution in large-labeled datasets. Nevertheless, in low-labeled data regime, the difference on perceptual biases poses difficulties to CNNs to mimic human performance, particularly if there exists a distributional shift between training and testing classes, such as in the few-shot learning scenario <ref type="bibr" target="#b6">[7]</ref>.</p><p>Thus, we argue that attenuating high-frequency local components in the feature space yields to a better generalization to unseen classes in the context of few-shot semantic segmentation. Our motivation is inspired by the findings in <ref type="bibr" target="#b3">[4]</ref>, who showed that CNNs have a strong texture inductive bias that limits their ability to leverage useful low-frequency (e.g, shape) information. Although they show that the representational power of CNN can be improved if CNNs are forced to use shape information (by modifying input images), how to design efficient algorithms that allow CNNs to meaningfully use low-frequency information remains an open problem. We tackle this issue by proposing a novel architecture ( <ref type="figure" target="#fig_0">Fig. 1)</ref> which integrates a set of difference of gaussians (DOGs) <ref type="bibr" target="#b7">[8]</ref> on the feature representations. At each scale-space of the DOGs, the original high-frequency signals are attenuated differently, according to the standard deviation values, σ, employed to model the Gaussian distribution in the spatial domain, which results in multiple versions of the feature maps for a single image. The DoG at two near standard deviations (σ 1 and σ 2 ) will smooth out the features, reducing textural information that the feature extractor may have propagated. Then, following the literature on few-shot segmentation, we generate class representative prototypes from the learned representations, with the difference that in our setting we have multiple prototypes per image, i.e., one at each scale-space of the DOG. Thus, for each query image, our model produces an ensemble of segmentations, each one associated with a prototype. To generate the final prediction, we cast the problem into a sequential segmentation task, where each segmentation on the ensemble represents a time-point. To efficiently fuse temporal, i.e., multiple segmentation masks, and spatial features we resort to a Bi-directional convolutional long-short-term memory (BConvLSTM) <ref type="bibr" target="#b8">[9]</ref>, which bidirectionally encourages information exchange between LSTM units. Furthermore, in the k-shot setting, our approach learns a parametric fusion of the different support images by jointly analyzing their contribution.</p><p>Our contributions can be summarized as follows: (1) we propose to reduce the texture bias in CNNs in the few-shot segmentation task by attenuating high-frequency local components on the feature space, (2) to merge the multiple segmentations produced at different scale-space representations we reformulate the problem as a sequential segmentation task and employ a bi-directional con-vLSTM to efficiently fuse all the information, and (3) we report very competitive results on few-shot segmentation across several public benchmarks, outperforming most recent literature while keeping a light architecture. It first applies a pyramid of difference of gaussians (DoG) on the learned support features to attenuate high-frequency local components on the feature space. To perform segmentation on a query image, the multiple scale-space support representations are combined with the query features, and later fed as input to a bi-directional convLSTM. The convLSTM merges the information from multiple representations and generates the final query segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Few-shot segmentation. Pioneer works on few-shot semantic segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> incorporated two independent branches: a conditional branch that generates the prototypes (e.g., embedding) from the support set, and a segmentation branch, which takes the learned prototypes and the query image as input and produces the segmentation masks. More recently, researchers have unified these dual-branch architectures into a single branch network which can derive better guidance features with the addition of a masked average pooling layer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. For example, a similarity guidance module is integrated in <ref type="bibr" target="#b12">[13]</ref> to recalibrate the query feature map based on a similarity score between the representative prototype and each spatial location on the query features. In <ref type="bibr" target="#b13">[14]</ref>, authors present an approach to generate the weights of the final segmentation layer for the novel classes via imprinting. Other works interchange support and query images for prototype alignment regularization <ref type="bibr" target="#b14">[15]</ref> or to concurrently make predictions <ref type="bibr" target="#b16">[17]</ref> with the goal of achieving better generalization. Nguyen et al. <ref type="bibr" target="#b15">[16]</ref> integrated a regularization that estimates feature relevance by encouraging jointly high-feature activations on the foreground and low-feature activations on the background. Deep attention has also been exploited to learn attention weights between support and query images for further label propagation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. More recently, some researchers have adopted graph CNNs to establish more robust correspondences between support and query images and enrich the prototype representation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Our work differs from previous approaches from a motivation and methodological perspective. While most of the current literature focuses on learning better prototypical representations or iteratively refining these, we approach this problem under the perspective of reducing the inductive texture bias of CNNs. Thus, from a methodological point of view, our approach is the first attempt to integrate a pyramidal set of DoG to address the problem of texture bias.</p><p>Semantic segmentation with conv LSTM. Conv Long-Short Term Memory (LSTM) was presented in <ref type="bibr" target="#b21">[22]</ref> to address the limitations of LSTMs in tasks such as semantic segmentation, where the learned intermediate representations of the input images must preserve the spatial information. Particularly, convLSTM addresses this by integrating a convolution operator in the state-to-state and inputto-state transitions. In the context of image segmentation on 3-dimensional data, e.g. videos or medical imaging, convLSTMs are integrated to encode the spatialtemporal relationships between frames or slices <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. If only 2D images are available instead, an alternative is to leverage convLSTM for multi-level feature fusion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Li et al. <ref type="bibr" target="#b25">[26]</ref> employed convLSTM units to progressively refine the segmentation masks from high-level to low-level features. In <ref type="bibr" target="#b26">[27]</ref>, features derived from the skip connections in the encoding path of UNet <ref type="bibr" target="#b27">[28]</ref> were non-linearly fused with their corresponding features in the decoding path by employing a bi-directional convLSTM, instead of a simple concatenation. In a related work, Hu et al. <ref type="bibr" target="#b17">[18]</ref> employ a ConvLSTM to merge multiple segmentations in a k-shot scenario (k &gt; 1), where each segmentation is generated from a different support image. This differs from our work, where our goal is to fuse the segmentations from a single support image (k = 1) derived from multiple scale-space representations. Furthermore, we use a bidirectional ConvLSTM to foster the exchange of information between the forward and backward path of each recurrent module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Following the standard notation and set-up in few-shot semantic segmentation, we define three datasets: a training set</p><formula xml:id="formula_0">D train = {(X t i , Y t i )} Ntrain i=1 , a support set D support = {(X s i , Y s i )} Nsupport i=1</formula><p>, and a test set D test = {(X q i )} Ntest i=1 . In this setting, X i ∈ R H×W ×3 denotes an RGB image, with H and W being the height and the width of the image, respectively, and Y i ∈ {0, 1} H×W is its corresponding pixel-level mask. Furthermore, each set contains N images. The classes, denoted as c ∈ C, are shared among the support and test set, and are disjoint with the training set, i.e., {C train } ∩ {C support } = ∅.</p><p>The purpose of few-shot learning is to train a neural network f θ (·) on the training set D train to have the ability to segment a novel class c / ∈ C train on the test set D test , based on k references from the support set D support . To reproduce this mechanism during training, the network is trained on D train following the episodic paradigm <ref type="bibr" target="#b28">[29]</ref>. Specifically, assuming a c-way k-shot learning task, each episode is generated by sampling: (1) a support training set</p><formula xml:id="formula_1">D S train = {(X t s , Y t s (c))} k s=1 ⊂ D train for each class c, where Y t s (c)</formula><p>is the binary mask for the class c corresponding to the image X t s and (2) a query set</p><formula xml:id="formula_2">D Q train = {X t q , Y t q (c)} ⊂ D train ,</formula><p>where X t q is the query image and Y t q (c) its corresponding binary mask for the class c. The input of the model is composed of the support training set and the query image, f θ (D S train , X t q ), which are employed to estimate the segmentation mask for the class c in the query image,Ŷ t q (c). Then, the neural network parameters θ are optimized by employing an objective function between Y t q (c) andŶ t q (c) <ref type="bibr" target="#b2">3</ref> . During the testing phase, the model f θ (·) is evaluated on the test set D test given k images from the support set D support .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Removing Texture Bias</head><p>Recent findings suggest that perceptual bias on CNNs do not correlate with those in the human visual cortex <ref type="bibr" target="#b3">[4]</ref>, which may limit the performance of these models in low-labeled data scenarios <ref type="bibr" target="#b6">[7]</ref>. Inspired by this, we propose to reduce the texture bias of CNNs in the context of few-shot segmentation. To achieve this, we integrate a set of difference of gaussians (DoGs) <ref type="bibr" target="#b7">[8]</ref> into the learned feature space to attenuate high-frequency local components, i.e., texture. First, we use a CNN to encode the input images into the latent space, resulting in F s ∈ R W ×H ×M and F q ∈ R W ×H ×M for the support and query samples. The variables W , H and M represent the width, height and feature dimensionality on the latent space, respectively. To encode the high-frequency information during training, we apply a DoG on each channel m ∈ M of the feature map from the support samples F s , which can be formulated as:</p><formula xml:id="formula_3">G s = Γ σ1,σ2 (F s ) = (F m s * 1 2πσ 2 2 exp − x 2 +y 2 2σ 2 ) − (F m s * 1 2πσ 2 1 exp − x 2 +y 2 2σ 1 ), ∀m ∈ M</formula><p>(1) where σ 1 and σ 2 are (σ 2 &gt; σ 1 ) are the variance of the Gaussian filters, x and y represent the spatial position in the encoded feature space and * denotes the convolution operator. To encode different frequency information we apply a pyramid of DoGs with increasing σ values, similar to <ref type="bibr" target="#b7">[8]</ref>. This results in L level representations (L = 4) for each support sample (See <ref type="figure" target="#fig_2">Fig. 2</ref>), where the novel feature maps at each level (l ∈ L) can be denoted as G l s ∈ R W ×H ×M . Support images can contain cluttered background, as well as multiple object categories. Thus, we need to find a representative embedding f s that corresponds exclusively to the target class. Since we have L feature representations, each of them encoding different high-frequency local components, we generate L prototypes per class. To obtain the class prototypes, the novel encoded feature maps at each scale G l s are averaged over the known foreground regions in the support mask Y s (c). Thus, at each level we can estimate f l s as:</p><formula xml:id="formula_4">f l s = 1 |Ỹ s (c)| W H i=1 G l sỸs (c)<label>(2)</label></formula><p>where the support mask Y s (c) is down-sampled toỸ s (c) ∈ {0, 1} H ×W to match the spatial resolution of the feature maps G l s and |Ỹ s (c)| = iỸ s,i (c) is the number of foreground locations inỸ s (c). Then, each prototype is unpooled to the  same spatial resolution as the query features F q and the upsampled prototypes are convolved with F q . We then define the scale-space representation (SSR), which will serve as input signal of the BConvLSTM. This representation can be formulated as a convolution operation between the class representative feature maps at each scale-space and the feature maps derived from the query image:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-Space Representation</head><formula xml:id="formula_5">′ × ′ × ′ × ′ × ∈ ′ × ′ × ′ × ′ × * * * * ∈ ′ × ′ × ∈ ′ × ′ × − ∈ ′ × ′ × Gaussian kernels DOG + × × × × × × × × ′ × ′ × ′ × ′ × ′ × ′ × ′ × ′ × Pool UnPool Support Mask ° ° ° ° Masking ∈ ′ × ′ × ′ ∈ ′ × ′ × ′ ∈ × × Conv &amp; Up-sample</formula><formula xml:id="formula_6">SSR = {BN (ψ l s * F q )}, ∀l ∈ L<label>(3)</label></formula><p>where ψ l s are the upsampled prototypes f l s , and BN denotes a batch normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Scale-Space Representation</head><p>Fusion of the query features F q with the multi-scale class representations from the support features ψ l s produces L joint feature maps, one at each scale-space representation. While logical or average operations may be a straightforward solution to obtain a unique representation, they fail to exploit the inner relationship between sequential scale-space representations. To efficiently solve this, we reformulate the problem as a sequential task, and integrate a bidirectional convolutional long short term memory (BConvLSTM) <ref type="bibr" target="#b8">[9]</ref> on the output of the CNN architecture ( <ref type="figure" target="#fig_2">Fig. 2)</ref>. Even though LSTM have been proposed to deal with sequential problems, this sequential processing strategy may fail to explicitly encode the spatial correlation, since they use full connections in input-to-state and state-to-state transitions. To overcome this limitation, ConvLSTM was proposed in <ref type="bibr" target="#b21">[22]</ref>, which leverages convolution operations into input-to-state and state-tostate transitions instead. Specifically, three gating functions are calculated in the ConvLSTM, which are defined as:</p><formula xml:id="formula_7">i t = σ(W xi * X t + W hi * H t−1 + W ci • C t−1 + b i ) (4) f t = σ(W xf * X t + W hf * H t−1 + W cf • C t−1 + b f ) (5) o t = σ(W xo * X t + W ho * H t−1 + W co • C t−1 + b 0 )<label>(6)</label></formula><p>where X t and H t denote the input (i.e., SSR in eq. (3)) and hidden state at time t, respectively, and b is used to represent the bias term in each state. Similarly, W x , W h and W c represent the set of learnable parameters. Last, '•' denotes the Hadamard product. The LSTM module generates a new proposal for the cell state by looking at the previous H and current X , resulting in:</p><formula xml:id="formula_8">C t = tanh(W xc * X t + W hc * H t−1 + b c )<label>(7)</label></formula><p>Now we linearly combine the newly generated proposalC t with the previous state C t−1 to generate the final cell state in the recurrent model:</p><formula xml:id="formula_9">C t = f t • C t−1 + i t •C t<label>(8)</label></formula><p>Finally, the new hidden state H can be estimated as:</p><formula xml:id="formula_10">H t = o t • tanh(C t )<label>(9)</label></formula><p>Inspired by <ref type="bibr" target="#b8">[9]</ref>, we employ in this work a BConvLSTM to encode the different scale-space representations (SSR) at the output of the convolutional network <ref type="figure" target="#fig_2">(Fig. 2)</ref>. The bidirectional modules with forward and backward paths allow to strength the spatio-temporal information exchanges between the two sides, facilitating the memorization of both past and future sequences. This contrasts with the standard convLSTM, where only the dependencies on the forward direction are employed for the predictions. Thus, the output prediction for a query image X q is given at the output of the BConvLSTM, which is defined as:</p><formula xml:id="formula_11">Y q = tanh(W − → H y * − → H + W ← − H y * ← − H + b)<label>(10)</label></formula><p>where − → H and ← − H represent the hidden states of the forward and backward con-vLSTM units, respectively, and b is the bias term. Last, the output of the BCon-vLSTM is passed through a series of convolutions, followed by upsampling and batch normalization layers to produce the final segmentation masks in the original input image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">k-shot Segmentation</head><p>To fuse the information from several support images (k &gt; 1), most previous works estimate the class prototype ψ by simply taking the average of the representation vectors among k samples (non-parametric approach) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Nevertheless, this strategy assumes that each k sample has equal importance, and thus fails to provide a robust category representation when dealing with noisy or corrupted samples. To deal with this limitation, we propose to use a non-linear parametric method to further improve the model performance on the k-shot setting. The key idea is to first generate the embedded representation between the query and each k support samples and then apply BConvLSTM on these representations to get the final representation in a non-linear parametric fashion. Moving k-shot setting inside the scale-space representation gives the BConvL-STM more freedom to generate better representations using various samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Weakly-supervised Few-shot Segmentation</head><p>To push further the idea of training with very few supervision, we explore the performance of our method when other forms different than full-supervision , i.e., full pixel-level masks, are available. Particularly, we investigate bounding box annotations, which are less time-consuming to obtain than exhaustive segmentation masks. In this context, we relax the support mask by considering all the area inside the bounding box as the foreground. We show in the experiments that, compared to pixel-level annotations, our model achieves very competitive results by employing sparse support annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the datasets employed to evaluate our method and the experimental setting in our experiments. We then report the results compared to state-of-the-art few-shot segmentation approaches, demonstrating the benefits of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform extensive evaluations on three few-shot semantic segmentation benchmarks, i.e., PASCAL-5 i , FSS-1000 and COCO, following standard procedures in the literature. Details are given in Supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Set-up</head><p>Network and implementation details. We employ VGG <ref type="bibr" target="#b29">[30]</ref> and ResNet-101 pre-trained on ImageNet as the backbones for feature extractor. The proposed model is trained end-to-end by using Adam <ref type="bibr" target="#b30">[31]</ref> for 50K episodes with a batch size of 5. The initial learning rate is set to 10 −4 and reduced by 10 −1 at every 10K iterations. The work is carried out using one NVidia Titan X GPU. The code is written in Keras with tensorflow as backend and the code is publicly available at https://github.com/rezazad68/fewshot-segmentation Evaluation protocol. To evaluate the performance of the few-shot segmentation models, we employ the average IoU over all classes (mIoU). As pointed out in <ref type="bibr" target="#b18">[19]</ref>, the mIoU is a better metric, compared to background-foreground IoU (FB-IoU), in the context of few-shot semantic segmentation for several reasons. First, if a given image contains very small objects, the model may completely fail to segment those objects. Nevertheless, the background IoU can still be very high, which misleads information about the real performance of the model. And second, FB-IoU is more suitable for binary segmentation problems, such as video or foreground segmentation, while our purpose is on semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Comparison with state-of-the-art. Comparison of the proposed model with state-of-the-art methods on the FSS-1000 and PASCAL-5 i datasets is reported in Tables 1 and 2, respectively 4 . Results in <ref type="table" target="#tab_0">Table 1</ref> show that the proposed model outperforms the state-of-the-art methods in both 1-shot and 5-shot settings employing the same backbone, i.e., VGG. Particularly, in the 1-shot task, our method achieves a significant improvement of 5.5% over the second best performing model. In the case of 5-shot learning, we found that fusing the segmentations from the different supports in a non-parametric way brings nearly 1% of improvement with respect to the 1-shot setting. Nevertheless, combining the 5 support segmentations in a parametric fashion, i.e., with BConvLSTM, increases the mIoU by 2.5%. It is noteworthy to mention that the method in DAN <ref type="bibr" target="#b20">[21]</ref> uses ResNet-101 as backbone, which might explain the differences between the different methods.  <ref type="table" target="#tab_5">Table 2</ref>. Results of 1-way 1-shot and 1-way 5-shot segmentation on PASCAL-5 i data set employing the mean Intersection-Over-Union (mIoU) metric. Best results for each backbone architecture are highlighted in bold. We employ ∇ to denote the difference between 1-and 5-shot settings. We now report in <ref type="table" target="#tab_5">Table 2</ref> an extensive evaluation of all previous works on PASCAL-5 i , the most common benchmark in few-shot semantic segmentation.To make a fair comparison under different feature extractor backbones, we split the table into three groups. The top group shows the approaches that rely on VGG-16 as backbone architecture, whereas the methods in the middle and bottom groups resort to ResNet-50 and ResNet-101 to extract features, respectively. From the reported values, we can observe that the proposed approach outperforms most previous methods, under the same backbone and in both 1-and 5-shot scenarios. Specifically, compared to the second best performing approach based on VGG-16 (i.e., <ref type="bibr" target="#b16">[17]</ref>), our method achieves nearly 3% and 2% of improvement in 1-and 5shot, respectively. Furthermore, our approach achieves the best and second best performance across all the methods in the 1-and 5-shot scenarios, respectively, regardless of the backbone architecture. These quantitative results demonstrate the strong learning and generalization capabilities of the proposed model in both 1-and 5-shot settings.</p><p>Qualitative results. We depict visual results of the proposed method on Pascal5 i in <ref type="figure" target="#fig_3">Fig 3.</ref> Particularly, the support image-mask pair and the segmentation generated by our method for multiple query images, as well as their corresponding ground truths for several categories are shown. Without any postprocessing step, the proposed model provides satisfying segmentation results on unseen classes with only one annotated support image. It is noteworthy to highlight that the same support image can be employed to segment multiple query images presenting high appearance variability. For example, our model can successfully segment cats (first row of <ref type="figure" target="#fig_3">Fig. 3</ref>) when only fractions of the target are shown, such as the head (first column) or even a partial head (third column). Looking at other categories, e.g., bike or table, we observe that the proposed method can also handle objects viewed from a different perspective or presenting different shapes. This illustrates that our model has a strong ability to successfully generalize to unseen classes from only a handful of labeled examples. Impact of the multiple scale-space representation fusion strategy Logical operations, such as OR, have been typically employed to fuse features from different support images in k-shot segmentation. Even though these operations are straightforward, their result is hard to interpret and they fail to efficiently model the relation between sequential data. Thus, in addition to the results in <ref type="table" target="#tab_0">Table 1</ref>, we show in <ref type="table" target="#tab_2">Table 3</ref> the impact of fusing the multiple scale-space representations by both the simple average operation or an additional convolutional layer. Particularly, employing a convolutional layer to combine multiple scalespace representations brings nearly 1% of improvement compared to the simple average. On the other hand, if we integrate the proposed strategy, the performance is improved by 3% and 2%, respectively. These results demonstrate that our fusion achieves better few-shot segmentation performance. Weakly supervised performance. We further evaluate the proposed model with weaker forms of annotations, e.g., bounding boxes. As reported in <ref type="table" target="#tab_3">Table 4</ref>, our method achieves comparable performance to full supervision when bounding boxes are available in the support set of novel categories. Furthermore, compared to the very recent PANet architecture <ref type="bibr" target="#b14">[15]</ref> our model brings 10% of performance gain in the context of weak supervision. This suggests that our model is able to deal efficiently with noise introduced by bounding box annotations, which ultimately results in more representative class prototypes that approach those obtained by pixel-level annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a novel segmentation network that tackles the challenging problem of few-shot learning from the perspective of reducing the inductive texture bias on CNNs. This contrasts with most prior literature, which focuses on explicitly enhancing the prototypes representation. Particularly, the proposed model presents two novel contributions. First, we integrated a pyramid of Difference of Gaussians to attenuate high-frequency local components in the feature space. Second, to merge information at multiple scale-space representations we reformulated the problem as a sequential task and resorted to bi-directional convolutional LSTMs. For evaluation purposes, we have compared the proposed method to prior work, and performed ablations on important elements of our model on public few-shot segmentation benchmarks. Results demonstrated that the proposed model outperforms most prior methods while maintaining a light architecture, achieving a new state-of-the-art performance on several few-shot semantic segmentation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we first present extensive details on the datasets used in our experiments. Then, we show additional ablation studies and results that support the satisfactory performance of the proposed method.</p><p>1 Extended details on the employed datasets PASCAL-5 i . PASCAL-5 i <ref type="bibr" target="#b9">[10]</ref> is the most popular few-shot segmentation benchmark, which inherits from the well-known PASCAL dataset <ref type="bibr" target="#b41">[42]</ref>. The images in PASCAL-5 i are split into 4 folds, each having 5 classes, with 3 folds used for training and 1 for evaluation. Following the standard procedure in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, we employ 1000 support-query pairs randomly sampled in the test split for each class at test time. More details on PASCAL-5 i are provided in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSS-1000.</head><p>A limitation of PASCAL-5 i is that it contains relatively few distinct tasks, i.e., 20 excluding background and unknown categories. FSS-1000 dataset <ref type="bibr" target="#b31">[32]</ref> alleviates this issue by introducing a more realistic dataset for few-shot semantic segmentation, which emphasizes the number of object classes rather than the number of images. Indeed, FSS-1000 contains a total of 1000 classes, where only 10 images and their corresponding ground truth for each category are provided. Out of the 1000 classes, 240 are dedicated to the test task and the remaining for training. The FSS-1000 dataset <ref type="bibr" target="#b31">[32]</ref> only provides pixel-level annotations. Thus, to investigate the effect of using weak annotations in this dataset we generated bounding box annotations. Each bounding box is obtained from one randomly chosen instance mask in each support image. The generated bounding box annotations are provided with the code employed in the experiments.</p><p>COCO is a challenging large-scale dataset, which contains 80 object categories. Following <ref type="bibr" target="#b25">[26]</ref>, we choose 40 classes for training, 20 classes for validation and 20 classes for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Importance of the pyramidal setting</head><p>The integration of DoG in our model is strongly inspired by the seminal work in <ref type="bibr" target="#b7">[8]</ref>. Thus, we followed the recommended setting, which suggests that 5 scale levels gives optimal results. To understand why employing a single DoG with a larger difference of σ between the gaussian kernels will not perform at the same level than a pyramid of progressive DoG we need to consider how we recognize images at different distances. When we try to recognize objects that are far away, we might be able to just identify rough details, while fine-grained object details become more clear as the image gets closer. Thus, the level of the scale-space is a key factor when trying to recognize discriminative features in an image. The problem, however, is that the optimal scale-space level to discriminate important features for each object is unknown. By blurring the image with different σ values each image represents a different scale-space level, each of them specializing on features at a given 'distance'. In contrast, if we assume a single DoG with a larger difference between the Gaussian kernel variance, intermediate scale-scape levels will be missed. To demonstrate this empirically, we investigated the setting where a single DoG with σ 0 and σ 4 is integrated into the CNN. Results reported in <ref type="table" target="#tab_0">Table 1</ref> shows that a single DoG obtains a mIoU value of 77.67 on the FSS-1000 dataset, underperforming by 3% the pyramidal setting. 3 Ablation study on multi-scale fusion features.</p><p>Similarly to <ref type="bibr" target="#b18">[19]</ref>, we investigated the effect of employing different levels of features, or a combination of those. Particularly, we investigated the three last blocks of VGG-16. In our case, block5 gives the best performance when a single block is used. If multiple blocks are used instead, we observed that combining the three blocks provides the best performance, even though the contribution of the block4 is marginal compared to the fused features from block3 and block5 (+0.26%). The low performance of shallower layers alone can be explained by the fact that they exploit lower-level cues, which are insufficient to properly find object regions. By integrating these with higher-level features, which correspond to object categories, our model can efficiently identify class-agnostic regions on new images. Furthermore, fusion of features at several levels of abstraction can help to handle larger scale object variations. Thus, the final multi-scale model employed in our experiments corresponds to the architecture combining the three last feature blocks. The functionality of the proposed method in the demand of computational resources is also investigated in this work. <ref type="table" target="#tab_2">Table 3</ref> shows the model complexity of several methods, as well as their segmentation results on Pascal5 i for 1-shot. In this table, we include the models that either report their number of parameters or provide reproducible code. We observe that the proposed method is ranked among the lightest methods, while typically achieving the best segmentation performance. Compared to similar methods, in terms of complexity (e.g., co-FCN <ref type="bibr" target="#b11">[12]</ref>, RPMM <ref type="bibr" target="#b19">[20]</ref> or SG-One <ref type="bibr" target="#b12">[13]</ref>), our model brings between 2 and 17% gain on improvement.  <ref type="table" target="#tab_3">Table 4</ref> reports the results for 1-and 5-shot segmentation on COCO dataset. As the backbone architecture plays an important role on the performance of the whole model, we split the results on methods relying on VGG-16 (top) and on ResNet (top). From these results we can see that the proposed method achieves the best performance for 1-shot setting on the VGG-16 group, also outperforming a recent approach with ResNet, i.e., <ref type="bibr" target="#b15">[16]</ref>. Regarding the results on 5-shot, our model obtains similar results, but slightly worst, to those obtained by several approaches with ResNet as backbone. This, together with results on FSS-1000 and Pascal5 i , supports our hypothesis that removing the texture bias can be more efficient in scenarios with very limited supervision (e.g., 1-shot), where our method consistently achieves the best results across three different datasets (under the exact same conditions, i.e., same architecture as backbone). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional visual results</head><p>We include additional qualitative results to assess the performance of our method. First, in <ref type="figure" target="#fig_0">Fig. 1</ref>, visual results on the FSS-1000 class dataset are shown. Similarly to the qualitative examples shown in the main paper, we can observe how our method satisfactorily handles target objects presenting high variability on shape or perspective. This is evident, for example, in the bat images, where our method is able to capture the whole context of a bat flying, while the support image just contained an image of three bats standing in a branch. Then, we also depict failure cases <ref type="figure" target="#fig_2">(Fig. 2)</ref>, where our method does not achieve satisfactory segmentations, or not as good as expected. Typically, these failures come in the form of incomplete segmentations, with small regions of the object not properly identified. The next figure <ref type="figure" target="#fig_3">(Fig. 3)</ref> depicts the results when a bounding box is employed as supervisory signal in the support sample (depicted in purple). Despite the fact that the support mask is noisy, the results achieved by our method are close to the ground truth masks. This, in addition to the quantitative results reported in <ref type="table" target="#tab_3">Table 4</ref> (main paper), shows that the proposed method, once trained on a base dataset, is robust to noise on the support masks. Last, in <ref type="figure">Figure 4</ref>, we depict few samples from the FSS-1000 class dataset, with their corresponding ground truth and the generated bounding box annotation.  <ref type="figure" target="#fig_0">Fig. 1</ref>. Visual results on FSS-1000 class dataset in 1-way 1-shot setting using the proposed method. The support set, as well as predictions on several query images with corresponding ground truths are shown. <ref type="figure" target="#fig_2">Fig. 2</ref>. Visual examples of bad segmentation results on the FSS-1000 class dataset in 1-way 1-shot setting using the proposed method. The support set, as well as predictions on several query images with corresponding ground truths are shown. <ref type="figure" target="#fig_3">Fig. 3</ref>. Visual examples of segmentation results on the FSS-1000 class dataset in 1-way 1-shot setting using the proposed method with bounding box annotations. The support set (i.e., image and its corresponding bounding box annotation), as well as predictions on several query images with corresponding ground truths are shown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed method (DoG-LSTM) for few-shot segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The scale-space encoding block in the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visual results on Pascal-5 i in 1-way 1-shot setting using the proposed method. The support set, as well as predictions on several query images with corresponding ground truths are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of 1-way 1-shot and 1-way 5-shot segmentation on the FSS-1000 data set employing the mean Intersection Over Union (mIoU) metric. Best results in bold.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell></cell><cell>1-shot</cell></row><row><cell>OSLSM [10]</cell><cell>70.3</cell></row><row><cell>co-FCN [12]</cell><cell>71.9</cell></row><row><cell>FSS-1000 [32]</cell><cell>73.5</cell></row><row><cell>FOMAML [33]</cell><cell>75.2</cell></row><row><cell>Baseline</cell><cell>74.2</cell></row><row><cell>Baseline+DoG</cell><cell>78.7</cell></row><row><cell>Baseline+DoG+BConvLSTM</cell><cell>80.8</cell></row><row><cell>DAN [21] (ResNet-101)</cell><cell>85.2</cell></row><row><cell></cell><cell>5-shot</cell></row><row><cell>OSLSM [10]</cell><cell>73.0</cell></row><row><cell>co-FCN [12]</cell><cell>74.3</cell></row><row><cell>FSS-1000 [32]</cell><cell>80.1</cell></row><row><cell>FOMAML+regularization [33]</cell><cell>80.6</cell></row><row><cell>FOMAML+regularization+UHO [33]</cell><cell>82.2</cell></row><row><cell cols="2">Baseline+DoG+BConvLSTM (non-param) 81.7</cell></row><row><cell>Baseline+DoG+BConvLSTM (param)</cell><cell>83.4</cell></row><row><cell>DAN [21] (ResNet-101)</cell><cell>88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We report the results where no additional unlabeled data is employed.</figDesc><table><row><cell></cell><cell cols="2">1-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="5">fold 1 fold 2 fold 3 fold 4 Mean</cell><cell>fold 1</cell><cell cols="5">fold 2 fold 3 fold 4 Mean ∇</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Backbone (VGG 16)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSLSM [10]</cell><cell>BMVC'18</cell><cell cols="5">33.6 55.3 40.9 33.5 40.8</cell><cell>35.9</cell><cell cols="5">58.1 42.7 39.1 43.9 3.1</cell></row><row><cell>co-FCN [12]</cell><cell>ICLRW'18</cell><cell cols="5">36.7 50.6 44.9 32.4 41.1</cell><cell>37.5</cell><cell cols="5">50.0 44.1 33.9 41.4 0.3</cell></row><row><cell>AMP [14]</cell><cell>ICCV'19</cell><cell cols="5">41.9 50.2 46.7 34.7 43.4</cell><cell>41.8</cell><cell cols="5">55.5 50.3 39.9 46.9 3.5</cell></row><row><cell>PANet [15]</cell><cell>ICCV'19</cell><cell cols="5">42.3 58.0 51.1 41.2 48.1</cell><cell>51.8</cell><cell cols="5">64.6 59.8 46.5 55.7 7.6</cell></row><row><cell>FWB[16]</cell><cell>ICCV'19</cell><cell cols="5">47.0 59.6 52.6 48.3 51.9</cell><cell>50.9</cell><cell cols="5">62.9 56.5 50.1 55.1 3.2</cell></row><row><cell cols="7">Meta-Seg [34] IEEE Access'19 42.2 59.6 48.1 44.4 48.6</cell><cell>43.1</cell><cell cols="5">62.5 49.9 45.3 50.2 1.6</cell></row><row><cell>MDL [35]</cell><cell cols="6">COMPSAC'19 39.7 58.3 46.7 36.3 45.3</cell><cell>40.6</cell><cell cols="5">58.5 47.7 36.6 45.9 0.6</cell></row><row><cell>SG-One [13]</cell><cell cols="6">IEEE SMC'20 40.2 58.4 48.4 38.4 46.3</cell><cell>41.9</cell><cell cols="5">58.6 48.6 39.4 47.1 0.8</cell></row><row><cell>OS Adv [36]</cell><cell>Inf. Sci.'20</cell><cell cols="5">46.9 59.2 49.3 43.4 49.7</cell><cell>47.2</cell><cell cols="5">58.8 48.8 47.4 50.6 0.9</cell></row><row><cell>ARNet [37]</cell><cell>ICASSP'20</cell><cell cols="5">42.6 59.5 50.2 40.2 48.1</cell><cell>43.3</cell><cell cols="5">59.8 51.7 41.4 49.1 1.0</cell></row><row><cell>CRNet [17]</cell><cell>CVPR'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">58.5 3.3</cell></row><row><cell>FSS-1000 [32]</cell><cell>CVPR'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.4</cell><cell cols="4">60.9 46.6 42.2 56.8</cell><cell>-</cell></row><row><cell>RPMM [20]</cell><cell>ECCV'20</cell><cell cols="5">47.1 65.8 50.6 48.5 53.0</cell><cell>50.0</cell><cell cols="5">66.5 51.9 47.6 54.0 1.0</cell></row><row><cell>PFNet [38]</cell><cell>TPAMI'20</cell><cell cols="5">56.9 68.2 54.4 52.4 58.0</cell><cell>59.0</cell><cell cols="5">69.1 54.8 52.9 59.0 1.0</cell></row><row><cell>Proposed</cell><cell>-</cell><cell cols="5">56.2 66.0 56.1 53.8 58.0</cell><cell>57.5</cell><cell cols="5">70.6 56.6 57.7 60.6 2.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Backbone (ResNet-50)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CANet [19]</cell><cell>CVPR'19</cell><cell cols="5">52.5 65.9 51.3 51.9 55.4</cell><cell>55.5</cell><cell cols="5">67.8 51.9 53.2 57.1 1.7</cell></row><row><cell>PGNet [39]</cell><cell>ICCV'19</cell><cell cols="5">56.0 66.9 50.6 50.4 56.0</cell><cell>57.7</cell><cell cols="5">68.7 52.9 54.6 58.5 2.5</cell></row><row><cell>LTM [40]</cell><cell>MMM'20</cell><cell cols="5">52.8 69.6 53.2 52.3 57.0</cell><cell>57.9</cell><cell cols="5">69.9 56.9 57.5 60.6 3.6</cell></row><row><cell>CRNet [17]</cell><cell>CVPR'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">58.8 2.9</cell></row><row><cell>PPNet [20]*</cell><cell>ECCV'20</cell><cell cols="5">47.8 58.8 53.8 45.6 51.5</cell><cell>58.4</cell><cell cols="5">67.8 64.9 56.7 62.0 10.5</cell></row><row><cell>RPMM [41]</cell><cell>ECCV'20</cell><cell cols="5">55.2 66.9 52.6 50.7 56.3</cell><cell>56.3</cell><cell cols="5">67.3 54.5 51.0 57.3 1.0</cell></row><row><cell>PFNet [38]</cell><cell>TPAMI'20</cell><cell cols="5">61.7 69.5 55.4 56.3 60.8</cell><cell>63.1</cell><cell cols="5">70.7 55.8 57.9 61.9 1.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Backbone (ResNet-101)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FWB [16]</cell><cell>ICCV'19</cell><cell cols="5">51.3 64.5 56.7 52.2 56.2</cell><cell>54.9</cell><cell cols="5">67.4 62.2 55.3 59.9 3.7</cell></row><row><cell>DAN [21]</cell><cell>ECCV'20</cell><cell cols="5">54.7 68.6 57.8 51.6 58.2</cell><cell>57.9</cell><cell cols="5">69.0 60.1 54.9 60.5 2.3</cell></row><row><cell>PFNet [38]</cell><cell>TPAMI'20</cell><cell cols="5">60.5 69.4 54.4 55.9 60.1</cell><cell>62.8</cell><cell cols="5">70.4 54.9 57.6 61.5 1.4</cell></row><row><cell>Proposed</cell><cell>-</cell><cell cols="5">57.0 67.2 56.1 54.3 58.7</cell><cell>57.3</cell><cell cols="5">68.5 61.5 56.3 60.9 2.2</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of 1-way 1-shot segmentation on the FSS-1000 dataset with different fusion strategies to combine multiple scale-space representations. Best results in bold.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell></cell><cell>1-shot</cell></row><row><cell>Average</cell><cell>77.6</cell></row><row><cell>CNN layer</cell><cell>78.7</cell></row><row><cell cols="2">Proposed (BConvLSTM) 80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Full supervision vs weak-supervision performance in the 1-shot scenario. Type of supervision in brackets.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU</cell></row><row><cell></cell><cell cols="2">FSS-1000 PASCAL</cell></row><row><cell>Proposed (Pixels)</cell><cell>80.8</cell><cell>58.0</cell></row><row><cell>Proposed (Bounding boxes)</cell><cell>78.2</cell><cell>56.4</cell></row><row><cell>PANet [15] (Bounding boxes)</cell><cell>-</cell><cell>45.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Effect of employing a single DoG with σ0 and σ4 vs. a pyramidal DoG with progressive σ values. Results for 1-shot on the FSS-1000 class dataset.</figDesc><table><row><cell></cell><cell>mIoU</cell></row><row><cell>Single DoG</cell><cell>77.7</cell></row><row><cell cols="2">Pyramidal DoG 80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Effect of combining different level feature maps in the encoder network. Best result is highlighted in bold.</figDesc><table><row><cell>Block 3</cell><cell>Block 4</cell><cell>Block 5</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>76.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>78.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>78.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>80.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Parameter complexity in different approaches and their performance (mIoU) on 1-shot segmentation on PASCAL-5 i . Methods are ordered based on number of learnable parameters.</figDesc><table><row><cell>Method</cell><cell>1-shot mIoU</cell><cell>#params(M)</cell></row><row><cell>OSLSM [10]</cell><cell>40.8</cell><cell>276.7</cell></row><row><cell>Meta-Seg [34]</cell><cell>48.6</cell><cell>268.5</cell></row><row><cell>AMP [14]</cell><cell>43.4</cell><cell>34.7</cell></row><row><cell>co-FCN [12]</cell><cell>41.1</cell><cell>34.2</cell></row><row><cell>Proposed</cell><cell>58.0</cell><cell>22.7</cell></row><row><cell>RPMM [41]  †</cell><cell>56.3</cell><cell>19.6</cell></row><row><cell>SG-One [13]</cell><cell>46.3</cell><cell>19.0</cell></row><row><cell>CANet [19] †</cell><cell>55.4</cell><cell>19.0</cell></row><row><cell>PGNet [39] †</cell><cell>56.0</cell><cell>17.2</cell></row><row><cell>Proposed  ‡</cell><cell>58.7</cell><cell>16.3</cell></row><row><cell>PANet [15]</cell><cell>48.1</cell><cell>14.7</cell></row><row><cell>PFNet [38] ‡</cell><cell>60.1</cell><cell>10.8</cell></row></table><note>*Employed architectures: , VGG, † ResNet50, ‡ ResNet101 5 Results on COCO</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Results of 1-way 1-shot and 5-shot segmentation on COCO-20 i data set employing the mean Intersection Over Union (mIoU) metric. Methods are divided according to the backbone used. fold 2 fold 3 fold 4 Mean fold 1 fold 2 fold 3 fold 4 Mean 38.5 40.4 46.8 43.2 40.5 42.7 Employed architectures: † ResNet50, ‡ ResNet101</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell><cell></cell><cell></cell><cell cols="2">5-shot</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="4">fold 1 Backbone (VGG-16)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PANet[15]</cell><cell>ICCV'19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.7</cell></row><row><cell>Proposed</cell><cell>-</cell><cell>20.2</cell><cell>17.8</cell><cell cols="8">21.6 26.8 21.6 22.6 22.0 24.2 31.7 25.1</cell></row><row><cell></cell><cell></cell><cell cols="3">Backbone (ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FWB[16]  ‡</cell><cell cols="2">ICCV'19 18.4</cell><cell>16.7</cell><cell cols="8">19.6 25.4 20.0 20.9 19.2 21.9 28.4 22.6</cell></row><row><cell>OANet [43]  ‡</cell><cell>Arxiv'20</cell><cell>29.6</cell><cell>22.9</cell><cell cols="8">20.3 17.5 22.6 36.6 27.1 25.9 21.9 27.9</cell></row><row><cell>DAN [21]  ‡</cell><cell>ECCV'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.6</cell></row><row><cell cols="3">RPMM (Baseline) [41]  † ECCV'20 25.1</cell><cell>30.3</cell><cell cols="8">24.5 24.7 26.1 26.0 32.4 26.1 27.0 27.9</cell></row><row><cell>RPMM [41]  †</cell><cell cols="2">ECCV'20 29.5</cell><cell>36.8</cell><cell cols="8">29.0 27.0 30.6 33.8 42.0 33.0 33.3 35.5</cell></row><row><cell>PPNet* [20]  †</cell><cell cols="2">ECCV'20 34.5</cell><cell>25.4</cell><cell cols="8">24.3 18.6 25.7 48.3 30.9 35.7 30.2 36.2</cell></row><row><cell>PFNet [38]  ‡</cell><cell cols="2">TPAMI'20 36.8</cell><cell>41.8</cell><cell cols="2">38.7 36.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Support Image Query Image Butterfly Crocodile Boat Ground truth Predicted mask Ground truth Predicted mask Predicted mask Ground truth Bat</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Typically the standard cross-entropy loss function is employed in the few-shot segmentation literature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Results on COCO are given in Supplemental Material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="321" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macleod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08519</idno>
		<title level="m">Texture bias of CNNs limits few-shot classification performance</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper ConvLSTM for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="715" to="731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SG-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">AMP: Adaptive masked proxies for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5249" to="5258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">PANet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CRNet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4165" to="4173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based multicontext guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8441" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combining fully convolutional and recurrent neural networks for 3D biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3036" to="3044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recurrent fully convolutional networks for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-level convolutional LSTM model for the segmentation of left ventricle myocardium in infarcted porcine cine mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Icke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dogdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parimal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sampath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="470" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5745" to="5753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bi-directional convlstm u-net with densley connected convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV Workshops</title>
		<imprint>
			<biblScope unit="page" from="0" to="0" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FSS-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hendryx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Morrison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06290</idno>
		<title level="m">Meta-learning initializations for image segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Meta-seg: A generalized meta-learning framework for multi-class few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="166109" to="166121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale discriminative location-aware network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing novel patterns via adversarial learning for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Arnet: Attention-based refinement network for fewshot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2238" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9587" to="9595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A new local transformation module for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Objectness-aware one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02945</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
