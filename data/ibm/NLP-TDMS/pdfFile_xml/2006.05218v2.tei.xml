<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Super-resolution Variational Auto-Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Gatopoulos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Stol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
						</author>
						<title level="a" type="main">Super-resolution Variational Auto-Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The framework of variational autoencoders (VAEs) provides a principled method for jointly learning latent-variable models and corresponding inference models. However, the main drawback of this approach is the blurriness of the generated images. Some studies link this effect to the objective function, namely, the (negative) log-likelihood (nll). Here, we propose to enhance VAEs by adding a random variable that is a downscaled version of the original image and still use the log-likelihood function as the learning objective. Further, by providing the downscaled image as an input to the decoder, it can be used in a manner similar to the superresolution. We present empirically that the proposed approach performs comparably to VAEs in terms of the nll, but it obtains a better Fréchet Inception Distance (FID) score in data synthesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unlike many other sensory systems, the human visual system (i.e. the components from the eye to neural circuits) develops largely after birth, especially in the first few years of life <ref type="bibr" target="#b1">(Banks &amp; Salapatek, 1978)</ref>. In the beginning, even though the visual structures are fully present, they are still immature in their potentials. As the neural circuits adapt to natural light, they learn to enhance the already known signals with the new information that is becoming available <ref type="figure" target="#fig_2">(Figure 1</ref>). Furthermore, <ref type="bibr" target="#b0">(Ayzenberg, 2019)</ref> suggested that the human visual system for object recognition tasks initially starts with the skeletal structure of the object and then maps other properties, such as textures and colors, onto it before it is classified. It seems that humans reinforce their capabilities by sequentially applying new content of <ref type="figure" target="#fig_2">Figure 1</ref>: Infant vision development during the first six months. The previous processed information is enriched with new signals in order for the sensory systems to be able to analyze high fidelity signals <ref type="bibr" target="#b11">(Haskett, 2019)</ref>.</p><p>information over time and some specific processes, like object detection, are divided into two or more simpler tasks.</p><p>Inspired by this learning procedure, we formulate a generative model that mimics, to some extent, the human visual process. Specifically, we enhance the framework of Variational Auto-Encoder (VAEs) by introducing a downscaled representation of the image as a random variable, and utilize it in a super-resolution manner <ref type="bibr" target="#b3">(Chang et al., 2004;</ref><ref type="bibr" target="#b8">Dong et al., 2015;</ref><ref type="bibr" target="#b9">Freeman et al., 2002)</ref> to generate high quality images. As a result, we obtain a two-level VAE with three latent variables, where one is the downscaled version of the original image.</p><p>In summary, our contributions are as follows:</p><p>• We present a powerful Variational Auto-Encoder that consists of a novel DenseNet-based encoder, a DenseNet-based decoder, and a flow-based prior. It achieves SOTA in terms of the log-likelihood function among singe-leveled VAEs.</p><p>• We propose a new class of VAEs that contain a superresolution part for generating crisp images, and is still trained using the log-likelihood objective.</p><p>• We present empirical results on CIFAR-10 and Ima-geNet32 where our approach achieves descent scores in terms of the bits per dimension (bpd) on CIFAR-10 and ImageNet32, and impressive FID scores. p ϑ (x) = p ϑ (x, z)dz, where ϑ denotes parameters. We consider the optimization through maximum likelihood estimation (MLE) of p ϑ (x), however, it becomes infeasible due to the intractability of the integration at hand. One possible way of overcoming this issue and obtaining a highly scalable framework is by introducing an amortized variational family Q in order to identify its member q φ (z|x) that minimizes the Kullback-Leibler divergence to the real posterior p(z|x). In consequence, we derive a tractable objective function, namely the evidence lower bound (ELBO) <ref type="bibr" target="#b16">(Jordan et al., 1999)</ref>:</p><formula xml:id="formula_0">logp ϑ (x) ≥ E q φ (z|x) log p θ (x, z) q φ (z|x) = E q φ (z|x) [log p θ (x|z) − log q φ (z|x) + log p λ (z)] ≡ L(θ, φ, λ),<label>(1)</label></formula><p>where q φ (z|x) is the variational posterior (or the encoder), p θ (x|z) is the likelihood function (or the decoder) and p λ (z) is the prior over the latent variables, parameterized by and φ, θ and λ respectively. The optimization is done efficiently by computing the expectation by Monte Carlo integration while exploiting the reparameterization trick in order to obtain an unbiased estimator of the gradients. This generative model framework is known as Variational Auto-Encoder (VAE) <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014)</ref>.</p><p>VAE with a bijective prior Even though the lowerbound suggests that the prior plays a crucial role in improving the variational bounds, usually it is modelled by a fixed distribution (i.e., a standard multivariate Gaussian). While being relatively simple and computationally cheap, a fixed prior is known to result in over-regularized models that tend to ignore more of the latent dimensions <ref type="bibr" target="#b2">(Burda et al., 2015;</ref><ref type="bibr">Tomczak &amp; Welling, 2017)</ref>. Moreover, as the objective function is optimised to match the variational posterior with the prior, <ref type="bibr">(Rosca et al., 2018)</ref> argued that even if the former becomes the optimal one, namely the aggregated posterior, it may still not match a unit Gaussian distribution.</p><p>However, it is possible to obtain a rich, multi-modal prior distribution p(z) by using a bijective model. Formally, given a latent code</p><formula xml:id="formula_1">z ∼ q Z (z|x), a base distribution p V (v) on a latent variable v ∈ V , and f : V − → Z consisting of a sequence of L diffeomorphic transformations 1 , where f i (v i−1 ) = v i , v 0 = v and v L = z,</formula><p>the sequential use of the change of variable can be used to express the distribution of z as a function of v as follows:</p><formula xml:id="formula_2">log p Z (z) = log p V (v) − L i=1 log ∂f i (v i−1 ) ∂v i−1 ,<label>(2)</label></formula><p>1 That is, invertible and differentiable transformations.</p><p>where ∂fi(vi−1) ∂vi−1 is the Jacobian-determinant of the i th transformation.</p><p>Thus, using the transformed prior we end up with the following training objective function:</p><formula xml:id="formula_3">L (θ, φ, λ) = E q φ (z|x) log p θ (x|z) − log q φ (z|x) + + log p V (v 0 ) + L i=1 log ∂f −1 i (v i ) ∂v i .<label>(3)</label></formula><p>In this paper, we utilize RealNVP <ref type="bibr" target="#b7">(Dinh et al., 2016)</ref> as the prior, however, any other flow-based model could be used <ref type="bibr">(van den Berg et al., 2018;</ref><ref type="bibr" target="#b18">Kingma &amp; Dhariwal, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model formulation</head><p>Let us introduce an additional variable y ∈ R C that is a compressed representation of x 2 , where C ≤ D. Further, let u ∈ R K and z ∈ R M be two stochastic latent variables that interact with the above observed ones in a way that is presented in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>From the dependencies of the considered probabilistic graphical model, we can write the joint probability as follows:</p><p>p(x, y, z, u) = p(x|y, z) p(z|y, u) p(y|u) p(u).</p><p>Then, we define the amortized variational posterior of p(y, z, u|x) as follows:</p><p>q(y, z, u|x) = q(z|y, x) q(u|y) q(y|x) ≡ q(w|x)</p><p>where w = {y, z, u}, and derive the corresponding lower bound of the likelihood function in the following manner:</p><formula xml:id="formula_4">log p(x) ≥ E q(w) log p(x, w) q(w) = E q(w) log p(x, w) − E q(w) log q(w)<label>(4)</label></formula><p>≡ L(x).</p><p>After expanding and rearranging the above objective function (please see A.1 for full derivation), we obtain:</p><formula xml:id="formula_5">L(x) =E q(z|x,y) q(y|x) log p θ (x|y, z)+ − E q(u|y) KL(q(y|x)||p θ (y|u))+ − E q(u|y)q(y|x) KL(q(z|x, y)||p(z|y, u))+ − E q(y|x) KL(q(u|y)||p(u)),<label>(5)</label></formula><p>where KL(·||·) denotes the Kullback-Leibler divergence. Our approach takes advantage of a compressed representation y of the data in the variational part, that is then utilized in the super-resolution in the generative part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties</head><p>There are two main properties that we are going to take advantage of.</p><formula xml:id="formula_6">A. IF q(y|x) IS BOTH DETERMINISTIC AND DISCRETE, THEN E q(y|x) log q(y|x) = 0.</formula><p>Dependence between two random variables can take a variety of forms, of which stochastic independence and functional dependence can be argued to be most opposite in character. In the former case, neither of the variables provide any information about each other, whereas in the latter, there is a full determination. Even though the proposed framework allows to model q(y|x) as a stochastic dependency, the choice of a deterministic relationship is more attractive, as the transformations to a compressed representation are usually available (e.g., a downscaled image), the optimization process would be faster, easier and the model overall will require less trainable parameters.</p><p>We will define this deterministic transformation as a degenerate probability distribution which provides a way to deal with constant values in a probabilistic framework. It trivially gives rise to a probability mass function satisfying P (Ω) = 1 and has an expectation of a constant value c ∈ R, a variance of 0 and most importantly, its entropy is also equal to 0. Thus, modelling the distribution q(y|x) as a discrete degenerate distribution yields:</p><formula xml:id="formula_7">H q(y|x) = 0 ⇔ E q(y|x) − log q(y|x) = 0</formula><p>which simplifies the derived objective function in (5).</p><p>B. THE DISTRIBUTION q(z|y, x) CAN BE SIMPLIFIED TO q(z|x).</p><p>One of the core motivations behind the architecture of the two staged approach is that the latent variable z will be able to capture the missing information between x and y. While u would allow to produce the global structure of the data (e.g., a shape of a horse), the variation of z will alter high-level features (e.g., varying z will result into a different color of a horse). Thus, since y is a compressed representation of x, it does not introduce any additional information about z that is not already in x. This intuitively allows to model z only using x, and, in essence, to replace q(z|y, x) with q(z|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FINAL ELBO</head><p>With these two properties in mind, the final lower bound of the marginal likelihood of x is the following:</p><formula xml:id="formula_8">L(x) = E q(z|x) q(y|x) log p θ (x|y, z) REx + + E q(u|y)q(y|x) log p θ (y|u) REy + − E q(u|y)q(y|x) KL(q(z|x)||p(z|y, u)) KLz + − E q(y|x) KL(q(u|y)||p(u)) KLu .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Super-resolution VAE (srVAE)</head><p>We choose the following distributions in our model:</p><formula xml:id="formula_9">q φ1 (u|y ) = N u|µ φ1 (y), diag (σ φ1 (y)) ) q(y|x) = δ(y = d(x)) q φ2 (z|x ) = N z|µ φ2 (x), diag (σ φ2 (x)) ) p λ (u ) = p(v) L i=1 det ∂f i (v i−1 ) ∂v i−1 −1 p(v) = N (v|0, 1) p θ1 (y|u ) = K i=1 π (u) i Dlogistic µ (u) i , s (u) i p θ2 (z|y, u ) = N z|µ θ2 (y, u), diag (σ θ2 (y, u) ) ) p θ3 (x|z, y ) = K i=1 π (z,y) i Dlogistic µ (z,y) i , s (z,y) i .</formula><p>where Dlogistic is defined as the discretized logistic distribution <ref type="bibr">(Salimans et al., 2017)</ref>, δ(·) is the Dirac's delta, and d(x) denotes the downscaling transformation that returns a discrete values. In VAEs, it is possible to use the following functionality:</p><p>• Generation: The model is able to generate new images through the following process: z ∼ p(z) → x ∼ p(x|z).</p><p>• Reconstruction: The model allows to reconstruct x by using the following scheme:</p><formula xml:id="formula_10">x → z ∼ q(z|x) → x ∼ p(x|z).</formula><p>Interestingly, our approach allows four operations:</p><p>• Generation: The model allows to generate novel content by applying the following hierarchical sampling process:</p><formula xml:id="formula_11">u ∼ p(u) − → y ∼ p(y|u) − → z ∼ p(z|u, y) − → x ∼ p(x|z, y).</formula><p>• Conditional Generation (or Super-Resolution Generation): Given y, we can sample the latent codes:</p><formula xml:id="formula_12">u ∼ q(u|y) − → z ∼ p(z|y, u), − → x ∼ p(x|z, y). • Reconstruction: Similarly to standard VAE, we can reconstruct x: y ∼ q(y|x) − → z ∼ q(z|x) − → x ∼ p(x|z, y).</formula><p>• Generative Reconstruction: Additionally, we can reconstruct x by combining the generation and the reconstruction:</p><formula xml:id="formula_13">y * ∼ q(y * |x) − → u ∼ q(u|y * ) − → y ∼ p(y|u) − → z ∼ p(z|y, u), − → x ∼ p(x|z, y).</formula><p>In order to highlight the super-resolution part in our model, we refer to it as the super-resolution VAE (srVAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We evaluated the following two models for density estimation; (i) a VAE (ii) the proposed two-level VAE (sr-VAE). In both models we employed RealNVP as a bijective prior <ref type="bibr" target="#b7">(Dinh et al., 2016)</ref>. Specifically, for our model, even though the compressed image y can be given by any deterministic and discrete transformation of the input data (i.e., the image label, grey-scale transformation, Fourier transform, sketch representation), we provide results with a 2× downscaled image. The downscaled images still preserve the global structure of the samples while they disregard the high resolution details. Moreover, it will allow us to evaluate the model for its ability to perform super-resolution tasks. We set K = M = 16 × 8 × 8 as latent dimentions in our experiments. The building blocks of the neural network implementation details are described in the Appendix, see <ref type="figure">Figure 4</ref>. We used a composition of DenseNets <ref type="bibr" target="#b15">(Huang et al., 2016)</ref> and channel attention <ref type="bibr">(Zhang et al., 2018)</ref> with ELUs <ref type="bibr" target="#b6">(Clevert et al., 2015)</ref> as activation functions.</p><p>We applied the proposed model to CIFAR-10 for quantitative and qualitative evaluation of natural images. Additionally, we applied the model trained on CIFAR-10 to ImageNet32, without any additional fine-tuning in order to illustrate its adaption performance to a similar dataset. We evaluate the density estimation performance by bits per dimension (bits/dim), L/(hwc · log(2)), where h, w and c denote the height, width, and channels, respectively and we use the Fréchet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref> as a metric for image generation quality. The negative log-likelihood value (nll) was estimated using 500 weighted samples <ref type="bibr" target="#b2">(Burda et al., 2015)</ref>, and for computing the FID scored we used 10k generated images and 10k real images from the test set, but also 50k generated images and 50k real images from the train set.</p><p>The code for this paper is available at https:// github.com/ioangatop/srVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Natural Images</head><p>Quantitative results The density estimation and the image generation scores on CIFAR-10 and ImageNet32 are presented in <ref type="table" target="#tab_0">Table 1</ref>. Even though the VAE with the Re-alNVP prior follows an architecture without the use of any auto-regressive components and a single stochastic la- tent variable, it achieves a very competitive log-likelihood score (see <ref type="table" target="#tab_2">Table 2</ref> in the Appendix). The importance of the data-driven prior is further supported by the low FID, as it manages to outperform various flow-based generative models (see the Appendix, <ref type="table" target="#tab_3">Table 3</ref>). Finally, analyzing results in <ref type="table" target="#tab_0">Table 1</ref>, we make two observations. First, the RE x is better in case of our approach. This result is to be expected since our model contains a super-resolution part. However, we pay a price for that, namely, we have an extra error coming from RE y . Second, both KL z and KL u are relatively large, and, thus, we claim the model does not suffer from the posterior collapse. Interestingly, the KL part of the VAE is 1966 and 1707 for CIFAR-10 and ImageNet32, respectively, while our model achieves the sum of KL z and KL u around 1500 on both datasets. This result suggests that introducing an additional random variable y helps to match the variational posteriors and the (conditional) priors, and the model does not bypass the latent variable z, verifying its importance.</p><p>Even though our model outperforms the VAE in the reconstruction loss of the original image (RE x ), due to the summation with the value of RE y it results in a poorer likelihood. However, we see that the srVAE significantly improves the FID score, as it produces more coherent and visually pleasing generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results</head><p>We test the performance of the two models on image generation, reconstruction, and in the case of our model, additionally for conditional generation (super-resolution) and generative reconstruction tasks on CIFAR-10. The results are illustrated in <ref type="figure">Figure 3</ref>. The VAE with the bijective prior showcases an excellent performance on the natural image reconstruction task, which is contrary to the performance that is often provided in the literature. This maybe be associated with the effectiveness of a powerful, invertible, data-driven prior like RealNVP and its ability to boost the performance significantly with negligible sacrifice on generation speed, and none on inference. The provided unconditional generations, instead of being characterised as blurry, manage to output images with a coherent global structure.</p><p>In contrast with the VAE, our approach breaks the generation of an image into a two-step process. It first generates a compressed sample through the latent variable u, and then adds local structure with the help of the stochastic variable z. The provided results illustrate that indeed the generations of the first step outputs an outline as a general concept which is then enriched with additional components, resulting in a sharp image. While a proportion of the generations of VAE tend to be noisy and abstract, the two-staged approach seems to generate smoother, higher fidelity results. Moreover, due to our choice of the model, i.e, the compressed image as a 2× downscaled representation, the unconditional generation functionality is essentially a super-resolution task. The model manages to perform accurate reconstructions of the original images, providing a novel generative approach.</p><p>More detailed results and analysis of the conducted experiments are provided in the Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a new type of generative model which is able to perform both conditional and unconditional sampling which demonstrate improved quantitative performance in terms of FID of the generating sample on standard image modelling benchmark. In addition, we demonstrate that VAEs employed with a RealNVP prior can result in a competitive density estimation performance, despite its non-autoregressive architecture form and a single stochastic latent variable. Our approach opens new directions in the VAE framework. First, it allows usage of the log-likelihood-based objective to generate crisp images. Second, the introduction of a downscaled image in the framework alleviates common issues in learning latent variables. Third, it introduces the super-resolution into the VAE framework. All these aspects could be further studied and developed to obtain better quality of generated images.  Interestingly, plugging the above terms back to (4) and rearranging them, we will have</p><formula xml:id="formula_14">L(x) (4) = E q(z|y,x)q(y|x) log p(x|y, z) + E q(z|y,x) q(u|y) q(y|x) log p(z|y, u) − E q(z|y,x) log q(z|y, x) A + + E q(u|y)q(y|x) log p(y|u) + E q(u|y)q(y|x) log p(u) − E q(y|x) log q(y|x) − E q(u|y)q(y|x) log q(u|y) B .</formula><p>Working with term B, one can see that</p><formula xml:id="formula_15">B = E q(u|y)q(y|x) log p(y|u)p(u) q(u|y)q(y|x) ,</formula><p>which denotes a (hidden) lower bound on of the marginal log p(y) with variational posterior q(u|y)q(y|x).</p><p>Thus, the resulted lower bound of the marginal likelihood of x would be L(x) = E q(z|x,y) q(y|x) log p θ (x|y, z) − KL(q(z|x, y)||p(z|y, u)) + E q(u) log p θ (y|u) − KL(q(u|y)||p(u)) = E q(z|x,y) q(y|x) log p θ (x|y, z) − KL(q(z|x, y)||p(z|y, u)) + E q(u|y)q(y|x) log p(y|u)p(u) q(u|y)q(y|x) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Neural Network Architecture</head><p>In <ref type="figure">Figure 4</ref> is depicted the main architecture of the VAE with the bijective prior as well as the optimization choices. The design choices for the encoder and decoder form the building blocks to every model that was trained and evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Supplementary results</head><p>The datasets CIFAR-10 and ImageNet32 were split as described in <ref type="bibr" target="#b14">(Hoogeboom et al., 2019)</ref>. We notice that some papers in the literature use different and, in our opinion, unfair data division.</p><p>Quantitative Results Additional quantitative results are presented in <ref type="table" target="#tab_2">Tables 2 and 3</ref>.</p><p>Qualitative Results Additional qualitative results of the VAE and the srVAE for CIFAR-10 are illustrated in <ref type="figure">Figures 5, 6</ref>, 7 and 10. For ImageNet32, the images are presented in <ref type="figure" target="#fig_6">Figures 8, 9</ref> and 11.  <ref type="figure">Figure 4</ref>: Architecture of our autoencoder. On the right, there are some basic buildings block of the network. The notation as 'G' on the Conv2D channels indicate the growth rate of the densely connected network. The indicates a random variable drawn from a standard Gaussian, which helps us to make use of the reparametrization trick. Until z, we refer to this architecture as Encoder NN and thereafter as Decoder NN. The former and the later form the building blocks to every model that we train and evaluate. All models that are evaluated had ∼ 35M trainable parameters, were trained for 2 thousands epochs, using AdaMax optimizer <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2014)</ref> and the dimensionality of all the latent variables kept at 8 × 8 × 16. We applied weight normalization on the parameters with data-depended initialisation <ref type="bibr" target="#b20">(Salimans &amp; Kingma, 2016)</ref>.    The model initially generates the ×2 downscaled (compressed) representation of the image which intends to captures the "global" structure and then adds the local structure (the "details") while increasing its receptive field. These results indicate that the model successfully captures the global structured information from data on the first stage with the latent variable u and then adds a local structure with the help of the latent variable z, whose responsibility is to capture the missing information between the compressed and the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super-resolution Variational Auto-Encoders</head><p>Super res on ImageNet32 VAE recon on ImageNet32 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super res on ImageNet32</head><p>VAE recon on ImageNet32  <ref type="figure" target="#fig_2">Figure 10</ref>: Comparison on the image reconstruction on CIFAR10 between the VAE with the bijective prior (second row) and the srVAE (third row). Even though both models output images very similar to the original one (top row), the srVAE seems to preserve more details from the ground truth (take a look at images 5, 10 and 15).  <ref type="figure" target="#fig_2">Figure 11</ref>: Comparison on the image reconstruction on ImageNet † between the VAE with the bijective prior (second row) and the srVAE (third row). Again, we can notice that the last row seems to be more similar to the ground truth samples (top row).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Stochastic dependencies of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, OG: Original, SR: Super-Resolution, RS: Reconstruction, GR: Generative Reconstruction Figure 3: Qualitative results of the VAE (a) and the srVAE (b) on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Appendix A. 1 .</head><label>1</label><figDesc>Derivation of the lower boundExpanding the lower bound from (4), from the first part we will haveE q(w) log p(x, w) = E q(w) log p(x|y, z) p(z|y, u) p(y|u) p(u) = E q(z|y,x)q(y|x) log p(x|y, z) + + E q(z|y,x) q(u|y) q(y|x) log p(z|y, u) + E q(u|y)q(y|x) log p(y|u) + E q(u|y)q(y|x) log p(u) ,and for the second E q(w) log q(w) = E q(z|y,x) log q(z|y, x) + E q(y|x) log q(y|x) + E q(u|y) q(y|x) log q(u|y) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Qualitative results on CIFAR-10 of the VAE with the bijective prior. (A) Interpolation (B) Reconstructions (OG: real images, RS: reconstructions) and (C) Unconditional Generations. Qualitative results on CIFAR-10 of the srVAE. (A) Interpolation (B) Reconstructions (OG: real images, RS: reconstructions) and (C) Super-Resolution (CM: downscaled images, OG: real images, RS: conditional generations (superresolution). Super-resolution Variational Auto-Encoders iii) Unconditional generated samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Unconditional generations of the srVAE trained on CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Reconstruction results on ImageNet † from the VAE with the bijective prior. The results show that the model can successfully reconstruct natural images from a different source though its 8 × 8 × 16 dimensional latent space. Top row indicates the ground truth samples and the second rows present the results after reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Super-Resolution results of the srVAE on ImageNet † . Even though the model was trained on CIFAR10, its performance showcases its robustness capabilities. The top, second and third row illustrate the 2× downscaled image, the original and the after the super resolution result of the model, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Negative log-likelihood for CIFAR-10 and ImageNet32 test set. The dataset marked with the symbol † denotes a model trained only on CIFAR-10 and evaluated on ImageNet32. For FID, we provide values obtained on the test set and the training set (in brackets).</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>nll (bits/dim)</cell><cell cols="2">reconstruction loss RE x RE y</cell><cell cols="2">regularization loss KL z KL u</cell><cell>FID</cell></row><row><cell>Cifar10</cell><cell>VAE srVAE</cell><cell>3.51 3.65</cell><cell>5540 5107</cell><cell>-1241</cell><cell>1966 619</cell><cell>-819</cell><cell>41.36 (37.25) 34.71 (29.95)</cell></row><row><cell>ImageNet32  †</cell><cell>VAE srVAE</cell><cell>3.80 4.00</cell><cell>6386 5907</cell><cell>-1257</cell><cell>1707 597</cell><cell>-805</cell><cell>51.82 (N/A) 45.37 (N/A)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks, 2015.Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models, 2014.</figDesc><table><row><cell>Rosca, M., Lakshminarayanan, B., and Mohamed, S. Dis-</cell></row><row><cell>tribution matching in variational inference, 2018.</cell></row><row><cell>Salimans, T. and Kingma, D. P. Weight normalization: A</cell></row><row><cell>simple reparameterization to accelerate training of deep</cell></row><row><cell>neural networks, 2016.</cell></row><row><cell>Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P.</cell></row><row><cell>Pixelcnn++: Improving the pixelcnn with discretized lo-</cell></row><row><cell>gistic mixture likelihood and other modifications, 2017.</cell></row><row><cell>Tomczak, J. M. and Welling, M. Vae with a vampprior.</cell></row><row><cell>arXiv preprint arXiv:1705.07120, 2017.</cell></row><row><cell>Vahdat, A., Macready, W. G., Bian, Z., Khoshaman, A.,</cell></row><row><cell>and Andriyash, E. Dvae++: Discrete variational autoen-</cell></row><row><cell>coders with overlapping transformations, 2018.</cell></row><row><cell>van den Berg, R., Hasenclever, L., Tomczak, J. M., and</cell></row><row><cell>Welling, M. Sylvester normalizing flows for variational</cell></row><row><cell>inference, 2018.</cell></row><row><cell>van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K.</cell></row><row><cell>Pixel recurrent neural networks, 2016a.</cell></row><row><cell>van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt,</cell></row><row><cell>L., Graves, A., and Kavukcuoglu, K. Conditional image</cell></row><row><cell>generation with pixelcnn decoders, 2016b.</cell></row><row><cell>Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., and Fu, Y.</cell></row><row><cell>Image super-resolution using very deep residual channel</cell></row><row><cell>attention networks, 2018.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Generative modelling performance in bits per dimension. The symbol † on the ImageNet32 data denotes that the obtained results were produced using a different downsampling method from the one that was introduced by (van den Oord et al., 2016a), indicating not a fair comparison with the other methods. In the case of Flow++, we provide the results of the variational dequantization in the brackets.</figDesc><table><row><cell>Model Family</cell><cell>Model</cell><cell>CIFAR-10</cell><cell>ImageNet 32x32</cell></row><row><cell></cell><cell>PixelCNN (van den Oord et al., 2016a)</cell><cell>3.14</cell><cell>-</cell></row><row><cell></cell><cell>PixelRNN (van den Oord et al., 2016a)</cell><cell>3.00</cell><cell>3.86</cell></row><row><cell></cell><cell>Gated PixelCNN (van den Oord et al., 2016b)</cell><cell>3.03</cell><cell>3.83</cell></row><row><cell>Autoregressive</cell><cell>PixelCNN++ (Salimans et al., 2017)</cell><cell>2.92</cell><cell>-</cell></row><row><cell></cell><cell>Image Transformer (Parmar et al., 2018)</cell><cell>2.90</cell><cell>3.77</cell></row><row><cell></cell><cell>PixelSNAIL (Chen et al., 2017)</cell><cell>2.85</cell><cell>3.80</cell></row><row><cell></cell><cell>RealNVP (Dinh et al., 2016)</cell><cell>3.49</cell><cell>4.28</cell></row><row><cell></cell><cell>DVAE++ (Vahdat et al., 2018)</cell><cell>3.38</cell><cell>-</cell></row><row><cell></cell><cell>Glow (Kingma &amp; Dhariwal, 2018)</cell><cell>3.35</cell><cell>4.09</cell></row><row><cell>Non-autoregressive</cell><cell>IAF-VAE (Kingma et al., 2016) BIVA (Maaløe et al., 2019)</cell><cell>3.11 3.08</cell><cell>-3.96</cell></row><row><cell></cell><cell>Flow++ (Ho et al., 2019)</cell><cell>3.29 (3.08)</cell><cell>-(3.86)</cell></row><row><cell></cell><cell>VAE with bijective prior (ours)</cell><cell>3.51</cell><cell>3.80  †</cell></row><row><cell></cell><cell>srVAE (ours)</cell><cell>3.65</cell><cell>4.00  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>FID scores obtained from different models trained on CIFAR-10. Lower FID implies better sample quality. All results except ours are taken from. In the case of our VAEs, we provide the values obtained on the test set and the training set (in brackets).</figDesc><table><row><cell>Model</cell><cell>FID</cell></row><row><cell>PixelCNN (van den Oord et al., 2016b)</cell><cell>65.93</cell></row><row><cell>PixelIQN (Ostrovski et al., 2018)</cell><cell>49.46</cell></row><row><cell>iResNet Flow (Liang et al., 2017)</cell><cell>65.01</cell></row><row><cell>GLOW (Kingma &amp; Dhariwal, 2018)</cell><cell>46.90</cell></row><row><cell>Residual Flow (Chen et al., 2019)</cell><cell>46.37</cell></row><row><cell>DCGAN (Radford et al., 2015)</cell><cell>37.11</cell></row><row><cell>WGAN-GP (Gulrajani et al., 2017)</cell><cell>36.40</cell></row><row><cell>VAE with bijective prior (ours)</cell><cell>41.36 (37.25)</cell></row><row><cell>srVAE (ours)</cell><cell>34.71 (29.95)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here, we consider a downscaled x however, our framework allows for any compressed transformation of the original data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skeletal descriptions of shape provide unique perceptual information for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ayzenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename></persName>
		</author>
		<idno>1552-5783</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Acuity and contrast sensitivity in 1-, 2-, and 3-month-old human infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salapatek</surname></persName>
		</author>
		<idno>1552-5783</idno>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology and Visual Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="1978-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Importance weighted autoencoders</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<title level="m">Residual flows for invertible generative modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Human Versus Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haskett</surname></persName>
		</author>
		<ptr target="https://blinkidentity.com/human-versus-computer-vision-2/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Integer discrete flows and lossless compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W T</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Densely connected convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MACHINE LEARNING</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image transformer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
