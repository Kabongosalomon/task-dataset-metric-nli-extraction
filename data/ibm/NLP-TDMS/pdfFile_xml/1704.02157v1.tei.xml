<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<email>eliricci@fbk.eu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Perugia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>4 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<email>niculae.sebe@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multiscale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through extensive experimental evaluation we demonstrate the effectiveness of the proposed approach and establish new state of the art results on publicly available datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While estimating the depth of a scene from a single image is a natural ability for humans, devising computational models for accurately predicting depth information from RGB data is a challenging task. Many attempts have been made to address this problem in the past. In particular, recent works have achieved remarkable performance thanks to powerful deep learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. Assuming the availability of a large training set of RGB-depth pairs, monocular depth prediction is casted as a pixel-level regression problem and Convolutional Neural Network (CNN) architectures are typically employed.</p><p>In the last few years significant effort have been made in the research community to improve the performance of CNN models for pixel-level prediction tasks (e.g. semantic segmentation, contour detection). Previous works have shown that, for depth estimation as well as for other pixellevel classification/regression problems, more accurate es-  <ref type="bibr" target="#b22">[23]</ref>) and fusing multi-layer representations (c) with the approach in <ref type="bibr" target="#b32">[33]</ref> and (d) with the proposed multi-scale CRF. timates can be obtained by combining information from multiple scales <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref>. This can be achieved in different ways, e.g. fusing feature maps corresponding to different network layers or designing an architecture with multiple inputs corresponding to images at different resolutions. Other works have demonstrated that, by adding a Conditional Random Field (CRF) in cascade to a convolutional neural architecture, the performance can be greatly enhanced and the CRF can be fully integrated within the deep model enabling end-to-end training with back-propagation <ref type="bibr" target="#b35">[36]</ref>. However, these works mainly focus on pixel-level prediction problems in the discrete domain (e.g. semantic segmentation). While complementary, so far these strategies have been only considered in isolation and no previous works have exploited multi-scale information within a CRF inference framework.</p><p>In this paper we argue that, benefiting from the flexibility and the representational power of graphical models, we can optimally fuse representations derived from multiple CNN side output layers, improving performance over traditional multi-scale strategies. By exploiting this idea, we introduce a novel framework to estimate depth maps from single still images. Opposite to previous work fusing multi-scale features by averaging or concatenation, we propose to integrate multi-layer side output information by devising a novel approach based on continuous CRFs. Specifically, we present two different methods. The first approach is based on a single multi-scale CRF model, while the other considers a cascade of scale-specific CRFs. We also show that, by introducing a common CNN implementation for mean-fields updates in continuous CRFs, both models are equivalent to sequential deep networks and an end-to-end approach can be devised for training. Through extensive experimental evaluation we demonstrate that the proposed CRF-based method produces more accurate depth maps than traditional multiscale approaches for pixel-level prediction tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref> ( <ref type="figure" target="#fig_0">Fig.1</ref>). Moreover, by performing experiments on the publicly available NYU Depth V2 <ref type="bibr" target="#b29">[30]</ref> and on the Make3D <ref type="bibr" target="#b28">[29]</ref> datasets, we show that our approach outperforms state of the art methods for monocular depth estimation.</p><p>To summarize, the contributions of this paper are threefold. First, we propose a novel approach for predicting depth maps from RGB inputs which exploits multi-scale estimations derived from CNN inner layers by fusing them within a CRF framework. Second, as the task of pixellevel depth prediction implies inferring a set of continuous values, we show how mean field (MF) updates can be implemented as sequential deep models, enabling end-toend training of the whole network. We believe that our MF implementation will be useful not only to researchers working on depth prediction, but also to those interested in other problems involving continuous variables. Therefore, our code is made publicly available 1 . Third, our experiments demonstrate that the proposed multi-scale CRF framework is superior to previous methods integrating information from intermediate network layers by combining multiple losses <ref type="bibr" target="#b32">[33]</ref> or by adopting feature concatenations <ref type="bibr" target="#b9">[10]</ref>. We also show that our approach outperforms state of the art depth estimation methods on public benchmarks and that the proposed CRF-based models can be employed in combination with different pre-trained CNN architectures, consistently enhancing their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Depth Estimation. Previous approaches for depth estimation from single images can be categorized into three main groups: (i) methods operating on hand crafted features, (ii) methods based on graphical models and (iii) methods adopting deep networks. 1 https://github.com/danxuhk/ContinuousCRF-CNN.git Earlier works addressing the depth prediction task belong to the first category. Hoiem et al. <ref type="bibr" target="#b11">[12]</ref> introduced photo pop-up, a fully automatic method for creating a basic 3D model from a single photograph. Karsch et al. <ref type="bibr" target="#b13">[14]</ref> developed Depth Transfer, a non parametric approach where the depth of an input image is reconstructed by transferring the depth of multiple similar images and then applying some warping and optimizing procedures. Ladicky <ref type="bibr" target="#b16">[17]</ref> demonstrated the benefit of combining semantic object labels with depth features.</p><p>Other works exploited the flexibility of graphical models to reconstruct depth information. For instance, Delage et al. <ref type="bibr" target="#b6">[7]</ref> proposed a dynamic Bayesian framework for recovering 3D information from indoor scenes. A discriminatively-trained multiscale Markov Random Field (MRF) was introduced in <ref type="bibr" target="#b27">[28]</ref>, in order to optimally fuse local and global features. Depth estimation was treated as an inference problem in a discrete-continuous CRF in <ref type="bibr" target="#b20">[21]</ref>. However, these works did not employ deep networks.</p><p>More recent approaches for depth estimation are based on CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18]</ref>. For instance, Eigen et al. <ref type="bibr" target="#b8">[9]</ref> proposed a multi-scale approach for depth prediction, considering two deep networks, one performing a coarse global prediction based on the entire image, and the other refining predictions locally. This approach was extended in <ref type="bibr" target="#b7">[8]</ref> to handle multiple tasks (e.g. semantic segmentation, surface normal estimation). Wang et al. <ref type="bibr" target="#b31">[32]</ref> introduced a CNN for joint depth estimation and semantic segmentation. The obtained estimates were further refined with a Hierarchical CRF. The most similar work to ours is <ref type="bibr" target="#b19">[20]</ref>, where the representational power of deep CNN and continuous CRF is jointly exploited for depth prediction. However, the method proposed in <ref type="bibr" target="#b19">[20]</ref> is based on superpixels and the information associated to multiple scales is not exploited. Multi-scale CNNs. The problem of combining informations from multiple scales for pixel-level prediction tasks have received considerable interest lately. In <ref type="bibr" target="#b32">[33]</ref> a deeply supervised fully convolutional neural network was proposed for edge detection. Skip-layer networks, where the feature maps derived from different levels of a primary network are jointly considered in an output layer, have also become very popular <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>. Other works considered multistream architectures, where multiple parallel networks receiving inputs at different scale are fused <ref type="bibr" target="#b3">[4]</ref>. Dilated convolutions (e.g. dilation orà trous) have been also employed in different deep network models in order to aggregate multi-scale contextual information <ref type="bibr" target="#b4">[5]</ref>. We are not aware of previous works exploiting multi-scale representations into a continuous CRF framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Scale Models for Depth Estimation</head><p>In this section we introduce our approach for depth estimation from single images. We first formalize the problem  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-MF C-MF C-MF C-MF C-MF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Fusion with Continuous CRFs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation and Overview</head><p>Following previous works we formulate the task of depth prediction from monocular RGB input as the problem of learning a non-linear mapping F : I → D from the image space I to the output depth space D. More formally,</p><formula xml:id="formula_0">let Q = {(r i ,d i )} Q i=1</formula><p>be a training set of Q pairs, where r i ∈ I denotes an input RGB image with N pixels and d i ∈ D represents its corresponding real-valued depth map.</p><p>For learning F we consider a deep model made of two main building blocks <ref type="figure">(Fig. 2</ref>). The first component is a CNN architecture with a set of intermediate side outputs S = {s l } L l=1 , s l ∈ R N , produced from L different layers with a mapping function f s (r; Θ, θ l ) → s l . For simplicity, we denote with Θ the set of all network layer parameters and with θ l the parameters of the network branch producing the side output associated to the l-th layer (see Section 4.1 for details of our implementation). In the following we denote this network as the front-end CNN.</p><p>The second component of our model is a fusion block. As shown in previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>, features generated from different CNN layers capture complementary information. The main idea behind the proposed fusion block is to use CRFs to effectively integrate the side output maps of our front-end CNN for robust depth prediction. Our approach develops from the intuition that these representations can be combined within a sequential framework, i.e. performing depth estimation at a certain scale and then refining the obtained estimates in the subsequent level. Specifically, we introduce and compare two different multi-scale models, both based on CRFs, and corresponding to two different version of the fusion block. The first model is based on a single multi-scale CRFs, which integrates information available from different scales and simultaneously enforces smoothness constraints between the estimated depth values of neighboring pixels and neighboring scales. The second model implements a cascade of scale-specific CRFs: at each scale l a CRF is employed to recover the depth information from side output maps s l and the outputs of each CRF model are used as additional observations for the subsequent model. In Section 3.2 we describe the two models in details, while in Section 3.3 we show how they can be implemented as sequential deep networks by stacking several elementary blocks. We call these blocks C-MF blocks, as they implement Mean Field updates for Continuous CRFs <ref type="figure">(Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fusing side outputs with continuous CRFs</head><p>We now describe the proposed CRF-based models for fusing multi-scale representations.</p><p>Multi-scale CRFs. Given an LN -dimensional vectorŝ obtained by concatenating the side output score maps {s 1 , . . . , s L } and an LN -dimensional vector d of realvalued output variables, we define a CRF modeling the conditional distribution:</p><formula xml:id="formula_1">P (d|ŝ) = 1 Z(ŝ) exp{−E(d,ŝ)} (1) where Z(ŝ) = d exp −E(d,ŝ)dd is the partition function.</formula><p>The energy function is defined as:</p><formula xml:id="formula_2">E(d,ŝ) = N i=1 L l=1 φ(d l i ,ŝ) + i,j l,k ψ(d l i , d k j ) (2)</formula><p>and d l i indicates the hidden variable associated to scale l and pixel i. The first term is the sum of quadratic unary terms defined as:</p><formula xml:id="formula_3">φ(d l i ,ŝ) = d l i − s l i 2 (3)</formula><p>where s l i is the regressed depth value at pixel i and scale l obtained with f s (r; Θ, θ l ). The second term is the sum of pairwise potentials describing the relationship between pairs of hidden variables d l i and d k j and is defined as follows:</p><formula xml:id="formula_4">ψ(d l i , d k i ) = M m=1 β m w m (i, j, l, k, r)(d l i − d k j ) 2 (4)</formula><p>where w m (i, j, l, k, r) is a weight which specifies the relationship between the estimated depth of the pixels i and j at scale l and k, respectively.</p><p>To perform inference we rely on mean-field approxima-</p><formula xml:id="formula_5">tion, i.e. Q(d|ŝ) = N i=1 L l=1 Q i,l (d l i |ŝ).</formula><p>Following <ref type="bibr" target="#b24">[25]</ref>, by considering J i,l = log Q i,l (d l i |ŝ) and rearranging its expression into an exponential form, the following mean-field updates can be derived:</p><formula xml:id="formula_6">γ i,l = 2 1 + 2 M m=1 β m k j,i w m (i, j, l, k, r) (5) µ i,l = 2 γ i,l s l i + 2 M m=1 β m k j,i w m (i, j, l, k, r)µ j,k<label>(6)</label></formula><p>To define the weights w m (i, j, l, k, r) we introduce the following assumptions. First, we assume that the estimated depth at scale l only depends on the depth estimated at previous scale. Second, for relating pixels at the same and at previous scale, we set weights depending on m Gaussian</p><formula xml:id="formula_7">kernels K ij m = exp − h m i −h m j 2 2 2θ 2 m .</formula><p>Here, h m i and h m j indicate some features derived from the input image r for pixels i and j. θ m are user defined parameters. Following previous works <ref type="bibr" target="#b14">[15]</ref>, we use pixel position and color values as features, leading to two Gaussian kernels (i.e. an appearance and a smoothness kernel) for modeling dependencies of pixels at scale l and other two for relating pixels at neighboring scales. Under these assumptions, the meanfield updates <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref> can be rewritten as:</p><formula xml:id="formula_8">γ i,l = 2 1 + 2 2 m=1 β m j =i K ij m + 2 4 m=3 β m j,i K ij m (7) µ i,l = 2 γ i,l s l i + 2 2 m=1 β m j =i K ij m µ j,l , +2 4 m=3 β m j,i K ij m µ j,l−1<label>(8)</label></formula><p>Given a new test image, the optimald can be computed maximizing the log conditional probability <ref type="bibr" target="#b24">[25]</ref>, i.e.d = arg max d log(Q(d|S)), whered = [µ 1,1 , ..., µ N,L ] is a vector of the LN mean values associated to Q(d|ŝ). We take the estimated variables at the finer scale L (i.e. µ N,1 , ..., µ N,L ) as our predicted depth map d .</p><p>Cascade CRFs. The cascade model is based on a set of L CRF models, each one associated to a specific scale l, which are progressively stacked such that the estimated depth at previous scale can be used to define the features of the CRF model in the following level. Each CRF is used to compute the output vector d l and it is constructed considering the side output representations s l and the estimated depth at the previous stepd l−1 as observed variables,</p><formula xml:id="formula_9">i.e. o l = [s l ,d l−1 ].</formula><p>The associated energy function is defined as:</p><formula xml:id="formula_10">E(d l , o l ) = N i=1 φ(d l i , o l ) + i =j ψ(d l i , d l j ).<label>(9)</label></formula><p>The unary and pairwise terms can be defined analogously to the unified model. In particular the unary term, reflecting the similarity between the observation o i l and the hidden depth value d l i , is:</p><formula xml:id="formula_11">φ(y l i , o l ) = d l i − o l i 2 (10) where o l</formula><p>i is obtained combining the regressed depth from side outputs s l and the map d l−1 estimated by CRF at previous scale. In our implementation we simply consider o l i = s l i +d l−1 i , but other strategies can be also considered. The pairwise potentials, used to force neighboring pixels with similar appearance to have close depth values, are:</p><formula xml:id="formula_12">ψ(d l i , d l j ) = M m=1 β m K ij m (d l i − d l j ) 2<label>(11)</label></formula><p>where we consider M = 2 Gaussian kernels, one for appearance features, the other accounting for pixel positions.</p><p>Similarly to the multi-scale model, under mean-field approximation, the following updates can be derived:</p><formula xml:id="formula_13">γ i,l = 2 1 + 2 M m=1 β m j =i K ij m<label>(12)</label></formula><formula xml:id="formula_14">µ i,l = 2 γ i,l o l i + 2 M m=1 β m j =i K ij m µ j,l<label>(13)</label></formula><p>At test time, we use the estimated variables corresponding to the CRF model of the finer scale L as our predicted depth map d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-scale models as sequential deep networks</head><p>In this section, we describe how the two proposed CRFsbased models can be implemented as sequential deep networks, enabling end-to-end training of our whole network model (front-end CNN and fusion module). We first show how the mean-field iterations derived for the multi-scale and the cascade models can be implemented by defining a common structure, the C-MF block, consisting into a stack of CNN layers. Then, we present the resulting sequential network structures and detail the training phase.</p><p>C-MF: a common CNN implementation for two models. By analyzing the two proposed CRF models, we can observe that the mean-field updates derived for the cascade and for the multi-scale models share common terms. As stated above, the main difference between the two is the way the estimated depth at previous scale is handled at the current scale. In the multi-scale CRFs, the relationship among neighboring scales is modeled in the hidden variable space, while in the cascade CRFs the depth estimated at previous scale acts as an observed variable.</p><p>Starting from this observation, in this section we show how the computation of Eq. <ref type="formula" target="#formula_8">(8)</ref> and <ref type="formula" target="#formula_14">(13)</ref> can be implemented with a common structure. <ref type="figure" target="#fig_3">Figure 3</ref> describes in details these computations. In the following, for the sake of clarity, we introduce matrices. Let S l ∈ R W ×H be the matrix obtained rearranging the N = W H pixels corresponding to the side outputs vector s l and µ t l ∈ R W ×H the matrix of the estimated output variables associated to scale l and mean field iteration t. To implement the multi-scale model at each iteration t, µ t−1 l and µ t l−1 are convolved by two Gaussian kernels. Following <ref type="bibr" target="#b14">[15]</ref>, we use a spatial and a bilateral kernel. As Gaussian convolutions represent the computational bottleneck in the mean-field iterations, we adopt the permutohedral lattice implementation <ref type="bibr" target="#b0">[1]</ref> to approximate filter response calculation reducing computational cost from quadratic to linear <ref type="bibr" target="#b24">[25]</ref>. The weighing of the parameters β m is performed as a convolution with a 1×1 filter. Then, the outputs are combined and are added to the side output maps S l . Finally, a normalization step follows, corresponding to the calculation of <ref type="bibr" target="#b6">(7)</ref>. The normalization matrix γ ∈ R W ×H is also computed by considering Gaussian kernels convolutions and weighting with parameters β m . It is worth noting that the normalization step in our mean-field updates for continuous CRFs is substantially different from that of discrete CRFs <ref type="bibr" target="#b35">[36]</ref> based on a softmax function.</p><p>In the cascade CRF model, differently from the multiscale CRF, µ t l−1 acts as an observed variable. To design a common C-MF block among the two models, we introduce two gate functions G1 and G2 <ref type="figure" target="#fig_3">(Fig. 3)</ref> controlling the computing flow and allowing to easily switch between the two approaches. Both gate functions accept a user-defined boolean parameter (here 1 corresponds to the multi-scale CRF and 0 to the cascade model). Specifically, if G1 is equal to 1, the gate function G1 passes µ t l−1 to the Gaussian filtering block, otherwise to the addition block with unary term. Similarly, G2 controls the computation of the normalization terms and switches between the computation of <ref type="formula">(7)</ref> and <ref type="bibr" target="#b11">(12)</ref>. Importantly, for each step in the C-MF block we implement the calculation of error differentials for the backpropogation as in <ref type="bibr" target="#b35">[36]</ref>. For optimizing the CRF parameters,  similar to <ref type="bibr" target="#b35">[36]</ref>, the bandwidth values θ m are fixed and we implement the differential computation for the weights of Gaussian kernels β m . In this way β m are learned automatically with back-propagation.</p><formula xml:id="formula_15">2 4 4 = J 2 2μ t 1 l,2 µ t 1 l,1 = K1 ⌦ µ t 1 l µ t 1 l 2 1μ t 1 l,1 µ t 1 l,2 = K2 ⌦ µ t 1 l µ t 1 l µ t l 1,1 = K3 ⌦ µ t l 1 2 4μ t l 1,2 2 3μ t l 1,1 µ t l µ t l 1 µ t l =μ t l ↵ µ t l µ t 1 lμ t l 1,2 = K4 ⌦ µ t l 1 µ t l = Sl μ t l 3 4 Sl 2 2( 2 J) 1 = K1 ⌦ J 2 = K2 ⌦ J Bilateral Filtering Spatial Filtering 3 = K3 ⌦ J 4 = K4 ⌦ J</formula><p>From mean-field updates to sequential deep networks. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the implementation of the proposed two CRF-based models using the C-MF block described above.</p><p>In the figure, each blue box is associated to a mean-field iteration. The cascade model ( <ref type="figure" target="#fig_4">Fig. 4-left)</ref> consists of L single-scale CRFs. At the l-th scale, t l mean-field iterations are performed and then the estimated outputs are passed to the CRF model of the subsequent scale after a Rectified Linear Unit (ReLU) operation. To implement a single CRF, we stack t l C-MF blocks and make them share the parameters, while we learn different parameters for different CRFs. For the multi-scale model, one full mean-field updates involves L scales simultaneously, obtained by combining L C-MF blocks. We further stack T iterations for learning and inference. The parameters corresponding to different scales and different mean-field iterations are shared. In this way, by using the common C-MF layer, we implement the two proposed CRFs models as deep sequential networks enabling end-to-end training with the front-end network.</p><p>Training the whole network. We train the network using a two phase scheme. In the first phase (pretraining), the parameters Θ and {θ l } L l=1 of the front-end network are learned by minimizing the sum of L distinct side losses as in <ref type="bibr" target="#b32">[33]</ref>, corresponding to L side outputs. We use a square loss over Q training samples:</p><formula xml:id="formula_16">L P = L l=1 Q i=1 s l,i −d i 2 2 .</formula><p>In the second phase (fine tuning), we initialize the front-end network with the learned parameters in the first phase, and jointly fine-tune with the proposed multi-scale CRF models to compute the optimal value of the parameters Θ, </p><formula xml:id="formula_17">{θ l } L l=1 · · · µ 2 1 · · · · · · … µ t2</formula><formula xml:id="formula_18">F = Q i=1 F (r i ; Θ, θ l , β) − d l i 2 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of the proposed multiscale CRF models for monocular depth prediction, we performed experiments on two publicly available datasets: the NYU Depth V2 <ref type="bibr" target="#b29">[30]</ref> and the Make3D <ref type="bibr" target="#b26">[27]</ref> datasets. In the following we describe the details of our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. The NYU Depth V2 dataset <ref type="bibr" target="#b29">[30]</ref> contains 120K unique pairs of RGB and depth images captured with a Microsoft Kinect. The datasets consists of 249 scenes for training and 215 scenes for testing. The images have a resolution of 640 × 480. To speed up the training phase, following previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref> we consider only a small subset of images. This subset has 1449 aligned RGB-depth pairs: 795 pairs are used for training, 654 for testing. Following <ref type="bibr" target="#b8">[9]</ref>, we perform data augmentation for the training samples. The RGB and depth images are scaled with a ratio ρ ∈ {1, 1.2, 1.5} and the depths are divided by ρ. Additionally, we horizontally flip all the samples and bilinearly down-sample them to 320 × 240 pixels. The data augmentation phase produces 4770 training pairs in total.</p><p>The Make3D dataset <ref type="bibr" target="#b26">[27]</ref> contains 534 RGB-depth pairs, split into 400 pairs for training and 134 for testing. We resize all the images to a resolution of 460 × 345 as done in <ref type="bibr" target="#b20">[21]</ref> to preserve the aspect ratio of the original images. We adopted the same data augmentation scheme used for NYU Depth V2 dataset but, for ρ = {1.2, 1.5} we generate two samples each, obtaining 4K training samples.</p><p>Front-end CNN Architectures. To study the influence of the frond-end CNN, we consider several network architectures including: (i) AlexNet <ref type="bibr" target="#b15">[16]</ref>, (ii) VGG16 <ref type="bibr" target="#b30">[31]</ref>, (iii) an encoder-decoder network derived from VGG (VGG-ED) <ref type="bibr" target="#b1">[2]</ref>, (iv) VGG Convolution-Deconvolution (VGG-CD) <ref type="bibr" target="#b22">[23]</ref>, and (v) ResNet50 <ref type="bibr" target="#b10">[11]</ref>. For AlexNet, VGG16 and ResNet50, we obtain the side outputs from different convolutional blocks in which each convolutional layer outputs feature maps with the same size using a similar scheme as in <ref type="bibr" target="#b32">[33]</ref>. The number of side-outputs is 5, 5 and 4 for AlexNet, VGG16 and ResNet50, respectively. As VGG-ED and VGG-CD have been widely used for pixellevel prediction tasks, we also consider them in our analysis. Both VGG-ED and VGG-CD have a symmetric structure, and we use the corresponding part of VGG16 for their encoder/convolutional block. Five side-outputs are then extracted from the convolutional blocks of the decoder/deconvolutional part.</p><p>Evaluation Metrics. Following previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>, we adopt the following evaluation metrics to quantitatively assess the performance of our depth prediction model. Specifically, we consider: (i) mean relative error (rel): 1</p><formula xml:id="formula_19">N i |di−d i | d i ;</formula><p>(ii) root mean squared error (rms):</p><formula xml:id="formula_20">1 N i (d i − d i ) 2 ;</formula><p>(iii) mean log10 error (log10):</p><p>1 N i log 10 (d i ) − log 10 (d i ) and (iv) accuracy with threshold t: percentage (%) of d i subject to max(</p><formula xml:id="formula_21">d ī di ,d i d i ) = δ &lt; t (t ∈ [1.25, 1.25 2 , 1.25 3 ]).</formula><p>Implementation Details. We implement the proposed deep model using the popular Caffe framework <ref type="bibr" target="#b12">[13]</ref> on a single Nvidia Tesla K80 GPU with 12 GB memory. As described in Section 3.3, training consists of a pretraining and a fine tuning phase. In the first phase, we train the frontend CNN with parameters initialized with the corresponding ImageNet pretrained models. For AlexNet, VGG16, VGG-ED and VGG-CD, the batch size is set to 12 and for ResNet50 to 8. The learning rate is initialized at 10 −11 and decreases by 10 times around every 50 epochs. 80 epochs are performed for pretraining in total. The momentum and the weight decay are set to 0.9 and 0.0005, respectively. When the pretraining is finished, we connect all the side outputs of the front-end CNN to our CRFs-based multiscale deep models for end-to-end training of the whole net-Method Error (lower is better) Accuracy (higher is better) rel log10 rms δ &lt; 1.  work. In this phase, the batch size is reduced to 6 and a fixed learning rate of 10 −12 is used. The same parameters of the pre-training phase are used for momentum and weight decay. The bandwidth weights for the Gaussian kernels are obtained through cross validation. The number of mean-field iterations is set to 5 for efficient training for both the cascade CRFs and multi-scale CRFs. We do not observe significant improvement using more than 5 iterations. Training the whole network takes around 25 hours on the Make3D dataset and ∼ 31 hours on the NYU v2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Analysis of different multi-scale fusion methods. In the first series of experiments we consider the NYU Depth V2 dataset. We evaluate the proposed CRF-based models and compare them with other methods for fusing multiscale CNN representations. Specifically, we consider: (i) the HED method in <ref type="bibr" target="#b32">[33]</ref>, where the sum of multiple side output losses is jointly minimized with a fusion loss (we use the square loss, rather than the cross-entropy, as our problem involves continuous variables), (ii) Hypercolumn <ref type="bibr" target="#b9">[10]</ref>, where multiple score maps are concatenated and (iii) a CRF applied on the prediction of the front-end network (last layer) a posteriori (no end-to-end training). In these experiments we consider VGG-CD as front-end CNN.</p><p>The results of our comparison are shown in <ref type="table">Table 1</ref>. It is evident that with our CRFs-based models more accurate depth maps can be obtained, confirming our idea that integrating complementary information derived from CNN side output maps within a graphical model framework is more effective than traditional fusion schemes. The table also compares the proposed cascade and multi-scale models. As expected, the multi-scale model produces more accurate depth maps, at the price of an increased computational cost. Finally, we analyze the impact of adopting multiple scales and compare our complete models (5 scales) with their version when only a single and three side output  layers are used. It is evident that the performance improves by increasing the number of scales.</p><p>As the proposed models are based on the idea of progressively refining the obtained prediction results from previous layers, we also analyze the influence of the stacking order on the performance of the cascade model ( <ref type="table" target="#tab_2">Table 2)</ref>. We compare two different schemes: the first indicating that the cascade model operates from the inner to the outer layers and the other representing the reverse order. Our results confirm the validity of our original assumption: a coarse to fine approach leads to more accurate depth maps.</p><p>Evaluation of different front-end deep architectures. As discussed above, the proposed multi-scale fusion models are general and different deep neural architectures can be employed in the front end network. In this section, we evaluate the impact of this choice on the depth estimation performance. The results of our analysis are shown in <ref type="table" target="#tab_4">Table 3</ref>, where we consider both the case of pretrained model (i.e. only side losses are employed but not CRF models), indicated with P, and the fine-tuned model with the cascade CRFs (CRF). Similar results are obtained in the case of the multi-scale CRF. As expected, in both cases deeper models produced more accurate predictions and ResNet50 outperforms other models. Moreover, VGG-CD is slightly better than VGG-ED, and both these models outperforms VGG16. Importantly, for all considered networks there is a significant increase in performance when applying the proposed CRF-based models. <ref type="figure">Figure 5</ref> depicts some examples of predicted depth maps on the NYU Depth V2 dataset. As shown in the figure, the proposed approach is able to generate robust depth predictions. By comparing the reconstructed depth images obtained with pretrained models (e.g. using VGG-CD and ResNet50 as front-end networks) with those computed with our models, it is clear that our multi-scale approach significantly improves prediction accuracy.</p><p>Comparison with state of the art. We also compare our approach with state of the art methods on both datasets. For previous works we directly report results taken from the original papers. <ref type="table" target="#tab_6">Table 4</ref> shows the results of the comparison on the NYU Depth V2 dataset. For our approach we RGB Image AlexNet VGG16 VGG-CD-Ours VGG-CD ResNet Groundtruth ResNet-Ours <ref type="figure">Figure 5</ref>. Examples of depth prediction results on the NYU v2 dataset. Different network architectures are compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error (lower is better) Accuracy (higher is better) rel log10 rms δ &lt; 1.   consider the cascade model and use two different training sets for pretraining: the small set of 4.7K pairs employed in all our experiments and a larger set of 95K images as in <ref type="bibr" target="#b17">[18]</ref>. Note that for fine tuning we only use the small set. As shown in the table, our approach outperforms all baseline methods and it is the second best model when we use only 4.7K images. This is remarkable considering that, for instance, in <ref type="bibr" target="#b7">[8]</ref> 120K image pairs are used for training. We also perform a comparison with state of the art on the Make3D dataset <ref type="table" target="#tab_7">(Table 5</ref>). Following <ref type="bibr" target="#b20">[21]</ref>, the error metrics are computed in two different settings, i.e. considering (C1) only the regions with ground-truth depth less than 70 and (C2) the entire image. It is clear that the proposed approach is significantly better than previous methods. In particular, comparing with Laina et al. <ref type="bibr" target="#b17">[18]</ref>, the best performing method in the literature, it is evident that our approach, both in case of the cascade and the multi-scale models, outperforms <ref type="bibr" target="#b17">[18]</ref> by a significant margin when Laina et al. also adopt a square loss. It is worth noting that in <ref type="bibr" target="#b17">[18]</ref> a training set of 15K image pairs is considered, while we employ much less training samples. By increasing our training data (i.e. ∼ 10K in the pretraining phase), our multi-scale CRF model also outperforms <ref type="bibr" target="#b17">[18]</ref> with Huber loss (log10 and rms metrics). Finally, it is very interesting to compare the proposed method with the approach in Liu et al. <ref type="bibr" target="#b19">[20]</ref>, since in <ref type="bibr" target="#b19">[20]</ref> a CRF model is also employed within a deep network trained end-to-end. Our method significantly outperforms <ref type="bibr" target="#b19">[20]</ref> in terms of accuracy. Moreover, in <ref type="bibr" target="#b19">[20]</ref> a time of 1.1sec is reported for performing inference on a test image but the time required by superpixels calculations is not taken into account. Oppositely, with our method computing the depth map for a single image takes about 1 sec in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We introduced a novel approach for predicting depth images from a single RGB input, which is also particularly useful for other cross-modal tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The core of the method is a novel framework based on continuous CRFs for fusing multi-scale representations derived from CNN side outputs. We demonstrated that this framework can be used in combination with several common CNN architectures and is suitable for end-to-end training. The extensive experiments confirmed the validity of the proposed multi-scale fusion approach. While this paper specifically addresses the problem of depth prediction, we believe that other tasks in computer vision involving pixel-level predictions of continuous variables, can also benefit from our implementation of mean-fields updates within the CNN framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Original RGB image. (b) Ground truth. Depth map obtained by considering a pre-trained CNN (e.g. VGG Convolution-Deconvolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Front</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The proposed C-MF block. J represents a W × H matrix with all elements equal to one. The symbols ⊕, , and ⊗ indicate element-wise addition, subtraction, division and Gaussian convolution, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The proposed cascade (left) and multi-scale (right) models as a sequential deep networks. The blue and yellow boxes indicate the estimated variables and observations, respectively. The parameters βm are used for mean-field updates. As in the cascade model parameters are not shared among different CRFs, we use the notation β l 1 , β l 2 to denote parameters associated to the l-th scale. and β, with β = {β m } M m=1 . The entire network is learned with Stochastic Gradient Descent (SGD) by minimizing a square loss L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>25 δ &lt; 1.25 2 δ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell>HED [33]</cell><cell></cell><cell cols="2">0.185 0.077 0.723</cell><cell>0.678</cell><cell>0.918</cell><cell>0.980</cell></row><row><cell cols="2">Hypercolumn [10]</cell><cell cols="2">0.189 0.080 0.730</cell><cell>0.667</cell><cell>0.911</cell><cell>0.978</cell></row><row><cell>CRF</cell><cell></cell><cell cols="2">0.193 0.082 0.742</cell><cell>0.662</cell><cell>0.909</cell><cell>0.976</cell></row><row><cell cols="2">Ours (single-scale)</cell><cell cols="2">0.187 0.079 0.727</cell><cell>0.674</cell><cell>0.916</cell><cell>0.980</cell></row><row><cell cols="2">Ours -Cascade (3-s)</cell><cell cols="2">0.176 0.074 0.695</cell><cell>0.689</cell><cell>0.920</cell><cell>0.980</cell></row><row><cell cols="2">Ours -Cascade (5-s)</cell><cell cols="2">0.169 0.071 0.673</cell><cell>0.698</cell><cell>0.923</cell><cell>0.981</cell></row><row><cell cols="4">Ours -Multi-scale (3-s) 0.172 0.072 0.683</cell><cell>0.691</cell><cell>0.922</cell><cell>0.981</cell></row><row><cell cols="4">Ours -Multi-scale (5-s) 0.163 0.069 0.655</cell><cell>0.706</cell><cell>0.925</cell><cell>0.981</cell></row><row><cell cols="6">Table 1. NYU Depth V2 dataset. Comparison of different multi-</cell></row><row><cell cols="6">scale fusion schemes. 3-s, 5-s denote 3 and 5 scales respectively.</cell></row><row><cell>Method</cell><cell cols="5">Error (lower is better) rel log10 rms δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Accuracy (higher is better)</cell></row><row><cell cols="3">Outer → Inner 0.175 0.072 0.688 Inner → Outer 0.169 0.071 0.673</cell><cell cols="2">0.689 0.698</cell><cell>0.919 0.923</cell><cell>0.979 0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>NYU Depth V2 dataset. Comparison between the proposed model and the associated pretrained network architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>NYU Depth V2 dataset. Comparison between the proposed model and the associated pretrained network architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>25 δ &lt; 1.25 2 δ &lt; 1.25 3 Karsch et al.</figDesc><table><row><cell>[29]</cell><cell>0.349</cell><cell>-</cell><cell>1.214</cell><cell>0.447</cell><cell>0.745</cell><cell>0.897</cell></row><row><cell>Ladicky et al. [14]</cell><cell cols="3">0.35 0.131 1.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [21]</cell><cell cols="3">0.335 0.127 1.06</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ladicky et al. [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.542</cell><cell>0.829</cell><cell>0.941</cell></row><row><cell>Zhuo et al. [37]</cell><cell cols="3">0.305 0.122 1.04</cell><cell>0.525</cell><cell>0.838</cell><cell>0.962</cell></row><row><cell>Liu et al. [20]</cell><cell cols="3">0.230 0.095 0.824</cell><cell>0.614</cell><cell>0.883</cell><cell>0.975</cell></row><row><cell>Wang et al. [32]</cell><cell cols="3">0.220 0.094 0.745</cell><cell>0.605</cell><cell>0.890</cell><cell>0.970</cell></row><row><cell>Eigen et al. [9]</cell><cell>0.215</cell><cell>-</cell><cell>0.907</cell><cell>0.611</cell><cell>0.887</cell><cell>0.971</cell></row><row><cell cols="4">Roi and Todorovic [26] 0.187 0.078 0.744</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Eigen and Fergus [8]</cell><cell>0.158</cell><cell>-</cell><cell>0.641</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell></row><row><cell>Laina et al. [18]</cell><cell cols="3">0.129 0.056 0.583</cell><cell>0.801</cell><cell>0.950</cell><cell>0.986</cell></row><row><cell cols="4">Ours (ResNet50-4.7K) 0.143 0.065 0.613</cell><cell>0.789</cell><cell>0.946</cell><cell>0.984</cell></row><row><cell cols="4">Ours (ResNet50-95K) 0.121 0.052 0.586</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>NYU Depth V2 dataset: comparison with state of the art.</figDesc><table><row><cell>Method</cell><cell>C1 Error rel log10 rms</cell><cell cols="2">C2 Error rel log10 rms</cell></row><row><cell>Karsch et al. [14]</cell><cell cols="3">0.355 0.127 9.20 0.361 0.148 15.10</cell></row><row><cell>Liu et al. [21]</cell><cell cols="3">0.335 0.137 9.49 0.338 0.134 12.60</cell></row><row><cell>Liu et al. [20]</cell><cell cols="3">0.314 0.119 8.60 0.307 0.125 12.89</cell></row><row><cell>Li et al. [19]</cell><cell cols="3">0.278 0.092 7.19 0.279 0.102 10.27</cell></row><row><cell>Laina et al. [18] ( 2 loss)</cell><cell>0.223 0.089 4.89</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Laina et al. [18] (Huber loss) 0.176 0.072 4.46</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (ResNet50-Cascade)</cell><cell cols="3">0.213 0.082 4.67 0.221 4.79 8.81</cell></row><row><cell cols="4">Ours (Resnet50-Multi-scale) 0.206 0.076 4.51 0.212 4.71 8.73</cell></row><row><cell>Ours (Resnet50-10K)</cell><cell cols="3">0.184 0.065 4.38 0.198 4.53 8.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Make3D dataset: comparison with state of the art.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale convolutional neural networks for vision-based classification of cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lézoray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00373</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth-aware deep representations for robotic perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Buló</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penate-Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="468" to="475" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous conditional random fields for efficient regression in large fully connected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ristovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3-d depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel hand posture recognition system based on sparse representation using color and depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning cross-modal deep representations for robust pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
