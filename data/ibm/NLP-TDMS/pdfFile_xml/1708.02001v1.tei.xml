<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tiwaki Co.Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fully convolutional neural networks (FCNs) have shown outstanding performance in many dense labeling problems.</p><p>One key pillar of these successes is mining relevant information from features in convolutional layers. However, how to better aggregate multi-level convolutional feature maps for salient object detection is underexplored. In this work, we present Amulet, a generic aggregating multi-level convolutional feature framework for salient object detection. Our framework first integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and fine details. Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features. Finally, the predicted results are efficiently fused to generate the final saliency map. In addition, to achieve accurate boundary inference and semantic enhancement, edge-aware feature maps in low-level layers and the predicted results of low resolution features are recursively embedded into the learning framework. By aggregating multi-level convolutional features in this efficient and flexible manner, the proposed saliency model provides accurate salient object labeling. Comprehensive experiments demonstrate that our method performs favorably against state-of-the-art approaches in terms of near all compared evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection, which aims to identify the most conspicuous objects or regions in an image, has received considerable amount of attention in recent years. As a preprocessing step in computer vision, saliency detection has shown a great success in ranges of visual applications, e.g. object retargeting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>, scene classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref>, visual tracking <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>, image retrieval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref> and semantic segmentation <ref type="bibr" target="#b7">[8]</ref>. Despite decades of valuable research, salient object detection still remains an unsolved research * Prof.Lu is the corresponding author. problem because there are large variety of aspects that can contribute to define visual saliency, and it's hard to combine all hand-tuned factors or cues in an appropriate way.</p><p>Inspired by human visual attention mechanisms, many early existing methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> in salient object detection leverage low-level visual features (e.g. color, texture and contrast) with heuristic priors to model and approximate human saliency. These generic techniques are known to be useful for keeping fine image structures and reducing computation. Representative methods have set the benchmark on several saliency detection datasets. However, such low-level features and priors can hardly capture highlevel semantic knowledge about the object and its surroundings. Thus, these low-level feature based methods are very far away from distinguishing salient objects from the clutter background and can not generate satisfied predictions.</p><p>In recent years, fully convolutional networks (FCNs), adaptively extracting high-level semantic information from raw images, have shown impressive results in many dense labeling tasks, such as image segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>, generic object extraction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref>, pose estimation <ref type="bibr" target="#b47">[48]</ref> and contour detection <ref type="bibr" target="#b44">[45]</ref>. Motivated by these achievements, several attempts to utilize high-level features of FCNs, have been performed and delivered superior performance in predicting saliency maps <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. Nevertheless, these state-of-the-art models mainly focus on the non-linear combination of high-level features extracted from the last convolutional layers. Due to the lack of low-level visual information such as object edge, the predicted results of these methods tend to have poorly localized object boundaries.</p><p>From above discussions, we note that 1) how to simultaneously utilize multi-level potential saliency cues, 2) how to conveniently find the optimal multi-level feature aggregation strategy, and 3) how to efficiently preserve salient objects' boundaries should become the most intrinsic problems in salient object detection. To resolve these problems, in this paper, we propose a generic aggregating multi-level convolutional feature framework, namely Amulet, which effectively utilizes multi-level features of FCNs for salient object detection.</p><p>Our main contributions are summarized as follows:</p><p>• We propose a multi-level feature aggregation network, dubbed AmuletNet, which utilizes convolutional features from multiple levels as saliency cues for salient object detection. AmuletNet integrates multi-level features into multiple resolutions, learns to combine these features at each resolution and predicts saliency maps in a recursive manner. • We propose a deeply recursive supervision learning framework. It effectively incorporates edge-aware feature maps in low-level layers and the predicted results from low resolution features, to achieve accurate object boundary inference and semantic enhancement. The resulting framework can be trained by end-to-end gradient learning, which uses single-resolution ground truth without additional annotations. • The proposed model (only trained on the MSRA10K dataset <ref type="bibr" target="#b4">[5]</ref>) achieves new state-of-the-art performance on other large-scale salient object detection datasets, including the recent DUTS <ref type="bibr" target="#b41">[42]</ref>, DUT-OMRON <ref type="bibr" target="#b46">[47]</ref>, ECSSD <ref type="bibr" target="#b45">[46]</ref>, HKU-IS <ref type="bibr" target="#b49">[50]</ref>, PASCAL-S <ref type="bibr" target="#b25">[26]</ref>, SED <ref type="bibr" target="#b1">[2]</ref> and SOD <ref type="bibr" target="#b45">[46]</ref>. In addition, the model is fast on modern GPUs, achieving a near real-time speed of 16 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review existing representative models for salient object detection. We also discuss the multi-level feature aggregation methods based on FCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Salient object detection</head><p>Over the past decades, lots of salient object detection methods have been developed. The majority of salient object detection methods are based on low-level hand-crafted features, e.g., image contrast <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>, color <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>, texture <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. A complete survey of these methods is beyond the scope of this paper and we refer the readers to a recent survey paper <ref type="bibr" target="#b2">[3]</ref> for details.</p><p>Recently, deep learning based approaches, in particular the convolutional neural networks (CNNs), have delivered remarkable performance in many recognition tasks. A lot of research efforts have been made to develop various deep architectures for useful features that characterize salient objects or regions. For instance, Wang et al. <ref type="bibr" target="#b40">[41]</ref> first propose two deep neural networks to integrate local pixel estimation and global proposal search for salient object detection. Li et al. <ref type="bibr" target="#b20">[21]</ref> predict the saliency degree of each superpixel by taking multi-scale features in multiple generic CNNs. Zhao et al. <ref type="bibr" target="#b49">[50]</ref> also predict the saliency degree of each superpixel by taking global and local context into account, and detect salient objects in a multi-context deep CNN. Though these methods achieve better results than traditional counterparts, none of them handle low-level details perfectly, and all of their models include several fully connected layers, which are computationally expensive and drop spatial information of input images. To remedy above problems, Lee et al. <ref type="bibr" target="#b19">[20]</ref> propose to encode low-level distance map and high-level sematic features of deep CNNs for salient object detection. Liu et al. <ref type="bibr" target="#b26">[27]</ref> propose a deep hierarchical saliency network to learn enough global structures and progressively refine the details of saliency maps step by step via integrating local context information. In addition, Li et al. <ref type="bibr" target="#b21">[22]</ref> design a pixel-level fully convolutional stream and a segment-level spatial pooling stream to produce pixel-level saliency predictions. Wang et al. <ref type="bibr" target="#b43">[44]</ref> develop deep recurrent FCNs to incorporate the coarse predictions as saliency priors and stage-wisely refine the generated predictions. In contrary to the above methods only used specific-level features, we observe that features from all levels are potential saliency cues and helpful for salient object detection. In light of this observation, we develop a new multi-level feature aggregation approach based on deep FCNs, and show that beyond refining the predicted saliency map, the approach can also jointly learn to preserve object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature aggregation in FCNs</head><p>Several works on visualizing deep CNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> indicate that convolutional features at different levels describe the object and its surroundings from different views. High-level semantic features helps the category recognition of image regions, while low-level visual features help to generate sharp, detailed boundaries for high-resolution prediction. However, how to effectively and efficiently exploit multi-level convolutional features remains an open question. To this end, several valuble attempts have been performed. The seminal FCN method <ref type="bibr" target="#b27">[28]</ref> introduces skipconnections and adds high-level prediction layers to intermediate layers to generate pixel-wise prediction results at multiple resolutions. The Hypercolumn method <ref type="bibr" target="#b12">[13]</ref> also integrates convolutional features from multiple middle layers and learns high-level dense classification layers. The SegNet <ref type="bibr" target="#b0">[1]</ref> and DeconvNet <ref type="bibr" target="#b30">[31]</ref> employ a convolutional encoder-decoder network with pooling index guided deconvolution modules to exploit the features from multi-level convolutional layers. Similarly, the U-Net <ref type="bibr" target="#b33">[34]</ref> apply multiple skip-connections to construct a contracting path to capture context and a symmetric expanding path that enables precise localization. The HED model <ref type="bibr" target="#b44">[45]</ref> employs deeply supervised structures, and automatically learns rich hierarchical representations that are fused to resolve the challenging ambiguity in edge and object boundary detection.</p><p>Our proposed approach clearly differs from the abovementioned methods in three aspects. Firstly, our method aggregates multi-level features at multiple resolutions. We use a pre-trained FCN and integrate all level features into multiple resolutions at once. Our method can simultaneously incorporate coarse semantics and fine details. Although all above methods seem to be useful for aggregating multilevel features, their aggregation is carried out in a stagewise manner rather than jointly integrating. Secondly, our method employs a bidirectional information stream, which facilitates complement effect in prediction. In contrary, all above-mentioned methods simply aggregate multiple level features from one direction, i.e., low to high or high to low. Thirdly, our method is able to refine the coarse high-level semantic predictions by exploiting low-level visual features. In particular, our method employs edge-aware feature maps of low-level layers into the prediction modules which help to preserve objects' boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Aggregating Convolutional Feature Model</head><p>In this section, we begin by describing the components of our proposed AmuletNet architecture in Section 3.1. Then we give the detailed formulas of our bidirectional information aggregating learning method in Section 3.2. In the end, we construct saliency inference based on the multi-level predictions of the proposed Amulet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AmuletNet architecture</head><p>Our proposed AmuletNet consists of four components: multi-level feature extraction, resolution-based feature integration, recursive saliency map prediction and boundary preserved refinement. The four main components are jointly trained to optimize the output saliency detection quality. The overall architecture is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>Multi-level feature extraction. The first component of our architecture is a deep feature extraction network, which takes the input image and produces feature maps for convolutional feature integration. We build our architecture on the VGG-16 model from <ref type="bibr" target="#b36">[37]</ref>, which is well known for its elegance and simplicity, and at the same time yields nearly state-of-the-art results in image classification and good generalization properties. In the VGG-16 model there are five max-pooling stages with kernel size 2 and stride 2. Given an input image with size W × H, the output feature maps have size W 2 5 , H 2 5 , thus a FCN model built upon the VGG-16 would output feature maps reduced by a factor of 32. To balance the semantic context and fine image details, we remove the last pooling stage and enlarge the size of the input image. This way, the output feature maps of our feature extraction network are rescaled by a factor of 16 with respect to the input image. We take feature maps at five levels from the VGG-16 model: conv1-2 (which contains 64 feature maps), conv2-2 (128 feature maps), conv3-3 (256 feature maps), conv4-3 (512 feature maps) and conv5-3 (512 feature maps). Note that our feature extraction network is extremely flexible in that it can be replaced and modified in various ways, such as using different layers or networks, e.g. VGG-19 <ref type="bibr" target="#b36">[37]</ref> and ResNet <ref type="bibr" target="#b15">[16]</ref>. Resolution-based feature integration. Considering the inconsistent resolution of multi-level convolutional features, we propose a novel resolution-based feature combination structure, named RFC. The RFC structure consists of both shrink and extend branches. Assume I is the input image; τ = W 2 l , H 2 l is the target resolution of integrated feature maps, and identified by feature level l(= 0, 1, ..., L); F n (I) denotes a 3D tensor, i.e., the feature maps generated by the feature extraction network with n × τ resolution. Thus, the proposed RFC generates the integrated feature maps by</p><formula xml:id="formula_0">F τ = W τ * Cat(S n (F n (I); ψ n ), ..., S 1 (F 1 (I); ψ 1 ), E 1 (F 1 (I); ϕ 1 ), ..., E m (F m (I); ϕ m )),<label>(1)</label></formula><p>where * represents convolution operation;S n (·; ψ n ) denotes the shrink operator parameterized by ψ n that aims to downsample the input high-resolution feature maps by a factor of n, while the extend operator E m (·; ϕ m ) aims to up-sample the low-resolution ones by a factor of m. The shrink operators can be convolution or pooling. The extend operators can be deconvolution or interpolation. Cat is the crosschannel concatenation. W τ is the parameter for combining the concatenated feature maps. The details of RFC are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. For our proposed AmuletNet, we take feature maps at five different levels (L = 4) from the above feature extraction network. We utilize RFCs to resize all level feature maps into the five spatial resolution by performing 64 convolution or deconvolution operations. The generated features are concatenated into a tensor with 320 channels at each resolution. Then we use a convolutional layer with 1×1 kernel size to weight the importance of each feature map. For computational efficiency, 64 convolutional kernels are used to combine each tensor into 64 integrated feature maps. This way, each integrated feature map will simultaneously incorporate coarse semantics and fine details.</p><p>Recursive saliency map prediction. The integrated feature maps already contains various saliency cues, so we can use them to predict the saliency map. A direct method is to deconvolute the integrated feature maps at each level into the size of the input image, and add a new convolutional layer to produce the predicted saliency map. Although this method can detect salient objects from different levels, the inner connection of different-level predictions is missing. As a result, the independent prediction is not satisfactory enough, both quantitatively and visually, and further postprocessing is needed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. To facilitate the interaction of multiple predictions, we propose a recursive prediction architecture, i.e. Deep Recursive Supervision (DRS) in <ref type="figure">Fig. 1</ref>, to hierarchically and progressively absorb high-level predictions and render pixel-wise supervised information. The proposed DRS includes saliency map prediction modules (SMP) and the deeply supervised learning mechanism <ref type="bibr" target="#b44">[45]</ref>.  <ref type="figure">Figure 1</ref>. The overall architecture of our proposed Amulet model. Each colorful box is considered as a feature block. The arrows between blocks indicate the information stream. Given an input image (256×256×3), multi-level features are first generated by the feature extraction network (VGG-16 <ref type="bibr" target="#b36">[37]</ref>). Then feature integration is performed by resolution-based feature combination modules (RFCs). After that, deep recursive supervision (DRS) is employed to improve the interaction of multiple predictions. Finally, boundary preserved refinements (BPRs) are used to refine the predicted saliency maps. The final saliency map is the fused output of multiple predicted saliency maps. The SMP incorporates autoregressive recurrent connections into the predictions from high-level to low. In each level l, the SMP takes integrated feature maps F τ and the high-level prediction P l+1 as input, and produces the new prediction of this level as</p><formula xml:id="formula_1">P l = W r * σ(W F τ s F τ + W P l+1 * P l+1 + b), l &lt; L W F τ s F τ + b, l = L<label>(2)</label></formula><p>where s represents deconvolution operation with stride s to ensure the same spatial size of the output prediction. W F τ and W P l+1 are the integrated feature weight and the output prediction weight, respectively. b is the bias parameter. σ is the ReLU activation function. W r is the recursive weight. From Eq.(2) and <ref type="figure">Fig. 1</ref>, we can see that multiple autoregressive recurrent connections ensure that the new prediction has multiple paths from the input to the output, which facilitates effective information exchanges. Besides, we employ deeply supervised learning into the SMPs. This way, the pixel-wise supervised information from ground truth will guide the recursive saliency map prediction at each level, making the SMPs be able to propagate fine details back to the predictions of large contexts. Thus, DRS can build a bidirectional information stream aggregation, which facilitates complement effect in prediction. We will fully elaborate the bidirectional information aggregating learning in Section 3.2. The experiments in Section 4.4 show the superiority of DRS over the deeply supervised learning in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Boundary preserved refinement. To further improve the detection accuracy, we add boundary refinements by introducing short connections to the predicted results. Our approach bases on the observation that low-level feature maps in the conv1-2 layer have edge-preserving properties <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b29">30]</ref>. We expect that these low-level features help to predict objects' boundary. Besides, the features also have the same spatial resolution with respect to the input image. For boundary refinement, a convolutional layer with 1 × 1 kernel size is first applied to the conv1-2 layer, yielding boundary predictions B l . Then B l are added to the raw prediction for better aligned object boundaries,</p><formula xml:id="formula_2">P l b = W b * σ(B l + P l ),<label>(3)</label></formula><p>where W b is the refinement parameter. ReLU is used so that the boundary prediction is in the range of zero to infinity.</p><p>Based on the boundary preserved refinements P b , a additional convolutional layer is applied and learned to produce the fusion saliency prediction (FSP) as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bidirectional information aggregating learning</head><p>Given the salient object detection training dataset S = {(X n , Y n )} N n=1 with N training pairs, where X n = {x n j , j = 1, ..., T } and Y n = {y n j , j = 1, ..., T } are the input image and the binary ground-truth image with T pixels, respectively. y n j = 1 denotes the foreground pixel and y n j = 0 denotes the background pixel. For notional simplicity, we subsequently drop the subscript n and consider each image independently. We denote W as the parameters of the feature extraction network and RFCs. Supposing the network has M predictions, including one fused prediction and M − 1 specific-level predictions. In our AmuletNet, we have M = 6. For the fused prediction, the loss function can be expressed as</p><formula xml:id="formula_3">L f (W, w f ) = −β j∈Y+ log Pr(y j = 1|X; W, w f ) −(1 − β) j∈Y− log Pr(y j = 0|X; W, w f ),<label>(4)</label></formula><p>where w f is the classifier parameter for the fused prediction. Y + and Y − denote the foreground and background label sets, respectively. The loss weight β = |Y + |/|Y |, and |Y + | and |Y − | denote the foreground and background pixel number, respectively. Pr(y j = 1|X; W; w f ) ∈ [0, 1] is the confidence score of the fused prediction that measures how likely the pixel belong to the foreground.</p><p>For the prediction at level l, the loss function can be represented by L l (W, θ l , w l ) = −β j∈Y+ log Pr(y j = 1|X; W, θ l , w l )</p><formula xml:id="formula_4">−(1 − β) j∈Y−</formula><p>log Pr(y j = 0|X; W, θ l , w l ),</p><formula xml:id="formula_5">(5) where θ l = (w r l , w b l )</formula><p>is the parameter of the recursive prediction component and boundary refinement component in the prediction module. w l is the classifier parameter for the prediction at level l. Thus, the joint loss function for all predictions is obtained by</p><formula xml:id="formula_6">L(W, θ, w) = α f L f (W, w f ) + L l=0 α l L l (W, θ l , w l ),<label>(6)</label></formula><p>where α f and α l are the loss weights to balance each loss term. For simplicity and fair comparison, we set α f = α l = 1 as used in <ref type="bibr" target="#b44">[45]</ref>. The above loss function is continuously differentiable, so we can use the stochastic gradient descent (SGD) method to obtain the optimal parameters, (W * , θ * , w * ) = arg min L(W, θ, w).</p><p>Our aggregating learning method has several significant differences with other deeply supervised implementations, i.e., DHS <ref type="bibr" target="#b26">[27]</ref> and HED <ref type="bibr" target="#b44">[45]</ref>. In DHS and HED, the deep supervision is directly applied on side-outputs, while in our method the deep supervision is applied on multiple same resolution predictions. According to Eq.(2), each recursive prediction contains the information of two predictions at least, endowing our method the capability to propagate the supervised information across deep layers in a bidirectional manner. The bold black arrows in <ref type="figure">Fig. 1</ref> illustrate the bidirectional information stream. Besides, DHS needs to specify scales for side-outputs to minimize the multi-scale error, which requires additional annotation for each scale. In contrast, the proposed method adaptively unify the scale information into the size of input images, without using multiscale annotations. In addition, different from the methods used sigmoid classifiers in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>, we use the following softmax classifier to evaluate the prediction scores:</p><formula xml:id="formula_8">Pr(y j = 1|X; W, θ, w) = e z1 e z0 + e z1 ,<label>(8)</label></formula><formula xml:id="formula_9">Pr(y j = 0|X; W, θ, w) = e z0 e z0 + e z1 ,<label>(9)</label></formula><p>where z 0 and z 1 are the score of each label of training data. In this way, each prediction of the AmultNet is composed of a foreground excitation map (M f e ) and a background excitation map (M be ). We utilize M f e and M be of all-level predictions to generate the final fusion. This strategy not only increases the pixel-level discrimination but also captures context contrast information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Saliency inference</head><p>Although the architecture we use in this work can produce M predictions computed by Eq.(8) with the optimal parameters (W * , θ * , w * ), we observe that the quality of the predictions at different levels varies widely. The more lower level, the better they are. The fused prediction generally appears much better than other predictions. For saliency inference, we can simply use the fused prediction as our final saliency map. However, saliency inference emphasize the contrast between foreground and background. Therefore, more biologically we utilize the mean contrast of different predictions to further improve the detection accuracy during saliency inference. Formally, let M f e l (M f e f ) and M be l (M be f ) denote the foreground excitation map and background excitation map at level l (of the fused prediction), respectively. They can be computed by Eq.(8) and Eq.(9). Thus, the final saliency map can be obtained by</p><formula xml:id="formula_10">S = σ(Mean( L l=0 (M f e l − M be l )) + (M f e f − M be f )), (10)</formula><p>where Mean is the pixel-wise mean and σ is the ReLU activation function for clipping the negative values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: For the training, we utilize the MSRA10K dataset <ref type="bibr" target="#b4">[5]</ref>, which includes 10,000 images with high quality pixel-wise annotations. Most of the images in this dataset contain only one salient object. To improve the varieties, we simply augment this dataset by mirror reflection and rotation techniques (0 • , 90 • , 180 • , 270 • ), producing 80,000 training images totally.</p><p>For the performance evaluation, we adopt seven public saliency detection datasets as follows.</p><p>DUT-OMRON <ref type="bibr" target="#b46">[47]</ref>. This dataset has 5,168 high quality images. Images of this dataset have one or more salient objects and relatively complex background. Thus this dataset is more difficult and challenging, and provides more space of improvement for related research in saliency detection.</p><p>DUTS <ref type="bibr" target="#b49">[50]</ref>. This dataset is currently the largest saliency detection benchmark, and contains 10,553 training images (DUTS-TR) and 5,019 test images (DUTS-TE) with high quality pixel-wise annotations. Both the training and test set contain very challenging scenarios for saliency detection.</p><p>ECSSD <ref type="bibr" target="#b45">[46]</ref>. This dataset contains 1,000 natural images, which include many semantically meaningful and complex structures in their ground truth segmentation.</p><p>HKU-IS <ref type="bibr" target="#b49">[50]</ref>. This dataset has 4,447 images with high quality pixel-wise annotations. Images of this dataset are well chosen to include multiple disconnected salient objects or objects touching the image boundary.</p><p>PASCAL-S <ref type="bibr" target="#b25">[26]</ref>. This dataset is generated from the PASCAL VOC dataset <ref type="bibr" target="#b8">[9]</ref> and contains 850 natural images.</p><p>SED <ref type="bibr" target="#b1">[2]</ref>. This dataset contains two subsets: SED1 and SED2. The SED1 has 100 images each containing only one salient object, while the SED2 has 100 images each containing two salient objects.</p><p>SOD <ref type="bibr" target="#b45">[46]</ref>. This dataset has 300 images, and it was originally designed for image segmentation. Pixel-wise annotation of salient objects was generated by <ref type="bibr" target="#b18">[19]</ref>. This dataset is challenging since many images contain multiple objects either with low contrast or touching the image boundary.</p><p>Implementation Details: We implement our approach based on the MATLAB R2014b platform with the Caffe toolbox <ref type="bibr" target="#b17">[18]</ref>. We run our approach in a quad-core PC machine with an i7-4790 CPU (with 16G memory) and a NVIDIA Titan X GPU (with 12G memory). We train our model using augmented images from the MSRA10K dataset. We do not use validation set and train the model until its training loss converges. The parameters of multilevel feature extraction layers are initialized from the VGG-16 model <ref type="bibr" target="#b36">[37]</ref>. For other convolutional layers, we initialize the weights by the "msra" method <ref type="bibr" target="#b14">[15]</ref>. We use the SGD method to train our network with a momentum 0.9 and a weight decay 0.0001. We set the base learning rate to 1e-8 and decrease the learning rate by 10% when training loss reaches a flat. The training process takes almost 16 hours and converges after 200k iterations with mini-batch size 8. When testing, the proposed salient object detection algorithm runs at about 16 fps with 256 × 256 resolution. The source code can be found at http://ice.dlut.edu.cn/lu/.</p><p>Evaluation Metrics: We utilize three main metrics to evaluate the performance of different salient object detection algorithms, including the precision-recall (PR) curves, F-measure and mean absolute error (MAE) <ref type="bibr" target="#b2">[3]</ref>. The precision and recall are computed by thresholding the predicted saliency map, and comparing the binary map with the ground truth. The PR curve of a dataset demonstrates the mean precision and recall of saliency maps at different thresholds. The F-measure is a harmonic mean of average precision and average recall, and can be calculated by</p><formula xml:id="formula_11">F β = (1 + β 2 ) × P recision × Recall β 2 × P recision × Recall .<label>(11)</label></formula><p>We set β 2 to be 0.3 to weigh precision more than recall as suggested in <ref type="bibr" target="#b45">[46]</ref> [41] [3] <ref type="bibr" target="#b46">[47]</ref>. We report the performance when each saliency map is binarized with an image-dependent threshold. The threshold is determined to be twice the mean saliency of the image:</p><formula xml:id="formula_12">T = 2 W × H W x=1 H y=1 S(x, y),<label>(12)</label></formula><p>where W and H are width and height of an image, S(x, y) is the saliency value of the pixel at (x, y). We report the average precision, recall and F-measure over each dataset. The above overlapping-based evaluations usually give higher score to methods which assign high saliency score to salient pixel correctly. However, the evaluation on nonsalient regions can be unfair especially for the methods which successfully detect non-salient regions, but miss the detection of salient regions. Therefore, we also calculate the mean absolute error (MAE) for fair comparisons as suggested by <ref type="bibr" target="#b2">[3]</ref>. The MAE evaluates the saliency detection accuracy by</p><formula xml:id="formula_13">M AE = 1 W × H W x=1 H y=1 |S(x, y) − G(x, y)|,<label>(13)</label></formula><p>where G is the binary ground truth mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison with State-of-the-art</head><p>We compare our algorithm with other 11 state-ofthe-art ones including 7 deep learning based algorithms (DCL <ref type="bibr" target="#b21">[22]</ref>, DHS <ref type="bibr" target="#b26">[27]</ref>, DS <ref type="bibr" target="#b23">[24]</ref>, ELD <ref type="bibr" target="#b19">[20]</ref>, LEGS <ref type="bibr" target="#b40">[41]</ref>, MDF <ref type="bibr" target="#b49">[50]</ref>, RFCN <ref type="bibr" target="#b43">[44]</ref>) and 4 conventional algorithms (BL <ref type="bibr" target="#b38">[39]</ref>, BSCA <ref type="bibr" target="#b31">[32]</ref>, DRFI <ref type="bibr" target="#b18">[19]</ref>, DSR <ref type="bibr" target="#b22">[23]</ref>). For fair comparison, we use either the implementations with recommended parameter settings or the saliency maps provided by the authors.   Quantitative Evaluation. As shown in Tab. 1 and <ref type="figure" target="#fig_2">Fig. 3</ref>, the Amulet model can largely outperform other compared counterparts across all the datasets in terms of near all evaluation metrics, which convincingly demonstrates the effectiveness of the proposed method. Results on the SED dataset and PR curves on the DUT-OMRON, SED and SOD datasets appear in the supplemental material due to the limitation of space. From the results, we have other fundamental observations: (1) Our model improves the F-measure with a considerable margin on most of datasets, especially on large-scale datasets, such as DUTS-TE, ECSSD, HKU-IS. And at the same time, our model generally decreases the MAE. This indicates that our model is more convinced of the predicted regions and provides more accurate saliency maps. (2) Although only trained on the MSRA10K dataset, our model significantly outperforms other algorithms that pre-trained on specific saliency datasets, such as LEGS and RFCN on PASCAL-S, MDF on HKU-IS. The superior performance confirms that our model have good generalization abilities on other large-scale datasets. (3) Our method is inferior to DHS on several datasets. However, these datasets are relatively small compared to the era of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUT-OMRON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTS-TE</head><formula xml:id="formula_14">ECSSD HKU-IS PASCAL-S SOD Methods F β M AE F β M AE F β M AE F β M AE F β M AE F β M AE</formula><p>Qualitative Evaluation. <ref type="figure">Fig. 4</ref> provides a visual comparison of our approach and other methods. It can be seen that our method generates more accurate saliency maps in various challenging cases, e.g., low contrast between the objects and backgrounds (the first two rows), objects near the image boundary (the 3-4 rows) and multiple disconnected salient objects (the 5-6 rows). What's more, with our BPR component, our saliency maps provide more accurate boundaries of salient objects (the 1, 3, 4, 6 rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Feature resolution effects. To verify the importance of resolutions of integrated features, we additionally evaluate several variants of the proposed Amulet model with different scales. Amulet-1/n denotes the model that takes the integrated features reduced by a factor not larger than n, with respect to the input image. The corresponding performance are also reported in Tab. 1. The results suggest that features of all levels are helpful for saliency detection, and with the increment of resolutions, our approach gradually achieves better performance. In addition, even our simplest model (i.e., Amulet-1/16) can achieve better results than most of existing methods. This fact further verifies the strength of our proposed methods.</p><p>Boundary refinements. To verify the contributions of our proposed BPR, we also implement our proposed approach without BPRs, named Amulet BP R − , and report the performance in Tab. 1. It can be observed that without BPRs, our approach decreases the performance but not too much in F-measure. But it leads to a large drop in MAE. This indicates that our proposed BPR is capable of detecting and localizing the boundary of most salient objects, while other methods often fail at this fact. Several visual examples are illustrated in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other Aggregation Methods</head><p>For fair comparison, we perform additional evaluations to verify the detection ability of different aggregation methods. Specifically, we use the same augmented MSRA10K dataset to train the FCN-8s <ref type="bibr" target="#b27">[28]</ref>, Hypercolumn (HC) <ref type="bibr" target="#b12">[13]</ref>, SegNet (SN) <ref type="bibr" target="#b0">[1]</ref>, DeconvNet(DN) <ref type="bibr" target="#b30">[31]</ref> and HED <ref type="bibr" target="#b44">[45]</ref> for saliency detection task. All compared methods are based on the same VGG-16 model pre-trained on the ImageNet classification task <ref type="bibr" target="#b36">[37]</ref>. We drop the unnecessary compo-Methods FCN-8s HC SN DN HED Ours F β 0.8116 0.8187 0.8145 0.8264 0.8321 0.8521 M AE 0.1343 0.1193 0.0947 0.1435 0.1022 0.0662 <ref type="table">Table 2</ref>. The performance of different aggregations on ECSSD dataset. Other datasets have the similar performance trend. nents in each model and only focus on the feature aggregation part. For our model, we use the simplest model (i.e., Amulet-1/16) without BPRs. For each method, we find the optimal parameters to achieve its' best results. The performance on the ECSSD dataset is listed in Tab. 2. As can be seen from Tab. 2, with the aggregation of multi-level features, our approach achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a generic aggregating multilevel convolutional feature framework for salient object detection. Our framework can integrate multi-level feature maps into multiple resolutions, learn to combine feature maps, and predict saliency maps with the integrated features. In addition, edge-aware maps and high-level predictions are embedded into the framework. Experiments demonstrate that our method performs favorably against state-of-the-art approaches in saliency detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Details of the RFC module. The RFC first takes feature maps with different resolutions and channels as input. Then shrink and extend operators resize the feature maps to the same spatial resolution and equal channels. Finally, the concatenation and 1×1 convolution are used to generate the integrated features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Amulet 0.6471 0.09761 0.7365 0.08517 0.8684 0.05874 0.8542 0.05214 0.7632 0.09824 0.7547 0.13998 Amulet-1/1 0.6413 0.10161 0.7320 0.08796 0.8678 0.05997 0.8460 0.05416 0.7634 0.09948 0.7512 0.14169 Amulet-1/2 0.6408 0.10178 0.7210 0.08807 0.8675 0.05998 0.8456 0.05421 0.7629 0.09965 0.7509 0.14177 Amulet-1/4 0.6392 0.10219 0.7169 0.08851 0.8659 0.06039 0.8439 0.05465 0.7615 0.10001 0.7503 0.14204 Amulet-1/8 0.6356 0.10282 0.6942 0.08933 0.8625 0.06137 0.8397 0.05570 0.7584 0.10067 0.7492 0.14262 Amulet-1/16 0.6266 0.10280 0.6891 0.09110 0.8523 0.06477 0.8327 0.05821 0.7469 0.10273 0.7421 0.14495 Amulet BP R − 0.6301 0.12062 0.6912 0.09761 0.8647 0.06572 0.8402 0.06302 0.7533 0.1240 0.7201 0.15340 DCL [22] 0.6842 0.15726 0.7141 0.14928 0.8293 0.14949 0.8533 0.13587 0.7141 0.18073 0.7413 0.19383 DHS [27] --0.7301 0.06578 0.8675 0.05948 0.8541 0.05308 0.7741 0.09426 0.7746 0.12840 DS [24] 0.6028 0.12038 0.6323 0.09070 0.8255 0.12157 0.7851 0.07797 0.6590 0.17597 0.6981 0.18894 ELD [20] 0.6109 0.09240 0.6277 0.09761 0.8102 0.07955 0.7694 0.07414 0.7180 0.12324 0.7116 0.15452 LEGS [41] 0.5915 0.13335 0.5846 0.13793 0.7853 0.11799 0.7228 0.11934 --0.6834 0.19548 MDF [50] 0.6442 0.09156 0.6732 0.09986 0.8070 0.10491 0.8006 0.09573 0.7087 0.14579 0.7205 0.16394 RFCN [44] 0.6265 0.11051 0.7120 0.09003 0.8340 0.10690 0.8349 0.08891 0.7512 0.13241 0.7426 0.16919 BL [39] 0.4988 0.23881 0.4897 0.23794 0.6841 0.21591 0.6597 0.20708 0.5742 0.24871 0.5798 0.26681 BSCA [32] 0.5091 0.19024 0.4996 0.19614 0.7048 0.18211 0.6544 0.17480 0.6006 0.22286 0.5835 0.25135 DRFI [19] 0.5504 0.13777 0.5407 0.17461 0.7331 0.16422 0.7218 0.14453 0.6182 0.20651 0.6343 0.22377 DSR [23] 0.5242 0.13886 0.5182 0.14548 0.6621 0.17837 0.6772 0.14219 0.5575 0.21488 0.5962 0.23394</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The PR curves of the proposed algorithm and other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Comparison of saliency maps. (a) Input images; (b) Ground truth; (c) Our method; (d) RFCN; (e) DCL; (f) DHS; (g) DS; (h) LEGS; (i) MDF; (j) ELD; (k) DRFI. The top four row and bottom two row images are from the ECSSD and SED dataset, respectively. Visual comparison of the Amulet algorithm with /without BPRs. (a)(d)(g) Input images; (b)(e)(h) Predictions of the Amulet; (c)(f)(i) Predictions of the Amulet BP R − . High resolution to see better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The F-measure and MAE of different saliency detection methods on six large-scale saliency detection datasets. The best three results are shown in red, green and blue. The proposed methods rank first or second on these datasets.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.5 0.6</cell><cell></cell><cell>Amulet BL BSCA DCL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.5 0.6</cell><cell></cell><cell>Amulet BL BSCA DCL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.5 0.6</cell><cell></cell><cell>Amulet BL BSCA DCL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.6 0.5</cell><cell></cell><cell>Amulet BL BSCA DCL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell>DHS DRFI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>DHS DRFI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>DHS DRFI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>DHS DRFI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell>DSR ELD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>DSR ELD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>DSR ELD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>DSR ELD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>LEGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LEGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LEGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LEGS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell>MDF RFCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>MDF RFCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>MDF RFCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>MDF RFCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0.1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0.1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0.1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This paper is supported by the Natural Science Foundation of China #61472060, #61502070 and #61528101.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is a salient object? a dataset and a baseline model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive object tracking by learning background context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Importance filtering for image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Federico Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3-d object retrieval and recognition with hypergraph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mobile product search with bag of hash bits and boundary reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3005" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Biologically inspired object tracking using center-surround saliency mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="541" to="554" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regionbased saliency detection and its application in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="769" to="779" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rapid biologically-inspired scene classification using features shared with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scale and object aware image retargeting for thumbnail browsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1511" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Salient object detection via bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
