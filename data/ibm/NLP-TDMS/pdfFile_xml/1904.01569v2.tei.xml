<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Randomly Wired Neural Networks for Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Randomly Wired Neural Networks for Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets <ref type="bibr" target="#b11">[11]</ref> and DenseNets <ref type="bibr" target="#b16">[16]</ref> is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What we call deep learning today descends from the connectionist approach to cognitive science <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b7">7]</ref>-a paradigm reflecting the hypothesis that how computational networks are wired is crucial for building intelligent machines. Echoing this perspective, recent advances in computer vision have been driven by moving from models with chain-like wiring <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref> to more elaborate connectivity patterns, e.g., ResNet <ref type="bibr" target="#b11">[11]</ref> and DenseNet <ref type="bibr" target="#b16">[16]</ref>, that are effective in large part because of how they are wired.</p><p>Advancing this trend, neural architecture search (NAS) <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref> has emerged as a promising direction for jointly searching wiring patterns and which operations to perform. NAS methods focus on search <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b27">27]</ref> while implicitly relying on an important-yet largely overlooked-component that we call a network generator (defined in §3.1). The NAS network generator defines a family of possible wiring patterns from which networks classifier classifier classifier conv 1 conv 1 conv 1 <ref type="figure">Figure 1</ref>. Randomly wired neural networks generated by the classical Watts-Strogatz (WS) <ref type="bibr" target="#b50">[50]</ref> model: these three instances of random networks achieve (left-to-right) 79.1%, 79.1%, 79.0% classification accuracy on ImageNet under a similar computational budget to ResNet-50, which has 77.1% accuracy.</p><p>are sampled subject to a learnable probability distribution. However, like the wiring patterns in ResNet and DenseNet, the NAS network generator is hand designed and the space of allowed wiring patterns is constrained in a small subset of all possible graphs. Given this perspective, we ask: What happens if we loosen this constraint and design novel network generators?</p><p>We explore this question through the lens of randomly wired neural networks that are sampled from stochastic network generators, in which a human-designed random process defines generation. To reduce bias from us-the authors of this paper-on the generators, we use three classical families of random graph models in graph theory <ref type="bibr" target="#b51">[51]</ref>: the Erdős-Rényi (ER) <ref type="bibr" target="#b5">[6]</ref>, Barabási-Albert (BA) <ref type="bibr" target="#b0">[1]</ref>, and Watts-Strogatz (WS) <ref type="bibr" target="#b50">[50]</ref> models. To define complete networks, we convert a random graph into a directed acyclic graph (DAG) and apply a simple mapping from nodes to their functional roles (e.g., to the same type of convolution).</p><p>The results are surprising: several variants of these random generators yield networks with competitive accuracy on ImageNet <ref type="bibr" target="#b39">[39]</ref>. The best generators, which use the WS model, produce multiple networks that outperform or are comparable to their fully manually designed counterparts and the networks found by various neural architecture search methods. We also observe that the variance of accuracy is low for different random networks produced by the same generator, yet there can be clear accuracy gaps between different generators. These observations suggest that the network generator design is important.</p><p>We note that these randomly wired networks are not "prior free" even though they are random. Many strong priors are in fact implicitly designed into the generator, including the choice of a particular rule and distribution to control the probability of wiring or not wiring certain nodes together. Each random graph model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b0">1]</ref> has certain probabilistic behaviors such that sampled graphs likely exhibit certain properties (e.g., WS is highly clustered <ref type="bibr" target="#b50">[50]</ref>). Ultimately, the generator design determines a probabilistic distribution over networks, and as a result these networks tend to have certain properties. The generator design underlies the prior and thus should not be overlooked.</p><p>Our work explores a direction orthogonal to concurrent work on random search for NAS <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>. These studies show that random search is competitive in "the NAS search space" <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref>, i.e., the "NAS network generator" in our perspective. Their results can be understood as showing that the prior induced by the NAS generator design tends to produce good models, similar to our observations. In contrast to <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>, our work goes beyond the design of established NAS generators and explores different random generator designs.</p><p>Finally, our work suggests a new transition from designing an individual network to designing a network generator may be possible, analogous to how our community have transitioned from designing features to designing a network that learns features. Rather than focusing primarily on search with a fixed generator, we suggest designing new network generators that produce new families of models for searching. The importance of the designed network generator (in NAS and elsewhere) also implies that machine learning has not been automated (c.f . "AutoML" <ref type="bibr" target="#b20">[20]</ref>)-the underlying human design and prior shift from network engineering to network generator engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Network wiring. Early recurrent and convolutional neural networks (RNNs and CNNs) <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b21">21]</ref> use chain-like wiring patterns. LSTMs <ref type="bibr" target="#b14">[14]</ref> use more sophisticated wiring to create a gating mechanism. Inception CNNs <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b44">44]</ref> concatenate multiple, irregular branching pathways, while ResNets <ref type="bibr" target="#b11">[11]</ref> use x + F(x) as a regular wiring template;</p><p>DenseNets <ref type="bibr" target="#b16">[16]</ref> use concatenation instead: [x, F(x)]. The LSTM, Inception, ResNet, and DenseNet wiring patterns are effective in general, beyond any individual instantiation.</p><p>Neural architecture search (NAS). Zoph and Le <ref type="bibr" target="#b55">[55]</ref> define a NAS search space and investigate reinforcement learning (RL) as an optimization algorithm. Recent research on NAS mainly focuses on optimization methods, including RL <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref>, progressive <ref type="bibr" target="#b26">[26]</ref>, gradient-based <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b27">27]</ref>, weight-sharing <ref type="bibr" target="#b33">[33]</ref>, evolutionary <ref type="bibr" target="#b34">[34]</ref>, and random search <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref> methods. The search space in these NAS works, determined by the network generator implicit in <ref type="bibr" target="#b55">[55]</ref>, is largely unchanged in these works. While this is reasonable for comparing optimization methods, it inherently limits the set of feasible solutions.</p><p>Randomly wired machines. Pioneers of artificial intelligence were originally interested in randomly wired hardware and their implementation in computer programs (i.e., artificial neural networks). In 1940s, Turing <ref type="bibr" target="#b47">[47]</ref> suggested a concept of unorganized machines, which is a form of the earliest randomly connected neural networks. One of the first neural network learning machines, designed by Minsky <ref type="bibr" target="#b31">[31]</ref> in 1950s and implemented using vacuum tubes, was randomly wired. In late 1950s the "Mark I Perceptron" visual recognition machine built by Rosenblatt <ref type="bibr" target="#b36">[36]</ref> used an array of randomly connected photocells.</p><p>Relation to neuroscience. Turing <ref type="bibr" target="#b47">[47]</ref> analogized the unorganized machines to an infant human's cortex. Rosenblatt <ref type="bibr" target="#b36">[36]</ref> pointed out that "the physical connections of the nervous system ... are not identical from one organism to another", and "at birth, the construction of the most important networks is largely random." Studies <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b48">48]</ref> have observed that the neural network of a nematode (a worm) with about 300 neurons is a graph with small-world properties <ref type="bibr" target="#b18">[18]</ref>. Random graph modeling has been used as a tool to study the neural networks of human brains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Random graphs in graph theory. Random graphs are widely studied in graph theory <ref type="bibr" target="#b51">[51]</ref>. Random graphs exhibit different probabilistic behaviors depending on the random process defined by the model (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">50]</ref>). The definition of the random graph model determines the prior knowledge encoded in the resulting graphs (e.g., smallworld <ref type="bibr" target="#b18">[18]</ref>) and may connect them to naturally occurring phenomena. As a result, random graph models are an effective tool for modeling and analyzing real-world graphs, e.g., social networks, world wide web, citation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We now introduce the concept of a network generator, which is the foundation of randomly wired neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Generators</head><p>We define a network generator as a mapping g from a parameter space Θ to a space of neural network architectures N , g: Θ → N . For a given θ ∈ Θ, g(θ) returns a neural network instance n ∈ N . The set N is typically a family of related networks, for example, VGG nets <ref type="bibr" target="#b43">[43]</ref>, ResNets <ref type="bibr" target="#b11">[11]</ref>, or DenseNets <ref type="bibr" target="#b16">[16]</ref>.</p><p>The generator g determines, among other concerns, how the computational graph is wired. For example, in ResNets a generator produces a stack of blocks that compute x + F(x). The parameters θ specify the instantiated network and may contain diverse information. For example, in a ResNet generator, θ can specify the number of stages, number of residual blocks for each stage, depth/width/filter sizes, activation types, etc.</p><p>Intuitively, one may think of g as a function in a programming language, e.g. Python, that takes a list of arguments (corresponding to θ), and returns a network architecture. The network representation n returned by the generator is symbolic, meaning that it specifies the type of operations that are performed and the flow of data; it does not include values of network weights, 1 which are learned from data after a network is generated.</p><p>Stochastic network generators. The above network generator g(θ) performs a deterministic mapping: given the same θ, it always returns the same network architecture n. We can extend g to accept an additional argument s that is the seed of a pseudo-random number generator that is used internally by g. Given this seed, one can construct a (pseudo) random family of networks by calling g(θ, s) multiple times, keeping θ fixed but changing the value of s = 1, 2, 3, . . .. For a fixed value of θ, a uniform probability distribution over all possible seed values induces a (likely non-uniform) probability distribution over N . We call generators of the form g(θ, s) stochastic network generators.</p><p>Before we discuss our method, we provide additional background by reinterpreting the work on NAS <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref> in the context of stochastic network generators.</p><p>NAS from the network generator perspective. The NAS methods of <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref> utilize an LSTM "controller" in the process of generating network architectures. But the LSTM is only part of the complete NAS network generator, which is in fact a stochastic network generator, as illustrated next.</p><p>The weight matrices of the LSTM are the parameters θ of the generator. The output of each LSTM time-step is a probability distribution conditioned on θ. Given this distribution and the seed s, each step samples a construction action (e.g., insert an operator, connect two nodes). The parameters θ of the LSTM controller, due to its probabilistic behavior, are optimized (searched for) by RL in <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>However, the LSTM is not the only component in the NAS network generator g(θ, s). There are also handdesigned rules defined to map the sampled actions to a computational DAG, and these rules are also part of g. Using the node/edge terminology in graph theory, for a NAS network in <ref type="bibr" target="#b56">[56]</ref>, if we map a combination operation (e.g., sum) to a node and a unary transformation (e.g., conv) to an edge (see the supplement), the rules of the NAS generator include, but are not limit to:</p><p>• A subgraph to be searched, called a cell <ref type="bibr" target="#b56">[56]</ref>, always accepts the activations of the output nodes from the 2 immediately preceding cells;</p><p>• Each cell contains 5 nodes that are wired to 2 and only 2 existing nodes, chosen by sampling from the probability distribution output by the LSTM;</p><p>• All nodes that have no output in a cell are concatenated by an extra node to form a valid DAG for the cell.</p><p>All of the generation rules, together with the choice of using an LSTM, and other hyper-parameters of the system (e.g., the number of nodes, say, 5), comprise the NAS network generator that produces a full DAG. It is also worth noticing that the view of "node as combination and edge as transformation" is not the only way to interpret a neural network as a graph, and so it is not the only way to turn a general graph into a neural network (we use a different mapping in §3.2). Encapsulating the complete generation process, as we have illustrated, reveals which components are optimized and which are hard-coded. It now becomes explicit that the network space N has been carefully restricted by handdesigned rules. For example, the rules listed above suggest that each of the 5 nodes in a cell always has precisely input degree 2 2 and output degree 1 (see the supplement). This does not cover all possible 5-(internal-)node graphs. It is in a highly restricted network space. Viewing NAS from the perspective of a network generator helps explain the recently demonstrated ineffectiveness of sophisticated optimization vs. random search <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>: the manual design in the NAS network generator is a strong prior, which represents a meta-optimization beyond the search over θ (by RL, e.g.) and s (by random search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Randomly Wired Neural Networks</head><p>Our analysis of NAS reveals that the network generator is hand-designed and encodes a prior from human knowledge. It is likely that the design of the network generator plays a considerable role-if so, current methods are short of achieving "AutoML" <ref type="bibr" target="#b20">[20]</ref> and still involve significant human effort (c.f . "Our experiments show that Neural Architecture Search can design good models from scratch." <ref type="bibr" target="#b55">[55]</ref>,  <ref type="figure">Figure 2</ref>. Node operations designed for our random graphs. Here we illustrate a node (blue circle) with 3 input edges and 4 output edges. The aggregation is done by weighted sum with learnable positive weights w0, w1, w2. The transformation is a ReLU-convolution-BN triplet, simply denoted as conv. The transformed data are sent out as 4 copies. emphasis added). To investigate how important the generator design is, it is not sufficient to compare different optimizers (sophisticated or random) for the same NAS generator; it is necessary to study new network generators that are substantially different from the NAS generator. This leads to our exploration of randomly wired neural networks. That is, we will define network generators that yield networks with random graphs, subject to different human-specific priors. To minimize the human bias from us-the authors of this paper-on the prior, we will use three classical random graph models in our study ( <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">50]</ref>; §3.3). Our methodology for generating randomly wired networks involves the following concepts:</p><p>Generating general graphs. Our network generator starts by generating a general graph (in the sense of graph theory). It generates a set of nodes and edges that connect nodes, without restricting how the graphs correspond to neural networks. This allows us to freely use any general graph generator from graph theory (ER/BA/WS). Once a graph is obtained, it is mapped to a computable neural network.</p><p>The mapping from a general graph to neural network operations is in itself arbitrary, and thus also human-designed. We intentionally use a simple mapping, discussed next, so that we can focus on graph wiring patterns.</p><p>Edge operations. Assuming by construction that the graph is directed, we define that edges are data flow, i.e., a directed edge sends data (a tensor) from one node to another node.</p><p>Node operations. A node in a directed graph may have some input edges and some output edges. We define the operations represented by one node <ref type="figure">(Figure 2</ref>) as:</p><p>-Aggregation: The input data (from one or more edges) to a node are combined via a weighted sum; the weights are learnable and positive. <ref type="bibr" target="#b2">3</ref> -Transformation: The aggregated data is processed by a transformation defined as a ReLU-convolution-BN triplet 4 <ref type="bibr" target="#b12">[12]</ref>. The same type of convolution is used for all nodes, e.g., a 3×3 separable convolution 5 by default. <ref type="bibr" target="#b2">3</ref> Applying sigmoid on unrestricted weights ensures they are positive. <ref type="bibr" target="#b3">4</ref> Instead of a triplet with a convolution followed by BN <ref type="bibr" target="#b17">[17]</ref> then ReLU <ref type="bibr" target="#b32">[32]</ref>, we use the ReLU-convolution-BN triplet, as it means the aggregation (at the next nodes) can receive positive and negative activation, preventing the aggregated activation from being inflated in case of a large input degree. <ref type="bibr" target="#b4">5</ref> Various implementations of separable convolutions exist. We use the -Distribution: The same copy of the transformed data is sent out by the output edges of the node.</p><p>These operations have some nice properties:</p><p>(i) Additive aggregation (unlike concatenation) maintains the same number of output channels as input channels, and this prevents the convolution that follows from growing large in computation, which may increase the importance of nodes with large input degree simply because they increase computation, not because of how they are wired.</p><p>(ii) The transformation should have the same number of output and input channels (unless switching stages; discussed later), to make sure the transformed data can be combined with the data from any other nodes. Fixing the channel count then keeps the FLOPs (floating-point operations) and parameter count unchanged for each node, regardless of its input and output degrees.</p><p>(iii) Aggregation and distribution are almost parameterfree (except for a negligible number of parameters for weighted summation), regardless of input and output degrees. Also, given that every edge is parameter-free the overall FLOPs and parameter count of a graph are roughly proportional to the number of nodes, and nearly independent of the number of edges.</p><p>These properties nearly decouple FLOPs and parameter count from network wiring, e.g., the deviation of FLOPs is typically ±2% among our random network instances or different generators. This enables the comparison of different graphs without inflating/deflating model complexity. Differences in task performance are therefore reflective of the properties of the wiring pattern.</p><p>Input and output nodes. Thus far, a general graph is not yet a valid neural network even given the edge/node operations, because it may have multiple input nodes (i.e., those without any input edge) and multiple output nodes. It is desirable to have a single input and a single output for typical neural networks, e.g., for image classification. We apply a simple post-processing step.</p><p>For a given general graph, we create a single extra node that is connected to all original input nodes. This is the unique input node that sends out the same copy of input data to all original input nodes. Similarly, we create a single extra node that is connected to all original output nodes. This is the unique output node; we have it compute the (unweighted) average from all original output nodes. These two nodes perform no convolution. When referring to the node count N , we exclude these two nodes.</p><p>Stages. With unique input and output nodes, it is sufficient for a graph to represent a valid neural network. Nevertheless, in image classification in particular, networks that form of <ref type="bibr" target="#b4">[5]</ref>: a 3×3 separable convolution is a 3×3 depth-wise convolution followed by a 1×1 convolution, with no non-linearity in between.  <ref type="table">Table 1</ref>. RandWire architectures for small and regular computation networks. A random graph is denoted by the node count (N ) and channel count for each node (C). We use conv to denote a ReLU-Conv-BN triplet (expect conv1 is Conv-BN). The input size is 224×224 pixels. The change of the output size implies a stride of 2 (omitted in table) in the convolutions that are right after the input of each stage. maintain the full input resolution throughout are not desirable. It is common <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b56">56]</ref> to divide a network into stages that progressively down-sample feature maps.</p><p>We use a simple strategy: the random graph generated above defines one stage. Analogous to the stages in a ResNet, e.g., conv 1,2,3,4,5 <ref type="bibr" target="#b11">[11]</ref>, our entire network consists of multiple stages. One random graph represents one stage, and it is connected to its preceding/succeeding stage by its unique input/output node. For all nodes that are directly connected to the input node, their transformations are modified to have a stride of 2. The channel count in a random graph is increased by 2× when going from one stage to the next stage, following <ref type="bibr" target="#b11">[11]</ref>. <ref type="table">Table 1</ref> summarizes the randomly wired neural networks, referred to as RandWire, used in our experiments. They come in small and regular complexity regimes (more in §4). For conv 1 and/or conv 2 we use a single convolutional layer for simplicity with multiple random graphs following. The network ends with a classifier output ( <ref type="table">Table 1</ref>, last row). <ref type="figure">Figure 1</ref> shows full computation graphs of three randomly wired network samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Random Graph Models</head><p>We now describe in brief the three classical random graph models used in our study. We emphasize that these random graph models are not proposed by this paper; we describe them for completeness. The three classical models all generate undirected graphs; we use a simple heuristic to turn them into DAGs (see the supplement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Erdős-Rényi (ER)</head><p>. In the ER model <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b5">6]</ref>, with N nodes, an edge between two nodes is connected with probability P , independent of all other nodes and edges. This process is iterated for all pairs of nodes. The ER generation model has only a single parameter P , and is denoted as ER(P ).</p><p>Any graph with N nodes has non-zero probability of being generated by the ER model, including graphs that are disconnected. However, a graph generated by ER(P ) has high probability of being a single connected component if P &gt; ln(N ) N <ref type="bibr" target="#b5">[6]</ref>. This provides one example of an implicit bias introduced by a generator.</p><p>Barabási-Albert (BA). The BA model <ref type="bibr" target="#b0">[1]</ref> generates a random graph by sequentially adding new nodes. The initial state is M nodes without any edges (1 ≤ M &lt; N ). The method sequentially adds a new node with M new edges. For a node to be added, it will be connected to an existing node v with probability proportional to v's degree. The new node repeatedly adds non-duplicate edges in this way until it has M edges. Then this is iterated until the graph has N nodes. The BA generation model has only a single parameter M , and is denoted as BA(M ).</p><p>Any graph generated by BA(M ) has exactly M ·(N −M ) edges. So the set of all graphs generated by BA(M ) is a subset of all possible N -node graphs-this gives one example on how an underlying prior can be introduced by the graph generator in spite of randomness.</p><p>Watts-Strogatz (WS). The WS model <ref type="bibr" target="#b50">[50]</ref> was defined to generate small-world graphs <ref type="bibr" target="#b18">[18]</ref>. Initially, the N nodes are regularly placed in a ring and each node is connected to its K/2 neighbors on both sides (K is an even number). Then, in a clockwise loop, for every node v, the edge that connects v to its clockwise i-th next node is rewired with probability P . "Rewiring" is defined as uniformly choosing a random node that is not v and that is not a duplicate edge. This loop is repeated K/2 times for 1≤i≤K/2. K and P are the only two parameters of the WS model, denoted as WS(K, P ).</p><p>Any graph generated by WS(K, P ) has exactly N ·K edges. WS(K, P ) only covers a small subset of all possible N -node graphs too, but this subset is different from the subset covered by BA. This provides an example on how a different underlying prior has been introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Design and Optimization</head><p>Our randomly wired neural networks are generated by a stochastic network generator g(θ, s). The random graph parameters, namely, P , M , (K, P ) in ER, BA, WS respectively, are part of the parameters θ. The "optimization" of such a 1-or 2-parameter space is essentially done by trialand-error by human designers, e.g., by line/grid search. Conceptually, such "optimization" is not distinct from many other designs involved in our and other models (including NAS), e.g., the number of nodes, stages, and filters.</p><p>Optimization can also be done by scanning the random seed s, which is an implementation of random search. Random search is possible for any stochastic network generator, including ours and NAS. But as we present by experiment,   <ref type="figure">Figure 4</ref>. Visualization of the random graphs generated by ER, BA, and WS. Each plot represents one random graph instance sampled by the specified generator. The generators are those in <ref type="figure" target="#fig_1">Figure 3</ref>. The node count is N =32 for each graph. A blue/red node denotes an input/output node, to which an extra unique input/output node (not shown) will be added (see §3.2).</p><p>the accuracy variation of our networks is small for different seeds s, suggesting that the benefit of random search may be small. So we perform no random search and report mean accuracy of multiple random network instances. As such, our network generator has minimal optimization (1-or 2-parameter grid search) beyond their hand-coded design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on the ImageNet 1000-class classification task <ref type="bibr" target="#b39">[39]</ref>. We train on the training set with ∼1.28M images and test on the 50K validation images. Architecture details. Our experiments span a small computation regime (e.g., MobileNet <ref type="bibr" target="#b15">[15]</ref> and ShuffleNet <ref type="bibr" target="#b54">[54]</ref>) and a regular computation regime (e.g., ResNet-50/101 <ref type="bibr" target="#b11">[11]</ref>). RandWire nets in these regimes are in <ref type="table">Table 1</ref>, where N nodes and C channels determine network complexity.</p><p>We set N =32, and then set C to the nearest integer such that target model complexity is met: C=78 in the small regime, and C=109 or 154 in the regular regime.</p><p>Random seeds. For each generator, we randomly sample 5 network instances (5 random seeds), train them from scratch, and evaluate accuracy for each instance. To emphasize that we perform no random search for each generator, we report the classification accuracy with "mean±std" for all 5 random seeds (i.e., we do not pick the best). We use the same seeds 1, . . ., 5 for all experiments.</p><p>Implementation details. We train our networks for 100 epochs, unless noted. We use a half-period-cosine shaped learning rate decay <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b16">16]</ref>. The initial learning rate is 0.1, the weight decay is 5e-5, and the momentum is 0.9. We use label smoothing regularization <ref type="bibr" target="#b44">[44]</ref> with a coefficient of 0.1. Other details of the training procedure are the same as <ref type="bibr" target="#b10">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis Experiments</head><p>Random graph generators. <ref type="figure" target="#fig_1">Figure 3</ref> compares the results of different generators in the small computation regime: each RandWire net has ∼580M FLOPs. <ref type="figure">Figure 4</ref> visualizes one example graph for each generator. The graph generator is specified by the random graph model (ER/BA/WS) and its set of parameters: e.g., ER(0.2). We observe:</p><p>All random generators provide decent accuracy over all 5 random network instances; none of them fails to converge. ER, BA, and WS all have certain settings that yield mean accuracy of &gt;73%, within a &lt;1% gap from the best mean accuracy of 73.8% from WS(4, 0.75).</p><p>Moreover, the variation among the random network instances is low. Almost all random generators in <ref type="figure" target="#fig_1">Figure 3</ref> have an standard deviation (std) of 0.2∼0.4%. As a comparison, training the same instance of a ResNet-50 multiple times has a typical std of 0.1∼0.2% <ref type="bibr" target="#b10">[10]</ref>. The observed low variance of our random generators suggests that even without random search (i.e., picking the best from several random instances), it is likely that the accuracy of a network instance is close to the mean accuracy, subject to some noise.</p><p>On the other hand, different random generators may have a gap between their mean accuracies, e.g., BA(1) has 70.7% accuracy and is ∼3% lower than WS(4, 0.75). This suggests that random generator design, including the wiring priors (BA vs. WS) and generation parameters, plays an important role in the accuracy of sampled network instances. <ref type="figure" target="#fig_1">Figure 3</ref> also includes a set of non-random generators: WS(K, P =0). "P =0" means no random rewiring. Interestingly, the results of WS(K, P =0) are all worse than their WS(K, P &gt;0) counterparts for any fixed K in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Graph damage. We explore graph damage by randomly removing one node or edge-an ablative setting inspired by <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b49">49]</ref>. Formally, given a network instance after training, we remove one node or one edge from the graph and evaluate the validation accuracy without any further training.</p><p>When a node is removed, we evaluate the accuracy loss (∆) vs. the output degree of that node <ref type="figure" target="#fig_2">(Figure 5, top)</ref>. It is clear that ER, BA, and WS behave differently under such damage. For networks generated by WS, the mean degradation of accuracy is larger when the output degree of the removed node is higher. This implies that "hub" nodes in WS that send information to many nodes are influential.</p><p>When an edge is removed, we evaluate the accuracy loss vs. the input degree of this edge's target node <ref type="figure" target="#fig_2">(Figure 5</ref>, bottom). If the input degree of an edge's target node is smaller, removing this edge tends to change a larger portion of the target node's inputs. This trend can be seen by the fact that the accuracy loss is generally decreasing along the x-axis in <ref type="figure" target="#fig_2">Figure 5</ref> (bottom). The ER model is less sensitive to edge removal, possibly because in ER's definition wiring of every edge is independent.  Node operations. Thus far, all models in our experiment use a 3×3 separable convolution as the "conv" in <ref type="figure">Figure 2</ref>. Next we evaluate alternative choices. We consider: (i) 3×3 (regular) convolution, and (ii) 3×3 max-/average-pooling followed by a 1×1 convolution. We replace the transformation of all nodes with the specified alternative. We adjust the factor C to keep the complexity of all alternative networks. <ref type="figure" target="#fig_3">Figure 6</ref> shows the mean accuracy for each of the generators listed in <ref type="figure" target="#fig_1">Figure 3</ref>. Interestingly, almost all networks still converge to non-trivial results. Even "3×3 pool with 1×1 conv" performs similarly to "3×3 conv". The network generators roughly maintain their accuracy ranking despite the operation replacement; in fact, the Pearson correlation between any two series in <ref type="figure" target="#fig_2">Figure 5</ref> is 0.91∼0.98. This suggests that the network wiring plays a role somewhat orthogonal to the role of the chosen operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons</head><p>Small computation regime. <ref type="table" target="#tab_3">Table 2</ref> compares our results in the small computation regime, a common setting studied in existing NAS papers. Instead of training for 100 epochs, here we train for 250 epochs following settings in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>     wiring (MobileNet/ShuffleNet) and NAS-based results, except for AmoebaNet-C <ref type="bibr" target="#b34">[34]</ref>. The mean accuracy achieved by RandWire is a competitive result, especially considering that we perform no random search in our random generators, and that we use a single operation type for all nodes.</p><p>Regular computation regime. Next we compare the RandWire networks with ResNet-50/101 <ref type="bibr" target="#b11">[11]</ref> under similar FLOPs. In this regime, we use a regularization method inspired by our edge removal analysis: for each training minibatch, we randomly remove one edge whose target node has input degree &gt; 1 with probability of 0.1. This regularization is similar to DropPath adopted in NAS <ref type="bibr" target="#b56">[56]</ref>. We train with a weight decay of 1e-5 and a DropOut <ref type="bibr" target="#b13">[13]</ref> rate of 0.2 in the classifier fc layer. Other settings are the same as the small computation regime. We train the ResNet/ResNeXt competitors using the recipe of <ref type="bibr" target="#b10">[10]</ref>, but with the cosine backbone AP AP 50 AP 75 AP S AP M AP L ResNet-50 <ref type="bibr" target="#b11">[11]</ref> 37.1 58. <ref type="bibr" target="#b8">8</ref>   <ref type="table">Table 5</ref>. COCO object detection results fine-tuned from the networks in <ref type="table" target="#tab_4">Table 3</ref>, reported on the val2017 set. The backbone networks have comparable FLOPs to ResNet-50 or ResNet-101. schedule and label smoothing, for fair comparisons. <ref type="table" target="#tab_4">Table 3</ref> compares RandWire with ResNet and ResNeXt under similar FLOPs as ResNet-50/101. Our mean accuracies are respectively 1.9% and 1.3% higher than ResNet-50 and ResNet-101, and are 0.6% higher than the ResNeXt counterparts. Both ResNe(X)t and RandWire can be thought of as hand-designed, but ResNe(X)t is based on designed wiring patterns, while RandWire uses a designed stochastic generator. These results illustrate different roles that manual design can play.</p><p>Larger computation. For completeness, we compare with the most accurate NAS-based networks, which use more computation. For simplicity, we use the same trained networks as in <ref type="table" target="#tab_4">Table 3</ref>, but only increase the test image size to 320×320 without retraining. <ref type="table" target="#tab_5">Table 4</ref> compares the results.</p><p>Our networks have mean accuracy 0.7%∼1.3% lower than the most accurate NAS results, but ours use only ∼2/3 FLOPs and ∼3/4 parameters. Our networks are trained for 100 epochs and not on the target image size, vs. the NAS methods which use &gt;250 epochs and train on the target 331×331 size. Our model has no search on operations, unlike NAS. These gaps will be explored in future work.</p><p>COCO object detection. Finally, we report the transferability results by fine-tuning the networks for COCO object detection <ref type="bibr" target="#b25">[25]</ref>. We use Faster R-CNN <ref type="bibr" target="#b35">[35]</ref> with FPN <ref type="bibr" target="#b24">[24]</ref> as the object detector. Our fine-tuning is based on 1× setting of the publicly available Detectron <ref type="bibr" target="#b9">[9]</ref>. We simply replace the backbones with those in <ref type="table" target="#tab_4">Table 3</ref> (regular regime). <ref type="table">Table 5</ref> compares the object detection results. A trend is observed similar to that in the ImageNet experiments in <ref type="table" target="#tab_4">Table 3</ref>. These results indicate that the features learned by our randomly wired networks can also transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We explored randomly wired neural networks driven by three classical random graph models from graph theory. The results were surprising: the mean accuracy of these models is competitive with hand-designed and optimized models from recent work on neural architecture search. Our exploration was enabled by the novel concept of a network generator. We hope that future work exploring new generator designs may yield new, powerful networks designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>Mapping a NAS cell to a graph. If one maps a combining op (e.g., addition, concatenation) to a node, and a unary transformation (e.g., 3×3 conv, 5×5 conv, identity) to an edge <ref type="figure" target="#fig_6">(Figure 7, right)</ref>, then all cells in the NAS search space share this property: internal nodes all have precisely input degree 2 and output degree 1. This is an implicit prior induced by the design.</p><p>The mapping from the NAS cell to a graph is not unique. One may map both combining and unary transformations to nodes, and data flow to edges <ref type="figure" target="#fig_6">(Figure 7, left)</ref>. The above property on the NAS search space can be instead described as: internal merging nodes all have precisely input degree 2 and output degree 1.   Converting undirected graphs into DAGs. ER, BA, and WS models generate random undirected graphs. We convert them to DAGs using a simple heuristic: we assign indices to all nodes in a graph, and set the direction of every edge as pointing from the smaller-index node to the larger-index one. This heuristic ensures that there is no cycle in the resulted directed graph. The node indexing strategies for the models are -ER: indices are assigned in a random order; BA: the initial M nodes are assigned indices 1 to M , and all other nodes are indexed following their order of adding to the graph; WS: indices are assigned sequentially in the clockwise order.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Comparison on random graph generators: ER, BA, and WS in the small computation regime. Each bar represents the results of a generator under a parameter setting for P , M , or (K, P ) (tagged in x-axis). The results are ImageNet top-1 accuracy, shown as mean and standard deviation (std) over 5 random network instances sampled by a generator. At the rightmost, WS(K, P =0) has no randomness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 .</head><label>5</label><figDesc>removed edge's target node Figure Graph damage ablation. We randomly remove one node (top) or remove one edge (bottom) from a graph after the network is trained, and evaluate the loss (∆) in accuracy on Im-ageNet. From left to right are ER, BA, and WS generators. Red circle: mean; gray bar: median; orange box: interquartile range; blue dot: an individual damaged instance.random graph models (ER, BA, WS with different P, M, (K, P)) max-pool 1×1 conv 3×3 avg-pool 1×1 conv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Alternative node operations. Each column is the mean accuracy of the same set of 5 random graphs equipped with different node operations, sorted by "3×3 separable conv" (fromFigure 3). The generators roughly maintain their orders of accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Mapping a NAS cell (left, credit:<ref type="bibr" target="#b56">[56]</ref>) to a graph (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>±0.25 92.2 ±0.15 583 ±6.2 5.6 ±0.1</figDesc><table><row><cell>network</cell><cell cols="4">top-1 acc. top-5 acc. FLOPs (M) params (M)</cell></row><row><cell>MobileNet [15]</cell><cell>70.6</cell><cell>89.5</cell><cell>569</cell><cell>4.2</cell></row><row><cell cols="2">MobileNet v2 [40] 74.7</cell><cell>-</cell><cell>585</cell><cell>6.9</cell></row><row><cell>ShuffleNet [54]</cell><cell>73.7</cell><cell>91.5</cell><cell>524</cell><cell>5.4</cell></row><row><cell cols="2">ShuffleNet v2 [30] 74.9</cell><cell>92.2</cell><cell>591</cell><cell>7.4</cell></row><row><cell>NASNet-A [56]</cell><cell>74.0</cell><cell>91.6</cell><cell>564</cell><cell>5.3</cell></row><row><cell>NASNet-B [56]</cell><cell>72.8</cell><cell>91.3</cell><cell>488</cell><cell>5.3</cell></row><row><cell>NASNet-C [56]</cell><cell>72.5</cell><cell>91.0</cell><cell>558</cell><cell>4.9</cell></row><row><cell>Amoeba-A [34]</cell><cell>74.5</cell><cell>92.0</cell><cell>555</cell><cell>5.1</cell></row><row><cell>Amoeba-B [34]</cell><cell>74.0</cell><cell>91.5</cell><cell>555</cell><cell>5.3</cell></row><row><cell>Amoeba-C [34]</cell><cell>75.7</cell><cell>92.4</cell><cell>570</cell><cell>6.4</cell></row><row><cell>PNAS [26]</cell><cell>74.2</cell><cell>91.9</cell><cell>588</cell><cell>5.1</cell></row><row><cell>DARTS [27]</cell><cell>73.1</cell><cell>91.0</cell><cell>595</cell><cell>4.9</cell></row><row><cell>RandWire-WS</cell><cell>74.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>for fair comparisons.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RandWire with WS(4, 0.75) has mean accuracy of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.7% (with min 74.4% and max 75.0%). This result is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>better than or comparable to all existing hand-designed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>ImageNet: small computation regime (i.e., &lt;600M RandWire-WS, C=109 79.0 ±0.17 94.4 ±0.11 4.0 ±0.09 31.9 ±0.66 RandWire-WS, C=154 80.1 ±0.19 94.8 ±0.18 7.9 ±0.18 61.5 ±1.32</figDesc><table><row><cell cols="5">FLOPs). RandWire results are the mean accuracy (±std) of 5 ran-</cell></row><row><cell cols="5">dom network instances, with WS(4, 0.75). Here we train for 250</cell></row><row><cell cols="4">epochs similar to [56, 34, 26, 27], for fair comparisons.</cell><cell></cell></row><row><cell>network</cell><cell cols="4">top-1 acc. top-5 acc. FLOPs (B) params (M)</cell></row><row><cell>ResNet-50 [11]</cell><cell>77.1</cell><cell>93.5</cell><cell>4.1</cell><cell>25.6</cell></row><row><cell>ResNeXt-50 [52]</cell><cell>78.4</cell><cell>94.0</cell><cell>4.2</cell><cell>25.0</cell></row><row><cell>ResNet-101 [11]</cell><cell>78.8</cell><cell>94.4</cell><cell>7.8</cell><cell>44.6</cell></row><row><cell>ResNeXt-101 [52]</cell><cell>79.5</cell><cell>94.6</cell><cell>8.0</cell><cell>44.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>ImageNet: regular computation regime with FLOPs comparable to ResNet-50 (top) and to ResNet-101 (bottom). ResNeXt is the 32×4 version [52]. RandWire is WS(4, 0.75). RandWire-WS 320 2 100 81.6 ±0.13 95.6 ±0.07 16.0 ±0.36 61.5 ±1.32</figDesc><table><row><cell>network</cell><cell>test size</cell><cell cols="3">epochs top-1 acc. top-5 acc. FLOPs (B) params (M)</cell></row><row><cell cols="3">NASNet-A [56] 331 2 &gt;250 82.7</cell><cell>96.2</cell><cell>23.8</cell><cell>88.9</cell></row><row><cell cols="3">Amoeba-B [34] 331 2 &gt;250 82.3</cell><cell>96.1</cell><cell>22.3</cell><cell>84.0</cell></row><row><cell cols="3">Amoeba-A [34] 331 2 &gt;250 82.8</cell><cell>96.1</cell><cell>23.1</cell><cell>86.7</cell></row><row><cell cols="3">PNASNet-5 [26] 331 2 &gt;250 82.9</cell><cell>96.2</cell><cell>25.0</cell><cell>86.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>ImageNet: large computation regime. Our networks are the same as inTable 3(C=154), but we evaluate on 320×320 images instead of 224×224. Ours are only trained for 100 epochs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>39.7 21.9 40.8 47.6 ResNeXt-50 [52] 38.2 60.5 41.3 23.0 41.5 48.8 RandWire-WS, C=109 39.9 61.9 43.3 23.6 43.5 52.7 ResNet-101 [11] 39.8 61.7 43.3 23.7 43.9 51.7 ResNeXt-101 [52] 40.7 62.9 44.5 24.4 44.8 52.7 RandWire-WS, C=154 41.1 63.1 44.6 24.6 45.1 53.0</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use parameters to refer to network generator arguments and weights to refer to the learnable weights and biases of a generated network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In graph theory, "degree" is the number of edges connected to a node. We refer to "input/output degree" as that of input/output edges to a node.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Small-world brain networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The neuroscientist</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="512" to="523" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">353</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Complex brain networks: graph theoretical analysis of structural and functional systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: A critical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1141" to="1144" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Small world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Ablex Pub</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Using machine learning to explore neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>ECCV. 2014. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Theory of neural-analog reinforcement systems and its application to the brain model problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Minsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
		<respStmt>
			<orgName>Princeton University.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">386</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Intelligent machinery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structural properties of the caenorhabditis elegans neuronal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paniagua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collective dynamics of smallworldnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Introduction to graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prentice hall Upper Saddle River</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>NJ</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
