<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Clinical Concept Extraction with Contextual Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Si</surname></persName>
							<email>yuqi.si@uth.tmc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution">The University of Texas Health Science Center at Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution">The University of Texas Health Science Center at Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution">The University of Texas Health Science Center at Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
							<email>kirk.roberts@uth.tmc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution">The University of Texas Health Science Center at Houston</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Clinical Concept Extraction with Contextual Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network-based representations ("embeddings") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (e.g., ELMo, BERT) have further pushed the stateof-the-art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText). Both offthe-shelf, open-domain embeddings and pretrained clinical embeddings from MIMIC-III are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings, and compare these on four concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pre-training time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings. Contextual embeddings pre-trained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1measures of 90. 25, 93.18 (partial), 80.74, and  81.65. We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate that contextual embeddings encode valuable semantic information not accounted for in traditional word representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Concept extraction is the most common clinical natural language processing (NLP) task <ref type="bibr" target="#b37">(Tang et al., 2013;</ref><ref type="bibr" target="#b17">Kundeti et al., 2016;</ref><ref type="bibr" target="#b39">Unanue et al., 2017;</ref><ref type="bibr" target="#b44">Wang et al., 2018b)</ref>, and a precursor to downstream tasks such as relations <ref type="bibr" target="#b28">(Rink et al., 2011)</ref>, frame parsing <ref type="bibr" target="#b12">(Gupta et al., 2018;</ref><ref type="bibr" target="#b33">Si and Roberts, 2018)</ref>, co-reference <ref type="bibr" target="#b19">(Lee et al., 2011)</ref>, and phenotyping <ref type="bibr" target="#b46">(Xu et al., 2011;</ref><ref type="bibr" target="#b42">Velupillai et al., 2018)</ref>. Corpora such as those from i2b2 <ref type="bibr" target="#b40">(Uzuner et al., 2011;</ref><ref type="bibr" target="#b35">Sun et al., 2013;</ref><ref type="bibr" target="#b34">Stubbs et al., 2015)</ref>, ShARe/CLEF <ref type="bibr" target="#b36">(Suominen et al., 2013;</ref><ref type="bibr" target="#b16">Kelly et al., 2014)</ref>, and SemEval <ref type="bibr" target="#b27">(Pradhan et al., 2014;</ref><ref type="bibr" target="#b9">Elhadad et al., 2015;</ref><ref type="bibr" target="#b2">Bethard et al., 2016)</ref> act as evaluation benchmarks and datasets for training machine learning (ML) models.</p><p>Meanwhile, neural network-based representations continue to advance nearly all areas of NLP, from question answering <ref type="bibr" target="#b32">(Shen et al., 2017)</ref> to named entity recognition <ref type="bibr" target="#b6">(Chang et al., 2015;</ref><ref type="bibr" target="#b13">Habibi et al., 2017;</ref><ref type="bibr" target="#b39">Unanue et al., 2017;</ref><ref type="bibr" target="#b10">Florez et al., 2018</ref>) (a close analog of concept extraction). Recent advances in contextualized representations, including ELMo <ref type="bibr" target="#b26">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, have pushed performance even further. These have demonstrated that relatively simple downstream models using contextualized embeddings can outperform complex models <ref type="bibr" target="#b31">(Seo et al., 2016)</ref> using embeddings such as word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>.</p><p>In this paper, we aim to explore the potential impact these representations have on clinical concept extraction. Our contributions include the following:</p><p>1. An evaluation exploring numerous embedding methods: word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>, GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>, fast-Text <ref type="bibr" target="#b3">(Bojanowski et al., 2016)</ref>, <ref type="bibr">ELMo (Peters et al., 2018)</ref>, and BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>.</p><p>2. An analysis covering four clinical concept corpora, demonstrating the generalizability of these methods.</p><p>3. A performance increase for clinical concept extraction that achieves state-of-the-art results on all four corpora.</p><p>4. A demonstration of the effect of pre-training on clinical corpora vs larger open domain corpora, an important trade-off in clinical NLP <ref type="bibr" target="#b29">(Roberts, 2016)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section introduces the theoretical knowledge that supports the shift from word-level embeddings to contextual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Embedding Models</head><p>Word-level vector representation methods learn a real-valued vector to represent a single word. One of the most prominent methods for word-level representation is word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>. So far, word2vec has widely established its effectiveness for achieving state-of-the-art performances in a variety of clinical NLP tasks <ref type="bibr" target="#b43">(Wang et al., 2018a)</ref>. GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref> is another unsupervised learning approach to obtain a vector representation for a single word. Unlike word2vec, GloVe is a statistical model that aggregates both a global matrix factorization and a local context window. The learning relies on dimensionality reduction on the co-occurrence count matrix based on how frequently a word appears in a context. fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2016)</ref> is also an established library for word representations. Unlike word2vec and GloVe, fastText considers individual words as character n-grams. For instance, cold is made of the n-grams c, co, col, cold, o, ol, old, l, ld, and d. This approach enables handling of infrequent words that are not present in the training vocabulary, alleviating some out-ofvocabulary issues. However, the effectiveness of word-level representations is hindered by the limitation that they conflate all possible meanings of a word into a single representation and so the embedding is not adjusted to the surrounding context. In order to tackle these deficiencies, advanced approaches have attempted to directly model the word's context into the vector representation. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates this with the word cold, in which a traditional word embedding assigns all senses of the word cold with a single vector, whereas a contextual representation varies the vector based on its meaning in context (e.g., cold temperature, medical symptom/condition, an unfriendly disposition). Although a fictional figure is shown here, we later demonstrate this on real data.</p><p>The first contextual word representation that we consider to overcome this issue is ELMo <ref type="bibr" target="#b26">(Peters et al., 2018)</ref>. Unlike the previously mentioned traditional word embeddings that constitute a single vector for each word and the vector remains stable in downstream tasks, this contextual word representation can capture the context information and dynamically alter a multilayer representation. At training time, a language model objective is used to learn the context-sensitive embeddings from a large text corpus. The training step of learning these context-sensitive embeddings is known as pre-training. After pre-training, the contextsensitive embedding of each word will be fed into the sentences for downstream tasks. The downstream task learns the shared weights of the inner state of pre-trained language model by optimizing the loss on the downstream task.</p><p>BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> is also a contextual word representation model, and, similar to ELMo, pre-training on an unlabeled corpus with a language model objective. Compared to ELMo, BERT is deeper in how it handles contex-tual information due to a deep bidirectional transformer for encoding sentences. It is based on a transformer architecture employing self-attention <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>. The deep bidirectional transformer is equipped with multi-headed selfattention to prevent locality bias and to achieve long-distance context comprehension. Additionally, in terms of the strategy for how to incorporate these models into the downstream task, ELMo is a feature-based language representation while BERT is a fine-tuning approach. The featurebased strategy is similar to traditional word embedding methods that considers the embedding as input features for the downstream task. The finetuning approach, on the other hand, adjusts the entire language model on the downstream task to achieve a task-specific architecture. So while the ELMo embeddings may be used as the input of a downstream model, with the BERT fine-tuning method, the entire BERT model is integrated into the downstream task. This fine-tuning strategy is more likely to make use of the encoded information in the pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clinical Concept Extraction</head><p>Clinical concept extraction is the task of identifying medical concepts (e.g., problem, test, treatment) from clinical notes. This is typically considered as a sequence tagging problem to be solved with machine learning-based models (e.g., Conditional Random Field) using hand-engineered clinical domain knowledge as features <ref type="bibr" target="#b44">(Wang et al., 2018b;</ref><ref type="bibr" target="#b7">De Bruijn et al., 2011)</ref>. Recent advances have demonstrated the effectiveness of deep learning-based models with word embeddings as input. Up to now, the most prominent model for clinical concept extraction is a bidirectional Long Short-Term Memory with Conditional Random Field (Bi-LSTM CRF) architecture <ref type="bibr" target="#b13">(Habibi et al., 2017;</ref><ref type="bibr" target="#b10">Florez et al., 2018;</ref><ref type="bibr" target="#b5">Chalapathy et al., 2016)</ref>. The bidirectional LSTM-based recurrent neural network captures both forward and backward information in the sentence and the CRF layer considers sequential output correlations in the decoding layer using the Viterbi algorithm.</p><p>Most similar to this paper, several recent works have applied contextual embedding methods to concept extraction, both for clinical text and biomedical literature. For instance, ELMo has shown excellent performance on clinical concept extraction <ref type="bibr" target="#b48">(Zhu et al., 2018)</ref>. BioBERT <ref type="bibr" target="#b20">(Lee et al., 2019)</ref> applied BERT primarily to literature concept extraction, pre-training on MEDLINE abstracts and PubMed Central articles, but also applied this model to the i2b2 2010 corpus without clinical pre-training (we include BioBERT in our experiments below). A recent preprint by <ref type="bibr" target="#b1">(Alsentzer et al., 2019)</ref> pre-trains on MIMIC-III, similar to our work, but achieves lower performance on the two tasks in common, i2b2 2010 and 2012. Their work does suggest potential value in only pre-training on MIMIC-III discharge summaries, as opposed to all notes, as well as combining clinical pre-training with literature pretraining. Finally, another recent preprint proposes the use of BERT not for concept extraction, but for clinical prediction tasks such as 30-day readmission prediction <ref type="bibr" target="#b14">(Huang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this paper, we consider both off-the-shelf embeddings from the open domain as well as pretraining clinical domain embeddings on clinical notes from MIMIC-III <ref type="bibr" target="#b15">(Johnson et al., 2016)</ref>, which is a public database of Intensive Care Unit (ICU) patients.</p><p>For the traditional word-embedding experiments, the static embeddings are fed into a Bi-LSTM CRF architecture. All words that occur at least five times in the corpus are included and infrequent words are denoted as UNK. To compensate for the loss due of those unknown words, character embeddings for each word are included.</p><p>For ELMo, the context-independent embeddings with trainable weights are used to form context-dependent embeddings, which are then fed into the downstream task. Specifically, the context-dependent embedding is obtained through a low-dimensional projection and a highway connection after a stacked layer of a character-based Convolutional Neural Network (char-CNN) and a two-layer Bi-LSTM language model (bi-LM). Thus, the contextual word embedding is formed with a trainable aggregation of highly-connected bi-LM. Because the context-independent embeddings already consider representation of characters, it is not necessary to learn a character embedding input for the Bi-LSTM in concept extraction. Finally, the contextual word embedding for each word is fed into the prior state-of-the-art sequence labeling architecture, Bi-LSTM CRF, to predict the label for each token.</p><p>For BERT, both the BERT BASE and BERT LARGE off-the-shelf models are used with additional Bi-LSTM layers at the top of the BERT architecture, which we refer to as BERT BASE (General) and BERT LARGE (General), respectively. For background, the BERT authors released two off-theshelf cased models: BERT BASE and BERT LARGE , with 110 million and 340 million total parameters, respectively. BERT BASE has 12 layers of transformer blocks, 768 hidden units, and 12 selfattention heads, while BERT LARGE has 24 layers of transformer blocks, 1024 hidden units, and 16 self-attention heads. So BERT LARGE is both "wider" and "deeper" in model structure, but is otherwise essentially the same architecture. The models initiated from BERT BASE (General) and BERT LARGE (General) are fine-tuned on the downstream task (e.g., clinical concept recognition in our case). Because BERT integrates sufficient label-correlation information, the CRF layer is abandoned and only a Bi-LSTM architecture is used for sequence labeling. Additionally, two clinical domain embedding models are pre-trained on MIMIC-III, initiated from the BERT BASE and BERT LARGE checkpoints, which we refer to as BERT BASE (MIMIC) and BERT LARGE (MIMIC), respectively.  Our experiments are performed on four widelystudied shared tasks, the 2010 i2b2/VA challenge <ref type="bibr" target="#b40">(Uzuner et al., 2011)</ref>, the 2012 i2b2 challenge <ref type="bibr" target="#b35">(Sun et al., 2013)</ref>, the SemEval 2014 Task 7 (Pradhan et al., 2014) and the SemEval 2015 Task 14 <ref type="bibr" target="#b9">(Elhadad et al., 2015)</ref>. The descriptive statistics for the datasets are shown in  <ref type="bibr" target="#b38">(Tang et al., 2015)</ref>.</p><p>The clinical embeddings are trained on MIMIC III <ref type="bibr" target="#b15">(Johnson et al., 2016)</ref>, which consists of almost 2 million clinical notes. Notes that have an ERROR tag are first removed, ending up with 1,908,359 notes and 786,414,528 tokens and a vocabulary of size 712,286. For pre-training traditional word embeddings, words are lowercased, as is standard practice. For pre-training ELMo and BERT, casing is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Concept Extraction</head><p>Concept extraction is based on the model proposed in <ref type="bibr" target="#b18">Lample et al., (2016)</ref>, a Bi-LSTM CRF architecture. For traditional embedding methods and ELMo embeddings, we use the same hyperparameters setting: hidden unit dimension at 512, dropout probability at 0.5, learning rate at 0.001, learning rate decay at 0.9, and Adam as the optimization algorithm. Early stopping of training is set to 5 epochs without improvement to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Pre-training of Clinical Embeddings</head><p>Across embedding methods, two different scenarios of pre-training are investigated and compared:</p><p>1. Off-the-shelf embeddings from the official release, referred to as the General model.</p><p>2. Pre-trained embeddings on MIMIC-III, referred to as the MIMIC model..</p><p>In the first scenario, more details related to the embedding models are shown in in Table 2. We also apply BioBERT <ref type="bibr" target="#b20">(Lee et al., 2019)</ref>, which is the most recent pre-trained model on biomedical literature initiated from BERT BASE .  In the second scenario, for all the traditional embedding methods, we pre-train 300 dimension embeddings from MIMIC-III clinical notes. We apply the following hyperparameter settings for all three traditional embedding methods including word2vec, GloVe, and fastText: window size of 15, minimum word count of 5, 15 iterations, and embedding size of 300 to match the off-the-shelf embeddings.</p><p>For ELMo, the hyperparameter setting for pretraining follows the default in Peters et al., <ref type="bibr">(2018)</ref>. Specifically, a char-CNN embedding layer is applied with 16-dimension character embeddings, filter widths of <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">7]</ref> with respective <ref type="bibr">[32,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024]</ref> number of filters. After that, a two-layer Bi-LSTM with 4,096 hidden units in each layer is added. The output of the final bi-LM language model is projected to 512 dimensions with a highway connection. MIMIC-III was split into a training corpus (80%) for pretraining and a held-out testing corpus (20%) for evaluating perplexity. The pre-training step is performed on the training corpus for 15 epochs. The average perplexity on the testing corpus is 9.929.</p><p>For BERT, two clinical-domain models initialized from BERT BASE and BERT LARGE are pretrained. Unless specified, we follow the authors detailed instructions to set up the pre-training parameters, as other options were tested and it has been concluded that this is a useful recipe when pre-training from their released model (e.g., poor model convergence). The vocabulary list consisting of 28,996 word-pieced tokens applied in BERT BASE and BERT LARGE is adopted. According to their paper, the performance on the downstream tasks decrease as the training steps in-crease, thus we decide to save the intermediate checkpoint (every 20,000 steps) and report the performance of intermediate models on the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fine-tuing BERT</head><p>Fine-tuning the BERT clinical models on the downstream task requires some adjustments. First, instead of randomly initializing the Bi-LSTM output weights, Xavier initialization is utilized (Glorot and Bengio, 2010), without which the BERT fine-tuning failed to converge (this was not necessary for ELMo). Second, early stopping of finetuning is set to 800 steps without improvement to prevent overfitting. Finally, post-processing steps are conducted to align the BERT output with the concept gold standard, including handling truncated sentences and word-pieced tokenization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Evaluation and Computational Costs</head><p>10% of the official training set is used as a development set and the official test set is used to report performance. The specific performance metrics are precision, recall, and F1-measure for exact matching. The pre-training BERT experiments are implemented in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> on a NVIDIA Tesla V100 GPU (32G), other experiments are performed on a NVIDIA Quadro M5000 (8G). The time for pre-training ELMo, BERT BASE and BERT LARGE for every 20K checkpoint is 4.83 hours, 3.25 hours, and 5.16 hours, respectively. These models were run until manually set to stop at 320K iterations (82.66 hours ∼ roughly 3.4 days), 700K iterations (113.75 hours ∼ roughly 4.7 days), and 700K iterations (180.83 hours ∼ roughly 7.5 days), respectively.  <ref type="bibr" target="#b48">(Zhu et al., 2018)</ref> } <ref type="bibr" target="#b21">(Liu et al., 2017)</ref> 80.3 <ref type="bibr" target="#b38">(Tang et al., 2015)</ref> 81.3 <ref type="bibr" target="#b47">(Zhang et al., 2014)</ref> } The SOTA on the i2b2 2012 task is only reported in partial-matching F1. That result, 92.29 <ref type="bibr" target="#b21">(Liu et al., 2017)</ref>, is below the equivalent we achieve on partial-matching F1 with BERTLARGE(MIMIC), 93.18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Comparison</head><p>The performance on the respective test sets for the embedding methods on the four clinical concept extraction tasks are reported in <ref type="table" target="#tab_5">Table 3</ref>. The performance is evaluated in exact matching F1. In general, embeddings pre-trained on the clinical corpus performed better than the same method pre-trained on an open domain corpus. For i2b2 2010, the best performance is achieved by BERT LARGE (MIMIC) with an F1 of 90.25. It improves the performance by 5.18 over the best performance of the traditional embeddings achieved by GloVe (MIMIC) with an F1 of 85.07. As expected, both ELMo and BERT clinical embeddings outperform the off-the-shelf embeddings with relative increase up to 10%.</p><p>The best performance on the i2b2 2012 task is achieved by BERT LARGE (MIMIC) with an F1 of 80.91 across all the alternative methods. It increases F1 by 5.64 over GloVe(MIMIC), which obtains the best score (75.27) among the traditional embedding methods. As expected, ELMo and BERT with pre-trained clinical embeddings exceed the off-the-shelf open domain models. The most efficient model for SemEval 2014 task achieved an exact matching with F1 of 80.74 by BERT LARGE (MIMIC). Notably, traditional embedding models pre-trained on clinical corpus such as GloVe(MIMIC) obtained a higher performance than contextual embedding model trained on open domain, namely ELMo(General). For the SemEval 2015 task, as the experiments are performed only in concept extraction, the models are evaluated using the official eval-uation script from the SemEval 2014 task. Note the training set (298 notes) for the SemEval 2015 task is the training (199 notes) and test set (99 notes) combined for the SemEval 2014 task. The best performance on the 2015 task is achieved by BERT LARGE (MIMIC) with an F1 of 81.65.</p><p>The detailed performance for each entity category including PROBLEM, TEST and TREATMENT on the 2010 task is shown in Table 4. Both ELMo and BERT show improvements to all three categories, with ELMo outperforming the traditional embeddings on all three, and BERT outperforming ELMo on all three. One notable aspect with BERT is that TREATMENTs see a larger jump: TREATMENT is the lowestperforming category for ELMo and the traditional embeddings despite there being slightly more TREATMENTs than TESTs in the data, but for BERT the TREATMENTs category outperforms TESTs.   <ref type="table">Table 5</ref> shows the results for each event type on the 2012 task with embeddings pre-trained from MIMIC-III. Generally, the biggest improvement by the contextual embeddings over the traditional embeddings is achieved on the PROBLEM type (BERT LARGE : 86.1, GloVe: 77.83). This is reasonable because in clinical notes, diseases and conditions normally appear in certain types of sur-rounding context with similar grammar structures. Thus, it is necessarily important to take advantage of contextual representations to capture the surrounding context for that particular concept. Interestingly, ELMo outperforms both BERT models for CLINICAL <ref type="bibr">DEPARTMENT</ref>   <ref type="table">Table 5</ref>: Performance of each label category with pretrained MIMIC models on i2b2 2012 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-training Evaluation</head><p>The efficiency of pre-trained ELMo and BERT models are investigated by reporting the loss during pre-training steps and by evaluating the intermediate checkpoints on downstream tasks. It is observed for both ELMo and BERT at their pre-training stages, the train perplexity or loss decreases as the steps increase, indicating that the language model is actually adapting to the clinical corpus. If there is no intervention to stop the pretraining process, it will lead to a very small loss value. However, this will ultimately cause overfitting on the pre-training corpus (shown in Supplemental <ref type="figure" target="#fig_1">Figure 1</ref>). However, this will bring to another common issue that the model might be overfitting on the training set. Using i2b2 2010 as the downstream task, the final performance at each intermediate checkpoint of the pre-trained model is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. For ELMo, as the pre-training proceeds, the performance of the downstream task remains stable after a certain number of iterations (the maximum F1 reaches 87.80 at step 280K). For BERT BASE , the performance on the downstream task is less steady and tends to decrease after achieving its optimal model, with the maximum F1 89.55 at step 340K. We theorize that this is due to initializing the MIMIC model with the open-domain BERT model: over many iterations on the MIMIC data, the information learned from the large open corpus (3.3 billion words) is lost and would eventually converge on a model similar to one initialized from scratch. Thus, limiting pre-training on a clinical corpus to a certain number of iterations provides a useful trade-off, balancing the benefits of a large open-domain corpus while still learning much from a clinical corpus. We hope this is a practical piece of guidance for the clinical NLP community when they intend to generate their own pre-trained model from a clinical corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This study explores the effects of numerous embedding methods on four clinical concept extraction tasks. Unsurprisingly, domain-specific embedding models outperform open-domain embedding models. All types of embeddings enable consistent gains in concept extraction tasks when pretrained on a clinical domain corpus. Further, the contextual embeddings outperform traditional embeddings in performance. Specifically, large improvements can be achieved by pre-training a deep language model from a large corpus, followed by a task-specific fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">State-of-the-art Comparison</head><p>Among the four clinical concept extraction corpora, the i2b2 2012 task reports the partial matching F1 as the organizers reported in <ref type="bibr" target="#b35">Sun et al., (2013)</ref> and the other three tasks report the exact matching F1. Currently, the state-of-theart models in i2b2 2010, i2b2 2012, SemEval 2014 task 7, and Semeval 2015 Task 14 are reported with F1 of 88.60 <ref type="bibr" target="#b48">(Zhu et al., 2018</ref><ref type="bibr">), 92.29 (Liu et al., 2017</ref><ref type="bibr">), 80.3 (Tang et al., 2015</ref> and 81.3 <ref type="bibr" target="#b47">(Zhang et al., 2014)</ref>, respectively. With the most advanced language model representation method pretrained on a large clinical corpus, namely BERT LARGE (MIMIC), we achieved new state-of-the-art performances across all tasks. BERT LARGE (MIMIC) outperform the state-ofthe-art models on all four tasks with respective F-measures of 90.25, 93.18 (partial F1), 80.74, and 81.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Semantic Information from Contextual Embeddings</head><p>Here, we explore the semantic information captured by the contextual representation and infer that the contextual embedding can encode information that a single word vector fails to. First, we select 30 sentences from both web texts and clinical notes in which the word cold appears (The actual sentences can be found in <ref type="table" target="#tab_1">Supplemental Table  1</ref>). The embedding vectors of cold in 30 sentences from four embedding models, ELMo(General), ELMo(MIMIC), BERT LARGE (General), and BERT LARGE (MIMIC), were derived. This results in 120 vectors for the same word across four embeddings. For each embedding method, Principal Component Analysis (PCA) is performed to reduce the dimensionality to 2. The PCA visualizations are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. As expected, the vectors of cold generated by ELMo(General) are mixed within two different meaning labels.</p><p>The vectors generated by BERT LARGE (General) and BERT LARGE (MIMIC) are more clearly clustered into two groups. ELMo(General) is unable to discriminate the different meanings of the word cold, specifically between temperature and symptom. The visualization result is also consistent with the performance on the concept extract tasks where ELMo(General) tends to get a poorer performance compared with the other three models.</p><p>Second, traditional word embeddings are commonly evaluated using lexical similarity tasks, such as those by <ref type="bibr" target="#b23">(Pakhomov et al., 2010</ref><ref type="bibr" target="#b24">(Pakhomov et al., , 2016</ref>, which compare two words outside any sentencelevel context. While not entirely appropriate for comparing contextual embeddings such as ELMo and BERT because the centroid of the embedding clusters are not necessarily meaningful, such lexical similarity tasks do provide motivation for investigating the clustering effects of lexically similar (and dissimilar) words. In Supplemental <ref type="figure" target="#fig_2">Figure  2</ref>, we compare four words from <ref type="bibr" target="#b24">(Pakhomov et al., 2016)</ref>: tylenol, motrin, pain, and herpes based on 50-sentence samples from MIMIC-III and the same 2-dimension PCA visualization technique. One would expect tylenol (acetaminophen) and motrin (ibuprofen) to be similar, and in fact the clusters overlap almost completely. Meanwhile, pain is a nearby cluster, while herpes is quite distant. So while contextual embeddings are not wellsuited to context-free lexical similarity tasks, the aggregate effects (clusters) still demonstrate similar spatial relationships as traditional word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Lexical Segmentation in BERT</head><p>One important notable difference between BERT and both ELMo and the traditional word embeddings is that BERT breaks words down into subword tokens, referred to as wordpieces <ref type="bibr" target="#b30">(Schuster and Nakajima, 2012)</ref>. This is accomplished via statistical analysis on a large corpus, as opposed to using a morphological lexicon. The concern for clinical NLP, then, is if a different word piece tokenization method is appropriate for clinical text as opposed to general text (i.e., books and Wikipedia for the pre-trained BERT models). Supplemental <ref type="table" target="#tab_4">Table 2</ref> shows the word piece tokenization for the medical words from the lexical similarity corpus developed by <ref type="bibr" target="#b24">(Pakhomov et al., 2016)</ref>. The results do not exactly conform to traditional medical term morphology (e.g., "appendicitis" is broken into "app", "-end", "-icit", "-is", as opposed to having the suffix "-itis"). Note that this isn't necessary a bad segmentation: it is possible this would outperform a word piece tokenization based on the SPECIALIST lexicon <ref type="bibr" target="#b4">(Browne et al., 2000)</ref>. What is not in dispute, however,is that further experimentation is required, such as determining word pieces from MIMIC-III. Note this is not as simple as it at first seems. The primary issue is that the BERT models we use in this paper were first pre-trained on a 3.3 billion word open-domain corpus, then fine-tuned on MIMIC-III. Performing word piece tokenization on MIMIC-III would at a minimum require repeating the pre-training process on the open-domain corpus (with the clinical word pieces) in order to get comparable embedding models. Given the range of experimentation necessary to determine the best word piece strategy, we leave this experimentation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present an analysis of different word embedding methods and investigate their effectiveness on four clinical concept extraction tasks. We compare between traditional word representation methods as well as the advanced contextual representation methods. We also com- pare pre-trained contextual embeddings using a large clinical corpus against the performance of off-the-shelf pre-trained models on open domain data. Primarily, the efficacy of contextual embeddings over traditional word vector representations are highlighted by comparing the performances on clinical concept extraction. Contextual embeddings also provide interesting semantic information that is not accounted for in traditional word representations. Further, our results highlight the benefits of embeddings through unsupervised pretraining on clinical text corpora, which achieve higher performance than off-the-shelf embedding models and result in new state-of-the-art performance across all tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>5. A detailed analysis of the effect of pretraining time when starting from pre-built open domain models, which is important due to the long pre-training time of methods such as ELMo and BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Fictional embedding vector points and clusters of "cold".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performances on the i2b2 2010 task governed by the steps of pre-training epochs on ELMo (MIMIC) and BERT BASE (MIMIC) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Principal Component Analysis (PCA) visualizations using embedding vectors of cold from embedding models (purple: cold as temperature meaning; red: cold as symptom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Descriptive statistics for concept extraction datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The 2010 i2b2/VA challenge data contains a total of 349 training and 477 testing reports with clinical concept types: PROBLEM, TEST and TREATMENT. The 2012 i2b2 challenge data contains 190 training and 120 testing discharge summaries, with 6 clinical concept types: PROBLEM, TEST, TREAT-MENT, CLINICAL DEPARTMENT, EVIDENTIAL, and OCCURRENCE. The SemEval 2014 Task 7 data contains 199 training and 99 testing reports with the concept type: DISEASE DISORDER. The SemEval 2015 Task 14 data consists of 298 training and 133 testing reports with the concept type: DISEASE DISORDER. For the two SemEval tasks, the disjoint concepts are handled with "BIOHD" tagging schema</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Resources of off-the-shelf embeddings from open domain. (</figDesc><table /><note>* Vocabulary size calculated after word-piece tokenization)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test set comparison in exact F1 of embedding methods across tasks. SOTA: state-of-the-art.</figDesc><table><row><cell>Method</cell><cell cols="2">i2b2 2010</cell><cell cols="2">i2b2 2012</cell><cell cols="2">Semeval 2014 Task 7</cell><cell cols="2">Semeval 2015 Task 14</cell></row><row><cell></cell><cell>General</cell><cell>MIMIC</cell><cell cols="3">General MIMIC General</cell><cell>MIMIC</cell><cell>General</cell><cell>MIMIC</cell></row><row><cell>word2vec</cell><cell>80.38</cell><cell>84.32</cell><cell>71.07</cell><cell>75.09</cell><cell>72.2</cell><cell>77.48</cell><cell>73.09</cell><cell>76.42</cell></row><row><cell>GloVe</cell><cell>84.08</cell><cell>85.07</cell><cell>74.95</cell><cell>75.27</cell><cell>70.22</cell><cell>77.73</cell><cell>72.13</cell><cell>76.68</cell></row><row><cell>fastText</cell><cell>83.46</cell><cell>84.19</cell><cell>73.24</cell><cell>74.83</cell><cell>69.87</cell><cell>76.47</cell><cell>72.67</cell><cell>77.85</cell></row><row><cell>ELMo</cell><cell>83.83</cell><cell>87.8</cell><cell>76.61</cell><cell>80.5</cell><cell>72.27</cell><cell>78.58</cell><cell>75.15</cell><cell>80.46</cell></row><row><cell>BERTbase</cell><cell>84.33</cell><cell>89.55</cell><cell>76.62</cell><cell>80.34</cell><cell>76.76</cell><cell>80.07</cell><cell>77.57</cell><cell>80.67</cell></row><row><cell>BERTlarge</cell><cell>85.48</cell><cell>90.25</cell><cell>78.14</cell><cell>80.91</cell><cell>78.75</cell><cell>80.74</cell><cell>77.97</cell><cell>81.65</cell></row><row><cell>BioBERT</cell><cell>84.76</cell><cell>-</cell><cell>77.77</cell><cell>-</cell><cell>77.91</cell><cell>-</cell><cell>79.97</cell><cell>-</cell></row><row><cell cols="2">Prior SOTA 88.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of each label category with pre-trained MIMIC models on i2b2 2010 task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>and OCCURRENCE.</figDesc><table><row><cell></cell><cell cols="5">word2vec GloVe FastText ELMo BERT BASE BERT LARGE</cell></row><row><cell>PROBLEM</cell><cell>76.49</cell><cell>77.83 75.35</cell><cell>84.1</cell><cell>85.91</cell><cell>86.1</cell></row><row><cell>TEST</cell><cell>78.12</cell><cell cols="2">81.26 76.94 84.76</cell><cell>86.88</cell><cell>86.56</cell></row><row><cell>TREATMENT</cell><cell>76.22</cell><cell>78.52 76.88</cell><cell>83.9</cell><cell>84.27</cell><cell>85.09</cell></row><row><cell cols="2">CLINICAL DEPT 78.18</cell><cell cols="2">77.92 77.27 83.71</cell><cell>77.92</cell><cell>78.23</cell></row><row><cell>EVIDENTIAL</cell><cell>73.14</cell><cell cols="2">74.26 72.94 72.95</cell><cell>74.21</cell><cell>74.96</cell></row><row><cell>OCCURRENCE</cell><cell>64.77</cell><cell cols="2">64.19 61.02 66.27</cell><cell>62.36</cell><cell>65.65</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. National Institutes of Health (NIH) and the Cancer Prevention and Research Institute of Texas (CPRIT). Specifically, NIH support comes from the National Library of Medicine (NLM) un-der award R00LM012104 and R01LM010681, as well as the National Cancer Institute under award U24CA194215. CPRIT support for computational resources was provided under awards RP170668 and RR180012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hung</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 12: Clinical tempeval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Te</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The specialist lexicon. National Library of Medicine Technical Reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename><forename type="middle">T</forename><surname>Allen C Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Mccray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="18" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf for clinical concept extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehsan Zare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Borzeshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piccardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08373</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application of word embeddings in biomedical named entity recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fx Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Relly</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Information Management</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machinelearned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Berry De Bruijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="557" to="562" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 14: Analysis of clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Named entity recognition using neural networks for clinical notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edson</forename><surname>Florez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Riveill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Pighetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Medication and Adverse Drug Event Detection</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic information extraction from unstructured mammography reports using distributed semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupama</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imon</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="78" to="86" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning with word embeddings improves biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Luis</forename><surname>Wiegandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
		</imprint>
	</monogr>
	<note>Scientific data</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of the share/clef ehealth evaluation lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gondy</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">L</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clinical named entity recognition: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Srinivasa Rao Kundeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Vijayananda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mujjiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1937" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stanford&apos;s multi-pass sieve coreference resolution system at the conll-2011 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth conference on computational natural language learning: Shared task</title>
		<meeting>the fifteenth conference on computational natural language learning: Shared task</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08746</idno>
		<title level="m">BioBERT: pre-trained biomedical language representation model for biomedical text mining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Entity recognition from clinical texts via recurrent neural network. BMC medical informatics and decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic similarity and relatedness between clinical terms: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serguei</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bridget</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Melton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page">572</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Corpus domain effects on distributional semantic modeling of medical terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Serguei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reed</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Mcewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3635" to="3644" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 7: Analysis of clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic extraction of relations between medical concepts in clinical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="594" to="600" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Assessing the corpus size vs. similarity trade-off for word embeddings in clinical nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Clinical Natural Language Processing Workshop (ClinicalNLP)</title>
		<meeting>the Clinical Natural Language Processing Workshop (ClinicalNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word embedding based correlation model for question/answer matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3511" to="3517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Frame-Based NLP System for Cancer-Related Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">1524</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><surname>Stubbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kotfila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uzuner</forename><surname>Andözlem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating temporal relations in clinical text: 2012 i2b2 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="806" to="813" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Overview of the share/clef ehealth evaluation lab 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanna</forename><surname>Salanterä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">L</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="212" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing clinical entities in hospital discharge summaries using structural support vector machines with word representation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognizing disjoint clinical concepts in clinical text using machine learning-based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1184</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with specialized word embeddings for healthdomain named-entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><forename type="middle">Zare</forename><surname>Iñigo Jauregi Unanue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Borzeshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">i2b2/va challenge on concepts, assertions, and relations in clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuying</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="552" to="556" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using clinical natural language processing for health outcomes research: Overview and actionable suggestions for future advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Osborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Downs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comparison of word embeddings for the biomedical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Rastegar-Mojarad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="12" to="20" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Clinical information extraction applications: a literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Rastegar-Mojarad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrim</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="34" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A study of neural word embeddings for named entity recognition in clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Extracting and integrating data from entire electronic health records for detecting colorectal cancer cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anushi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neeraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxia</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramani</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">1564</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">UTH CCB: a report for semeval 2014-task 7 analysis of clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Se-mEval</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="802" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ioannis Ch Paschalidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tahmasebi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10566</idno>
		<title level="m">Clinical concept extraction with contextual word embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
