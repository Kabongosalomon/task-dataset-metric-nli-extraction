<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 RANDOM FEATURE ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<email>hapeng@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
							<email>npappas@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<email>dyogatama@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
							<email>roys@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♦</forename><surname>Lingpeng Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">DeepMind ♦ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit3">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 RANDOM FEATURE ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints. * The majority of this work was done while these authors were at DeepMind.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformer architectures <ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref> have achieved tremendous success on a variety of sequence modeling tasks <ref type="bibr" target="#b54">Radford et al., 2018;</ref><ref type="bibr" target="#b48">Parmar et al., 2018;</ref><ref type="bibr" target="#b21">Devlin et al., 2019;</ref><ref type="bibr">Parisotto et al., 2020, inter alia)</ref>. Under the hood, the key component is attention <ref type="bibr" target="#b6">(Bahdanau et al., 2015)</ref>, which models pairwise interactions of the inputs, regardless of their distances from each other. This comes with quadratic time and memory costs, making the transformers computationally expensive, especially for long sequences. A large body of research has been devoted to improving their time and memory efficiency <ref type="bibr" target="#b68">(Tay et al., 2020c)</ref>. Although better asymptotic complexity and prominent gains for long sequences have been achieved <ref type="bibr" target="#b14">Child et al., 2019;</ref><ref type="bibr">Beltagy et al., 2020, inter alia)</ref>, in practice, many existing approaches are less well-suited for moderatelength ones: the additional computation steps required by some approaches can overshadow the time and memory they save <ref type="bibr" target="#b34">(Kitaev et al., 2020;</ref><ref type="bibr">Roy et al., 2020, inter alia)</ref>.</p><p>This work proposes random feature attention (RFA), an efficient attention variant that scales linearly in sequence length in terms of time and space, and achieves practical gains for both long and moderate length sequences. RFA builds on a kernel perspective of softmax <ref type="bibr" target="#b57">(Rawat et al., 2019)</ref>. Using the well-established random feature maps <ref type="bibr" target="#b56">(Rahimi &amp; Recht, 2007;</ref><ref type="bibr" target="#b2">Avron et al., 2016;</ref><ref type="bibr">§2)</ref>, RFA approximates the dot-then-exponentiate function with a kernel trick <ref type="bibr" target="#b28">(Hofmann et al., 2008)</ref>: exp(x · y) ≈ φ(x) · φ(y). Inspired by its connections to gated recurrent neural networks <ref type="bibr" target="#b27">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b15">Cho et al., 2014)</ref> and fast weights <ref type="bibr" target="#b60">(Schmidhuber, 1992)</ref>, we further augment RFA with an optional gating mechanism, offering a straightforward way of learning with recency bias when locality is desired.</p><p>RFA and its gated variant ( §3) can be used as a drop-in substitute for the canonical softmax attention, and increase the number of parameters by less than 0.1%. We explore its applications in transformers on language modeling, machine translation, and long text classification ( §4). Our experiments show that RFA achieves comparable performance to vanilla transformer baselines in all tasks, while outperforming a recent related approach <ref type="bibr" target="#b31">(Katharopoulos et al., 2020)</ref>. The gating mechanism proves particularly useful in language modeling: the gated variant of RFA outperforms the transformer baseline on WikiText-103. RFA shines in decoding, even for shorter sequences. In our head-to-head comparison on machine translation benchmarks, RFA decodes around 2× faster than a transformer baseline, without accuracy loss. Comparisons to several recent efficient transformer variants on three long text classification datasets show that RFA is competitive in terms of both accuracy and efficiency. Our analysis ( §5) shows that more significant time and memory efficiency improvements can be achieved for longer sequences: 12× decoding speedup with less than 10% of the memory for 2,048-length outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 ATTENTION IN SEQUENCE MODELING</head><p>The attention mechanism <ref type="bibr" target="#b6">(Bahdanau et al., 2015)</ref> has been widely used in many sequence modeling tasks. Its dot-product variant is the key building block for the state-of-the-art transformer architectures <ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref>. Let {q t } N t=1 denote a sequence of N query vectors, that attend to sequences of M key and value vectors. At each timestep, the attention linearly combines the values weighted by the outputs of a softmax:</p><formula xml:id="formula_0">attn (q t , {k i }, {v i }) = i exp (q t · k i /τ ) j exp (q t · k j /τ ) v i .</formula><p>(1)</p><p>τ is the temperature hyperparameter determining how "flat" the softmax is <ref type="bibr" target="#b25">(Hinton et al., 2015)</ref>. <ref type="bibr">1</ref> Calculating attention for a single query takes O(M ) time and space. For the full sequence of N queries the space amounts to O(M N ). When the computation cannot be parallelized across the queries, e.g., in autoregressive decoding, the time complexity is quadratic in the sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RANDOM FEATURE METHODS</head><p>The theoretical backbone of this work is the unbiased estimation of the Gaussian kernel by <ref type="bibr" target="#b56">Rahimi &amp; Recht (2007)</ref>. Based on Bochner's theorem <ref type="bibr" target="#b8">(Bochner, 1955)</ref>, <ref type="bibr" target="#b56">Rahimi &amp; Recht (2007)</ref> proposed random Fourier features to approximate a desired shift-invariant kernel. The method nonlinearly transforms a pair of vectors x and y using a random feature map φ; the inner product between φ(x) and φ(y) approximates the kernel evaluation on x and y. More precisely: Theorem 1 <ref type="bibr" target="#b56">(Rahimi &amp; Recht, 2007)</ref>. Let φ : R d → R 2D be a nonlinear transformation:</p><p>φ (x) = 1/D sin (w 1 · x) , . . . , sin (w D · x) , cos (w 1 · x) , . . . , cos (w D · x) .</p><p>When d-dimensional random vectors w i are independently sampled from N (0, σ 2 I d ),</p><formula xml:id="formula_2">E wi [φ (x) · φ (y)] = exp − x − y 2 /2σ 2 .<label>(3)</label></formula><p>Variance of the estimation is inversely proportional to D (Appendix <ref type="bibr">A.2;</ref><ref type="bibr" target="#b77">Yu et al., 2016)</ref>.</p><p>Random feature methods proved successful in speeding up kernel methods <ref type="bibr" target="#b45">(Oliva et al., 2015;</ref><ref type="bibr" target="#b3">Avron et al., 2017;</ref><ref type="bibr">Sun, 2019, inter alia)</ref>, and more recently are used to efficiently approximate softmax <ref type="bibr" target="#b57">(Rawat et al., 2019)</ref>. In §3.1, we use it to derive an unbiased estimate to exp( · , · ) and then an efficient approximation to softmax attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>This section presents RFA ( §3.1) and its gated variant ( §3.2). In §3.3 we lay out several design choices and relate RFA to prior works. We close by practically analyzing RFA's complexity ( §3.4).  3 Z k g j 8 i 1 / b S X 9 N j k G v + T S l Z 7 v v V K g E B w + f 7 S I 5 m 3 s P Y s e 8 l u l r 9 O X f n 7 r 3 7 D x 4 + K p V / e f z r k 6 f P n r 9 o J 1 G q H N F y I j 9 S X Z s n w v d C 0 d K e 9 k U 3 V o I H t i 8 6 9 n B l X O 9 c C J V 4 U X i i R 7 H o B V y G n u s 5 X B f U s e T B m 7 3 9 3 z 8 9 m 6 8 u V C e X O T t g 0 8 G 8 M b 2 a n 5 6 X 5 q x + 5 K S B C L X j 8 y Q 5 Z 9 V Y 9 z K u t O f 4 I i 9 b a S J i 7 g y 5         </p><formula xml:id="formula_3">v i m J W i C M J Z / A w A z t h h o I Z S c N J b X q P M i k R d a E u V E A F d A g d Q s k s J d S D e l A F V d A R d A T 1 o T 7 0 C n o F v Y Z e Q w N o Q N Y P G k I j a E T 2 A B p D E 2 g C / Q D 9 A N V Q D U 2 h K d l A q I G S 7 S a b f Q O 9 g d 5 C b 6 E f o R + h Y + g Y</formula><formula xml:id="formula_4">F O f F M O S B S H r Z Z L 6 5 + V s h f d O N V P E L t T l R + k T G g y Q Z B X b R G X A 9 S G 7 X x v h / t f N U u / V e 5 o V x q k X o X H / I T X 1 T R + b 4 z 5 t 9 T w l H + 6 N i w B 3 l F X M 1 n Q F X 3 N H F E p W t v n C L Z Z x M J w t G t p + K P D v a a O R Z b b l S r 1 d Y r Z r f b l K i P + 1 h d V Z Z f l + p L 8 / 0 R I q H 8 u Z V i 9 V a x W S 1 t x V z a W m m M 0 5 V 7 N 9 0 s r d F Z + 1 d 0 b 0 0 + 0 6 p h A h v Z l d M j b G K u f g + L 5 c n j d b F l V B R l l n j J b L d r J r n + b Q Q h Q L O 4 E E K</formula><formula xml:id="formula_5">i M b h B E f u U 4 Y + Z E b h B E g u U k Y C Z J b h B E h u U 0 Y G Z I 7 h B E i u U s Y K Z J 7 h B E j u U 8 Y O Z I H h B E k e U g Y S Z J H h B E l e U w Y W Z J N w g i T P C G M N M l T w o i T P C O M P M l z w g i U b B F G o m S b M C I l O 4 S R K d k l j F D J C 8 J I l b w k P I 2 V F Y p b J w o C H g 4 y K 4 x U k F + x X m b 5 w t W W 3 x Z K L z F L e X K o</formula><formula xml:id="formula_6">x / d Y s Y 7 p E F R o p n W u q s G 4 = " &gt; A A A K L n i c b d b L b t t G F A Z g O k 3 b S O n F a b L L h q h R I A U E Q 3 R k R 9 k l 8 v 0 u X 3 S x T c E Y D o c j x r x l O L Q j E 3 y X b t t N n q a L L o p u + x i l Z L n / q V U C g k b f O a Q O Z v 4 F n S T w U 1 2 v / z H 3 6 I v H X 3 7 1 9 Z N K 9 e k 3 3 3 7 3 / f y z H 7 p p n C k u O j w O Y t V 3 W C o C P x I d 7 e t A 9 B M l W O g E o u d c r Y 7 r v W u h U j + O T v U o E Y O Q</formula><formula xml:id="formula_7">C + v F 0 U x L c S R g F v w M A P b Y Y a C H g r N S G 1 y j z I p E X W g D p R D O d S F u l A y S w H 1 o B 5 U Q i V 0 C B 1 C f a g P f Q d 9 B 7 2 C X k E D a E D W D x p C I 2 h E 9 g A a Q x N o A n 0 P f Q 9 V U A V N o S n Z Q K i G k u 0 m m 3 0 N v Y b e Q G + g H 6 A f o C P o C H o L v R 3 r H Y d v w W / / a w 5 b 0 B Z 0 F b o K X Y O u Q d e h 6 9 A N 6 A Z 0 E 7 o J 3 Y J u Q b e h 2 9 A d 6 A 5 0 F 7 o L 3 Y P u Q f e h + 9 A D 6 A H 0 E H o I b U P b 0 C P o E f Q Y e g w 9 g Z 5 A T 6 G n 0 A 6 0 A + 1 C u 9 A e t A f t Q / v Q M + g Z 9 B x 6 j v D I + / B w F p D 0 y B Z h x E e u E k Z + 5 B p h B E i u E 0 a C 5 A Z h R E h u E k a G 5 B Z h h E h u E 0 a K 5 A 5 h x E j u E k a O 5 B 5 h B E n u E 0 a S 5 A F h R E k e E k</formula><formula xml:id="formula_8">C + v F 0 U x L c S R g F v w M A P b Y Y a C H g r N S G 1 y j z I p E X W g D p R D O d S F u l A y S w H 1 o B 5 U Q i V 0 C B 1 C f a g P f Q d 9 B 7 2 C X k E D a E D W D x p C I 2 h E 9 g A a Q x N o A n 0 P f Q 9 V U A V N o S n Z Q K i G k u 0 m m 3 0 N v Y b e Q G + g H 6 A f o C P o C H o L v R 3 r H Y d v w W / / a w 5 b 0 B Z 0 F b o K X Y O u Q d e h 6 9 A N 6 A Z 0 E 7 o J 3 Y J u Q b e h 2 9 A d 6 A 5 0 F 7 o L 3 Y P u Q f e h + 9 A D 6 A H 0 E H o I b U P b 0 C P o E f Q Y e g w 9 g Z 5 A T 6 G n 0 A 6 0 A + 1 C u 9 A e t A f t Q / v Q M + g Z 9 B x 6 j v D I + / B w F p D 0 y B Z h x E e u E k Z + 5 B p h B E i u E 0 a C 5 A Z h R E h u E k a G 5 B Z h h E h u E 0 a K 5 A 5 h x E j u E k a O 5 B 5 h B E n u E 0 a S 5 A F h R E k e E k</formula><formula xml:id="formula_9">v i m J W i C M J Z / A w A z t h h o I Z S c N J b X q P M i k R d a E u V E A F d A g d Q s k s J d S D e l A F V d A R d A T 1 o T 7 0 C n o F v Y Z e Q w N o Q N Y P G k I j a E T 2 A B p D E 2 g C / Q D 9 A N V Q D U 2 h K d l A q I G S 7 S a b f Q O 9 g d 5 C b 6 E f o R + h Y + g Y e g e 9 m + g 9 h + / A 7 / 5 t D l v Q F n Q N u g Z d h 6 5 D N 6 A b 0 E 3 o J n Q L u g X d h m 5 D d 6 A 7 0 F 3 o L n Q P u g f d h + 5 D D 6 A H 0 E P o I f Q I e g R t Q 9 v Q Y + g x 9 A R 6 A j 2 F n k L P o G f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 B z 6 D n 0 A n q B 8 K i H 8 A g e k P S o F m H E R 6 0 R R n 7 U O m E E S G 0 Q R o L U J m F E S G 0 R R o b U N m G E S O 0 Q R o r U L m H E S O 0 R R o 7 U P m E E S R 0 Q R p L U I W F E S R 0 R R p Z U m z D C p I 4 J I 0 3 q h D D i p E 4 J I 0 / q j D A C p T q E k S j V J Y x I q R 5 h Z E r 1 C S N U 6 p w w U q U u C E</formula><formula xml:id="formula_10">v i m J W i C M J Z / A w A z t h h o I Z S c N J b X q P M i k R d a E u V E A F d A g d Q s k s J d S D e l A F V d A R d A T 1 o T 7 0 C n o F v Y Z e Q w N o Q N Y P G k I j a E T 2 A B p D E 2 g C / Q D 9 A N V Q D U 2 h K d l A q I G S 7 S a b f Q O 9 g d 5 C b 6 E f o R + h Y + g Y e g e 9 m + g 9 h + / A 7 / 5 t D l v Q F n Q N u g Z d h 6 5 D N 6 A b 0 E 3 o J n Q L u g X d h m 5 D d 6 A 7 0 F 3 o L n Q P u g f d h + 5 D D 6 A H 0 E P o I f Q I e g R t Q 9 v Q Y + g x 9 A R 6 A j 2 F n k L P o G f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 B z 6 D n 0 A n q B 8 K i H 8 A g e k P S o F m H E R 6 0 R R n 7 U O m E E S G 0 Q R o L U J m F E S G 0 R R o b U N m G E S O 0 Q R o r U L m H E S O 0 R R o 7 U P m E E S R 0 Q R p L U I W F E S R 0 R R p Z U m z D C p I 4 J I 0 3 q h D D i p E 4 J I 0 / q j D A C p T q E k S j V J Y x I q R 5 h Z E r 1 C S N U 6 p w w U q U u C E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RANDOM FEATURE ATTENTION</head><p>RFA builds on an unbiased estimate to exp( · , · ) from Theorem 1, which we begin with:</p><formula xml:id="formula_11">exp x · y/σ 2 = exp x 2 /2σ 2 + y 2 /2σ 2 exp − x − y 2 /2σ 2 ≈ exp x 2 /2σ 2 + y 2 /2σ 2 φ (x) · φ (y) .<label>(4)</label></formula><p>The last line does not have any nonlinear interaction between φ(x) and φ(y), allowing for a linear time/space approximation to attention. For clarity we assume the query and keys are unit vectors. 2</p><formula xml:id="formula_12">attn (q t , {k i }, {v i }) = i exp q t · k i /σ 2 j exp (q t · k j /σ 2 ) v i ≈ i φ (q t ) φ (k i ) v i j φ (q t ) · φ (k j ) = φ (q t ) i φ (k i ) ⊗ v i φ (q t ) · j φ (k j ) = RFA (q t , {k i }, {v i }) .<label>(5)</label></formula><p>⊗ denotes the outer product between vectors, and σ 2 corresponds to the temperature term τ in Eq. 1.</p><p>RFA can be used as a drop-in-replacement for softmax-attention.</p><p>(a) The input is revealed in full to cross attention and encoder self-attention. Here RFA calculates attention using Eq. 5. (b) In causal attention RFA attends only to the prefix. 3 This allows for a recurrent computation. Tuple (S t ∈ R 2D×d , z t ∈ R 2D ) is used as the "hidden state" at time step t to keep track of the history, similar to those in RNNs. Then RFA(q t ,</p><formula xml:id="formula_13">{k i } i≤t , {v i } i≤t ) = φ(q t ) S t /(φ(q t ) · z t ), where S t = S t−1 + φ (k t ) ⊗ v t , z t = z t−1 + φ (k t ) .<label>(6)</label></formula><p>2D denotes the size of φ(·). Appendix A.1 summarizes the computation procedure of RFA, and <ref type="figure" target="#fig_9">Figure 1</ref> compares it against the softmax attention. Appendix A.3 derives causal RFA in detail.</p><p>Analogously to the softmax attention, RFA has its multiheaded variant <ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref>. In our experiments we use causal RFA in a transformer language model ( §4.1), and both cross and causal RFA in the decoder of a sequence-to-sequence machine translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RFA-GATE: LEARNING WITH RECENCY BIAS</head><p>The canonical softmax attention does not have any explicit modeling of distance or locality. In learning problems where such inductive bias is crucial <ref type="bibr" target="#b4">(Ba et al., 2016;</ref><ref type="bibr" target="#b48">Parmar et al., 2018;</ref><ref type="bibr" target="#b41">Miconi et al., 2018;</ref><ref type="bibr">Li et al., 2019, inter alia)</ref>, transformers heavily rely on positional encodings. Answering to this, many approaches have been proposed, e.g., learning the attention spans <ref type="bibr" target="#b64">(Sukhbaatar et al., 2019;</ref>, and enhancing the attention computation with recurrent <ref type="bibr" target="#b24">(Hao et al., 2019;</ref> or convolutional <ref type="bibr" target="#b74">(Wu et al., 2019;</ref><ref type="bibr" target="#b43">Mohamed et al., 2019)</ref> components.</p><p>RFA faces the same issue, but its causal attention variant (Eq. 6) offers a straightforward way of learning with recency bias. We draw inspiration from its connections to RNNs, and augment RFA with a learned gating mechanism <ref type="bibr" target="#b27">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b15">Cho et al., 2014;</ref><ref type="bibr">Peng et al., 2018, inter alia)</ref>:</p><formula xml:id="formula_14">g t = sigmoid(w g · x t + b g ), S t = g t S t−1 + (1 − g t ) φ (k t ) ⊗ v t , z t = g t z t−1 + (1 − g t ) φ (k t ) .<label>(7)</label></formula><p>w g and b g are learned parameters, and x t is the input representation at timestep t. 4 By multiplying the learned scalar gates 0 &lt; g t &lt; 1 against the hidden state (S t , z t ), history is exponentially decayed, favoring more recent context.</p><p>The gating mechanism shows another benefit of RFA: it would be otherwise more difficult to build similar techniques into the softmax attention, where there is no clear sense of "recurrence" (Appendix A.5). It proves useful in our language modeling experiments ( §4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DISCUSSION</head><p>On query and key norms, and learned random feature variance. Eq. 5 assumes both the query and keys are of norm-1. It therefore approximates a softmax attention that normalizes the queries and keys before multiplying them, and then scales the logits by dividing them by σ 2 . Empirically, this normalization step scales down the logits <ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref> and enforces that −1 ≤ q k ≤ 1.</p><p>In consequence, the softmax outputs would be "flattened" if not for σ, which can be set a priori as a hyperparameter <ref type="bibr" target="#b77">(Yu et al., 2016;</ref><ref type="bibr" target="#b3">Avron et al., 2017;</ref><ref type="bibr">Sun, 2019, inter alia)</ref>. Here we instead learn it from data with the reparameterization trick <ref type="bibr" target="#b33">(Kingma &amp; Welling, 2014)</ref>:</p><formula xml:id="formula_15">w i ∼ N (0, I d ), w i = σ • w i .<label>(8)</label></formula><p>I d is the d × d identity matrix, and • denotes elementwise product between vectors. d-dimensional vector σ is learned, but random vectors w i are not. 5</p><p>This norm-1 constraint is never mandatory. Rather, we employ it for notation clarity and easier implementation. In preliminary experiments we find it has little impact on the performance when σ is set properly or learned from data. Eq. 12 in Appendix A presents RFA without imposing it.</p><p>Going beyond the Gaussian kernel. More broadly, random feature methods can be applied to a family of shift-invariant kernels, with the Gaussian kernel being one of them. In the same family, the order-1 arc-cosine kernel <ref type="bibr" target="#b16">(Cho &amp; Saul, 2009</ref>) can be approximated with feature map: φ arccos (x) = 1/D[ReLU(w 1 · x), . . . , ReLU(w D · x)] <ref type="bibr" target="#b1">(Alber et al., 2017)</ref>. <ref type="bibr">6</ref> In our experiments, the Gaussian and arc-cosine variants achieve similar performance. This supplements the exploration of alternatives to softmax in attention <ref type="bibr" target="#b70">(Tsai et al., 2019;</ref><ref type="bibr" target="#b23">Gao et al., 2019)</ref>.</p><p>Relations to prior work. <ref type="bibr" target="#b31">Katharopoulos et al. (2020)</ref> inspire the causal attention variant of RFA. They use a feature map based on the exponential linear unit activation <ref type="bibr" target="#b18">(Clevert et al., 2016)</ref>: elu(·) + 1. It significantly underperforms both the baseline and RFA in our controlled experiments, showing the importance of a properly-chosen feature map. Random feature approximation of attention is also explored by a concurrent work <ref type="bibr" target="#b17">(Choromanski et al., 2020)</ref>, with applications in masked language modeling for proteins. They propose positive random features to approximate softmax, aiming for a lower variance in critical regions. RFA instead normalizes the queries and keys before random projection to reduce variance. Going beyond both, RFA establishes the benefits of random feature methods as a more universal substitute for softmax across all attention variants, facilitating its applications in, e.g., sequence-to-sequence learning.</p><p>There are interesting connections between gated RFA and fast weights <ref type="bibr" target="#b60">(Schmidhuber, 1992;</ref><ref type="bibr" target="#b61">1993;</ref><ref type="bibr" target="#b4">Ba et al., 2016;</ref><ref type="bibr">Miconi et al., 2018, inter alia)</ref>. Emphasizing recent patterns, they learn a temporal memory to store history similarly to Eqs. 7. The main difference is that RFA additionally normalizes the output using φ(q t ) · z as in Eq. 6, a by-product of approximating softmax's partition function.</p><p>It is intriguing to study the role of this normalization term, which we leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COMPLEXITY ANALYSIS</head><p>Time. Scaling linearly in the sequence lengths, RFA needs less computation (in terms of number of operations) for long sequences. This implies speedup wherever the quadratic-time softmax attention cannot be fully-parallelized across time steps. More specifically:</p><p>• Significant speedup can be expected in autoregressive decoding, both conditional (e.g., machine translation) and unconditional (e.g., sampling from a language model). For example, 1.9× speedup is achieved in our machine translation experiments ( §4.2); and more for longer sequences (e.g., 12× for 2,048-length ones; §5). • Some applications (e.g., language modeling, text classification) reveal inputs to the model in full. 7 When there are enough threads to parallelize softmax attention across time steps, hardly any speedup from RFA can be achieved; when there are not, typically for very long sequences (&gt;1,000), substantial speed gain is possible. For example, RFA does not achieve any speedup when working with 512-length context ( §4.1), but achieves a 5.3× speedup with 4,000-length context ( §4.2).</p><p>Memory. Asymptotically, RFA has a better memory efficiency than its softmax counterpart (linear vs. quadratic). To reach a more practical conclusion, we include in our analysis the cost of the feature maps. φ's memory overhead largely depends on its size D. For example, let's consider the cross attention of a decoder. RFA uses O(4D + 2Dd) space to store φ(q t ), i φ(k i ) ⊗ v i , and i φ(k i ) (Eq. 5; line 12 of Algo. 2). <ref type="bibr">8</ref> In contrast, softmax cross attention stores the encoder outputs with O(M d) memory, with M being the source length. In this case RFA has a lower memory overhead when 2D M . Typically D should be no less than d in order for reasonable approximation <ref type="bibr" target="#b77">(Yu et al., 2016)</ref>; In a transformer model, d is the size of an attention head, which is usually around 64 or 128 <ref type="bibr" target="#b71">(Vaswani et al., 2017;</ref>. This suggests that RFA can achieve significant memory saving with longer sequences, which is supported by our empirical analysis in §5. Further, using moderate sized feature maps is also desirable, so that its overhead does not overshadow the time and memory RFA saves. We experiment with D at d and 2d; the benefit of using D &gt; 2d is marginal.</p><p>Appendix A.6 discusses the time and space complexity in more detail, and Appendix C.2 studies the effect of random feature size on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate RFA on language modeling, machine translation, and long text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LANGUAGE MODELING</head><p>Setting. We experiment with WikiText-103 <ref type="bibr" target="#b39">(Merity et al., 2017)</ref>. It is based on English Wikipedia. <ref type="table" target="#tab_7">Table 5</ref> in Appendix B summarizes some of its statistics. We compare the following models:</p><p>• BASE is our implementation of the strong transformer-based language model by . • RFA builds on BASE, but replaces the softmax attention with random feature attention. We experiment with both Gaussian and arc-cosine kernel variants. • RFA-GATE additionally learns a sigmoid gate on top of RFA ( §3.2). It also has a Gaussian kernel variant and a arc-cosine kernel one. 9 • φ elu is a baseline to RFA. Instead of the random feature methods it uses the elu(·) + 1 feature map, as in <ref type="bibr" target="#b31">Katharopoulos et al. (2020)</ref>.</p><p>To ensure fair comparisons, we use comparable implementations, tuning, and training procedure. All models use a 512 block size during both training and evaluation, i.e., they read as input a segment of 512 consecutive tokens, without access to the context from previous mini-batches. RFA variants use 64-dimensional random feature maps. We experiment with two model size settings, small (around 38M parameters) and big (around 242M parameters); they are described in Appendix B.1 along with other implementation details. Results. <ref type="table">Table 1</ref> compares the models' performance in perplexity on WikiText-103 development and test data. Both kernel variants of RFA, without gating, outperform φ elu by more than 2.4 and 2.1 test perplexity for the small and big model respectively, confirming the benefits from using random feature approximation. 10 Yet both underperform BASE, with RFA-Gaussian having a smaller gap. Comparing RFA against its gated variants, a more than 1.8 perplexity improvement can be attributed to the gating mechanism; and the gap is larger for small models. Notably, RFA-GATE-Gaussian outperforms BASE under both size settings by at least 1.2 perplexity. In general, RFA models with Gaussian feature maps outperform their arc-cosine counterparts. 11 From the analysis in §3.4 we would not expect speedup by RFA models, nor do we see any in the experiments. 12</p><p>Closing this section, we explore a "stateful" variant of RFA-GATE-Gaussian. It passes the last hidden state (S t , z t ) to the next mini-batch during both training and evaluation, a technique commonly used in RNN language models <ref type="bibr" target="#b40">(Merity et al., 2018)</ref>. This is a consequence of RFA's RNN-style computation, and is less straightforward to be applicable in the vanilla transformer models. <ref type="bibr">13</ref> From the last row of <ref type="table">Table 1</ref> we see that this brings a more than 1.5 test perplexity improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MACHINE TRANSLATION</head><p>Datasets. We experiment with three standard machine translation datasets.</p><p>• WMT14 EN-DE and EN-FR <ref type="bibr" target="#b9">(Bojar et al., 2014)</ref>. Our data split and preprocessing follow those of <ref type="bibr" target="#b71">Vaswani et al. (2017)</ref>. We share the source and target vocabularies within each language pair, with 32,768 byte pair encoding types (BPE; <ref type="bibr" target="#b62">Sennrich et al., 2016)</ref>. • IWSLT14 DE-EN <ref type="bibr" target="#b12">(Cettolo et al., 2014)</ref> is based on TED talks. The preprocessing follows . Separate vocabularies of 9K/7K BPE types are used for the source and target. <ref type="table" target="#tab_7">Table 5</ref> in Appendix B summarizes some statistics of the datasets. 10 All models are trained for 150K steps; this could be part of the reason behind the suboptimal performance of φ elu : it may need 3 times more gradient updates to reach similar performance to the softmax attention baseline <ref type="bibr" target="#b31">(Katharopoulos et al., 2020)</ref>. <ref type="bibr">11</ref> We observe that RFA Gaussian variants are more stable and easier to train than the arc-cosine ones as well as φ elu . We conjecture that this is because the outputs of the Gaussian feature maps have an 2-norm of 1, which can help stabilize training. To see why, sin 2 (x) + cos 2 (x) = cos(x − x) = 1.</p><p>12 In fact, RFA trains around 15% slower than BASE due to the additional overhead from the feature maps. 13 Some transformer models use a text segment from the previous mini-batch as a prefix <ref type="bibr" target="#b19">Dai et al., 2019)</ref>. Unlike RFA, this gives the model access to only a limited amount of context, and significantly increases the memory overhead.</p><p>Setting. We compare the RFA variants described in §4.1. They build on a BASE model that is our implementation of the base-sized transformer <ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref>. All RFA models apply random feature attention in decoder cross and causal attention, but use softmax attention in encoders. This setting yields the greatest decoding time and memory savings ( §3.4). We use 128/64 for D in cross/causal attention. RFA-GATE learns sigmoid gates in the decoder causal attention. The φ elu baseline uses the same setting and applies feature map in both decoder cross and causal attention, but not in the encoders. Further details are described in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT14 IWSLT14</head><p>Model  Results. <ref type="table" target="#tab_3">Table 2</ref> compares the models' test set BLEU on three machine translation datasets. Overall both Gaussian and arc-cosine variants of RFA achieve similar performance to BASE on all three datasets, significantly outperforming <ref type="bibr" target="#b31">Katharopoulos et al. (2020)</ref>. Differently from the trends in the language modeling experiments, here the gating mechanism does not lead to substantial gains. Notably, all RFA variants decode more than 1.8× faster than BASE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LONG TEXT CLASSIFICATION</head><p>We further evaluate RFA's accuracy and efficiency when used as text encoders on three NLP tasks from the recently proposed Long Range Arena benchmark <ref type="bibr">(Tay et al., 2021)</ref>, designed to evaluate efficient Transformer variants on tasks that require processing long sequences. 14 Experimental setting and datasets. We compare RFA against baselines on the following datasets:</p><p>• ListOps (LO; <ref type="bibr" target="#b44">Nangia &amp; Bowman, 2018)</ref> aims to diagnose the capability of modelling hierarchically structured data. Given a sequence of operations on single-digit integers, the model predicts the solution, also a single-digit integer. It is formulated as a 10-way classification. We follow <ref type="bibr">Tay et al. (2021)</ref> and consider sequences with 500-2,000 symbols. • Character-level text classification with the IMDb movie review dataset <ref type="bibr" target="#b38">(Maas et al., 2011)</ref>. This is a binary sentiment classification task. • Character-level document retrieval with the ACL Anthology Network (AAN; <ref type="bibr" target="#b53">Radev et al., 2009</ref>) dataset. The model classifies whether there is a citation between a pair of papers.</p><p>To ensure fair comparisons, we implement RFA on top of the transformer baseline by <ref type="bibr">Tay et al. (2021)</ref>, and closely follow their preprocessing, data split, model size, and training procedure. Speed and memory are evaluated on the IMDb dataset. For our RFA model, we use D = 64 for the IMDb dataset, and D = 128 for others. We refer the readers to <ref type="bibr">Tay et al. (2021)</ref> for further details.</p><p>Results. From <ref type="table" target="#tab_4">Table 3</ref> we can see that RFA outperforms the transformer baseline on two out of the three datasets, achieving the best performance on IMDb with 66% accuracy. Averaging across three datasets, RFA outperforms the transformer by 0.3% accuracy, second only to <ref type="bibr" target="#b78">Zaheer et al. (2020)</ref> with a 0.1% accuracy gap. In terms of time and memory efficiency, RFA is among the strongest. RFA speeds up over the transformer by 1.1-5.3×, varying by sequence length. Importantly, compared to the only two baselines that perform comparably to the baseline transformer model <ref type="bibr" target="#b66">(Tay et al., 2020a;</ref><ref type="bibr" target="#b78">Zaheer et al., 2020)</ref>, RFA has a clear advantage in both speed and memory efficiency, and is the only model that is competitive in both accuracy and efficiency.  <ref type="bibr">Tay et al. (2021)</ref>.</p><p>2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Sequence Length   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS</head><p>Decoding time and memory varying by sequence length. §3.4 shows that RFA can potentially achieve more significant speedup and memory saving for longer sequences, which we now explore.</p><p>We use a simulation conditional generation experiment on to compare RFA's sequence-to-sequence decoding speed and memory overhead against the baseline's. Here we assume the input and output sequences are of the same length. The compared models are of the same size as those described in §4.2, with 6-layer encoders and decoders. Other hyperparameters are summarized in Appendix B.2. All models are tested using greedy decoding with the same batch size of 16, on a TPU v2 accelerator.</p><p>From <ref type="figure" target="#fig_12">Figures 2 (a)</ref> and (b) we observe clear trends. Varying the lengths, both RFA variants achieve consistent decoding speed with nearly-constant memory overhead. In contrast, the baseline decodes slower for longer sequences, taking an increasing amount of memory. Notably, for 2,048-length sequences, RFA decodes around 12× faster than the baseline while using less than 10% of the memory. RFA-arccos slightly outperforms RFA-Gaussian in terms of speed and memory efficiency. This is because when using the same D (as we do here), the φ arccos is half the size of φ Gaussian . These results suggest that RFA can be particularly useful in sequence-to-sequence tasks with longer sequences, e.g., document-level machine translation <ref type="bibr" target="#b42">(Miculicich et al., 2018)</ref>. <ref type="figure" target="#fig_14">Figure 3</ref> in Appendix C.1 compares the speed and memory consumption in unconditional decoding (e.g., sampling from a language model). The overall trends are similar to those in <ref type="figure" target="#fig_12">Figure 2</ref>.</p><p>Notes on decoding speed. With a lower memory overhead, RFA can use a larger batch size than the baseline. As noted by <ref type="bibr" target="#b31">Katharopoulos et al. (2020)</ref> and <ref type="bibr">Kasai et al. (2021)</ref>, if we had used mini-batches as large as the hardware allows, RFA could have achieved a more significant speed gain. Nonetheless, we control for batch size even though it is not the most favorable setting for RFA, since the conclusion translates better to common applications where one generates a single sequence at a time (e.g., instantaneous machine translation). For the softmax attention baseline, we follow  and cache previously computed query/key/value representations, which significantly improves its decoding speed (over not caching).</p><p>Further analysis results. RFA achieves comparable performance to softmax attention. Appendix C.3 empirically shows that this cannot be attributed to RFA learning a good approximation to softmax: when we train with one attention but evaluate with the other, the performance is hardly better than randomly-initialized untrained models. Yet, an RFA model initialized from a pretrained softmax transformer achieves decent training loss after a moderate amount of finetuning steps (Appendix C.4). This suggests some potential applications, e.g., transferring knowledge from a pretrained transformer (e.g., <ref type="bibr">GPT-3;</ref><ref type="bibr">Brown et al., 2020)</ref> to an RFA model that is more efficient to sample from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>One common motivation across the following studies, that is shared by this work and the research we have already discussed, is to scale transformers to long sequences. Note that there are plenty orthogonal choices for improving efficiency such as weight sharing <ref type="bibr" target="#b20">(Dehghani et al., 2019)</ref>, quantization <ref type="bibr" target="#b63">(Shen et al., 2020)</ref>, knowledge distillation <ref type="bibr" target="#b59">(Sanh et al., 2020)</ref>, and adapters <ref type="bibr" target="#b29">(Houlsby et al., 2019)</ref>. For a detailed overview we refer the reader to <ref type="bibr" target="#b68">Tay et al. (2020c)</ref>.</p><p>Sparse attention patterns. The idea behind these methods is to limit the reception field of attention computation. It motivates earlier attempts in improving attention's efficiency, and still receives lots of interest. The sparse patterns can be set a priori <ref type="bibr" target="#b37">(Liu et al., 2018;</ref><ref type="bibr" target="#b52">Qiu et al., 2020;</ref><ref type="bibr" target="#b26">Ho et al., 2020;</ref><ref type="bibr">You et al., 2020, inter alia)</ref> or learned from data <ref type="bibr" target="#b64">(Sukhbaatar et al., 2019;</ref><ref type="bibr">Roy et al., 2020, inter alia)</ref>. For most of these approaches, it is yet to be empirically verified that they are suitable for large-scale sequence-to-sequence learning; few of them have recorded decoding speed benefits.</p><p>Compressed context.  compress the context along the timesteps so that the effective sequence length for attention computation is reduced. Another line of work aims to store past context into a memory module with limited size <ref type="bibr">Rae et al., 2020, inter alia)</ref>, so that accessing longer history only moderately increases the overhead. Reminiscent of RNN language models, RFA attends beyond a fixed context window through a stateful computation, without increasing time or memory overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented random feature attention (RFA). It views the softmax attention through the lens of kernel methods, and approximates it with random feature methods. With an optional gating mechanism, RFA provides a straightforward way of learning with recency bias. RFA's time and space complexity is linear in the sequence length. We use RFA as a drop-in substitute for softmax attention in transformer models. On language modeling, machine translation, and long text classification benchmarks, RFA achieves comparable or better performance than strong baselines. In the machine translation experiment, RFA decodes twice as fast. Further time and memory efficiency improvements can be achieved for longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DERIVATION OF CAUSAL RFA</head><p>This section presents a detailed derivation of causal RFA as in §3.1. Following Eq. 5 but changing the attended keys and values to the prefix:</p><formula xml:id="formula_16">RFA(q t , {k i } i≤t , {v i } i≤t ) = φ (q t ) i≤t φ (k i ) ⊗ v i φ (q t ) · j≤t φ (k j )<label>(10)</label></formula><p>Let S t i≤t φ(k i ) ⊗ v i , and z t i≤t φ(k i ); both can be calculated recurrently. Assuming S 0 = 0 and z 0 = 0:</p><formula xml:id="formula_17">S t = S t−1 + φ (k t ) ⊗ v t , z t = z t−1 + φ (k t ) , t ≥ 1.<label>(11)</label></formula><p>This completes the derivation of causal RFA as in §3.1.</p><p>A.4 RFA WITHOUT NORM-1 CONSTRAINTS §3.1 assumes that the queries and keys are unit vectors. This norm-1 constraint is not a must. Here we present a RFA without imposing this constraint. Let</p><formula xml:id="formula_18">C(x) = exp( x 2 /2σ 2 ). From Eq. 4 we have attn (q t , {k i }, {v i }) = i exp q t · k i /σ 2 j exp (q t · k j /σ 2 ) v i ≈ i C(q t ) C(k i ) φ (q t ) φ (k i ) v i j C(q t ) C(k j ) φ (q t ) · φ (k j ) = φ (q t ) i C(k i ) φ (k i ) ⊗ v i φ (q t ) · j C(k j ) φ (k j ) .<label>(12)</label></formula><p>The specific attention computation is similar to those in §3.1. In sum, lifting the norm-1 constraint brings an additional scalar term C(·).</p><p>A.5 RELATING RFA-GATE TO SOFTMAX ATTENTION Drawing inspiration from gated RNNs, §3.2 introduces a gated variant of RFA. Now we study its "softmax counterpart."</p><formula xml:id="formula_19">k i = k i (1 − g i ) t j=i+1 g j , v i = v i (1 − g i ) t j=i+1 g j , i = 1, . . . , t h t = attn(q t , { k i } i≤t , { v i } i≤t ).<label>(13)</label></formula><p>h t is the output at timestep t and is used for onward computation.</p><p>At each step, all prefix keys and values are decayed by a gate value before calculating the attention. This implies that the attention computation for q t+1 cannot start until that of q t is finished. Combined with the linear complexity of softmax normalization, this amounts to quadratic time in sequence length, even for language modeling training.</p><p>The above model is less intuitive and more expensive in practice, without the RFA perspective. This shows that RFA brings some benefits in developing new attention models.</p><p>A.6 DETAILED COMPLEXITY ANALYSIS     During training, we sample a different random projection matrix for each attention head. Preliminary experiments suggest this performs better than using the same random projection throughout training <ref type="table" target="#tab_9">(Table 6</ref>). Our conjecture is that this helps keep the attention heads from "over committing" to any particular random projection <ref type="bibr" target="#b50">(Peng et al., 2020)</ref>. To avoid the overhead of sampling from Gaussian during training, we do this in an offline manner. I.e., before training we construct a pool of random matrices (typically 200), at each training step we draw from the pool. At test time each attention head uses the same random projection, since no accuracy benefit is observed by using different ones for different test instances.</p><formula xml:id="formula_20">O(M ) O(M ) O(N ) O(M 2 ) O(M N ) O(N 2 ) RFA O(M ) O(M ) O(N ) O(M ) O(M + N ) O(N ) Decoding softmax O(M ) O(M N ) O(N 2 ) O(M 2 ) O(M N ) O(N 2 ) RFA O(M ) O(M + N ) O(N ) O(M ) O(M + N ) O(N )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 LANGUAGE MODELING</head><p>We compare the models using two model size settings, summarized in <ref type="table" target="#tab_11">Table 7</ref>. We use the fixed sinusoidal position embeddings by <ref type="bibr" target="#b71">Vaswani et al. (2017)</ref>. All models are trained for up to 150K gradient steps using the Adam optimizer <ref type="bibr" target="#b32">(Kingma &amp; Ba, 2015)</ref>. No 2 -regularization is used. We apply early stopping based on development set perplexity. All models are trained using 16 TPU v3 accelerators, and tested using a single TPU v2 accelerator.  WMT14. We use the fixed sinusoidal position embeddings by <ref type="bibr" target="#b71">Vaswani et al. (2017)</ref>. For both EN-DE and EN-FR experiments, we train the models using the Adam (with β 1 = 0.1, β 2 = 0.98, and = 10 −9 ) optimizer for up to 350K gradient steps. We use a batch size of 1,024 instances for EN-DE, while 4,096 for the much larger EN-FR dataset. The learning rate follows that by <ref type="bibr" target="#b71">Vaswani et al. (2017)</ref>. Early stopping is applied based on development set BLEU. No 2 regularization or gradient clipping is used. All models are trained using 16 TPU v3 accelerators, and tested using a single TPU v2 accelerator. Following standard practice, we average 10 most recent checkpoints at test time. We evaluate the models using SacreBLEU <ref type="bibr" target="#b51">(Post, 2018)</ref>. <ref type="bibr">16</ref> A beam search with beam size 4 and length penalty 0.6 is used. Other hyperparameters are summarized in <ref type="table" target="#tab_13">Table 8</ref>.   <ref type="figure" target="#fig_14">Figure 3</ref> compares the RFA's unconditional decoding speed and memory against the softmax attention. The setting is the same as that in §5 except that here the models do not have an encoder. This experiment aims to simulate the applications such as sampling from a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EFFECT OF RANDOM FEATURE SIZE</head><p>This section studies how the size of φ(·) affects the performance. <ref type="table" target="#tab_14">Table 9</ref> summarize RFA-Gaussian's performance on WMT14 EN-DE development set. The model and training are the same as that used in §4.2 except random feature size. Recall from §2.2 that the size of φ(·) is 2D for 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Sequence Length  RFA-Gaussian. When the size of φ(·) is too small (32 or 64 for cross attention, 32 for causal attention), training does not converge. We observe accuracy improvements by using random features sufficiently large (256 for cross attention and 128 for causal attention); going beyond that, the benefit is marginal.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 TRAIN AND EVALUATE WITH DIFFERENT ATTENTION FUNCTIONS</head><p>RFA achieves comparable performance to its softmax counterpart. Does this imply that it learns a good approximation to the softmax attention? To answer this question, we consider:</p><p>(i) an RFA-Gaussian model initialized from a pretrained softmax-transformer;</p><p>(ii) a softmax-transformer initialized from a pretrained an RFA-Gaussian model.</p><p>If RFA's good performance can be attributed to learning a good approximation to softmax, both, without finetunining, should perform similarly to the pretrained models. However, this is not the case on IWSLT14 DE-EN. Both pretrained models achieve more than 35.2 development set BLEU. In contrast, (i) and (ii) respectively get 2.3 and 1.1 BLEU without finetuning, hardly beating a randomly-initialized untrained model. This result aligns with the observation by <ref type="bibr" target="#b17">Choromanski et al. (2020)</ref>, and suggests that it is not the case that RFA performs well because it learns to imitate softmax attention's outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 KNOWLEDGE TRANSFER FROM SOFTMAX ATTENTION TO RFA</head><p>We first supplement the observation in Appendix C.3 by finetuning (i) on the same pretraining data. <ref type="figure" target="#fig_16">Figure 4</ref> plots the learning curves. It takes RFA roughly 1,500 steps to reach similar training loss to the pretrained model. As a baseline, "RFA Reset" resets the multihead attention parameters (i.e., those for query, key, value, and output projections) to randomly initialized ones. Its learning curve is similar to that of (i), suggesting that the pretrained multihead attention parameters are no more useful to RFA than randomly initialized ones. To further confirm this observation, "softmax Reset" Takeaway. From the above results on IWSLT14, pretrained knowledge in a softmax transformer cannot be directly transferred to an RFA model. However, from <ref type="figure" target="#fig_16">Figure 4</ref> and a much larger-scale experiment by <ref type="bibr" target="#b17">Choromanski et al. (2020)</ref>, we do observe that RFA can recover the pretraining loss, and the computation cost of finetuning is much less than training a model from scratch. This suggests some potential applications. For example, one might be able to initialize an RFA language model from a softmax transformer pretrained on large-scale data (e.g., <ref type="bibr">GPT-3;</ref><ref type="bibr">Brown et al., 2020)</ref>, and finetune it at a low cost. The outcome would be an RFA model retaining most of the pretraining knowledge, but is much faster and more memory-friendly to sample from. We leave such exploration to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " i T N 1 6 P U F E d d 6 7 b c g 7 v L U / G F M T 0 M = " &gt; A A A C A H i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 t F 8 F Q S K e i x 6 M V j B W u F N p T N d t M u 3 c 2 G 3 Y l Y Y g / + F S 8 e F M S r P 8 O b / 8 Z N m 4 O 2 P h h 4 v D f D z L w w E d y A 5 3 0 7 p a X l l d W 1 8 n p l Y 3 N r e 8 f d 3 b s 1 K t W U t a g S S t + F x D D B Y 9 Y C D o L d J Z o R G Q r W D k e X u d + + Z 9 p w F d / A O G G B J I O Y R 5 w S s F L P P e i q h G k C S s d E s s y o C C R 5 m P T c q l f z p s C L x C 9 I F R V o 9 t y v b l / R V L I Y q C D G d H w v g S A j G j g V b F L p p o Y l h I 7 I g H U s z Z e Z I J v e P 8 H H V u n j S G l b M e C p + n s i I 9 K Y s Q x t p y Q w N P N e L v 7 n d V K I z o O M x 0 k K L K a z R V E q M C i c h 4 H 7 X D M K Y m w J o Z r b W z E d E k 0 o 2 M g q N g R / / u V F 0 j 6 t + f W a 7 1 / X q 4 2 L I o 8 y O k R H 6 A T 5 6 A w 1 0 B V q o h a i 6 B E 9 o 1 f 0 5 j w 5 L 8 6 7 8 z F r L T n F z D 7 6 A + f z B + x 9 l y M = &lt; / l a t e x i t &gt; softmax &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E K G F j e p b x 5 c a f J 1 5 n y I y H U x E J U c = " &gt; A A A K H n i c b d Z J b 9 t G G A Z g O t 0 i d U n S H n s h a h R I A c H Q O L K j 3 B J 5 3 + V F i 2 0 K w X A 0 H N H m l u H Q j k z w P / T a X v p r e i x 6 b f 9 N K V n u + 9 U q A Q H D 5 / t I j m b e w 7 h J 4 K e m X v 9 7 4 c k n n 3 7 2 + R d P K 9 U v v / r 6 m 2 f P X 3 z b T e N M C 9 k R c R D r v s t T G f i R 7 B j f B L K f a M l D N 5 A 9 9 3 p t U u / d S J 3 6 c X R m x o k c h F x F v u c L b k r q O u r o 5 c F P 7 5 8 v 1 p f q 0 8 u e H 7 D Z Y N G a X e 3 3 L y o L z j A W W S g j I w K e p p e s n p h B z r X x R S C L q p O l M u H i m i t 5 W Q 4 j H s p 0 k E + n W 9 g / l j K 0 v V i X v 8 j Y U 6 V P 5 D x M 0 3 H o l p 0 h N 6 P 0 c W 2 C / 1 e 7 z I z X H O R + l G R G R u L + Q 1 4 W 2 C a 2 J / / d H v p a C h O M y w E X 2 i / n a o s R 1 1 y Y c o W q z l B 6 5 S p O p 5 O H Y z f I Z J G f b L W K v L F a a z Z r r F E v H j d p O Z z 1 s C a r r b 6 p N V f n e m L N I / X w q u V 6 o 2 a z x q u a v b I y 1 5 l k O g k e O t m r s r P x u u x e m X + n 0 l J G D 7 M r p 8 Z Y z V 5 + U 1 S r 0 0 b n 5 k 7 q O M + d y R K 5 X l 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>e g e 9 m + g 9 h + / A 7 / 5 t D l v Q F n Q N u g Z d h 6 5 D N 6 A b 0 E 3 o J n Q L u g X d h m 5 D d 6 A 7 0 F 3 o L n Q P u g f d h + 5 D D 6 A H 0 E P o I f Q I e g R t Q 9 v Q Y + g x 9 A R 6 A j 2 F n k L P o G f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 B z 6 D n 0 A n q B 8K i H 8 A g e k P S o F m H E R 6 0 R R n 7 U O m E E S G 0 Q R o L U J m F E S G 0 R R o b U N m G E S O 0 Q R o r U L m H E S O 0 R R o 7 U P m E E S R 0 Q R p L U I W F E S R 0 R R p Z Um z D C p I 4 J I 0 3 q h D D i p E 4 J I 0 / q j D A C p T q E k S j V J Y x I q R 5 h Z E r 1 C S N U 6 p w w U q U u C E 9 j 5 U T y V s R h y K N h 7 k S x D o t L N s i d Q H r G C b p S m 0 X m a F + N j K M n d 0 V 5 a G K P j 0 j z g 9 7 y E m s s M X b c W H z b m p 2 f n l r f W z 9 Y L y 1 m v b b e W t t W 2 + p Y w r q y f r Z + s X 6 t / F b 5 v f J H 5 c / 7 1 i c L s 2 e + s / 5 z V f 7 6 B 4 b o x 3 s = &lt; / l a t e x i t &gt; O(M ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x v 9 6 / M 5 N / v l e B K W 7 w D I f 4 y 0 8 j r M = " &gt; A A A K H 3 i c b d Z J b 9 t G G A Z g O r v U N O s x F y J G g R Q Q D I 0 j O 8 o t k f d d X r T Y p h A M q e G I E b c M h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>k 2 C S N M 8 p A w 0 i S P C C N O 8 p g w 8 i R P C C N Q s k U Y i Z J t w o i U 7 B B G p m S X M E I l T w k j V f K M 8 C R W V i g u n S g I e N j P r D B S Q X 7 O e p n l C 1 d b f l s o P c 8 s 5 c m B t t T 4 L i 8 O T e z 2 E W l 2 0 F l c Y L U F x g 5 r 8 x 8 a 0 / P T I + O V 8 d p 4 Y z D j n f H B 2 D S a R s t w j K H x z f j D + L P 0 V + n v 0 v f S j + v W O 3 P T Z 1 4 a / 7 l K P / 8 B q y / H 0 w = = &lt; / l a t e x i t &gt; O(MN) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3 k f S 6 H 5 o f O X / m R p F 1 7 d 8 R U q W C 4 = " &gt; A A A K L X i c b d Z J b 9 t G G A Z g O t 0 i p Y v T H H s h a h T o Q T A 0 j u w o h w K J v O / y o s U 2 V W F I D U e M u X k 4 l C M T / C 2 9 t p f + m l w C B L 3 2 b 5 S S 5 b 5 f r Q 4 g Y P h 8 H 6 n B z H s Y O / a 9 R F e r H x e e f P b 5 F 1 9 + 9 b R U f v b 1 N 9 9 + t / j 8 + 3 Y S p c o R L S f y I 9 W 1 e S J 8 L x Q t 7 W l f d G M l e G D 7 o m N f r 0 / q n Z F Q i R e F 5 3 o c i 1 7 A Z e i 5 n s N 1 Q f 3 F F 1 Z m j W 7 6 n p X 3 M + 8 X l v + a H e X 9 x a X q c n U 6 z P k J m 0 2 W j N l o 9 p + X F q x B 5 K S B C L X j 8 y S 5 Y t V Y 9 z K u t O f 4 I i 9 b a S J i 7 l x z K a 6 K a c g D k f S y 6 e p z 8 6 d C B q Y b q e I X a n O q 9 I 2 M B 0 k y D u y i M + B 6 m D y u T f D / a l e p d u u 9 z A v j V I v Q u f 8 j N / V N H Z m T r T A H n h K O 9 s f F h D v K K 9 Z q O k O u u K O L D S t b A + E W m z p d T h a M b T 8 V e X a 6 3 c i z 2 l q l X q + w W j V / 3 K T E Y N b D 6 q y y 9 r p S X 5 v r i R Q P 5 c O n V q q 1 i s l q L y v m 6 u p c Z 5 y q 2 H / o Z C + L z t q r o n t 1 / p t S C R E + r K 5 Y G m M V c + V 1 X i 5 P G 6 3 R n V B R l l m T L b L d r J r n + a w Q h Q L O 4 E E K t o I U B T 0 U m p P a 9 B l l U i J q Q 2 2 o A 3 W g A + g A S l Y p o C 7 U h U q o h A 6 h Q 6 g H 9 a D v o O + g 1 9 B r q A / 1 y f 5 B A 2 g I D c k Z Q C N o D I 2 h N 9 A b q I I q a A J N y A F C N Z Q c N z n s E X Q E v Y X e Q t 9 D 3 0 P H 0 D H 0 D n o 3 0 X s O 3 o L f / t s c N K A N 6 D p 0 H b o B 3 Y B u Q j e h W 9 A t 6 D Z 0 G 7 o D 3 Y H u Q n e h e 9 A 9 6 D 5 0 H 3 o A P Y A e Q g + h R 9 A j 6 D H 0 G N q E N q E n 0 B P o K f Q U e g Y 9 g 5 5 D z 6 E t a A v a h r a h H W g H 2 o V 2 o R f Q C + g l 9 B L h k Q / h c b h P 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>L T V 5 m l y a 2 O M r 0 v y k s 7 L M a s u M n d S W 3 j R m 9 6 e n x g / G j 8 b P B j N e G W + M H a N p t A z H G B u / G b 8 b f 5 T + L H 0 o f S r 9 d d / 6 Z G H 2 z g v j P 6 P 0 9 z 9 1 7 M 4 L &lt; / l a t e x i t &gt; {q i } N i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 H + N 8 b 0 t Y v / J e C 1 w 8 N x p 5 Y 5 / J + U = " &gt; A A A K L X i c b d Z J b 9 t G G A Z g O u k S q Z u T H H s h a h T o Q T A 0 j u w o h w K J v O / y o s U 2 V W F I D U e M u G U 4 l C M T / C 2 9 t p f + m l 4 K F L 3 2 b 5 S S 5 b 5 f r Q 4 g Y P h 8 H 6 n B z H s Y O / a 9 R F e r f y w 9 e f r J p 5 9 9 / q x U / u L L r 7 7 + Z v n 5 i 3 Y S p c o R L S f y I 9 W 1 e S J 8 L x Q t 7 W l f d G M l e G D 7 o m O P N q f 1 z l i o x I v C S z 2 J R S / g M v R c z + G 6 o P 7 y S y u z x q O + Z + X 9 z P u R 5 T 9 l x 3 l / e a W 6 W p 0 N c 3 H C 5 p M V Y z 6 a / e e l J W s Q O W k g Q u 3 4 P E l u W D X W v Y w r 7 T m + y M t W m o i Y O y M u x U 0 x D X k g k l 4 2 W 3 1 u f l / I w H Q j V f x C b c 6 U v p H x I E k m g V 1 0 B l w P k 8 e 1 K f 5 f 7 S b V b r 2 X e W G c a h E 6 9 3 / k p r 6 p I 3 O 6 F e b A U 8L R / q S Y c E d 5 x V p N Z 8 g V d 3 S x Y W V r I N x i U 2 f L y Y K J 7 a c i z 8 5 3 G 3 l W 2 6 j U 6 x V W q + a P m 5 Q Y z H t Y n V U 2 3 l T q G w s 9 k e K h f P j U W r V W M V n t V c V c X 1 / o j F M V + w + d 7 F X R W X t d d K 8 v f l M q I c K H 1 R V L Y 6 x i r r 3 J y + V Z o z W + E y r K M m u 6 R b a b V f M 8 n x e i U M A Z P E j B V p C i o I d C c 1 K b P a N M S k R t q A 1 1 o A 5 0 A B 1 A y S o F 1 I W 6 U A m V 0 C F 0 C P W g H v Q9 9 D 1 0 B B 1 B f a h P 9 g 8 a Q E N o S M 4 A G k F j a A z 9 A P 0 A V V A F T a A J O U C o h p L j J o c 9 h o 6 h t 9 B b 6 E f o R + g E O o H e Q e + m e s / B O / C 7 f 5 u D B r Q B 3 Y R u Q r e g W 9 B t 6 D Z 0 B 7 o D 3 Y X u Q v e g e 9 B 9 6 D 7 0 A H o A P Y Q e Q o + g R 9 B j 6 D H 0 B H o C P Y W e Q p v Q J v Q M e g Y 9 h 5 5 D L 6 A X 0 E v o J b Q F b U H b 0 D a 0 A + 1 A u 9 A u 9 A p 6 B b 2 G X i M 8 8 i E 8 D v d J e m S D M O I j N w k j P 3 K L M A I k t w k j Q X K H M C I k d w k j Q 3 K P M E I k 9 w k j R f K A M G I k D w k j R / K I M I I k j w k j S f K E M K I k T w k j S 7 J J G G G S Z 4 S R J n l O G H G S F 4 S R J 3 l J G I G S L c J I l G w T R q R k h z A y J b u E E S p 5 R R i p k t e E Z 7 G y Q n H r R E H A w 0 F m h Z E K 8 h v W y y x f u N r y 2 0 L p F W Y p T w 6 1 p a Z P 0 0 s T e 3 x F W p x 0 1 l Z Z b Z W x s 9 r K 2 8 b 8 / v T M + N b 4 z v j B Y M Z r 4 6 2 x Z z S N l u E Y E + N n 4 x f j 1 9 J v p d 9 L f 5 b + u m 9 9 s j R / 5 6 X x n 1 H 6 + x 8 x p M 4 E &lt; / l a t e x i t &gt; {k i } M i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O e G p z y C c y 6 ZC o 0 o E D G M V u J c b Z C I = " &gt; A A A K L X i c b d b L b t t G F A Z g O k n b S O n F S Z b d E D U K d C E Y G k d 2 l E W B R L 7 f 5 Y s u t q k K Q 2 o 4 Y s x b h k M 5 M s Fn y b b d 9 G m y K V B 0 2 9 c o J c v 9 T 6 0 O I G D 4 n U N q M P M v x o 5 9 L 9 H V 6 h 8 L j x 4 / + e L L r 5 6 W y s + + / u b b 7 x a f v 2 g n U a o c 0 X I i P 1 J d m y f C 9 0 L R 0 p 7 2 R T d W g g e 2 L z r 2 9 f q k 3 h k J l X h R e K 7 H s e g F X I a e 6 z l c F 9 R f f G l l 1 m j U 9 6 y 8 n 3 k / s / y X 7 D D v L y 5 V l 6 v T Y c 5 P 2 G y y Z M x G s / + 8 t G A N I i c N R K g d n y f J F a v G u p d x p T 3 H F 3 n Z S h M R c + e a S 3 F V T E M e i K S X T V e f m z 8W M j D d S B W / U J t T p W 9 k P E i S c W A X n Q H X w + R h b Y L / V 7 t K t V v v Z V 4 Y p 1 q E z t 0 f u a l v 6 s i c b I U 5 8 J R w t D 8 u J t x R X r F W 0 x l y x R 1 d b F j Z G g i 3 2 N T p c r J g b P u p y L P T 7 U a e 1 d Y q 9 X q F 1 a r 5 w y Y l B r M e V m e V t T e V + t p c T 6 R 4 K O 8 / t V K t V U x W e 1 U x V 1 f n O u N U x f 5 9 J 3 t V d N Z e F 9 2 r 8 9 + U S o j w f n X F 0 h i r m C t v 8 n J 5 2 m i N b o W K s s y a b J H t Z t U 8 z 2 e F K B R w B g 9 S s B W k K O i h 0 J z U p s 8 o k x J R G 2 p D H a g D H U A H U L J K A X W h L l R C J X Q I H U I 9 q A d 9 D 3 0 P v Y Z e Q 3 2 o T / Y P G k B D a E j O A B p B Y 2 g M / Q D 9 A F V Q B U 2 g C T l A q I a S 4 y a H P Y K O o D f Q G + h H 6 E f o G D q G 3 k J v J 3r H w T v w u 3 + b g w a 0 A V 2 H r k M 3 o B v Q T e g m d A u 6 B d 2 G b k N 3 o D v Q X e g u d A + 6 B 9 2 H 7 k M P o A f Q Q + g h 9 A h 6 B D 2 G H k O b 0 C b 0 B H o C P Y W e Q s + g Z 9 B z 6 D m 0 B W 1 B 2 9 A 2 t A P t Q L v Q L v Q C e g G 9 h F 4 i P P I + P A 7 3 S X p k g z D i I 9 c J I z 9 y g z A C J D c J I 0 F y i z A i J L c J I 0 N y h z B C J H c J I 0 V y j z B i J P c J I 0 f y g D C C J A 8 J I 0 n y i D C i J I 8 J I 0 u y S R h h k i e E k S Z 5 S h h x k m e E k S d 5 T h i B k i 3 C S J R s E 0 a k Z I c w M i W 7 h B E q e U E Y q Z K X h K e x s k J x 4 0 R B w M N B Z o W R C v I r 1 s s s X 7 j a 8 t t C 6 S V m K U 8 O t a U m T 5 N L E 3 t 4 R Z q f d F a W W W 2 Z s Z P a 0 t v G 7 P 7 0 1 P j e + M H 4 y W D G a + O t s W M 0 j Z b h G G P j k / G r 8 V v p 9 9 L n 0 p + l v + 5 a H y 3 M 3 n l p / G e U / v 4 H n Q X O D w = = &lt; / l a t e x i t &gt; {v i } M i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m D v r 6 a 7 P Y g P p F H Q f A F d 1 u G l y T d k = " &gt; A A A K L X i c b d b L b t t G F A Z g O k n b S O n F S Z b d E D U K d C E Y G k d 2 l E W B R L 7 f 5 Y s u t q k K Q 2 o 4 Y s x b h k M 5 M s F n y b b d 9 G m y K V B 0 2 9 c o J c v 9 T 6 0 O I G D 4 n U N q M P M v x o 5 9 L 9 H V 6 h 8 L j x 4 / + e L L r 5 6 W y s + + / u b b 7 x a f v 2 g n U a o c 0 X I i P 1 J d m y f C 9 0 L R 0 p 7 2 R T d W g g e 2 L z r 2 9 f q k 3 h k J l X h R e K 7 H s e g F X I a e 6 z l c F 9 R f f G l l 1 m j Y 9 6 y 8 n 3 k / s / y X 7 C j v L y 5 V l 6 v T Y c 5 P 2 G y y Z M x G s / + 8 t G A N I i c N R K g d n y f J F a v G u p d x p T 3 H F 3 n Z S h M R c + e a S 3 F V T E M e i K S X T V e f m z 8 W M j D d S B W / U J t T p W 9 k P E i S c W A X n Q H X w + R h b Y L / V 7 t K t V v v Z V 4 Y p 1 q E z t 0 f u a l v 6 s i c b I U 5 8 J R w t D 8 u J t x R X r F W 0 x l y x R 1 d b F j Z G g i 3 2 N T p c r J g b P u p y L P T 7 U a e 1 d Y q 9 X q F 1 a r 5 w y Y l B r M e V m e V t T e V + t p c T 6 R 4 K O 8 / t V K t V U x W e 1 U x V 1 f n O u N U x f 5 9 J 3 t V d N Z e F 9 2 r 8 9 + U S o j w f n X F 0 h i r m C t v 8 n J 5 2 m i N b o W K s s y a b J H t Z t U 8 z 2 e F K B R w B g 9 S s B W k K O i h 0 J z U p s 8 o k x J R G 2 p D H a g D H U A H U L J K A X W h L l R C J X Q I H U I 9 q A d 9 D 3 0 P v Y Z e Q 3 2 o T / Y P G k B D a E j O A B p B Y 2 g M / Q D 9 A F V Q B U 2 g C T l A q I a S 4 y a H P Y K O o D f Q G + h H 6 E f o G D q G 3 k J v J 3 r H w T v w u 3 + b g w a 0 A V 2 H r k M 3 o B v Q T e g m d A u 6 B d 2 G b k N 3 o D v Q X e g u d A + 6 B 9 2 H 7 k M P o A f Q Q + g h 9 A h 6 B D 2 G H k O b 0 C b 0 B H o C P Y W e Q s + g Z 9 B z 6 D m 0 B W 1 B 2 9 A 2 t A P t Q L v Q L v Q C e g G 9 h F 4 i P P I + P A 7 3 S X p k g z D i I 9 c J I z 9 y g z A C J D c J I 0 F y i z A i J L c J I 0 N y h z B C J H c J I 0 V y j z B i J P c J I 0 f y g D C C J A 8 J I 0 n y i D C i J I 8 J I 0 u y S R h h k i e E k S Z 5 S h h x k m e E k S d 5 T h i B k i 3 C S J R s E 0 a k Z I c w M i W 7 h B E q e U E Y q Z K X h K e x s k J x 4 0 R B w M N B Z o W R C v I r 1 s s s X 7 j a 8 t t C 6 S V m K U 8 O t a U m T 5 N L E 3 t 4 R Z q f d F a W W W 2 Z s Z P a 0 t v G 7 P 7 0 1 P j e + M H 4 y W D G a + O t s W M 0 j Z b h G G P j k / G r 8 V v p 9 9 L n 0 p + l v + 5 a H y 3 M 3 n l p / G e U / v 4 H H h H O A g = = &lt; / l a t e x i t &gt; {h i } N i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z W e Y L p o x / d Y s Y 7 p E F R o p n W u q s G 4 = " &gt; A A A K L n i c b d b L b t t G F A Z g O k 3 b S O n F a b L L h q h R I A U E Q 3 R k R 9 k l 8 v 0 u X 3 S x T c E Y D o c j x r x l O L Q j E 3 y X b t t N n q a L L o p u + x i l Z L n / q V U C g k b f O a Q O Z v 4 F n S T w U 1 2 v / z H 3 6 I v H X 3 7 1 9 Z N K 9 e k 3 3 3 7 3 / f y z H 7 p p n C k u O j w O Y t V 3 W C o C P x I d 7 e t A 9 B M l W O g E o u d c r Y 7 r v W u h U j + O T v U o E Y O Q y c j 3 f M 5 0 S Z f z L 1 7 Z T h y 4 6 S g s v 3 K b u 7 E u f r 6 c X 6 g v 1 i e X O b u w p o s F Y 3 q 1 L 5 9 V 5 m w 3 5 l k o I s 0 D l q Y X V j 3 R g 5 w p 7 f N A F F U 7 S 0 X C + B W T 4 q J c R i w U 6 S C f j F + Y P 5 X i m l 6 s y k + k z Y n S O 3 I W p u M B y 8 6 Q 6 W H 6 s D b G / 6 t d Z N p r D n I / S j I t I n 7 3 R 1 4 W m D o 2 x 3 t h u r 4 S X A e j c s G 4 8 s t Z T T 5 k i n F d 7 l j V d o V X 7 u p k n D w c O U E m i v x 4 s 1 X k j Z V a s 1 m z G v X i Y Z M S 7 r T H a l q 1 l b e 1 5 s p M T 6 x Y J O 8 f t V R v 1 E y r 8 b p m L i / P d C a Z S o L 7 T u t 1 2 d l 4 U 3 Y v z z 5 T K i G i + + n K 0 S y r Z i 6 9 L a r V S a N 9 f S t U n O f 2 e I s c L 6 8 X R T E t x J G A W / A w A 9 t h h o I e C s 1 I b f I b Z V I i 6 k A d K I d y q A t 1 o W R K A f W g H l R C J X Q I H U J 9 q A / 9 A P 0 A v Y J e Q Q N o Q P Y P G k I j a E T O A B p D E 2 g C / Q j 9 C F V Q B U 2 h K T l A q I a S 4 y a H f Q 2 9 h t 5 A b 6 C f o J + g I + g I e g u 9 H e s d h + / B 7 / 9 t D l v Q F n Q V u g p d g 6 5 B 1 6 H r 0 A 3 o B n Q T u g n d g m 5 B t 6 H b 0 B 3 o D n Q X u g v d g + 5 B 9 6 H 7 0 A P o A f Q Q e g h t Q 9 v Q I + g R 9 B h 6 D D 2 B n k B P o a f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 A z 6 B n 0 H H q O 8 M j 7 8 H A W k P T I F m H E R 6 4 S R n 7 k G m E E S K 4 T R o L k B m F E S G 4 S R o b k F m G E S G 4 T R o r k D m H E S O 4 S R o 7 k H m E E S e 4 T R p L k A W F E S R 4 S R p Z k m z D C J I 8 I I 0 3 y m D D i J E 8 I I 0 / y l D A C J T u E k S j Z J Y x I y R 5 h Z E r 2 C S N U 8 o w w U i X P C U 9 i Z U f i h s d h y C I 3 t 6 N Y h c W F N c j t Q H j a D r p C 6 Q X L V r 4 c a l u N f x X l S 5 P 1 8 B V p d t F b W r Q a i 5 Z 1 1 F h 4 1 5 q + P z 0 x X h o / G q 8 M y 3 h j v D O 2 j L b R M b h x a / x i / G r 8 V v l c + b 3 y Z + W v u 9 Z H c 9 N 7 n h v / u S p / / w P h a M 5 q &lt; / l a t e x i t &gt; (·) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z W e Y L p o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>y c j 3 f M 5 0 S Z f z L 1 7 Z T h y 4 6 S g s v 3 K b u 7 E u f r 6 c X 6 g v 1 i e X O b u w p o s F Y 3 q 1 L 5 9 V 5 m w 3 5 l k o I s 0 D l q Y X V j 3 R g 5 w p 7 f N A F F U 7 S 0 X C + B W T 4 q J c R i w U 6 S C f j F + Y P 5 X i m l 6 s y k + k z Y n S O 3 I W p u M B y 8 6 Q 6 W H 6 s D b G / 6 t d Z N p r D n I / S j I t I n 7 3 R 1 4 W m D o 2 x 3 t h u r 4 S X A e j c s G 4 8 s t Z T T 5 k i n F d 7 l j V d o V X 7 u p k n D w c O U E m i v x 4 s 1 X k j Z V a s 1 m z G v X i Y Z M S 7 r T H a l q 1 l b e 1 5 s p M T 6 x Y J O 8 f t V R v 1 E y r 8 b p m L i / P d C a Z S o L 7 T u t 1 2 d l 4 U 3 Y v z z 5 T K i G i + + n K 0 S y r Z i 6 9 L a r V S a N 9 f S t U n O f 2 e I s c L 6 8 X R T E t x J G A W / A w A 9 t h h o I e C s 1 I b f I b Z V I i 6 k A d K I d y q A t 1 o W R K A f W g H l R C J X Q I H U J 9 q A / 9 A P 0 A v Y J e Q Q N o Q P Y P G k I j a E T O A B p D E 2 g C / Q j 9 C F V Q B U 2 h K T l A q I a S 4 y a H f Q 2 9 h t 5 A b 6 C f o J + g I + g I e g u 9 H e s d h + / B 7 / 9 t D l v Q F n Q V u g p d g 6 5 B 1 6 H r 0 A 3 o B n Q T u g n d g m 5 B t 6 H b 0 B 3 o D n Q X u g v d g + 5 B 9 6 H 7 0 A P o A f Q Q e g h t Q 9 v Q I + g R 9 B h 6 D D 2 B n k B P o a f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 A z 6 B n 0 H H q O 8 M j 7 8 H A W k P T I F m H E R 6 4 S R n 7 k G m E E S K 4 T R o L k B m F E S G 4 S R o b k F m G E S G 4 T R o r k D m H E S O 4 S R o 7 k H m E E S e 4 T R p L k A W F E S R 4 S R p Z k m z D C J I 8 I I 0 3 y m D D i J E 8 I I 0 / y l D A C J T u E k S j Z J Y x I y R 5 h Z E r 2 C S N U 8 o w w U i X P C U 9 i Z U f i h s d h y C I 3 t 6 N Y h c W F N c j t Q H j a D r p C 6 Q X L V r 4 c a l u N f x X l S 5 P 1 8 B V p d t F b W r Q a i 5 Z 1 1 F h 4 1 5 q + P z 0 x X h o / G q 8 M y 3 h j v D O 2 j L b R M b h x a / x i / G r 8 V v l c + b 3 y Z + W v u 9 Z H c 9 N 7 n h v / u S p / / w P h a M 5 q &lt; / l a t e x i t &gt; (·) (a) Softmax attention.&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H A Z Z A J T h m B q k O Z h O P W i t 3 O O Z a O k = " &gt; A A A K P 3 i c b d b L b t t G FA Z g O k 3 b S L 0 5 7 b I b I k a A F B A M 0 Z E d Z Z f I 9 7 t 8 0 c U 2 B W M 4 H I 4 Y 8 5 b h 0 I 5 M 8 A n y N N m 2 m 7 5 G X 6 D L o t v u S s l y / x O r B A Q N v 3 N I j W b + x T h J 4 K e 6 X v 9 j 7 t E X j 7 / 8 6 u s n l e o 3 3 3 7 3 / Q / z T 3 / s p n G m u O j w O I h V 3 2 G p C P x I d L S v A 9 F P l G C h E 4 i e c 7 U 6 r v e u h U r 9 O D r V o 0 Q M Q i Y j 3 / M 5 0 y V d z j + 3 n T h w 0 1 F Y f u V 2 M v S L F 5 8 J d 2 N d / H I 5 v 1 B f r E 8 u c 3 Z g T Q c L x v R q X z 6 t z N l u z L N Q R J o H L E 0 v r H q i B z l T 2 u e B K K p 2 l o q E 8 S s m x U U 5 j F g o 0 k E + + T + F + b w U 1 / R i V X 4 i b U 6 U P p G z M B 1 P s O w M m R 6 m D 2 t j / L / a R a a 9 5 i D 3 o y T T I u J 3 P + R l g a l j c 7 w 4 p u s r w X U w K g e M K 7 + c q 8 m H T D G u y y W s 2 q 7 w y m W e T C c P R 0 6 Q i S I / 3 m w V e W O l 1 m z W r E a 9 e N i k h D v t s Z p W b e V 1 r b k y 0 x M r F s n 7 V y 3 V G z X T a r y s m c v L M 5 1 J p p L g v t N 6 W X Y 2 X p X d y 7 P v l E q I 6 H 5 2 5 d Q s q 2 Y u v S 6 q 1 U m j f X 0 r V J z n 9 n i J H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a W Z J s w w i S P C C N N 8 p g w 4 i R P C C N P 8 p Q w A i U 7 h J E o 2 S W M S M k e Y W R K 9 g k j V P K M M F I l z w l P Y m V H 4 o b H Y c g i N 7 e j W I X F h T X I 7 U B 4 2 g 6 6 Q u k F y 1 a + H G p b j e + K 8 t B k P T w i z Q 5 6 S 4 t W Y 9 G y j h o L b 1 r T 8 9 M T 4 2 f j m f H C s I x X x h t j y 2 g b H Y M b H 4 1 P x q / G b 5 X f K 3 9 W / q r 8 f d f 6 a G 7 6 z E / G Z 1 f l n 3 8 B K G 3 W P A = = &lt; / l a t e x i t &gt; (·)&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H A Z Z A J T h m B q k O Z h O P W i t 3 O O Z a O k = " &gt; A AA K P 3 i c b d b L b t t G F A Z g O k 3 b S L 0 5 7 b I b I k a A F B A M 0 Z E d Z Z f I9 7 t 8 0 c U 2 B W M 4 H I 4 Y 8 5 b h 0 I 5 M 8 A n y N N m 2 m 7 5 G X 6 D L o t v u S s l y / x O r B A Q N v 3 N I j W b + x T h J 4 K e 6 X v 9 j 7 t E X j 7 / 8 6 u s n l e o 3 3 3 7 3 / Q / z T 3 / s p n G m u O j w O I h V 3 2 G p C P x I d L S v A 9 F P l G C h E 4 i e c 7 U 6 r v e u h U r 9 O D r V o 0 Q M Q i Y j 3 / M 5 0 y V d z j + 3 n T h w 0 1 F Y f u V 2 M v S L F 5 8 J d 2 N d / H I 5 v 1 B f r E 8 u c 3 Z g T Q c L x v R q X z 6 t z N l u z L N Q R J o H L E 0 v r H q i B z l T 2 u e B K K p 2 l o q E 8 S s m x U U 5 j F g o 0 k E + + T + F + b w U 1 / R i V X 4 i b U 6 U P p G z M B 1 P s O w M m R 6 m D 2 t j / L / a R a a 9 5 i D 3 o y T T I u J 3 P + R l g a l j c 7 w 4 p u s r w X U w K g e M K 7 + c q 8 m H T D G u y y W s 2 q 7 w y m W e T C c P R 0 6 Q i S I / 3 m w V e W O l 1 m z W r E a 9 e N i k h D v t s Z p W b e V 1 r b k y 0 x M r F s n 7 V y 3 V G z X T a r y s m c v L M 5 1 J p p L g v t N 6 W X Y 2 X p X d y 7 P v l E q I 6 H 5 2 5 d Q s q 2 Y u v S 6 q 1 U m j f X 0 r V J z n 9 n i J H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>a W Z J s w w i S P C C N N 8 p g w 4 i R P C C N P 8 p Q w A i U 7 h J E o 2 S W M S M k e Y W R K 9 g k j V P K M M F I l z w l P Y m V H 4 o b H Y c g i N 7 e j W I X F h T X I 7 U B 4 2 g 6 6 Q u k F y 1 a + H G p b j e + K 8 t B k P T w i z Q 5 6 S 4 t W Y 9 G y j h o L b 1 r T 8 9 M T 4 2 f j m f H C s I x X x h t j y 2 g b H Y M b H 4 1 P x q / G b 5 X f K 3 9 W / q r 8 f d f 6 a G 7 6 z E / G Z 1 f l n 3 8 B K G 3 W P A = = &lt; / l a t e x i t &gt; (·) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E K G F j e p b x 5 c a f J 1 5 n y I y H U x E J U c = " &gt; A A A K H n i c b d Z J b 9 t G G A Z g O t 0 i d U n S H n s h a h R I A c H Q O L K j 3 B J 5 3 + V F i 2 0 K w X A 0 H N H m l u H Q j k z w P / T a X v p r e i x 6 b f 9 N K V n u + 9 U q A Q H D 5 / t I j m b e w 7 h J 4 K e m X v 9 7 4 c k n n 3 7 2 + R d P K 9 U v v / r 6 m 2 f P X 3 z b T e N M C 9 k R c R D r v s t T G f i R 7 B j f B L K f a M l D N 5 A 9 9 3 p t U u / d S J 3 6 c X R m x o k c h F x F v u c L b k r q O u r o 5 c F P 7 5 8 v 1 p f q 0 8 u e H 7 D Z Y N G a X e 3 3 L y o L z j A W W S g j I w K e p p e s n p h B z r X x R S C L q p O l M u H i m i t 5 W Q 4 j H s p 0 k E + n W 9 g / l j K 0 v V i X v 8 j Y U 6 V P 5 D x M 0 3 H o l p 0 h N 6 P 0 c W 2 C / 1 e 7 z I z X H O R + l G R G R u L + Q 1 4 W 2 C a 2 J / / d H v p a C h O M y w E X 2 i / n a o s R 1 1 y Y c o W q z l B 6 5 S p O p 5 O H Y z f I Z J G f b L W K v L F a a z Z r r F E v H j d p O Z z 1 s C a r r b 6 p N V f n e m L N I / X w q u V 6 o 2 a z x q u a v b I y 1 5 l k O g k e O t m r s r P x u u x e m X + n 0 l J G D 7 M r p 8 Z Y z V 5 + U 1 S r 0 0 b n 5 k 7 q O M + d y R K 5 X l 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>9 j 5 U T y V s R h y K N h 7 k S x D o t L N s i d Q H r G C b p S m 0 X m a F + N j K M n d 0 V 5 a G K P j 0 j z g 9 7 y E m s s M X b c W H z b m p 2 f n l r f W z 9 Y L y 1 m v b b e W t t W 2 + p Y w r q y f r Z + s X 6 t / F b 5 v f J H 5 c / 7 1 i c L s 2 e + s / 5 z V f 7 6 B 4 b o x 3 s = &lt; / l a t e x i t &gt; O(M ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M A 8 A Z n 8 N / V o N T E G j 9 R t Q q 4 F J F z 0 = " &gt; A A A K H n i c b d Z J b 9 t G G A Z g O t 0 i d U n S H n s h a h R I A c H Q O L K j 3 B J 5 3 + V F i 2 0 K w X A 0 H N H m l u H Q j k z w P / T a X v p r e i x 6 b f 9 N K V n u + 9 U q A Q H D 5 / t I j m b e w 7 h J 4 K e m X v 9 7 4 c k n n 3 7 2 + R d P K 9 U v v / r 6 m 2 f P X 3 z b T e N M C 9 k R c R D r v s t T G f i R 7 B j f B L K f a M l D N 5 A 9 9 3 p t U u / d S J 3 6 c X R m x o k c h F x F v u c L b k r q O u r o 5 e F P 7 5 8 v 1 p f q 0 8 u e H 7 D Z Y N G a X e 3 3 L y o L z j A W W S g j I w K e p p e s n p h B z r X x R S C L q p O l M u H i m i t 5 W Q 4 j H s p 0 k E + n W 9 g / l j K 0 v V i X v 8 j Y U 6 V P 5 D x M 0 3 H o l p 0 h N 6 P 0 c W 2 C / 1 e 7 z I z X H O R + l G R G R u L + Q 1 4 W 2 C a 2 J / / d H v p a C h O M y w E X 2 i / n a o s R 1 1 y Y c o W q z l B 6 5 S p O p 5 O H Y z f I Z J G f b L W K v L F a a z Z r r F E v H j d p O Z z 1 s C a r r b 6 p N V f n e m L N I / X w q u V 6 o 2 a z x q u a v b I y 1 5 l k O g k e O t m r s r P x u u x e m X + n 0 l J G D 7 M r p 8 Z Y z V 5 + U 1 S r 0 0 b n 5 k 7 q O M + d y R K 5 X l 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>9 j 5 U T y V s R h y K N h 7 k S x D o t L N s i d Q H r G C b p S m 0 X m a F + N j K M n d 0 V 5 a G K P j 0 j z g 9 7 y E m s s M X b c W H z b m p 2 f n l r f W z 9 Y L y 1 m v b b e W t t W 2 + p Y w r q y f r Z + s X 6 t / F b 5 v f J H 5 c / 7 1 i c L s 2 e + s / 5 z V f 7 6 B 5 C e x 3 w = &lt; / l a t e x i t &gt; O(N ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3 k f S 6 H 5 o f O X / m R p F 1 7 d 8 R U q W C 4 = " &gt; A A A K L X i c b d Z J b 9 t G G A Z g O t 0 i p Y v T H H s h a h T o Q T A 0 j u w o h w K J v O / y o s U 2 V W F I D U e M u X k 4 l C M T / C 2 9 t p f + m l w C B L 3 2 b 5 S S 5 b 5 f r Q 4 g Y P h 8 H 6 n B z H s Y O / a 9 R F e r Hx e e f P b 5 F 1 9 + 9 b R U f v b 1 N 9 9 + t / j 8 + 3 Y S p c o R L S f y I 9 W 1 e S J 8 L x Q t 7 W l f d G M l e G D 7 o m N f r 0 / q n Z F Q i R e F 5 3 o c i 1 7 A Z e i 5 n s N 1 Q f 3 F F 1 Z m j W 7 6 n p X 3 M + 8 X l v + a H e X 9 x a X q c n U 6 z P k J m 0 2 W j N l o 9 p + X F q x B 5 K S B C L X j 8 y S 5 Y t V Y 9 z K u t O f 4 I i 9 b a S J i 7 l x z K a 6 K a c g D k f S y 6 e p z 8 6 d C B q Y b q e I X a n O q 9 I 2 M B 0 k y D u y i M + B 6 m D y u T f D / a l e p d u u 9 z A v j V I v Q u f 8 j N / V N H Z m T r T A H n h K O 9 s f F h D v K K 9 Z q O k O u u K O L D S t b A + E W m z p d T h a M b T 8 V e X a 6 3 c i z 2 l q l X q + w W j V / 3 K T E Y N b D 6 q y y 9 r p S X 5 v r i R Q P 5 c O n V q q 1 i s l q L y v m 6 u p c Z 5 y q 2 H / o Z C + L z t q r o n t 1 / p t S C R E + r K 5 Y G m M V c + V 1 X i 5 P G 6 3 R n V B R l l m T L b L d r J r n + a w Q h Q L O 4 E E K t o I U B T 0 U m p P a 9 B l l U i J q Q 2 2 o A 3 W g A + g A S l Y p o C 7 U h U q o h A 6 h Q 6 g H 9 a D v o O + g 1 9 B r q A / 1 y f 5 B A 2 g I D c k Z Q C N o D I 2 h N 9 A b q I I q a A J N y A F C N Z Q c N z n s E X Q E v Y X e Q t 9 D 3 0 P H 0 D H 0 D n o 3 0 X s O 3 o L f / t s c N K A N 6 D p 0 H b o B 3 Y B u Q j e h W 9 A t 6 D Z 0 G 7 o D 3 Y H u Q n e h e 9 A 9 6 D 5 0 H 3 o A P Y A e Q g + h R 9 A j 6 D H 0 G N q E N q E n 0 B P o K f Q U e g Y 9 g 5 5 D z 6 E t a A v a h r a h H W g H2 o V 2 o R f Q C + g l 9 B L h k Q / h c b h P 0 i M b h B E f u U 4 Y + Z E b h B E g u U k Y C Z J b h B E h u U 0 Y G Z I 7 h B E i u U s Y K Z J 7 h B E j u U 8 Y O Z I H h B E k e U g Y S Z J H h B E l e U w Y W Z J N w g i T P C G M N M l T w o i T P C O M P M l z w g i U b B F G o m S b M C I l O 4 S R K d k l j F D J C 8 J I l b w k P I 2 V F Y p b J w o C H g 4 y K 4 x U k F + x X m b 5 w t W W 3 x Z K L z F L e X K oL T V 5 m l y a 2 O M r 0 v y k s 7 L M a s u M n d S W 3 j R m 9 6 e n x g / G j 8 b P B j N e G W + M H a N p t A z H G B u / G b 8 b f 5 T + L H 0 o f S r 9 d d / 6 Z G H 2 z g v j P 6 P 0 9 z 9 1 7 M 4 L &lt; / l a t e x i t &gt; {q i } N i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 H + N 8 b 0 t Y v / J e C 1 w 8 N x p 5 Y 5 / J + U = " &gt; A A A K L X i c b d Z J b 9 t G G A Z g O u k S q Z u T H H s h a h T o Q T A 0 j u w o h w K J v O / y o s U 2 V W F I D U e M u G U 4 l C M T / C 2 9 t p f + m l 4 K F L 3 2 b 5 S S 5 b 5 f r Q 4 g Y P h 8 H 6 n B z H s Y O / a 9 R F e r f y w 9 e f r J p 5 9 9 / q x U / u L L r 7 7 + Z v n 5 i 3 Y S p c o R L S f y I 9 W 1 e S J 8 L x Q t 7 W l f d G M l e G D 7 o m O P N q f 1 z l i o x I v C S z 2 J R S / g M v R c z + G 6 o P 7 y S y u z x q O + Z + X 9 z P u R 5 T 9 l x 3 l / e a W 6 W p 0 N c 3 H C 5 p M V Y z 6 a / e e l J W s Q O W k g Q u 3 4 P E l u W D X W v Y w r 7 T m + y M t W m o i Y O y M u x U 0 x D X k g k l 4 2 W 3 1 u f l / I w H Q j V f x C b c 6 U v p H x I E k m g V 1 0 B l w P k 8 e 1 K f 5 f 7 S b V b r 2 X e W G c a h E 6 9 3 / k p r 6 p I 3 O 6 F e b A U 8L R / q S Y c E d 5 x V p N Z 8 g V d 3 S x Y W V r I N x i U 2 f L y Y K J 7 a c i z 8 5 3 G 3 l W 2 6 j U 6 x V W q + a P m 5 Q Y z H t Y n V U 2 3 l T q G w s 9 k e K h f P j U W r V W M V n t V c V c X 1 / o j F M V + w + d 7 F X R W X t d d K 8 v f l M q I c K H 1 R V L Y 6 x i r r 3 J y + V Z o z W + E y r K M m u 6 R b a b V f M 8 n x e i U M A Z P E j B V p C i o I d C c 1 K b P a N M S k R t q A 1 1 o A 5 0 A B 1 A y S o F 1 I W 6 U A m V 0 C F 0 C P W g H v Q 9 9 D 1 0 B B 1 B f a h P 9 g 8 a Q E N o S M 4 A G k F j a A z 9 A P 0 A V V A F T a A J O U C o h p L j J o c 9 h o 6 h t 9 B b 6 E f o R + g E O o H e Q e + m e s / B O / C 7 f 5 u D B r Q B 3 Y R u Q r e g W 9 B t 6 D Z 0 B 7 o D 3 Y X u Q v e g e 9 B 9 6 D 7 0 A H o A P Y Q e Q o + g R 9 B j 6 D H 0 B H o C P Y W e Q p v Q J v Q M e g Y 9 h 5 5 D L 6 A X 0 E v o J b Q F b U H b 0 D a 0 A + 1 A u 9 A u 9 A p 6 B b 2 G X i M 8 8 i E 8 D v d J e m S D M O I j N w k j P 3 K L M A I k t w k j Q X K H M C I k d w k j Q 3 K P M E I k 9 w k j R f K A M G I k D w k j R / K I M I I k j w k j S f K E M K I k T w k j S 7 J J G G G S Z 4 S R J n l O G H G S F 4 S R J 3 l J G I G S L c J I l G w T R q R k h z A y J b u E E S p 5 R R i p k t e E Z 7 G y Q n H r R E H A w 0 F m h Z E K 8 h v Wy y x f u N r y 2 0 L p F W Y p T w 6 1 p a Z P 0 0 s T e 3 x F W p x 0 1 l Z Z b Z W x s 9 r K 2 8 b 8 / v T M + N b 4 z v j B Y M Z r 4 6 2 x Z z S N l u E Y E + N n 4 x f j 1 9 J v p d 9 L f 5 b + u m 9 9 s j R / 5 6 X x n 1 H 6 + x 8 x p M 4 E &lt; / l a t e x i t &gt; {k i } M i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O e G p z y C c y 6 ZC o 0 o E D G M V u J c b Z C I = " &gt; A A A K L X i c b d b L b t t G F A Z g O k n b S O n F S Z b d E D U K d C E Y G k d 2 l E W B R L 7 f 5 Y s u t q k K Q 2 o 4 Y s x b h k M 5 M s Fn y b b d 9 G m y K V B 0 2 9 c o J c v 9 T 6 0 O I G D 4 n U N q M P M v x o 5 9 L 9 H V 6 h 8 L j x 4 / + e L L r 5 6 W y s + + / u b b 7 x a f v 2 g n U a o c 0 X I i P 1 J d m y f C 9 0 L R 0 p 7 2 R T d W g g e 2 L z r 2 9 f q k 3 h k J l X h R e K 7 H s e g F X I a e 6 z l c F 9 R f f G l l 1 m j U 9 6 y 8 n 3 k / s / y X 7 D D v L y 5 V l 6 v T Y c 5 P 2 G y y Z M x G s / + 8 t G A N I i c N R K g d n y f J F a v G u p d x p T 3 H F 3 n Z S h M R c + e a S 3 F V T E M e i K S X T V e f m z 8W M j D d S B W / U J t T p W 9 k P E i S c W A X n Q H X w + R h b Y L / V 7 t K t V v v Z V 4 Y p 1 q E z t 0 f u a l v 6 s i c b I U 5 8 J R w t D 8 u J t x R X r F W 0 x l y x R 1 d b F j Z G g i3 2 N T p c r J g b P u p y L P T 7 U a e 1 d Y q 9 X q F 1 a r 5 w y Y l B r M e V m e V t T e V + t p c T 6 R 4 K O 8 / t V K t V U x W e 1 U x V 1 f n O u N U x f 5 9 J 3 t V d N Z e F 9 2 r 8 9 + U S o j w f n X F 0 h i r m C t v 8 n J 5 2 m i N b o W K s s y a b J H t Z t U 8 z 2 e F K B R w B g 9 S s B W k K O i h 0 J z U p s 8 o k x J R G 2 p D H a g D H U A H U L J K A X W h L l R C J X Q I H U I 9 q A d 9 D 3 0 P v Y Z e Q 3 2 o T / Y P G k B D a E j O A B p B Y 2 g M / Q D 9 A F V Q B U 2 g C T l A q I a S 4 y a H P Y K O o D f Q G + h H 6 E f o G D q G 3 k J v J 3 r H w T v w u 3 + b g w a 0 A V 2 H r k M 3 o B v Q T e g m d A u 6 B d 2 G b k N 3 o D v Q X e g u d A + 6 B 9 2 H 7 k M P o A f Q Q + g h 9 A h 6 B D 2 G H k O b 0 C b 0 B H o C P Y W e Q s + g Z 9 B z 6 D m 0 B W 1 B 2 9 A 2 t A P t Q L v Q L v Q C e g G 9 h F 4 i P P I + P A 7 3 S X p k g z D i I 9 c J I z 9 y g z A C J D c J I 0 F y i z A i J L c J I 0 N y h z B C J H c J I 0 V y j z B i J P c J I 0 f y g D C C J A 8 J I 0 n y i D C i J I 8 J I 0 u y S R h h k i e E k S Z 5 S h h x k m e E k S d 5 T h i B k i 3 C S J R s E 0 a k Z I c w M i W 7 h B E q e U E Y q Z K X h K e x s k J x 4 0 R B w M N B Z o W R C v I r 1 s s s X 7 j a 8 t t C 6 S V m K U 8 O t a U m T 5 N L E 3 t 4 R Z q f d F a W W W 2 Z s Z P a 0 t v G 7 P 7 0 1 P j e + M H 4 y W D G a + O t s W M 0 j Z b h G G P j k / G r 8 V v p 9 9 L n 0 p + l v + 5 a H y 3 M 3 n l p / G e U / v 4 H n Q X O D w = = &lt; / l a t e x i t &gt; {v i } M i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m D v r 6 a 7 P Y g P p F H Q f A F d 1 u G l y T d k = " &gt; A A A K L X i c b d b L b t t G F A Z g O k n b S O n F S Z b d E D U K d C E Y G k d 2 l E W B R L 7 f 5 Y s u t q k K Q 2 o 4 Y s x b h k M 5 M s F n y b b d 9 G m y K V B 0 2 9 c o J c v 9 T 6 0 O I G D 4 n U N q M P M v x o 5 9 L 9 H V 6 h 8 L j x 4 / + e L L r 5 6 W y s + + / u b b 7 x a f v 2 g n U a o c 0 X I i P 1 J d m y f C 9 0 L R 0 p 7 2 R T d W g g e 2 L z r 2 9 f q k 3 h k J l X h R e K 7 H s e g F X I a e 6 z l c F 9 R f f G l l 1 m j Y 9 6 y 8 n 3 k / s / y X 7 C j v L y 5 V l 6 v T Y c 5 P 2 G y y Z M x G s / + 8 t G A N I i c N R K g d n y f J F a v G u p d x p T 3 H F 3 n Z S h M R c + e a S 3 F V T E M e i K S X T V e f m z 8 W M j D d S B W / U J t T p W 9 k P E i S c W A X n Q H X w + R h b Y L / V 7 t K t V v v Z V 4 Y p 1 q E z t 0 f u a l v 6 s i c b I U 5 8 J R w t D 8 u J t x R X r F W 0 x l y x R 1 d b F j Z G g i 3 2 N T p c r J g b P u p y L P T 7 U a e 1 d Y q 9 X q F 1 a r 5 w y Y l B r M e V m e V t T e V + t p c T 6 R 4 K O 8 / t V K t V U x W e 1 U x V 1 f n O u N U x f 5 9 J 3 t V d N Z e F 9 2 r 8 9 + U S o j w f n X F 0 h i r m C t v 8 n J 5 2 m i N b o W K s s y a b J H t Z t U 8 z 2 e F K B R w B g 9 S s B W k K O i h 0 J z U p s 8 o k x J R G 2p D H a g D H U A H U L J K A X W h L l R C J X Q I H U I 9 q A d 9 D 3 0 P v Y Z e Q 3 2 o T / Y P G k B D a E j O A B p B Y 2 g M / Q D 9 A F V Q B U 2 g C T l A q I a S 4 y a H P Y K O o D f Q G + h H 6 E f o G D q G 3 k J v J 3 r H w T v w u 3 + b g w a 0 A V 2 H r k M 3 o B v Q T e g m dA u 6 B d 2 G b k N 3 o D v Q X e g u d A + 6 B 9 2 H 7 k M P o A f Q Q + g h 9 A h 6 B D 2 G H k O b 0 C b 0 B H o C P Y W e Q s + g Z 9 B z 6 D m 0 B W 1 B 2 9 A 2 t A P t Q L v Q L v Q C e g G 9 h F 4 i P P I + P A 7 3 S X p k g z D i I 9 c J I z 9 y g z A C J D c J I 0 F y i z A i J L c J I 0 N y h z B C J H c J I 0 V y j z B i J P c J I 0 f y g D C C J A 8 J I 0 n y i D C i J I 8 J I 0 u y S R h h k i e E k S Z 5 S h h x k m e E k S d 5 T h i B k i 3 C S J R s E 0 a k Z I c w M i W 7 h B E q e U E Y q Z K X h K e x s k J x 4 0 R B w M N B Z o W R C v I r 1 s s s X 7 j a 8 t t C 6 S V m K U 8 O t a U m T 5 N L E 3 t 4 R Z q f d F a W W W 2 Z s Z P a 0 t v G 7 P 7 0 1 P j e + M H 4 y W D G a + O t s W M 0 j Z b h G G P j k / G r 8 V v p 9 9 L n 0 p + l v + 5 a H y 3 M 3 n l p / G e U / v 4 H H h H O A g = = &lt; / l a t e x i t &gt; {h i } N i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z W e Y L p o x / d Y s Y 7 p E F R o p n W u q s G 4 = " &gt; A A A K L n i c b d b L b t t G F A Z g O k 3 b S O n F a b L L h q h R I A U E Q 3 R k R 9 k l 8 v 0 u X 3 S x T c E Y D o c j x r x l O L Q j E 3 y X b t t N n q a L L o p u + x i l Z L n / q V U C g k b f O a Q O Z v 4 F n S T w U 1 2 v / z H 3 6 I v H X 3 7 1 9 Z N K 9 e k 3 3 3 7 3 / f y z H 7 p p n C k u O j w O Y t V 3 W C o C P x I d 7 e t A 9 B M l W O g E o u d c r Y 7 r v W u h U j + O T v U o E Y O Q y c j 3 f M 5 0 S Z f z L 1 7 Z T h y 4 6 S g s v 3 K b u 7 E u f r 6 c X 6 g v 1 i e X O b u w p o s F Y 3 q 1 L 5 9 V 5 m w 3 5 l k o I s 0 D l q Y X V j 3 R g 5 w p 7 f N A F F U 7 S 0 X C + B W T 4 q J c R i w U 6 S C f j F + Y P 5 X i m l 6 s y k + k z Y n S O 3 I W p u M B y 8 6 Q 6 W H 6 s D b G / 6 t d Z N p r D n I / S j I t I n 7 3 R 1 4 W m D o 2 x 3 t h u r 4 S X A e j c s G 4 8 s t Z T T 5 k i n F d 7 l j V d o V X 7 u p k n D w c O U E m i v x 4 s 1 X k j Z V a s 1 m z G v X i Y Z M S 7 r T H a l q 1 l b e 1 5 s p M T 6 x Y J O 8 f t V R v 1 E y r 8 b p m L i / P d C a Z S o L 7 T u t 1 2 d l 4 U 3 Y v z z 5 T K i G i + + n K 0 S y r Z i 6 9 L a r V S a N 9 f S t U n O f 2 e I s c L 6 8 X R T E t x J G A W / A w A 9 t h h o I e C s 1 I b f I b Z V I i 6 k A d K I d y q A t 1 o W R K A f W g H l R C J X Q I H U J 9 q A / 9 A P 0 A v Y J e Q Q N o Q P Y P G k I j a E T O A B p D E 2 g C / Q j 9 C F V Q B U 2 h K T l A q I a S 4 y a H f Q 2 9 h t 5 A b 6 C f o J + g I + g I e g u 9 H e s d h + / B 7 / 9 t D l v Q F n Q V u g p d g 6 5 B 1 6 H r 0 A 3 o B n Q T u g n d g m 5 B t 6 H b 0 B 3 o D n Q X u g v d g + 5 B 9 6 H 7 0 A P o A f Q Q e g h t Q 9 v Q I + g R 9 B h 6 D D 2 B n k B P o a f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 A z 6 B n 0 H H q O 8 M j 7 8 H A W k P T I F m H E R 6 4 S R n 7 k G m E E S K 4 T R o L k B m F E S G 4 S R o b k F m G E S G 4 T R o r k D m H E S O 4 S R o 7 k H m E E S e 4 T R p L k A W F E S R 4 S R p Z k m z D C J I 8 I I 0 3 y m D D i J E 8 I I 0 / y l D A C J T u E k S j Z J Y x I y R 5 h Z E r 2 C S N U 8 o w w U i X P C U 9 i Z U f i h s d h y C I 3 t 6 N Y h c W F N c j t Q H j a D r p C 6 Q X L V r 4 c a l u N f x X l S 5 P 1 8 B V p d t F b W r Q a i 5 Z 1 1 F h 4 1 5 q + P z 0 x X h o / G q 8 M y 3 h j v D O 2 j L b R M b h x a / x i / G r 8 V v l c + b 3 y Z + W v u 9 Z H c 9 N 7 n h v / u S p / / w P h a M 5 q &lt; / l a t e x i t &gt; (·) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z W e Y L p o x / d Y s Y 7 p E F R o p n W u q s G 4 = " &gt; A A A K L n i c b d b L b t t G F A Z g O k 3 b S O n F a b L L h q h R I A U E Q 3 R k R 9 k l 8 v 0 u X 3 S x T c E Y D o c j x r x l O L Q j E 3 y X b t t N n q a L L o p u + x i l Z L n / q V U C g k b f O a Q O Z v 4 F n S T w U 1 2 v / z H 3 6 I v H X 3 7 1 9 Z N K 9 e k 3 3 3 7 3 / f y z H 7 p p n C k u O j w O Y t V 3 W C o C P x I d 7 e t A 9 B M l W O g E o u d c r Y 7 r v W u h U j + O T v U o E Y O Q y c j 3 f M 5 0 S Z f z L 1 7 Z T h y 4 6 S g s v 3 K b u 7 E u f r 6 c X 6 g v 1 i e X O b u w p o s F Y 3 q 1 L 5 9 V 5 m w 3 5 l k o I s 0 D l q Y X V j 3 R g 5 w p 7 f N A F F U 7 S 0 X C + B W T 4 q J c R i w U 6 S C f j F + Y P 5 X i m l 6 s y k + k z Y n S O 3 I W p u M B y 8 6 Q 6 W H 6 s D b G / 6 t d Z N p r D n I / S j I t I n 7 3 R 1 4 W m D o 2 x 3 t h u r 4 S X A e j c s G 4 8 s t Z T T 5 k i n F d 7 l j V d o V X 7 u p k n D w c O U E m i v x 4 s 1 X k j Z V a s 1 m z G v X i Y Z M S 7 r T H a l q 1 l b e 1 5 s p M T 6 x Y J O 8 f t V R v 1 E y r 8 b p m L i / P d C a Z S o L 7 T u t 1 2 d l 4 U 3 Y v z z 5 T K i G i + + n K 0 S y r Z i 6 9 L a r V S a N 9 f S t U n O f 2 e I s c L 6 8 X R T E t x J G A W / A w A 9 t h h o I e C s 1 I b f I b Z V I i 6 k A d K I d y q A t 1 o W R K A f W g H l R C J X Q I H U J 9 q A / 9 A P 0 A v Y J e Q Q N o Q P Y P G k I j a E T O A B p D E 2 g C / Q j 9 C F V Q B U 2 h K T l A q I a S 4 y a H f Q 2 9 h t 5 A b 6 C f o J + g I + g I e g u 9 H e s d h + / B 7 / 9 t D l v Q F n Q V u g p d g 6 5 B 1 6 H r 0 A 3 o B n Q T u g n d g m 5 B t 6 H b 0 B 3 o D n Q X u g v d g + 5 B 9 6 H 7 0 A P o A f Q Q e g h t Q 9 v Q I + g R 9 B h 6 D D 2 B n k B P o a f Q D r Q D 7 U K 7 0 B 6 0 B + 1 D + 9 A z 6 B n 0 H H q O 8 M j 7 8 H A W k P T I F m H E R 6 4 S R n 7 k G m E E S K 4 T R o L k B m F E S G 4 S R o b k F m G E S G 4 T R o r k D m H E S O 4 S R o 7 k H m E E S e 4 T R p L k A W F E S R 4 S R p Z k m z D C J I 8 I I 0 3 y m D D i J E 8 I I 0 / y l D A C J T u E k S j Z J Y x I y R 5 h Z E r 2 C S N U 8 o w w U i X P C U 9 i Z U f i h s d h y C I 3 t 6 N Y h c W F N c j t Q H j a D r p C 6 Q X L V r 4 c a l u N f x X l S 5 P 1 8 B V p d t F b W r Q a i 5 Z 1 1 F h 4 1 5 q + P z 0 x X h o / G q 8 M y 3 h j v D O 2 j L b R M b h x a / x i / G r 8 V v l c + b 3 y Z + W v u 9 Z H c 9 N 7 n h v / u S p / / w P h a M 5 q &lt; / l a t e x i t &gt; (·) (b) Random feature attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 1 :</head><label>1</label><figDesc>Computation graphs for softmax attention (left) and random feature attention (right). Here, we assume cross attention with source length M and target length N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 :</head><label>2</label><figDesc>Conditional decoding speed (left) and memory overhead (right) varying the output lengths. All models are tested on a single TPU v2 accelerator, with greedy decoding and batch size 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 :</head><label>3</label><figDesc>Unconditional decoding speed (left) and memory overhead (right) varying the output lengths. All models are tested on a single TPU v2 accelerator, with greedy decoding and batch size 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Varying causal attention φ sizes while fixing that of cross attention to be 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 4 :</head><label>4</label><figDesc>Finetuning an RFA-Gaussian model with its parameters initialized from a pretrained softmax-transformer. "Reset" indicates resetting the multihead attention parameters to randomlyinitialized ones. The dashed line indicates the training loss of the pretrained model. resets the multihead attention parameters without changing the attention functions. It converges to the pretraining loss in less than 200 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Machine translation test set BLEU. The decoding speed (last column) is relative to BASE. All models are tested on a single TPU v2 accelerator, with batch size 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Accuracy (higher is better) of different models on LO, IMDb, and AAN, along with their</cell></row><row><cell>speed (higher is better) and peak memory consumption (lower is better) varying sequence lengths</cell></row><row><cell>(1-4K). Speed and memory are evaluated on the IMDb dataset and relative to the transformer's.</cell></row><row><cell>Bold font indicates the best performance in each column, and underlined numbers outperform the</cell></row><row><cell>transformer in accuracy. Transformer's and previous works' numbers are due to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>considers a sequence-to-sequence model, and breaks down the comparisons to training (with teacher forcing;<ref type="bibr" target="#b73">Williams &amp; Zipser, 1989</ref>) and autoregressive decoding. Here we assume enough threads to fully parallelize softmax attention across timesteps when the inputs are revealed to the model in full. RFA has a lower space complexity, since it never explicitly populates the attention matrices. As for time, RFA trains in linear time, and so does the softmax attention: in teacher-forcing training a standard transformer decoder parallelizes the attention computation across time steps. The trend of the time comparison differs during decoding: when only one output token is produced at a time, RFA decodes linearly in the output length, while softmax attention decodes quadratically.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Time Complexity</cell><cell cols="3">Space Complexity</cell></row><row><cell>Setting</cell><cell>Model</cell><cell>Encoder</cell><cell>Cross</cell><cell>Causal</cell><cell>Encoder</cell><cell>Cross</cell><cell>Causal</cell></row><row><cell>Training w/</cell><cell>softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>teacher forcing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Time and space complexity comparisons between RFA and its softmax counterpart in a sequence-to-sequence attentive model, assuming an infinite amount of available threads. M and N denote the lengths of the source and target sequences respectively. Teacher forcing training<ref type="bibr" target="#b73">(Williams &amp; Zipser, 1989</ref>) and autoregressive decoding are assumed. Blue color indicates the cases where RFA asymptotically outperforms softmax attention.</figDesc><table><row><cell>Data</cell><cell cols="2">Train Dev.</cell><cell cols="2">Test Vocab.</cell></row><row><cell>WikiText-103</cell><cell cols="3">103M 218K 246K</cell><cell>268K</cell></row><row><cell>WMT14 EN-DE</cell><cell>4.5M</cell><cell>3K</cell><cell>3K</cell><cell>32K</cell></row><row><cell>WMT14 EN-FR</cell><cell>4.5M</cell><cell>3K</cell><cell>3K</cell><cell>32K</cell></row><row><cell cols="2">IWSLT14 DE-EN 160K</cell><cell>7K</cell><cell>7K</cell><cell>9K/7K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Some statistics for the datasets. WikiText-103 split sizes are in number of tokens, while others are in number of instances.</figDesc><table /><note>B EXPERIMENTAL DETAILS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>summarizes some statistics of the datasets used in our experiments. Our implementation is based on JAX.15    </figDesc><table><row><cell># Random Matrices</cell><cell>1</cell><cell>50</cell><cell>100 200</cell></row><row><cell>BLEU</cell><cell cols="3">24.0 25.7 25.8 25.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>WMT14 EN-DE development set performance varying the number of random matrices to sample from during training. No beam search or checkpoint averaging is used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>× 10 −4 , 2.5 × 10 −4 , 5 × 10 −4 ]</figDesc><table><row><cell>Hyperprams.</cell><cell></cell><cell>Small</cell><cell>Big</cell></row><row><cell># Layers</cell><cell></cell><cell>6</cell><cell>16</cell></row><row><cell># Heads</cell><cell></cell><cell>8</cell><cell>16</cell></row><row><cell>Embedding Size</cell><cell></cell><cell>512</cell><cell>1024</cell></row><row><cell>Head Size</cell><cell></cell><cell>64</cell><cell>64</cell></row><row><cell>FFN Size</cell><cell></cell><cell>2048</cell><cell>4096</cell></row><row><cell>Batch Size</cell><cell></cell><cell>64</cell><cell>64</cell></row><row><cell cols="2">Learning Rate [1 Warmup Steps</cell><cell>6000</cell><cell>6000</cell></row><row><cell>Gradient Clipping Norm</cell><cell></cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>Dropout</cell><cell cols="2">[0.05, 0.1]</cell><cell>[0.2, 0.25, 0.3]</cell></row><row><cell>Random Feature Map Size</cell><cell></cell><cell>64</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters used in the language modeling experiments.</figDesc><table /><note>B.2 MACHINE TRANSLATION</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters used in the machine translation experiments. C MORE ANALYSIS RESULTS C.1 MORE RESULTS ON DECODING SPEED AND MEMORY OVERHEAD</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>WMT14 EN-DE development set performance of RFA-Gaussian (the size of φ is 2D; §2.2) varying the random feature sizes. N/A indicates training does not converge. No beam search or checkpoint averaging is used.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">M = N in self-attention; they may differ, e.g., in the cross attention of a sequence-to-sequence model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This can be achieved by 2-normalizing the query and keys. See §3.3 for a related discussion. 3 It is also sometimes called "decoder self-attention" or "autoregressive attention."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In multihead attention<ref type="bibr" target="#b71">(Vaswani et al., 2017)</ref>, kt and vt are calculated from xt using learned affine transformations.5  This departs from Eq. 2 by lifting the isotropic assumption imposed on the Gaussian distribution: note the difference between the vector σ in Eq. 8 and the scalar σ in Eq. 3. We find this improves the performance in practice ( §4), even though the same result in Theorem 1 may not directly apply.6  Apart from replacing the sinusoid functions with ReLU, it constructs wi in the same way as Eq. 8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">A causal masking is usually used to prevent the model from accessing future tokens in language models. 8 RFA never constructs the M × 2D × d tensor [φ(ki) ⊗ vi]i, but sequentially processes the sequence. 9 This gating technique is specific to RFA variants, in the sense that it is less intuitive to apply it in BASE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://github.com/google-research/long-range-arena</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://github.com/google/jax.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://github.com/mjpost/sacrebleu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Phil Blunsom, Chris Dyer, Nando de Freitas, Jungo Kasai, Adhiguna Kuncoro, Dianqi Li, Ofir Press, Lianhui Qin, Swabha Swayamdipta, Sam Thomson, the language team at DeepMind and the ARK group at the University of Washington for their helpful feedback. We also thank Tay Yi for helping run the Long Range Arena experiments, Richard Tanburn for the advice on implementations, and the anonymous reviewers for their thoughtful comments. This work was supported in part by NSF grant 1562364 and a Google Fellowship. Nikolaos Pappas was supported by the Swiss National Science Foundation under grant number P400P2 183911 "UNISON."</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Causal random feature attention.</p><p>z is a D-dimensional vector 4:</p><p>S, z ← 0, 0 5:</p><p>10:</p><p>end for 11:</p><p>12: end procedure Algorithm 2 Cross random feature attention.</p><p>S is a D × d matrix 3:</p><p>z is a D-dimensional vector 4:</p><p>S, z ← 0, 0 5:</p><p>end for 10:</p><p>for i = 1 to N do 11:</p><p>end for 14:</p><p>15: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 VARIANCE OF RANDOM FOURIER FEATURES</head><p>The following result is due to <ref type="bibr" target="#b77">Yu et al. (2016)</ref>. Using the same notation as in §2.2:</p><p>Var(φ (x) · φ (y)) = 1 2D 1 − e −z 2 2 ,</p><p>where z = x − y /σ.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ETC: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical study on the properties of random bases for kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quasi-Monte Carlo feature maps for shift-invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Haim Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">120</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster kernel ridge regression using sketching and preconditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Kenneth</forename><surname>Haim Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM J. Matrix Analysis Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Harmonic Analysis and the Theory of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bochner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<editor>Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent positional embedding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring kernel functions in the softmax layer for contextual word classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling recurrence for transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPs Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kernel methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1171" to="1220" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularizing and Optimizing LSTM Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Differentiable plasticity: training plastic neural networks with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Miconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
	</analytic>
	<monogr>
		<title level="j">Transformers with convolutional context for ASR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ListOps: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL Student Research Workshop</title>
		<meeting>of NAACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast function to function regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy</forename><surname>Trac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stabilizing transformers for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proc. of ICML</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rational recurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A mixture of h − 1 heads is better than h heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The ACL Anthology network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qazvinian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Text and Citation Analysis for Scholarly Digital Libraries</title>
		<meeting>of the Workshop on Text and Citation Analysis for Scholarly Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sampled softmax with random Fourier features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Xinnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICANN</title>
		<meeting>of ICANN</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian based ultra low precision quantization of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Random Features Methods in Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>The University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Hard-coded Gaussian attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Orthogonal random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Xinnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
