<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Explaining Structures Improve NLP Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Fan</surname></persName>
							<email>fanchun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Center</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Qinghong</roleName><forename type="first">Han</forename><forename type="middle">♣</forename><surname>Xiaofei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><forename type="middle">♣</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiwei_li@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Laboratory, ♣</roleName><forename type="first">Peng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Explaining Structures Improve NLP Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches to explaining deep learning models in NLP usually suffer from two major drawbacks: (1) the main model and the explaining model are decoupled: an additional probing or surrogate model is used to interpret an existing model, and thus existing explaining tools are not self-explainable; (2) the probing model is only able to explain a model's predictions by operating on low-level features by computing saliency scores for individual words but are clumsy at high-level text units such as phrases, sentences, or paragraphs.</p><p>To deal with these two issues, in this paper, we propose a simple yet general and effective self-explaining framework for deep learning models in NLP. The key point of the proposed framework is to put an additional layer, as is called by the interpretation layer, on top of any existing NLP model. This layer aggregates the information for each text span, which is then associated with a specific weight, and their weighted combination is fed to the softmax function for the final prediction.</p><p>The proposed model comes with the following merits: (1) span weights make the model selfexplainable and do not require an additional probing model for interpretation; (2) the proposed model is general and can be adapted to any existing deep learning structures in NLP;</p><p>(3) the weight associated with each text span provides direct importance scores for higherlevel text units such as phrases and sentences. We for the first time show that interpretability does not come at the cost of performance: a neural model of self-explaining features obtains better performances than its counterpart without the self-explaining nature, achieving a new SOTA performance of 59.1 on SST-5 and a new SOTA performance of 92.3 on SNLI. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches to explaining deep learning models in NLP usually suffer from two major drawbacks: (1) the main model and the explaining model are decoupled: an additional probing or surrogate model is used to interpret an existing model, and thus existing explaining tools are not self-explainable; (2) the probing model is only able to explain a model's predictions by operating on low-level features by computing saliency scores for individual words but are clumsy at high-level text units such as phrases, sentences, or paragraphs.</p><p>To deal with these two issues, in this paper, we propose a simple yet general and effective self-explaining framework for deep learning models in NLP. The key point of the proposed framework is to put an additional layer, as is called by the interpretation layer, on top of any existing NLP model. This layer aggregates the information for each text span, which is then associated with a specific weight, and their weighted combination is fed to the softmax function for the final prediction.</p><p>The proposed model comes with the following merits: (1) span weights make the model selfexplainable and do not require an additional probing model for interpretation; (2) the proposed model is general and can be adapted to any existing deep learning structures in NLP;</p><p>(3) the weight associated with each text span provides direct importance scores for higherlevel text units such as phrases and sentences. We for the first time show that interpretability does not come at the cost of performance: a neural model of self-explaining features obtains better performances than its counterpart without the self-explaining nature, achieving a new SOTA performance of 59.1 on SST-5 and a new SOTA performance of 92.3 on SNLI. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A long-term criticism against deep learning models is the lack of interpretability <ref type="bibr" target="#b60">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b4">Bach et al., 2015a;</ref><ref type="bibr">Montavon et al., 2017;</ref><ref type="bibr">Kindermans et al., 2017)</ref>. The black-box nature of neural models not only significantly limits the scope of applications of deep learning models (e.g., in biomedical or legal domains), but also hinders model behavior analysis and error analysis.</p><p>To enhance neural models' interpretability, various approaches to rationalize neural models' predictions have been proposed (details see Section 2). Existing interpretation models have two major drawbacks. Firstly, they are not self-explainable and require a probing model to be additionally built to interpret the original model. Building the probing model is not only an additional burden, but more importantly, the intrinsic decoupling nature of the two models makes the probing model only able to provide approximate interpretation results. Take the gradient-based saliency model <ref type="bibr" target="#b60">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b45">Li et al., 2015;</ref><ref type="bibr" target="#b54">Selvaraju et al., 2017)</ref> as an example, the derivative of the target probability (or the logit) w.r.t. the input dimensions is only an approximate feature weight in the first-order Taylor expansion, with all higher order items omitted.</p><p>Secondly, interpretation models mostly focus on learning word-level importance scores assigned by the the main model and are hard to be adapted to higher-level text units such as phrases, sentences, or paragraphs. A straightforward way to compute the saliency score for a phrase or a sentence could be averaging the scores for its constituent words. Unfortunately, this over-simplified strategy is inadequate to capture the semantic composition in language, which involves multiple layers of highly non-linear operations in neural nets, and can thus result in high risk of misinterpretation. As pointed by <ref type="bibr" target="#b46">Murdoch et al. (2018)</ref>, interpreting neural net predictions should go beyond word level.</p><p>We raise the following question: what makes a good interpretation method for NLP? Firstly, the method should be self-explainable and no additional probing model is needed. Secondly, the model should offer precise and clear saliency scores for any level of text units. Thirdly, the selfexplainable nature does not tradeoff performances.</p><p>Towards these three purposes, in this paper, we propose a self-explainable framework for deep neural models in the context of NLP. The key point of the proposed framework is to put an additional layer, as is called the interpretation layer on top of any existing NLP model, and this layer aggregates the information for all (i.e., O(n 2 )) text spans. Each text span is associated with a specific weight, and their weighted combination is fed to the softmax function for the final prediction. The proposed structure offers following advantages: (1) the model is self-explainable: the interpretation layer is trained along with the objective, with no probing model needed. Weights at the interpretation layer can directly be used as saliency scores for corresponding text spans; (2) since the saliency score for any text span can be straightforwardly derived from the interpretation layer, the model offers direct, precise and clear saliency scores for any level of text units beyond word level. (3) the interpretation layer collects information for each text span in the form of representations, and forwards the weighted sum to the final prediction layer. Therefore, no additional information is incorporated, nor any information is lost at the interpretation layer. This makes the model not come at the cost of performances.</p><p>The proposed framework is general, and can be adapted to any prevalent deep learning structure in NLP. We show that the proposed framework (1) offers clear model interpretations; (2) facilitates error analysis; (3) can help downstream NLP tasks such as adversarial example generation; and (4) most importantly, for the first time proves that selfexplainable nature does not come at the cost of performances, but rather, leads to better results in NLP, achieving a new SOTA performance of 59.1 on SST-5 and a new SOTA performance of 92.3 on SNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Rationalizing model predictions is of growing interest <ref type="bibr" target="#b50">(Ribeiro et al., 2016;</ref><ref type="bibr" target="#b59">Shrikumar et al., 2017;</ref><ref type="bibr" target="#b5">Bach et al., 2015b;</ref><ref type="bibr">Kindermans et al., 2017;</ref><ref type="bibr">Montavon et al., 2017;</ref><ref type="bibr" target="#b53">Schwab and Karlen, 2019;</ref><ref type="bibr" target="#b3">Arrieta et al., 2020)</ref>. In NLP, approaches to interpret neural models include extracting pieces of input text, called "rationales", as justifications to model predictions <ref type="bibr" target="#b33">(Lei et al., 2016;</ref><ref type="bibr" target="#b10">Chang et al., 2019;</ref><ref type="bibr" target="#b16">DeYoung et al., 2020;</ref>, studying the efficacy and dynamics of hidden states in recurrent networks <ref type="bibr" target="#b26">(Karpathy et al., 2015;</ref><ref type="bibr" target="#b58">Shi et al., 2016;</ref><ref type="bibr" target="#b21">Greff et al., 2016;</ref><ref type="bibr" target="#b64">Strobelt et al., 2016)</ref>, and applying variants of the attention mechanism <ref type="bibr" target="#b6">(Bahdanau et al., 2014)</ref> to interpret model behaviors <ref type="bibr" target="#b23">(Jain and Wallace, 2019;</ref><ref type="bibr" target="#b57">Serrano and Smith, 2019;</ref><ref type="bibr" target="#b69">Wiegreffe and Pinter, 2019;</ref><ref type="bibr" target="#b66">Vashishth et al., 2019;</ref><ref type="bibr" target="#b51">Rogers et al., 2020)</ref>. <ref type="bibr" target="#b33">Lei et al. (2016)</ref> proposed to extract text snippets as model explanations. <ref type="bibr" target="#b48">Rajani et al. (2019)</ref> collected human rationales for commonsense reasoning. Other works <ref type="bibr" target="#b12">(Chen et al., 2019;</ref><ref type="bibr" target="#b32">Lehman et al., 2019;</ref><ref type="bibr" target="#b9">Chai et al., 2020)</ref> trained independent models to extract supporting sentences as auxiliary guidelines for downstream tasks. Using attentions as a tool for model interpretation, <ref type="bibr" target="#b20">Ghaeini et al. (2018)</ref> visualized attention heatmaps to understand how natural language inference models build interactions between two sentences; <ref type="bibr" target="#b68">Vig and Belinkov (2019)</ref>; <ref type="bibr" target="#b65">Tenney et al. (2019)</ref>; <ref type="bibr" target="#b13">Clark et al. (2019);</ref><ref type="bibr" target="#b22">Htut et al. (2019)</ref> analyzed the attention structures by plotting heatmaps, and found that meaningful linguistic patterns exist in different heads and layers. Despite the interpretability the attention mechanism offers, Serrano and Smith (2019); <ref type="bibr" target="#b23">Jain and Wallace (2019)</ref> observed the highly inconsistency between attentions and predictors, and suggested that attentions should not be treated as justification for a decision.</p><p>Saliency methods are widely used in computer vision <ref type="bibr" target="#b60">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b73">Zeiler and Fergus, 2014;</ref><ref type="bibr" target="#b62">Springenberg et al., 2014;</ref><ref type="bibr" target="#b0">Adler et al., 2018;</ref><ref type="bibr" target="#b14">Datta et al., 2016;</ref><ref type="bibr" target="#b63">Srinivas and Fleuret, 2019)</ref> and NLP <ref type="bibr" target="#b15">(Denil et al., 2015;</ref><ref type="bibr" target="#b45">Li et al., 2015</ref><ref type="bibr" target="#b35">Li et al., , 2016</ref><ref type="bibr" target="#b2">Arras et al., 2016;</ref><ref type="bibr" target="#b17">Ebrahimi et al., 2017;</ref><ref type="bibr" target="#b19">Feng et al., 2018;</ref> for model interpretation. The key idea is to find the salient features responsible for a model's prediction. <ref type="bibr" target="#b60">Simonyan et al. (2013)</ref>; <ref type="bibr" target="#b63">Srinivas and Fleuret (2019)</ref> visualized the contributions of input pixels by compute the derivatives of the label logit in the output layer with respect to the input pixel. <ref type="bibr" target="#b0">Adler et al. (2018)</ref>; <ref type="bibr" target="#b14">Datta et al. (2016)</ref> explained neural models by perturbing different parts of the input, and compared the performance change in downstream tasks to measure the importance of perturbed features. In the context of NLP, <ref type="bibr" target="#b15">Denil et al. (2015)</ref> used saliency maps to identify and extract task-specific salient sentences from documents to maximally preserve document topics and semantics; <ref type="bibr" target="#b45">Li et al. (2015)</ref> visualized word-level saliency maps to understand how individual words affect model predictions; <ref type="bibr" target="#b17">Ebrahimi et al. (2017)</ref> crafted white-box adversarial examples to find the most salient text-editing operations (flip, insertion and deletion) to trick models by computing derivatives w.r.t. these editing operations;  combined the saliency method and the influence function <ref type="bibr" target="#b30">(Koh and Liang, 2017)</ref> to interpret model predictions from both learning history and inputs.</p><p>Our work is inspired by Melis and Jaakkola (2018), which achieves "self-explaining" by jointly training a deep learning model and a linear regression model with human-interpretable features. It is worth noting that the model in Melis and Jaakkola (2018) still requires a surrogate model, i.e., the linear regression model with human-interpretable features. Instead, the proposed framework does not require a surrogate model. Our work is also inspired by Selvaraju et al. <ref type="bibr" target="#b44">(2017)</ref> in computer vision, which computes the gradient w.r.t. feature maps of the high-level convolutional layer to obtain high-level saliency scores.</p><p>3 The Self-Explaining Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Given an input sequence x = {x 1 , x 2 , ..., x N }, where N denotes the length of x. Let x(i, j) denote the text span starting at index i, ending at index</p><formula xml:id="formula_0">j, where x(i, j) = {x i , x i+1 , ..., x j−1 , x j }.</formula><p>We wish to predict the label y for x based on p(y|x). Let V denote vocabulary size, and word representations are stored in W ∈ R V ×D . The input x is associated with the label y ∈ Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>For illustration purposes, we use transformers <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> as the backbone to show how the proposed framework works. The framework can be easily extended to other models such as BiLSTMs or CNNs. An overview of the proposed model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>the movie is my favorite</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Layer</head><p>(4,5) (1,3)</p><p>(2,4) (4,5) (1,3)</p><p>(2,4) Intermediate Layer On top of the input layer are the K encoder stacks, where each stack consists of multi-head attentions, layer normalization and residual connections. The representation for k-th layer at position t is denoted by h k t ∈ R 1×D . Specially, the presentation for the last layer at position t is denoted by h K t ∈ R 1×D . Span Infor Collecting Layer (SIC) To enable direct measurement for saliency of an arbitrary text span, we place a Span Infor Collecting Layer on top of intermediate layer. For an arbitrary text span x(i, j), we first obtain a dense representation h(i, j) to represent x(i, j), and h(i, j) needs to contain all semantic and syntactic information stored in x(i, j). Concretely, h(i, j) is obtained by taking the representation for the starting index at the last intermediate layer h K i , and the representation for the ending index h K j :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation Layer</head><formula xml:id="formula_1">h(i, j) = F (h K i , h K j )<label>(1)</label></formula><p>where F denotes the mapping function, which be FFN or other forms. The details of F will be discussed in section 3.3. The strategy of using the starting and ending representations to represent a text span has been used in many recent works on span-level features in texts <ref type="bibr" target="#b25">Joshi et al., 2020;</ref>. The SIC layer iterates over all text spans, and collects h(i, j) for all</p><formula xml:id="formula_2">i ∈ [1, N ], j ∈ [i, N ], with the complexity being O(N 2 ).</formula><p>Interpretation Layer The interpretation layer aggregates information from all text spans h(i, j): this is achieved by first assigning weights α(i, j) to each span x(i, j) and combing these representations using weighted sum. The weight α(i, j) can be obtained by first mapping h(i, j) to a scalar, and then normalizing all α(i, j):</p><formula xml:id="formula_3">o(i, j) =ĥ h(i, j) α(i, j) = exp(o(i, j)) i∈[1,N ],j∈[i,N ] exp(o(i, j))<label>(2)</label></formula><p>whereĥ ∈ R 1×D . The outputh ∈ R 1×D from the interpretation layer is the weighted average of all span representations:</p><formula xml:id="formula_4">h = i∈[1,N ],j∈[i,N ] α(i, j)h(i, j)<label>(3)</label></formula><p>Output Layer Similar to the standard setup, the output layer of the proposed framework is the probability distribution over labels using the softmax function:</p><formula xml:id="formula_5">p(y|x) = u yh y∈Y u yh<label>(4)</label></formula><p>where u y ∈ R D×1 . As can be seen, α(i, j) measures how much x(i, j) contributes to the final representationh, and thus indicates the importance of the text span x(i, j). The proposed strategy is similar to gradient-based interpretation methods <ref type="bibr" target="#b60">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b45">Li et al., 2015)</ref>. Using the chain rule, we have</p><formula xml:id="formula_6">∂p(y|x) ∂h(i, j) = ∂p(y|x) ∂h ∂h ∂h(i, j) = ∂p(y|x) ∂h α(i, j) + h(i, j) ∂α(i, j) ∂h(i, j)<label>(5)</label></formula><p>Omitting the second part, ∂p(y|x)/∂h is approximately in proportion to α(i, j). There is also a key advantage of the proposed framework over existing gradient-based interpretation methods, where the interpretation layer allows gradients to be straightforwardly computed with respect to an arbitrary text span. This is not feasible for vanilla gradientbased methods: because of the highly entanglement of neural networks, it's impossible to filter out the information of a specific text span from intermediate layers. Only gradients w.r.t. the input layer offers non-disentangling saliency scores, making the model only able to perform word-level explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Computing</head><p>A practical issue with Eq.1 is the computation cost. If F takes the form of FFN, the computational complexity for one single text span is O(D 2 ), leading to a final complexity of O(N 2 D 2 ) if all spans are iterated over. This is computationally unaffordable for long texts. Towards efficient computations, we propose that F takes the following form:</p><formula xml:id="formula_7">F (h i , h j ) = tanh[W (h i , h j , h i − h j , h i ⊗ h j )] (6) where W = [W 1 , W 2 , W 3 , W 4 ], W i ∈ R D×D . ⊗ denotes the pairwise dot between two vectors. Elements in (h i , h j , h i − h j , h i ⊗ h j )</formula><p>respectively captures concatenation, element-wise difference and element-wise closeness between the two vectors, the strategy of which is been used in recent work to model interactions between two semantic representations <ref type="bibr" target="#b45">(Mou et al., 2015;</ref><ref type="bibr" target="#b56">Seo et al., 2016)</ref>.</p><p>In Eq.6, W 1 h i , W 2 h j , W 3 h i and W 3 h j can be computed in advance for all i and j, leading to a computational complexity of O(N D). For</p><formula xml:id="formula_8">W 4 h i ⊗ h j , it can be factorized as √ W 4 h i ⊗ √ W 4 h j , with each of the two parts being com- puted in advance. The final computational com- plexity of W (h i , h j , h i − h j , h i ⊗ h j ) is thus O(N D). The element-wise tanh operation for all N 2 spans leads to a cost of O(N 2 D), giving a complexity of O(N 2 D) + O(N D) = O(N 2 D)</formula><p>for the SIC layer, which significantly cuts the cost. The computational cost for the interpretation layer, which requires the dot product betweenĥ and all h(i, j), is also O(N 2 D), leading to the final computational complexity O(N 2 D). In this way, we reduce the computation cost from O(N 2 D 2 ) to O(N 2 D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The training objective is the standard cross entropy loss. An additional regularization on α is needed: the model should only focus on a very small number of text spans. We thus wish the distribution of α to be sharp. We thus propose the following training objective:</p><formula xml:id="formula_9">L = log p(y|x) + λ i,j α 2 (i, j)<label>(7)</label></formula><p>which uses i,j α 2 (i, j) as the regularizer. Under the constraint of i,j α(i, j) = 1, i,j α 2 (i, j) achieves the highest value with one element of α being 1 and the rest being 0, and the lowest value if all α(i, j) have the same value. <ref type="bibr">2</ref> The model can be trained in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We would like an automatic evaluation method that is both flexible in evaluating the quality of model explanations under any task and dataset, and able to offer the ability of accurately reflecting the faithfulness of model explanations, i.e., the extracted rationales ought to semantically influence the model's prediction for the same instance, as suggested by <ref type="bibr" target="#b16">DeYoung et al. (2020)</ref>.</p><p>Intuitively, if extracted rationales can faithfully represent, in other words, be equivalent to, their corresponding text inputs with respect to model predictions, the following should hold:</p><p>(1) a model trained on the original inputs should perform comparably well when tested on the extracted rationales;</p><p>(2) a model trained on the extracted rationales should perform comparably well when tested on the original inputs;</p><p>(3) a model trained on the extracted rationales should perform comparably well when tested on other extracted rationales. Higher performances on these three aspects indicate more faithful extracted rationales, and consequently better interpretation models. These strategies are inspired by <ref type="bibr" target="#b33">Lei et al. (2016)</ref>, who trained a rationale generator to extract text pieces as explanations for different sentiment aspects for the task of sentiment analysis, achieving high precision based on sentence-level aspect annotations.</p><p>More formally, we use full to refer to the situation of training or testing a model on the original texts, and span to refer to the situation of training or testing a model on the extracted rationales. By denoting the original full dataset by D train , D dev and D test , and the newly constructed rationale-based dataset by D train , D dev and D test , the settings described above are as follows:</p><p>• FullTrain-SpanTest: The model is trained on D train and tested on D test , with hyperparameters selected on D dev .</p><p>• SpanTrain-FullTest: The model is trained on D train and tested on D test , with hyperparameters selected on D dev .</p><p>• SpanTrain-SpanTest: The model is trained on D train and tested on D test , with hyperparameters selected on D dev .</p><p>To construct the new rationale-based dataset, we train a rationale-extraction model on D train to extract rationales from the original text in D train , D dev and D test , and then replace the original full input text with the corresponding extracted rationales, with the labels remaining unchanged.</p><p>It is also worthing that the proposed three evaluation metrics are far from perfect: the system can be gamed if the extracted plan is just the same as the original span. The proposed evaluations thus need to be combined with other evaluations for complement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks and Datasets</head><p>We conduct experiments on three NLP tasks: text classification on the SST-5 dataset <ref type="bibr" target="#b61">(Socher et al., 2013)</ref>, natural language inference on the SNLI dataset <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref> and machine translation on the IWSLT2014 En→De dataset. Please refer to the appendix section for descriptions of the three datasets and training details.</p><p>For text classification and natural language inference, we use RoBERT <ref type="bibr" target="#b39">(Liu et al., 2019b)</ref> as the model backbone. For reference purposes, we also train a self-explaining model with α set to be uniform, i.e., α(i, j) = 1/M , where M is the total number of text spans. This model is denoted by AvgSelf-Explaining. For neural machine translation, we use the Transformer-base <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> model for evaluation. <ref type="table" target="#tab_1">Table 1</ref> shows the results for the SST-5 and SNLI datasets, and <ref type="table" target="#tab_2">Table 2</ref> shows the results for IWSLT 2014 En→De. We can see from the tables that BCN+SuBiLSTM+CoVe <ref type="bibr" target="#b8">(Brahma, 2018)</ref> 56.2 BERT-base <ref type="bibr" target="#b11">(Cheang et al., 2020)</ref> 54.9 BERT-large <ref type="bibr" target="#b11">(Cheang et al., 2020)</ref> 56.2 SentiBERT <ref type="bibr" target="#b72">(Yin et al., 2020)</ref> 56.9 SentiLARE (Ke et al., 2020) † 58.6 RoBERTa-base+AvgSelf-Explaining 56.2 RoBERTa-base <ref type="bibr" target="#b39">(Liu et al., 2019b)</ref> 56.4 RoBERTa-base+Self-Explaining 57.8 (+1.4) RoBERTa-large <ref type="bibr" target="#b39">(Liu et al., 2019b)</ref> 57.9 RoBERTa-large+Self-Explaining 59.1 (+1.2) SNLI BERT-base  89.2 BERT-large  90.4 SJRC    A surprising observation is that in spite of using the SIC layer and the interpretation layer, AvgSelf-Explaining still underperforms RoBERTa, which indicates that the learned attention weights α are important for model performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interpretability Evaluation</head><p>We compare our proposed Self-Explaining model with the following three widely used interpretation models:</p><p>• AvgGradient: The method of averaging word saliency scores within a text span <ref type="bibr" target="#b19">Feng et al., 2018)</ref>. The saliency score of a word w is computed as the derivative of the probability of the ouput label with respect to the word embedding: s y (w) = ∇ w p(y|x) 1 , where p(y|x) is the predicted probability of the ground-truth label y, w is the corresponding word embedding of word w and · 1 is the L1 norm. We take the average of saliency scores of all words within a text span as the span-level saliency score. We select the span with the highest span-level saliency score as the rationale.</p><p>• AvgAttention: The method of averaging attention scores within a text span <ref type="bibr" target="#b68">(Vig and Belinkov, 2019;</ref><ref type="bibr" target="#b65">Tenney et al., 2019;</ref><ref type="bibr" target="#b13">Clark et al., 2019)</ref>. The attention score for each word w is the normalized attentive probability of the special token [CLS] with respect to word w in the last intermediate layer. We take the average of attention scores of all words within a text span as the span-level attention score. The span with the highest span-level attention score is selected as the rationale.</p><p>• Rationale: The rationale extraction model proposed by <ref type="bibr" target="#b33">Lei et al. (2016)</ref>. One encoder is used to encode input texts into representative features, and one generator is used to extract text spans as rationales. These two models are jointly trained to minimize the expected cost function using the REINFORCE algorithm <ref type="bibr" target="#b70">(Williams, 1992)</ref>.</p><p>All models use RoBERTa <ref type="bibr" target="#b39">(Liu et al., 2019b)</ref> as the backbone and are trained separately for comparison.</p><p>Results are shown in <ref type="table">Table 4</ref>. For all the three setups FullTrain-SpanTest, SpanTrain-FullTest and SpanTrain-SpanTest, we can observe that the proposed Self-Explaining outperforms other interpretation methods by a large margin on both SST-5 and SNLI. On SST, Self-Explaining outperforms Rationale by +3.2, +5.0 and +6.7 respectively for the F-S, S-F and S-S setup. On SNLI, Self-Explaining outperforms Rationale by +3.5, +10.0 and 7.3 respectively for the F-S, S-F and S-S setup. Comparing AvgGradient, AvgAttention and Rationale, we can see that there is no method that shows consistent superiority to the other two. For example,   <ref type="table">Table 4</ref>: Performances of different models on the three evaluation methods defined in Section 4. "F" refers to Full and "S" refers to Span. Accuracy is reported in each cell.</p><p>Rationale outperforms AvgGradient and AvgAttention under the F-S and S-F setup, but underperforms AvgGradient under the S-S setup. Instead, Self-Explaining consistently achieves significantly better results compared to all baselines under all setups. These results demonstrate the better interpretability of the proposed Self-Explaining method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Effect of λ</head><p>We would like to explore the effect of λ in the additional regularization term in Eq.7. A larger λ has a stronger effect on sharpening the distribution of α, leading to the preference to a small fraction of text spans. Intuitively, a reasonable value of λ is crucial for model performances: too small λ means the selected text spans are not confident enough to support model predictions, while too large λ means the model only attends to a single text span for its prediction. The former may cause over-numbered selected text spans, which incurs more noise for model predictions. The latter may force the model the attend to a single span that is not important for predictions. Therefore, a sensible λ should be neither too small nor too large, balancing the number of attended text spans. <ref type="figure" target="#fig_1">Figure 2</ref> shows the results. As we can see from the figure, λ = 1.5 leads to the best result on SST-5 and λ = 1.0 gives the best result on SNLI, significantly outperforming models with small λ and large λ, as well as the baseline. As the value of λ keeps increasing, the performance drastically drops. When λ = 5.0 for SST-5 and λ = 2.0 for SNLI, the accuracies are respectively 56.4 and 90.3, even underperforming baselines without self-explaining structures. To our surprise, though the model performance with λ = 0 is worse than the best model with λ = 1.5 and λ = 1.0, it still achieves better results compared to baselines (57.0 vs. 56.4 on SST-5 and 91.4 vs. 90.7 on SNLI). This observation verifies that the proposed method can actually improve NLP models in terms of both performance and interpretability.</p><p>6 Analysis 6.1 Examples <ref type="table" target="#tab_5">Table 5</ref> lists several examples to illustrate how different methods extract text spans to interpret model predictions. Extracted spans are in bold.</p><p>As can be seen from the table, all methods including Self-Explaining are able to extract corresponding text spans as evidential explanation for the model prediction. For example, both AvgAttention and the proposed Self-Explaining are able to extract "overproduced and generally disappointing" as rationale for predicting the label Very Negative, and AvgGradient selects the key term "disappoint-Label Model Text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Very Negative</head><p>(1) this overproduced and generally disappointing effort isn't likely to rouse the rush hour crowd (2) this overproduced and generally disappointing effort isn't likely to rouse the rush hour crowd (3) this overproduced and generally disappointing effort isn't likely to rouse the rush hour crowd</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative</head><p>(1) However, it lacks grandeur and that epic quality often associated with Stevenson's tale as well as with earlier Disney efforts (2) However, it lacks grandeur and that epic quality often associated with Stevenson's tale as well as with earlier Disney efforts (3) However, it lacks grandeur and that epic quality often associated with Stevenson's tale] as well as with earlier Disney efforts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative</head><p>(1) Though everything might be literate and smart, it never took off and always seemed static (2) Though everything might be literate and smart, it never took off and always seemed static (3)</p><p>Though everything might be literate and smart, it never took off and always seemed static</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Very Positive</head><p>(1) One of the greatest family-oriented, fantasy-adventure movies ever (2)</p><p>One of the greatest family-oriented, fantasy-adventure movies ever (3)</p><p>One of the greatest family-oriented, fantasy-adventure movies ever  <ref type="table">Table 6</ref>: Examples of mis-classified texts and the corresponding extracted text spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Text</head><p>Positive →Negative</p><p>In the film, Lumumba, we see the faces behind the monumental shift in the Congo's history after it is reclaimed from the Belgians, and we see the motives behind those men into whose hands the raped and starving country fell. Lumumba is not a movie for the hyper masses; it demands the attention of its viewers with raw, truthful acting and intricate, packed dialogue. Little of the main plot is shown through action, it relies almost solely on words, but there is a recurring strand that is only action, and it is the stroke of genius that makes the film an enlightening and powerful panorama of the tense political struggle that the Congo's independence gave birth to.This film is real. It is raw inits depiction of those in power, and those on the streets. It is eye-opening in its content. And it is moving in the passions and emotions of its superbly portrayed characters. Whether you are a history fan, a film buff, or simply like good stories, Lumumba is a must-see [Whether you are a history fan, a cinephile or just love good stories, Lumumba is a must-have]</p><p>.</p><p>Negative →Positive I'm a huge Zack Allan fan and was disappointed that he only got one scene in the movie [I'm a huge fan of Zack Allan and I was disappointed that he only has one scene in the film]. This was also my favourite scene where he confiscates a character's weapons and directs her to Down Below. Unfortunately unlike Thirdspace &amp; River of Souls, most of the action took place off station. I didn't care much for Garibaldi after the first three seasons and think Sheridan is okay but no Sinclair. I like Lochley but she only had limited screen time. If you like Crusade or space battles you should enjoy it. <ref type="table">Table 7</ref>: Examples of generated adversarial text spans on the SST-5 dataset. The extracted and back-translated text spans are in bold. The text in parentheses [·] is the corresponding paraphrase to flip the prediction.</p><p>ing", which also makes sense in this case. However, compared to AvgGradient and AvgAttention, Self-Explaining can select more global and comprehensive text spans. For example, Self-Explaining extracts "it lacks grandeur and that epic quality often associated with Stevenson's tale" for predicting the Negative label, while AvgAttention and AvgGradient only extract "lacks grandeur", which may not be comprehensive for making the decision. Self-Explaining is able to extract "Though everything might be literate and smart" as rationale for predicting Negative, while AvgAttention and Avg-Gradient only extract local text spans "smart" and "literate and smart", respectively. Although they all make correct predictions, the rationales provided by AvgAttention and AvgGradient can not explain  their behaviors of making the decision. By contrast, Self-Explaining successfully captures the conjunction "though", a word that reverses the sentiment from positive to negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Error Analysis</head><p>By examining extracted text spans from erroneously classified examples, the model provides a direct way for performing error analysis. The biggest advantage of the proposed model over previous interpretability methods is that, instead of focusing merely on word-level features as in <ref type="bibr" target="#b45">Li et al. (2015</ref><ref type="bibr" target="#b35">Li et al. ( , 2016</ref>, the proposed model operates at arbitrary levels of text units, providing more direct and accurate views of why models make mistakes. By examining erroneously classified examples shown in <ref type="table">Table 6</ref>, we can clearly identify a few patterns that make neural models fail: (1) the model emphasizes the part of sentences that should not be attended to in the contrast conjunction, e.g., " [the story is naturally poignant], but first -time screenwriter paul pender overloads it with sugary bits of business";</p><p>(2) the model cannot recognize a word used in a context that changes its sentiment, e.g., " [a well acted and well intention] ed snoozer" and "There [isn't a weak or careless performance] amongst them";</p><p>(3) the model cannot recognize irony: "george, [hire a real director] and good writers for the next installment, please"; (4) the model cannot recognize analogy: "It's [like a poem]", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Span-based Adversarial Example Generation</head><p>There has been a growing interest in generating adversarial examples to attack a neural network model in NLP <ref type="bibr" target="#b1">(Alzantot et al., 2018;</ref><ref type="bibr" target="#b49">Ren et al., 2019;</ref><ref type="bibr" target="#b74">Zhang et al., 2020a)</ref>. The current protocol for adversarial example generation is based on word substitution <ref type="bibr" target="#b17">Ebrahimi et al., 2017;</ref><ref type="bibr" target="#b52">Samanta and Mehta, 2017;</ref><ref type="bibr" target="#b49">Ren et al., 2019)</ref>, where an important word is selected by saliency and then replaced by its synonym if the replacement can flip the prediction. The shortcoming for current approaches is that it can only operate at the word level and perform word-level substitutions. This is rooted in the fact that saliency scores can only be reliably computed at the word level.</p><p>The proposed span-based model naturally addresses this issue: we first identify the most salient span in the input based on α, replace it with its paraphrase if the replacement flips the label. To the best of our knowledge, this is the first feasible approach that operates at higher levels beyond words for adversarial example generation in NLP. Paraphrases are generated by back-translation <ref type="bibr" target="#b55">(Sennrich et al., 2016;</ref><ref type="bibr">Edunov et al., 2018). 3</ref> Following Ren et al. <ref type="formula" target="#formula_1">(2019)</ref>, we use two datasets for test, IMDB <ref type="bibr" target="#b40">(Maas et al., 2011)</ref> and Yahoo! Answers. The details of the two datasets are described in Appendix. For fair comparisons, we follow <ref type="bibr" target="#b49">Ren et al. (2019)</ref> to use the Bi-LSTMs as the model backbone and compare our proposed Self-Explaining+Paraphrase model with the following attacking methods: (1) Random; (2) Gradient;</p><p>(3) Traversing in Word Order (TiWO); (4) Word Saliency (WS); (5) Probability Weighted Word Saliency (PWWS). We refer readers to Ren et al.</p><p>(2019) for more details of these methods. <ref type="table" target="#tab_7">Table 8</ref> shows the classification accuracies of different methods on the original datasets and the adversarial samples generated by these attacking methods. Results show that our proposed Self-Explaining+Paraphrase method reduces the classification accuracy to the most extent. Compared to the original performance, Self-Explaining+Paraphrase reduces the classification accuracies on IMDB and Yahoo! Answers by 84% and 48.86%, respectively, indicating the effectiveness of the proposed Self-Explaining+Paraphrase method for generating adversarial samples. <ref type="table">Table 7</ref> gives two examples of flipping model predictions by using the Self-Explaining+Paraphrase method to generate adversarial text spans. Through the examples, we can see that the selected text span is crucial for the model prediction and a slight perturbation injected to this span would flip the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present a light but effective selfexplainable structure to improve both performance and interpretability in the context of NLP. The idea of the proposed method is to introduce an interpretation layer, aggregating information for each text span, which is then assigned a specific weight representing its contribution to interpreting the model prediction. Extensive experiments show the effectiveness of the proposed method in terms of improving performances for the task of sentiment classification and natural language inference, as well as endowing the model with the ability of selfexplaining, avoiding heavy recourse to additional external interpretation models such as probing models and surrogate models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the proposed model. Input Layer Similar to standard deep learning models in NLP, the input layer for the proposed model consists of the stack of word representations for words in the input sequence, where x t is represented as a D-dimensional vector W [x t , :].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performances of the proposed model with respect to different values of λ. Accuracy is reported for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performances of different models on the SST-5 and SNLI datasets. denotes using RoBERTa-base as the model backbone, denotes using RoBERTa-large as the model backbone, and † denotes using external training resources.</figDesc><table><row><cell>Model</cell><cell>BLEU↑</cell></row><row><cell>IWSLT 2014 En→De</cell><cell></cell></row><row><cell cols="2">Transformer-base (Vaswani et al., 2017) 28.4</cell></row><row><cell>Transformer-base+Self-Explaining</cell><cell>28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performances of different models on the IWSLT 2014 En→De dataset.</figDesc><table><row><cell>the proposed Self-Explaining method significantly</cell></row><row><cell>boosts performances over strong RoBERTa base-</cell></row><row><cell>lines on SST-5 and SNLI. Using RoBerta-large, we</cell></row><row><cell>achieve a new SOTA performance of 59.1 on SST-</cell></row><row><cell>5 and a new SOTA performance of 92.3 on SNLI.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performances of different models on MovieReview and SNLI of the ERASER benchmark.</figDesc><table><row><cell>Model</cell><cell>F-S</cell><cell>S-F</cell><cell>S-S</cell></row><row><cell>SST-5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AvgGradient</cell><cell cols="3">34.1 45.6 36.9</cell></row><row><cell>AvgAttention</cell><cell cols="3">32.5 40.6 35.8</cell></row><row><cell cols="4">Rationale (Lei et al., 2016) 35.2 49.9 36.2</cell></row><row><cell>Self-Explaining</cell><cell cols="3">38.4 54.9 42.9</cell></row><row><cell>SNLI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AvgGradient</cell><cell cols="3">70.7 74.5 73.1</cell></row><row><cell>AvgAttention</cell><cell cols="3">64.5 72.5 70.4</cell></row><row><cell cols="4">Rationale (Lei et al., 2016) 71.0 78.5 71.9</cell></row><row><cell>Self-Explaining</cell><cell cols="3">74.5 88.5 79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Examples of correctly classified texts and the corresponding extracted text spans by different models.</figDesc><table><row><cell cols="3">(1): Self-Explaining; (2): AvgGradient; (3): AvgAttention.</cell></row><row><cell>True</cell><cell>Predicted</cell><cell>Text</cell></row><row><cell cols="2">Negative Positive</cell><cell>the story is naturally poignant, but first -time screen-writer paul pender overloads it with</cell></row><row><cell></cell><cell></cell><cell>sugary bits ofbusiness</cell></row><row><cell cols="2">Negative Neutral</cell><cell>a well acted and well intentioned snoozer</cell></row><row><cell cols="2">Negative Positive</cell><cell>george, hire a real director and good writers forthe next installment, please</cell></row><row><cell>Positive</cell><cell>Negative</cell><cell>It 's like a poem</cell></row><row><cell>Positive</cell><cell cols="2">Very Negative There isn't a weak or careless performance amongst them</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Classification accuracy of each model on</cell></row><row><cell>the original datasets (the first row) and the perturbed</cell></row><row><cell>datasets using different adversarial methods. A lower</cell></row><row><cell>classification accuracy corresponds to a more effective</cell></row><row><cell>attacking method.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/ ShannonAI/Self_Explaining_Structures_ Improve_NLP_Models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Another regularization that can be used to fulfill the same purpose of obtaining sharpening distributions is the entropy − i,j αi,j log αi,j which can be viewed as the KLdivergence between the distribution and the uniform distribution. Empirically, we find that the two strategies perform almost the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the off-the-shelf google translator to implement En→De→En back-translation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Training Details</head><p>The Stanford Sentiment Treebank (SST) <ref type="bibr" target="#b61">(Socher et al., 2013)</ref> is a widely used benchmark for text classification. The task is to perform both finegrained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive and negative) sentiment classification at both the phrase and sentence level. We adopt the fine-grained setup in this work. We use Adam (Kingma and Ba, 2014) to optimize all modesl, with β 1 = 0.9, β 2 = 0.98, = 10 −8 . All hyperparameters including batch size, dropout and learning rate are tuned on the validation set.</p><p>The Stanford Natural Language Inference (SNLI) Corpus <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref> is a collection of 570k human-written English sentence pairs for the task of natural language inference (NLI). It contains a balanced amount of sentence pairs with labels entailment, contradiction, and neutral. We follow the official train/dev/test splits for both datasets. We also use Adam (Kingma and Ba, 2014) for optimization. Hyperparameters including batch size, dropout and learning rate are tuned on the validation set IWSLT2014 En→De contains 160k training sequences pairs and 7k validation sentence pairs. We use the concatenation of dev2010, tst2010, tst2011 and tst2011 as the test set. A joint BPE vocabulary of about 10k tokens is constructed. We minimize the cross entropy loss with label smoothing of value 0.1 during training. Adam <ref type="bibr" target="#b29">(Kingma and Ba, 2014)</ref> is used for optimization.</p><p>IMDB contains an even number of positive and negative reviews. The training and test sets respectively contain 25k and 25k examples. Yahoo! Answers is a large dataset for topic classification over ten largest main categories from Yahoo! Answers Comprehensive Questions and Answers v1.0. This dataset contains 1,400k training exmaples and 60k test examples respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Auditing black-box models for indirect influence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tionney</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Rybeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07998</idno>
		<title level="m">Generating natural language adversarial examples</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07298</idno>
		<title level="m">Explaining predictions of non-linear classifiers in nlp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Del</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved sentence modeling using suffix bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Description based text classification with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A game theoretic approach to classwise selective rationalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10055" to="10065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language representation models for fine-grained sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailey</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howey</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masud</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13619</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seeing things from a different angle:discovering diverse perspectives about claims</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayak</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE symposium on security and privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Extraction of salient sentences from labelled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ERASER: A benchmark to evaluate rationalized NLP models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06751</idno>
		<title level="m">Hotflip: White-box adversarial examples for text classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07781</idno>
		<title level="m">Pathologies of neural models make interpretations difficult</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interpreting recurrent and attention-based neural models: a case study on natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadepalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do attention heads in bert track syntactic dependencies?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention is not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to faithfully rationalize by construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SentiLARE: Sentiment-aware language representation learning with linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6975" to="6988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Dumitru Erhan, Been Kim, and Sven Dähne. 2017. Learning how to explain neural networks: Patternnet and patternattribution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Nile: Natural language inference with faithful natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12116</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inferring which medical treatments work from reports of clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Rationalizing neural predictions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01066</idno>
		<title level="m">Visualizing and understanding neural models in nlp</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A unified mrc framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11476</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaoqiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchang</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08006</idno>
		<title level="m">Deep text classification can be fooled</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7775" to="7784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06943</idno>
		<title level="m">Pair the dots: Jointly examining training history and test stimuli for model interpretability</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep taylor decomposition</title>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08422</idno>
		<title level="m">Natural language inference by tree-based convolution and heuristic matching</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Beyond word importance: Contextual decomposition to extract interactions from lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05453</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Conditionally adaptive multi-task learning: Improving transfer learning in nlp using fewer parameters &amp; less data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Elhattami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02361</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples through probability weighted word saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Shuhuai Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1085" to="1097" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>why should i trust you?&quot;: Explaining the predictions of any classifier</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suranjana</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameep</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02812</idno>
		<title level="m">Towards crafting text adversarial samples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cxplain: Causal explanations for model interpretation under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Karlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10220" to="10230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03731</idno>
		<title level="m">Is attention interpretable? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Why neural translations are the right length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2278" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fullgradient representation for neural network visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4124" to="4133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07461</idno>
		<title level="m">Visual analysis of hidden state dynamics in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<title level="m">Bert rediscovers the classical nlp pipeline</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11218</idno>
		<title level="m">Attention interpretability across nlp tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Analyzing the structure of attention in a transformer language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04284</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01746</idno>
		<title level="m">Coreference resolution as query-based span prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Sentibert: A transferable transformer-based architecture for compositional sentiment semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adversarial attacks on deeplearning models in natural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Emma</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahoud</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Alhazmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Explicit contextual semantics for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 33rd Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>PACLIC 33</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Semantics-aware bert for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9628" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
