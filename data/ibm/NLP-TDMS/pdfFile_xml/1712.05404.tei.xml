<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEE: Towards Semi-Supervised End-to-End Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
							<email>christian.bartz@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
							<email>haojin.yang@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
							<email>meinel@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEE: Towards Semi-Supervised End-to-End Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In recent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present SEE, a step towards semi-supervised neural networks for scene text detection and recognition, that can be optimized end-to-end. Most existing works consist of multiple deep neural networks and several pre-processing steps. In contrast to this, we propose to use a single deep neural network, that learns to detect and recognize text from natural images, in a semi-supervised way. SEE is a network that integrates and jointly learns a spatial transformer network, which can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We introduce the idea behind our novel approach and show its feasibility, by performing a range of experiments on standard benchmark datasets, where we achieve competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text is ubiquitous in our daily lives. Text can be found on documents, road signs, billboards, and other objects like cars or telephones. Automatically detecting and reading text from natural scene images is an important part of systems, that are to be used for several challenging tasks, such as image-based machine translation, autonomous cars or image/video indexing. In recent years the task of detecting text and recognizing text in natural scenes has seen much interest from the computer vision and document analysis community. Furthermore, recent breakthroughs <ref type="bibr" target="#b9">(He et al. 2016a;</ref><ref type="bibr" target="#b14">Jaderberg et al. 2015b;</ref><ref type="bibr" target="#b21">Redmon et al. 2016;</ref><ref type="bibr" target="#b22">Ren et al. 2015)</ref> in other areas of computer vision enabled the creation of even better scene text detection and recognition systems than before <ref type="bibr" target="#b8">(Gómez and Karatzas 2017;</ref><ref type="bibr" target="#b7">Gupta, Vedaldi, and Zisserman 2016;</ref>. Although the problem of Optical Character Recognition (OCR) can be seen as solved for text in printed documents, it is still challenging to detect and recognize text in natural scene images. Images containing natural scenes exhibit large variations of illumination, perspective distortions, image qualities, text fonts, diverse backgrounds, etc.</p><p>The majority of existing research works developed endto-end scene text recognition systems that consist of com-  <ref type="figure">Figure 1</ref>: Schematic overview of our proposed system. The input image is fed to a single neural network that consists of a text detection part and a text recognition part. The text detection part learns to detect text in a semi-supervised way, by being jointly trained with the recognition part. plex two-step pipelines, where the first step is to detect regions of text in an image and the second step is to recognize the textual content of that identified region. Most of the existing works only concentrate on one of these two steps.</p><p>In this paper, we present a solution that consists of a single Deep Neural Network (DNN) that can learn to detect and recognize text in a semi-supervised way. In this setting the network only receives the image and the textual labels as input. We do not supply any groundtruth bounding boxes. The text detection is learned by the network itself. This is contrary to existing works, where text detection and text recognition systems are trained separately in a fully-supervised way. Recent work <ref type="bibr" target="#b3">(Dai, He, and Sun 2016)</ref> showed that Convolutional Neural Networks (CNNs) are capable of learning how to solve complex multi-task problems, while being trained in an end-to-end manner. Our motivation is to use these capabilities of CNNs and create an end-to-end trainable scene text recognition system, that can be trained on weakly labelled data. In order to create such a system, we learn a single DNN that is able to find single characters, words or even lines of text in the input image and recognize their content. This is achieved by jointly learning a localization network that uses a recurrent spatial transformer <ref type="bibr" target="#b14">(Jaderberg et al. 2015b;</ref><ref type="bibr" target="#b28">Sønderby et al. 2015)</ref> as attention mechanism and a text recognition network. <ref type="figure">Figure 1</ref> provides a schematic overview of our proposed system.</p><p>Our contributions are as follows: (1) We present a novel end-to-end trainable system for scene text detection and recognition by integrating spatial transformer networks.</p><p>(2) We propose methods that can improve and ease the work with spatial transformer networks. (3) We train our proposed system end-to-end, in a semi-supervised way. (4) We demonstrate that our approach is able to reach competitive performance on standard benchmark datasets. (5) We provide our code 1 and trained models 2 to the research community.</p><p>This paper is structured in the following way: We first outline work of other researchers that is related to ours. Second, we describe our proposed system in detail. We then show and discuss our results on standard benchmark datasets and finally conclude our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Over the course of years a rich environment of different approaches to scene text detection and recognition have been developed and published. Nearly all systems use a two-step process for performing end-to-end recognition of scene text. The first step, is to detect regions of text and extract these regions from the input image. The second step, is to recognize the textual content and return the text strings of the extracted text regions.</p><p>It is further possible to divide these approaches into three broad categories: (1) Systems relying on hand crafted features and human knowledge for text detection and text recognition.</p><p>(2) Systems using deep learning approaches, together with hand crafted features, or two different deep networks for each of the two steps. (3) Systems that do not consist of a two step approach but rather perform text detection and recognition using a single deep neural network. For each category, we will discuss some of these systems.</p><p>Hand Crafted Features In the beginning, methods based on hand crafted features and human knowledge have been used to perform text detection and recognition. These systems used features like MSERs <ref type="bibr" target="#b20">(Neumann and Matas 2010)</ref>, Stroke Width Transforms <ref type="bibr" target="#b4">(Epshtein, Ofek, and Wexler 2010)</ref> or HOG-Features <ref type="bibr" target="#b31">(Wang, Babenko, and Belongie 2011)</ref> to identify regions of text and provide them to the text recognition stage of the system. In the text recognition stage sliding window classifiers <ref type="bibr" target="#b17">(Mishra, Alahari, and Jawahar 2012)</ref> and ensembles of SVMs <ref type="bibr" target="#b33">(Yao et al. 2014)</ref> or k-Nearest Neighbor classifiers using HOG features <ref type="bibr" target="#b30">(Wang and Belongie 2010)</ref> were used. All of these approaches use hand crafted features that have a large variety of hyper parameters that need expert knowledge to correctly tune them for achieving the best results.</p><p>Deep Learning Approaches More recent systems exchange approaches based on hand crafted features in one or both steps of recognition systems by approaches using DNNs. <ref type="bibr" target="#b8">Gómez and Karatzas (Gómez and Karatzas 2017)</ref> propose a text-specific selective search algorithm that, together with a DNN, can be used to detect (distorted) text regions in natural scene images. <ref type="bibr" target="#b7">Gupta et al. (Gupta, Vedaldi, and Zisserman 2016)</ref> propose a text detection model based on the YOLO-Architecture <ref type="bibr" target="#b21">(Redmon et al. 2016</ref>) that uses a fully convolutional deep neural network to identify text regions.</p><p>Bissacco et al. <ref type="bibr" target="#b1">(Bissacco et al. 2013</ref>) propose a complete end-to-end architecture that performs text detection using hand crafted features. Jaderberg et al. <ref type="bibr" target="#b13">(Jaderberg et al. 2015a;</ref><ref type="bibr" target="#b15">Jaderberg, Vedaldi, and Zisserman 2014)</ref> propose several systems that use deep neural networks for text detection and text recognition. In <ref type="bibr" target="#b13">(Jaderberg et al. 2015a</ref>) Jaderberg et al. propose to use a region proposal network with an extra bounding box regression CNN for text detection. A CNN that takes the whole text region as input is used for text recognition. The output of this CNN is constrained to a pre-defined dictionary of words, making this approach only applicable to one given language.</p><p>Goodfellow et al. <ref type="bibr">(Goodfellow et al. 2014)</ref> propose a text recognition system for house numbers, that has been refined by Jaderberg et al. <ref type="bibr" target="#b15">(Jaderberg, Vedaldi, and Zisserman 2014)</ref> for unconstrained text recognition. This system uses a single CNN, taking the whole extracted text region as input, and recognizing the text using one independent classifier for each possible character in the given word. Based on this idea He et al. <ref type="bibr" target="#b10">(He et al. 2016b</ref>) and  propose text recognition systems that treat the recognition of characters from the extracted text region as a sequence recognition problem. ) later improved their approach by firstly adding an extra step that utilizes the rectification capabilities of Spatial Transformer Networks <ref type="bibr" target="#b14">(Jaderberg et al. 2015b</ref>) for rectifying extracted text lines. Secondly they added a soft-attention mechanism to their network that helps to produce the sequence of characters in the input image. In their work Shi et al. make use of Spatial Transformers as an extra pre-processing step to make it easier for the recognition network to recognize the text in the image. In our system we use the Spatial Transformer as a core building block for detecting text in a semi-supervised way.</p><p>End-to-End trainable Approaches The presented systems always use a two-step approach for detecting and recognizing text from scene text images. Although recent approaches make use of deep neural networks they are still using a huge amount of hand crafted knowledge in either of the steps or at the point where the results of both steps are fused together. Smith et al. <ref type="bibr" target="#b26">(Smith et al. 2016</ref>) and Wojna et al. <ref type="bibr" target="#b32">(Wojna et al. 2017)</ref> propose an end-to-end trainable system that is able to recognize text on French street name signs, using a single DNN. In contrast to our system it is not possible for the system to provide the location of the text in the image, only the textual content can be extracted. Recently Li et al. <ref type="bibr" target="#b16">(Li, Wang, and Shen 2017)</ref> proposed an end-to-end system consisting of a single, complex DNN that is trained end-to-end and can perform text detection and text recognition in a single forward pass. This system is trained using groundtruth bounding boxes and groundtruth labels for each word in the input images, which stands in contrast to our method, where we only use groundtruth labels for each word in the input image, as the detection of text is learned by the network itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed System</head><p>A human trying to find and read text will do so in a sequential manner. The first action is to put attention on a word, read each character sequentially and then attend to the next word. Most current end-to-end systems for scene text recognition do not behave in that way. These systems rather try to solve the problem by extracting all information from the image at once. Our system first tries to attend sequentially to different text regions in the image and then recognize their textual content. In order to do this, we created a single DNN consisting of two stages: (1) text detection, and (2) text recognition. In this section we will introduce the attention concept used by the text detection stage and the overall structure of the proposed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detecting Text with Spatial Transformers</head><p>A spatial transformer proposed by <ref type="bibr" target="#b14">Jaderberg et al. (Jaderberg et al. 2015b</ref>) is a differentiable module for DNNs that takes an input feature map I and applies a spatial transformation to this feature map, producing an output feature map O. Such a spatial transformer module is a combination of three parts. The first part is a localization network computing a function f loc , that predicts the parameters θ of the spatial transformation to be applied. These predicted parameters are used in the second part to create a sampling grid, which defines a set of points where the input map should be sampled. The third part is a differentiable interpolation method, that takes the generated sampling grid and produces the spatially transformed output feature map O. We will shortly describe each component in the following paragraphs.</p><p>Localization Network The localization network takes the input feature map I ∈ R C×H×W , with C channels, height H and width W and outputs the parameters θ of the transformation that shall be applied. In our system we use the localization network (f loc ) to predict N two-dimensional affine transformation matrices A n θ , where n ∈ {0, . . . , N − 1}:</p><formula xml:id="formula_0">f loc (I) = A n θ = θ n 1 θ n 2 θ n 3 θ n 4 θ n 5 θ n 6<label>(1)</label></formula><p>N is thereby the number of characters, words or textlines the localization network shall localize. The affine transformation matrices predicted in that way allow the network to apply translation, rotation, zoom and skew to the input image.</p><p>In our system the N transformation matrices A n θ are produced by using a feed-forward CNN together with a Recurrent Neural Network (RNN). Each of the N transformation matrices is computed using the globally extracted convolutional features c and the hidden state h n of each time-step of the RNN:</p><formula xml:id="formula_1">c = f conv loc (I) (2) h n = f rnn loc (c, h n−1 ) (3) A n θ = g loc (h n )<label>(4)</label></formula><p>where g loc is another feed-forward/recurrent network. We use a variant of the well known ResNet architecture <ref type="bibr" target="#b9">(He et al. 2016a)</ref> as CNN for our localization network. We use this network architecture, because we found that with this network structure our system learns faster and more successfully, as compared to experiments with other network structures, such as the VGGNet <ref type="bibr" target="#b25">(Simonyan and Zisserman 2015)</ref>. We argue that this is due to the fact that the residual connections of the ResNet help with retaining a strong gradient down to the very first convolutional layers. The RNN used in the localization network is a Long-Short Term Memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber 1997)</ref> unit. This LSTM is used to generate the hidden states h n , which in turn are used to predict the affine transformation matrices. We used the same structure of the network for all our experiments we report in the next section. <ref type="figure">Figure 2</ref> provides a structural overview of this network.</p><p>Rotation Dropout During our experiments, we found that the network tends to predict transformation parameters, which include excessive rotation. In order to mitigate such a behavior, we propose a mechanism that works similarly to dropout <ref type="bibr" target="#b27">(Srivastava et al. 2014)</ref>, which we call rotation dropout. Rotation dropout works by randomly dropping the parameters of the affine transformation, which are responsible for rotation. This prevents the localization network to output transformation matrices that perform excessive rotation. <ref type="figure" target="#fig_3">Figure 3</ref> shows a comparison of the localization result of a localization network trained without rotation dropout (top) and one trained with rotation dropout (middle). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Generator</head><p>During inference we can extract the N resulting grids G n , which contain the bounding boxes of the text regions found by the localization network. Height H o and width W o can be chosen freely.</p><p>Localization specific regularizers The datasets used by us, do not contain any samples, where text is mirrored either along the x-or y-axis. Therefore, we found it beneficial to add additional regularization terms that penalizes grid, which are mirrored along any axis. We furthermore found that the network tends to predict grids that get larger over the time of training, hence we included a further regularizer that penalizes large grids, based on their area. Lastly, we also included a regularizer that encourages the network to predict grids that have a greater width than height, as text is normally written in horizontal direction and typically wider than high. The main purpose of these localization specific BBoxes of text regions = <ref type="figure">Figure 2</ref>: The network used in our work consists of two major parts. The first is the localization network that takes the input image and predicts N transformation matrices, which are used to create N different sampling grids. The generated sampling grids are used in two ways: (1) for calculating the bounding boxes of the identified text regions (2) for extracting N text regions. The recognition network then performs text recognition on these extracted regions. The whole system is trained end-to-end by only supplying information about the text labels for each text region.</p><p>regularizers is to enable faster convergence. Without these regularizers, the network will eventually converge, but it will take a very long time and might need several restarts of the training. Equation 7 shows how these regularizers are used for calculating the overall loss of the network.</p><p>Image Sampling The N sampling grids G n produced by the grid generator are now used to sample values of the feature map I at the coordinates u n i , v n j for each n ∈ N . Naturally these points will not always perfectly align with the discrete grid of values in the input feature map. Because of that we use bilinear sampling and define the values of the N output feature maps O n at a given location i, j where i ∈ H o and j ∈ W o to be:</p><formula xml:id="formula_3">O n ij = H h W w I hw max(0, 1−|u n i −h|)max(0, 1−|v n j −w|)<label>(6)</label></formula><p>This bilinear sampling is (sub-)differentiable, hence it is possible to propagate error gradients to the localization network, using standard backpropagation.</p><p>The combination of localization network, grid generator and image sampler forms a spatial transformer and can in general be used in every part of a DNN. In our system we use the spatial transformer as the first step of our network. <ref type="figure">Figure 4</ref> provides a visual explanation of the operation method of grid generator and image sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Recognition Stage</head><p>The image sampler of the text detection stage produces a set of N regions, that are extracted from the original input image. The text recognition stage (a structural overview of this stage can be found in <ref type="figure">Figure 2</ref>) uses each of these N different regions and processes them independently of each other. The processing of the N different regions is handled by a CNN. This CNN is also based on the ResNet architecture as we found that we could only achieve good results, while using a variant of the ResNet architecture for our recognition network. We argue that using a ResNet in the recognition stage is even more important than in the detection stage, because the detection stage needs to receive strong gradient information from the recognition stage in order to successfully update the weights of the localization network. The CNN of the recognition stage predicts a probability distributionŷ over the label space L , where L = L ∪ { }, with L being the alphabet used for recognition, and representing the blank label. The network is trained by running a LSTM for a fixed number of T timesteps and calculating the crossentropy loss for the output of each timestep. The choice of number of timesteps T is based on the number of characters, of the longest word, in the dataset. The loss L is computed as follows:</p><formula xml:id="formula_4">L n grid = λ 1 × L ar (G n ) + λ 2 × L as (G n ) + L di (G n ) (7) L = N n=1 ( T t=1 (P (l n t |O n )) + L n grid )<label>(8)</label></formula><p>Where L ar (G n ) is the regularization term based on the area of the predicted grid n, L as (G n ) is the regularization term based on the aspect ratio of the predicted grid n, and L di (G n ) is the regularization term based on the direction of the grid n, that penalizes mirrored grids. λ 1 and λ 2 are scaling parameters that can be chosen freely. The typical range of these parameters is 0 &lt; λ 1 , λ 2 &lt; 0.5. l n t is the label l at time step t for the n-th word in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>The training set X used for training the model consists of a set of input images I and a set of text labels L I for each input image. We do not use any labels for training the text detection stage. The text detection stage is learning to detect regions of text by using only the error gradients, obtained by calculating the cross-entropy loss, of the predictions and the textual labels, for each character of each word. During our experiments we found that, when trained from scratch, a network that shall detect and recognize more than two text lines does not converge. In order to overcome this problem we designed a curriculum learning strategy <ref type="bibr" target="#b0">(Bengio et al. 2009</ref>) for training the system. The complexity of the supplied training images under this curriculum is gradually increasing, once the accuracy on the validation set has settled.</p><p>During our experiments we observed that the performance of the localization network stagnates, as the accuracy of the recognition network increases. We found that restarting the training with the localization network initialized using the weights obtained by the last training and the recognition network initialized with random weights, enables the localization network to improve its predictions and thus improve the overall performance of the trained network. We argue that this happens because the values of the gradients propagated to the localization network decrease, as the loss decreases, leading to vanishing gradients in the localization network and hence nearly no improvement of the localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section we evaluate our presented network architecture on standard scene text detection/recognition benchmark datasets. While performing our experiments we tried to answer the following questions: (1) Is the concept of letting the network automatically learn to detect text feasible? (2) Can we apply the method on a real world dataset? (3) Can we get any insights on what kind of features the network is trying <ref type="figure">Figure 4</ref>: Operation method of grid generator and image sampler. First the grid generator uses the N affine transformation matrices A n θ to create N equally spaced sampling grids (red and yellow grids on the left side). These sampling grids are used by the image sampler to extract the image pixels at that location, in this case producing the two output images O 1 and O 2 . The corners of the generated sampling grids provide the vertices of the bounding box for each text region, that has been found by the network.</p><formula xml:id="formula_5">I 1 O 2 O</formula><p>to extract?</p><p>In order to answer these questions, we used different datasets. On the one hand we used standard benchmark datasets for scene text recognition. On the other hand we generated some datasets on our own. First, we performed experiments on the SVHN dataset <ref type="bibr" target="#b19">(Netzer et al. 2011</ref>), that we used to prove that our concept as such is feasible. Second, we generated more complex datasets based on SVHN images, to see how our system performs on images that contain several words in different locations. The third dataset we exerimented with, was the French Street Name Signs (FSNS) dataset <ref type="bibr" target="#b26">(Smith et al. 2016</ref>). This dataset is the most challenging we used, as it contains a vast amount of irregular, low resolution text lines, that are more difficult to locate and recognize than text lines from the SVHN datasets. We begin this section by introducing our experimental setup. We will then present the results and characteristics of the experiments for each of the aforementioned datasets. We will conclude this section with a brief explanation of what kinds of features the network seems to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Localization Network The localization network used in every experiment is based on the ResNet architecture <ref type="bibr" target="#b9">(He et al. 2016a)</ref>. The input to the network is the image where text shall be localized and later recognized. Before the first residual block the network performs a 3 × 3 convolution, followed by batch normalization <ref type="bibr" target="#b12">(Ioffe and Szegedy 2015)</ref>, ReLU <ref type="bibr" target="#b18">(Nair and Hinton 2010)</ref>, and a 2 × 2 average pooling layer with stride 2. After these layers three residual blocks with two 3×3 convolutions, each followed by batch normal-ization and ReLU, are used. The number of convolutional filters is 32, 48 and 48 respectively. A 2 × 2 max-pooling with stride 2 follows after the second residual block. The last residual block is followed by a 5 × 5 average pooling layer and this layer is followed by a LSTM with 256 hidden units. Each time step of the LSTM is fed into another LSTM with 6 hidden units. This layer predicts the affine transformation matrix, which is used to generate the sampling grid for the bilinear interpolation. We apply rotation dropout to each predicted affine transformation matrix, in order to overcome problems with excessive rotation predicted by the network.</p><p>Recognition Network The inputs to the recognition network are N crops from the original input image, representing the text regions found by the localization network. In our SVHN experiments, the recognition network has the same structure as the localization network, but the number of convolutional filters is higher. The number of convolutional filters is 32, 64 and 128 respectively. We use an ensemble of T independent softmax classifiers as used in <ref type="bibr">(Goodfellow et al. 2014)</ref> and <ref type="bibr" target="#b15">(Jaderberg, Vedaldi, and Zisserman 2014)</ref> for generating our predictions. In our experiments on the FSNS dataset we found that using ResNet-18 <ref type="bibr" target="#b9">(He et al. 2016a</ref>) significantly improves the obtained recognition accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment of Groundtruth</head><p>During training we assume that all groundtruth labels are sorted in western reading direction, that means they appear in the following order: 1. from top to bottom, and 2. from left to right. We stress that currently it is very important to have a consistent ordering of the groundtruth labels, because if the labels are in a random order, the network rather predicts large bounding boxes that span over all areas of text in the image. We hope to overcome this limitation, in the future, by developing a method that allows random ordering of groundtruth labels.</p><p>Implementation We implemented all our experiments using Chainer <ref type="bibr" target="#b29">(Tokui et al. 2015)</ref>. We conducted all our experiments on a work station which has an Intel(R) Core(TM) i7-6900K CPU, 64 GB RAM and 4 TITAN X (Pascal) GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on the SVHN dataset</head><p>With our first experiments on the SVHN dataset <ref type="bibr" target="#b19">(Netzer et al. 2011)</ref> we wanted to prove that our concept works. We therefore first conducted experiments, similar to the experiments in <ref type="bibr" target="#b14">(Jaderberg et al. 2015b</ref>), on SVHN image crops with a single house number in each image crop, that is centered around the number and also contains background noise. <ref type="table">Table 1</ref> shows that we are able to reach competitive recognition accuracies.</p><p>Based on this experiment we wanted to determine whether our model is able to detect different lines of text that are arranged in a regular grid, or placed at random locations in the image. In <ref type="figure">Figure 5</ref> we show samples from our two generated datasets, that we used for our other experiments based on SVHN data. We found that our network performs well on the task of finding and recognizing house numbers that are arranged in a regular grid.</p><p>During our experiments on the second dataset, created by us, we found that it is not possible to train a model from scratch, which can find and recognize more than two textlines that are scattered across the whole image. We therefore resorted to designing a curriculum learning strategy that starts with easier samples first and then gradually increases the complexity of the train images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on the FSNS dataset</head><p>Following our scheme of increasing the difficulty of the task that should be solved by the network, we chose the French Street Name Signs (FSNS) dataset by Smith et al. <ref type="bibr" target="#b26">(Smith et al. 2016)</ref> to be our next dataset to perform experiments on. The FSNS dataset contains more than 1 million images of French street name signs, which have been extracted from Google Streetview. This dataset is the most challenging dataset for our approach as it (1) contains multiple lines of text with varying length, which are embedded in natural scenes with distracting backgrounds, and (2) contains a lot of images where the text is occluded, not correct, or nearly unreadable for humans.</p><p>During our first experiments with that dataset, we found that our model is not able to converge, when trained on the supplied groundtruth. We argue that this is because our network was not able to learn the alignment of the supplied labels with the text in the images of the dataset. We therefore chose a different approach, and started with experiments where we tried to find individual words instead of textlines with more than one word. <ref type="table">Table 2</ref> shows the performance of our proposed system on the FSNS benchmark dataset. <ref type="figure">Figure 6</ref>: Samples from the FSNS dataset, these examples show the variety of different samples in the dataset and also how well our system copes with these samples. The bottom row shows two samples, where our system fails to recognize the correct text. The right image is especially interesting, as the system here tries to mix information, extracted from two different street signs, that should not be together in one sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sequence Accuracy <ref type="bibr" target="#b26">(Smith et al. 2016)</ref> 72.5 % <ref type="bibr" target="#b32">(Wojna et al. 2017)</ref> 84.2 % Ours 78.0 % <ref type="table">Table 2</ref>: Recognition accuracies on the FSNS benchmark dataset.</p><p>We are currently able to achieve competitive performance on this dataset. We are still behind the results reported by <ref type="bibr" target="#b32">Wojna et al. (Wojna et al. 2017</ref>). This likely due to the fact that we used a feature extractor that is weaker (ResNet-18) compared to the one used by . Also recall that our method is not only able to determine the text in the images, but also able to extract the location of the text, although we never explicitly told the network where to find the text! The network learned this completely on its own in a semi-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insights</head><p>During the training of our networks, we used Visualbackprop <ref type="bibr" target="#b2">(Bojarski et al. 2016)</ref> to visualize the regions that the network deems to be the most interesting. Using this visualization technique, we could observe that our system seems to learn different types of features for each subtask. <ref type="figure" target="#fig_3">Figure 3</ref> (bottom) shows that the localization network learns to extract features that resemble edges of text and the recognition network learns to find strokes of the individual characters in each cropped word region. This is an interesting observation, as it shows that our DNN tries to learn features that are closely related to the features used by systems based on hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we presented a system that can be seen as a step towards solving end-to-end scene text recognition, only using a single multi-task deep neural network. We trained the text detection component of our model in a semi-supervised way and are able to extract the localization results of the text detection component. The network architecture of our system is simple, but it is not easy to train this system, as a successful training requires a clever curriculum learning strategy. We also showed that our network architecture can be used to reach competitive results on different public benchmark datasets for scene text detection/recognition. At the current state we note that our models are not fully capable of detecting text in arbitrary locations in the image, as we saw during our experiments with the FSNS dataset. Right now our model is also constrained to a fixed number of maximum words that can be detected with one forward pass. In our future work, we want to redesign the network in a way that makes it possible for the network to determine the number of textlines in an image by itself.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The grid generator uses a regularly spaced grid G o with coordinates y ho , x wo , of height H o and width W o . The grid G o is used together with the affine transformation matrices A n θ to produce N regular grids G n with coordinates u n i , v n j of the input feature map I, where i ∈ H o and j ∈ W o :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Top: predicted bounding boxes of network trained without rotation dropout. Middle: predicted bounding boxes of network trained with rotation dropout. Bottom: visualization of image parts that have the highest influence on the outcome of the prediction. This visualization has been created using Visualbackprop<ref type="bibr" target="#b2">(Bojarski et al. 2016</ref>).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Bartzi/see 2 https://bartzi.de/research/see</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05418</idno>
		<title level="m">Visualbackprop: efficient visualization of cnns</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Textproposals: A textspecific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3501" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014, number 8692 in Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards end-toend text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2012-23rd British Machine Vision Conference, 127.1-127.11. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2010</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end interpretation of the french street name signs dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="411" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.05329</idno>
		<title level="m">Recurrent spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010, number 6311 in Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention-based extraction of structured information from street view imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
