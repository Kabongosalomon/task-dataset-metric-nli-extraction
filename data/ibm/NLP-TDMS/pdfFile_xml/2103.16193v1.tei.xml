<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MediaSpeech: Multilanguage ASR Benchmark and Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rostislav</forename><surname>Kolobov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tomsk Polytechnic University</orgName>
								<address>
									<settlement>Tomsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NTR Labs</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Okhapkina</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">AO HTSTS</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Omelchishina</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Higher School of Economics -State University</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Platunov</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NTR Labs</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bedyakin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">AO HTSTS</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vyacheslav</forename><surname>Moshkin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">AO HTSTS</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Menshikov</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NTR Labs</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Mikhaylovskiy</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NTR Labs</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Tomsk State University</orgName>
								<address>
									<settlement>Tomsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MediaSpeech: Multilanguage ASR Benchmark and Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: ASR evaluation</term>
					<term>speech recognition dataset</term>
					<term>baseline ASR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of automated speech recognition (ASR) systems is well known to differ for varied application domains. At the same time, vendors and research groups typically report ASR quality results either for limited use simplistic domains (audiobooks, TED talks), or proprietary datasets. To fill this gap, we provide an open-source 10-hour ASR system evaluation dataset NTR MediaSpeech for 4 languages: Spanish, French, Turkish and Arabic. The dataset was collected from the official youtube channels of media in the respective languages, and manually transcribed. We estimate that the WER of the dataset is under 5%.</p><p>We have benchmarked many ASR systems available both commercially and freely, and provide the benchmark results.</p><p>We also open-source baseline QuartzNet models for each language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The availability of ImageNet <ref type="bibr" target="#b0">[1]</ref> as an open and large image dataset have played a critical role for the development of deep learning paradigm. In the area of speech recognition, LibriSpeech <ref type="bibr" target="#b1">[2]</ref> and AISHELL-1 <ref type="bibr" target="#b2">[3]</ref> have probably played a similar role, providing ASR corpora widely used by both research and industry communities.</p><p>The performance of ASR systems is well known to differ for different application domains. Some researchers and practitioners call for further research democratization by making public more datasets from different application domains <ref type="bibr" target="#b3">[4]</ref>. With the exploding popularity of semi-supervised learning <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref> and availability of open-source tools <ref type="bibr" target="#b9">[10]</ref> to collect audio datasets from YouTube providing general purpose domain ASR training datasets may be an overkill in 2021. Still, new datasets keep appearing, some of them with a very significant positive impact on the research community <ref type="bibr" target="#b10">[11]</ref>[12] <ref type="bibr" target="#b12">[13]</ref>.</p><p>At the same time, vendors and research groups typically report ASR quality results either for limited use simplistic domains (audiobooks), or proprietary datasets, because the datasets collected in an automated fashion typically have low transcription accuracy. The problem is especially acute for nona Work performed during internship at NTR Labs {English, Mandarin} languages. To fill this gap, we provide an open-source 10-hour ASR system evaluation dataset for 4 languages: Spanish, French, Turkish, and Arabic, and benchmark various ASR systems against it.</p><p>We also open-source baseline QuartzNet <ref type="bibr" target="#b13">[14]</ref> models for each language.</p><p>The dataset and models are available at https://github.com/NTRLab/MediaSpeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data processing pipeline 2.1. Candidate audio selection</head><p>For the MediaSpeech dataset construction, we have selected a list of media with sufficient YouTube presence in each of the languages (see <ref type="table" target="#tab_0">Table 1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Audio segmentation</head><p>Segmentation of audio data was performed using the Vosk speech recognition toolkit <ref type="bibr">[17]</ref>. Namely, Vosk was used to extract timestamps for each recognized word. Based on these timestamps, we sliced the audio track into segments less than 15 seconds long. Utterances that contain segments with no speech longer than 4 seconds were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Transcription</head><p>A system for manual transcription has been built specifically for the project. Each utterance has been first transcribed by an open-source ASR <ref type="bibr">[17]</ref>. The transcription was used as a prompt for human transcribers to speed up transcription. Later analysis revealed that some transcribers just copied the automated transcription for more complex audio fragments. These utterances have been removed on the post-processing stage.</p><p>For each human transcriber, a transcription pipeline is built by the transcription system. For the quality control purposes, 5% of the utterances were taken from an existing spoken corpus (Mozilla Common Voice <ref type="bibr" target="#b14">[18]</ref>). While the error rates of transcribers do not generalize to the new speech domain, control over these values allowed us to manage transcribers and make sure that the quality of transcription is consistent over time.</p><p>The initial attempts at crowdsourcing have shown that the transcription quality was inappropriate. Thus, we have hired full time at least three native speakers for each language. <ref type="table" target="#tab_1">Table  2</ref> lists WER for some transcribers (due to the bugs in the early versions of the transcription management system some data on the transcriber WER was lost). Each utterance has been transcribed by two human transcribers. In the case where the relative WER of transcriptions was over 5%, the third transcriber resolved the conflict.</p><p>All text files are encoded in UTF8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Post-processing</head><p>Symbols such as &lt;, &gt;, [, ], ˜, /, \, =, etc., are removed. Text normalization is applied towards numbers. Utterances containing URLs are removed. Abbreviations such as CEO are presented in lowercase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Alphabet normalization</head><p>Some target languages, such as French and Spanish have alphabets that contain non-latin symbols such as "ae", "ü", "ă", etc. We have brought the alphabets used in the dataset into compliance with the modern polygraphic standards. The final alphabets are presented in <ref type="table" target="#tab_2">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Speech recognition baseline</head><p>In our experiments, we finetuned English Quartznet 15x5 model <ref type="bibr" target="#b13">[14]</ref>. We use a batch size of 16 per GPU on a single server with 4 T4 GPUs. All models were trained for 50 epochs with learning rate=0.005, using NovoGrad optimizer and SpecAugment data augmentation on proprietary in-domain datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ASR comparison 4.1. ASRs tested</head><p>ASRs with the following models and parameters were tested on the dataset:  <ref type="table" target="#tab_4">Table 4</ref> below lists Word Error Rates for each language and ASR system benchmarked. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Unsurprisingly, the research ASR systems trained on our-ofdomain data, like Common Voice (DeepSpeech, wav2vec 2.0) perform worse on the media domain than the commercial quality systems trained on diverse data. Wit has somewhat surprisingly shown fantastic performance on our dataset. We assume a large portion of media in the Wit training dataset.</p><p>The multitude of Arabic dialects most likely accounts for low results on standard media Arabic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The list of media usedFor each of the channels listed inTable 1, video recordings containing speech were selected for a total of 20 hours per language. Audio tracks were loaded from the selected videos using the youtube_dl tool [15] library. Each audio track was converted to single-channel 16 kHz 16-bit PCM encoded WAV files using the FFmpeg library[16]    </figDesc><table><row><cell cols="4">Language Channel name Language Channel name</cell></row><row><cell></cell><cell>Al Arabiya</cell><cell></cell><cell>RT France</cell></row><row><cell>AR</cell><cell>FRANCE 24 Arabic</cell><cell></cell><cell>France 24</cell></row><row><cell></cell><cell></cell><cell>FR</cell><cell></cell></row><row><cell></cell><cell>BBC News ‫ﻋﺮﺑﻲ‬</cell><cell></cell><cell>Russia Today</cell></row><row><cell></cell><cell>Euronews</cell><cell></cell><cell>Euronews</cell></row><row><cell></cell><cell>BBC World</cell><cell></cell><cell>FOX Haber</cell></row><row><cell>ES</cell><cell>CNN</cell><cell>TR</cell><cell>Show Ana</cell></row><row><cell></cell><cell>International</cell><cell></cell><cell>Haber</cell></row><row><cell></cell><cell>Russia Today</cell><cell></cell><cell></cell></row><row><cell cols="2">2.2. Downloading</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Transcriber WER on CommonVoice corpus excerpts</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Normalized Alphabets</figDesc><table><row><cell>Language</cell><cell>Alphabet</cell></row><row><cell>French</cell><cell>azertyuiopqsdfghjklmùwxcvbné'èçàêôâûoe</cell></row><row><cell>Spanish</cell><cell>abcdefghijklmnñopqrstuvwxyzáéíóúüé'</cell></row><row><cell>Arabic</cell><cell>‫سيﺮإلىمحةاقثعهذفبئضودجصكخشزطءغظآؤ‬ ‫أنت‬</cell></row><row><cell>Turkish</cell><cell>abcçdefgğhıijklmnoöprsştuüvyz'</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>WER benchmark</figDesc><table><row><cell>ASR</cell><cell>AR</cell><cell>FR</cell><cell>TR</cell><cell>ES</cell></row><row><cell>Azure</cell><cell>0.3016</cell><cell>0.1683</cell><cell>0.2296</cell><cell>0.1292</cell></row><row><cell>Google</cell><cell>0.4464</cell><cell>0.2385</cell><cell>0.2707</cell><cell>0.2176</cell></row><row><cell>VOSK</cell><cell>0.3085</cell><cell>0.2111</cell><cell>0.3050</cell><cell>0.1970</cell></row><row><cell>Silero</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.3070</cell></row><row><cell>Wit</cell><cell>0.2333</cell><cell>0.1759</cell><cell>0.0768</cell><cell>0.0879</cell></row><row><cell>Deepspeech</cell><cell>-</cell><cell>0.4741</cell><cell>-</cell><cell>0.4236</cell></row><row><cell cols="2">Quartznet (ours) 0.1300</cell><cell>0.1915</cell><cell>0.1422</cell><cell>0.1826</cell></row><row><cell>wav2vec2</cell><cell>0.9596</cell><cell>0.3113</cell><cell>0.5812</cell><cell>0.2469</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors are grateful to colleagues at NTR Labs Machine Learning Research group and AO HTSTS for the discussions and support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2009.5206848</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2015.7178964</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<publisher>Augus</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSDA.2017.8384449</idno>
	</analytic>
	<monogr>
		<title level="m">2017 20th Conference of the Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques</title>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards an ImageNet Moment for Speech-to-Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Veysov</surname></persName>
		</author>
		<ptr target="https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/" />
	</analytic>
	<monogr>
		<title level="m">The Gradient, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>James Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative pseudolabeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-1800</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1006" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-Training for End-to-End Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9054295</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="7084" to="7088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised ASR by end-to-end self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-1280</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</meeting>
		<imprint>
			<publisher>Octob</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2787" to="2791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KT-Speech-Crawler: Automatic dataset construction for speech recognition from YouTube videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2016</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018 -Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MLS: A large-scale multilingual dataset for speech research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-2826</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="2757" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building corpora of transcribed speech from open access sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Iakushkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Fedoseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Shaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Sedova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2267</biblScope>
			<biblScope unit="page" from="475" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quartznet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9053889</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="6124" to="6128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Common Voice: A Massively-Multilingual Speech Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</title>
		<meeting>the 12th Conference on Language Resources and Evaluation (LREC 2020)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4211" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<ptr target="http://kaldi.sf.net/" />
	</analytic>
	<monogr>
		<title level="j">IEEE 2011 Work. Autom. Speech Recognit. Underst</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
