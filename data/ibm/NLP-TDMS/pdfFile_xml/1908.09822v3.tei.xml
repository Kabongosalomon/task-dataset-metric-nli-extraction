<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confidence Regularized Self-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<email>zhidingy@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
							<email>liuxiaofeng@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">General Motors R&amp;D</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Confidence Regularized Self-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST. Network retraining (b) Label regularized self-training (c) Model regularized self-training (a) Self-training without confidence regularization Backbone Network Car Person Bus Network retraining Network retraining Figure 1: Illustration of proposed confidence regularization. (a) Self-training without confidence regularization generates and retrains with hard pseudo-labels, resulting in sharp network output. (b) Label regularized self-training introduces soft pseudo-labels, therefore enabling outputs to be smooth. (c) Model regularized self-training also retrains with hard pseudo-labels, but incorporates a regularizer to directly promote output smoothness.</p><p>More recently, self-training with networks emerged as a promising alternative towards domain adaptation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b68">68]</ref>. Self-training iteratively generates a set of one-hot (or hard) pseudo-labels corresponding to large selection scores (i.e., prediction confidence) in target domain, and then retrains network based on these pseudo-labels with target data. Recently, [68] proposes class-balanced selftraining (CBST) and formulates self-training as a unified loss minimization with pseudo-labels that can be solved in an end-to-end manner. Instead of reducing domain gap by minimizing both the task loss and domain adversarial loss, the self-training loss implicitly encourages cross-domain feature alignment for each class by learning from both labeled source data and pseudo-labeled target data.</p><p>Early work <ref type="bibr" target="#b29">[29]</ref> shows that the essence of deep selftraining is entropy minimization -pushing network output to be as sharp as hard pseudo-label. However, 100% accuracy cannot always be guaranteed for pseudo-labels. Trusting all selected pseudo-labels as "ground truth" by encoding arXiv:1908.09822v3 [cs.CV] 15 Jul 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transferring knowledge learned by deep neural networks from label-rich domains to a new target domain is an important but challenging problem. Such domain change naturally occurs in many applications, such as synthetic data training <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b46">46]</ref> and simulation for robotics/autonomous driving. The existence of cross-domain differences often leads to considerably decreased model performance, and unsupervised domain adaptation (UDA) aims to address this problem by adapting source model to target domain with the aid of unlabeled target data. To this end, a predominant stream of adversarial learning based UDA methods were proposed to reduce the discrepancy between source and target domain features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b60">60]</ref>. them as hard labels can lead to overconfident mistakes and propagated errors. In addition, semantic labels of natural images can be highly ambiguous. Taking a sample image from VisDA17 <ref type="bibr" target="#b42">[42]</ref> (see <ref type="figure" target="#fig_5">Fig. 1</ref>) as an example: both person and car dominate significant portions of this image. Enforcing a model to be very confident on only one of the class during training can hurt the learning behavior <ref type="bibr" target="#b1">[2]</ref>, particularly within the under-determined context of UDA.</p><p>The above issues motivate us to prevent infinite entropy minimization in self-training via confidence regularization. A natural idea is to generate soft pseudo-label that redistributes a certain amount of confidence to other classes. Learning with soft pseudo-labels attenuates the misleading effect brought by incorrect or ambiguous supervision. Alternatively, to achieve the same goal, one can also encourage the smoothness of output probabilities and prevent overconfident prediction in network training. Both ideas are illustrated in <ref type="figure" target="#fig_5">Fig. 1</ref>. At high-level, the major goal of CRST is still aligned with entropy minimization. However, the confidence regularization serves as a safety measure to prevent infinite entropy minimization and degraded performance.</p><p>In this work, we choose CBST <ref type="bibr" target="#b68">[68]</ref> as a state-of-the-art non-regularized self-training baseline, and propose a variety of specific confidence regularizers to comprehensively validate CRST. Our contributions are listed as follows:</p><p>• In Section 3, We generalize CBST to continuous CBST as a necessary preliminary for introducing our CRST, where we relax the feasible space of pseudo-labels from one-hot vectors to a probability simplex, • In Section 4.1, we introduce label regularized selftraining (CRST-LR). CRST-LR generates soft pseudolabels for self-training. Specifically, we propose a label entropy regularizer (LRENT). In Section 4.2, we introduce model regularized self-training (CRST-MR). CRST-MR introduces an output smoothing regularizer to network training. Specifically, we introduce three model regularizers, including L 2 (MRL2), entropy (MRENT), and KLD (MRKLD). • In Section 5, we investigate theoretical properties of CRST, and prove that CRST is equivalent to regularized Classification Maximum Likelihood which can be solved via Classification Expectation Maximization (CEM). We also prove the convergence of CRST, and show that LRENT-regularized pseudo-label is equivalent to a generalized softmax with temperature <ref type="bibr" target="#b23">[23]</ref>. • In Section 6, we comprehensively evaluate CRST on multiple domain adaptation tasks, including image classification (visDA17/Office-31) and semantic segmentation (GTA5/SYNTHIA → Cityscapes).</p><p>We demonstrate state-of-the-art or competitive results from the proposed framework, and discuss the comparison between different regularizers in Section 7. We also show that LR+MR may benefit self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Self-training: Self-training has been widely investigated in semi-supervised learning <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">19</ref>]. An overview of different self-training techniques is presented in <ref type="bibr" target="#b59">[59]</ref>. Recent interests in self-training were revitalized with deep neural networks <ref type="bibr" target="#b29">[29]</ref>. A subtle difference between selftraining on fixed features and deep self-training is that the latter involves the learning of embeddings which renders greater flexibility towards domain alignment than classifierlevel adaptation. Within this context, <ref type="bibr" target="#b68">[68]</ref> proposed classbalanced self-training and achieved state-of-the-art performance in cross-domain semantic segmentation. Domain adaptation: (Unsupervised) domain adaptation (UDA) has recently gained considerable interests. For UDA with deep networks, a major principle is to let the network learn domain invariant embeddings by minimizing the cross-domain difference of feature distributions with certain criteria. Examples of these methods include maximum mean discrepancy (MMD) <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b62">62]</ref>, deep correlation alignment (CORAL) <ref type="bibr" target="#b56">[56]</ref>, sliced Wasserstein discrepancy <ref type="bibr" target="#b28">[28]</ref>, adversarial learning at input-level <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b12">13]</ref>, feature level <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b64">64]</ref>, output space level <ref type="bibr" target="#b60">[60]</ref>, and a variety of follow up works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b53">53]</ref> etc. Open set domain adaptation <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b52">52]</ref> focuses on the problem where classes are not totally shared between source and target domains. More recently, there have been multiple deep selftraining/pseudo-label based methods that are proposed for domain adaptation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b68">68]</ref>. Semi-supervised learning (SSL): There exist a natural strong connection between domain adaptation and semisupervised learning with their problem definitions. A series of teacher-student based approaches have been recently proposed for both SSL <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b37">37]</ref> and UDA problems <ref type="bibr" target="#b13">[14]</ref>. Noisy label learning: Self-training can also be regarded as noisy label learning <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b66">66]</ref> due to potential mistakes on pseudo-labels. <ref type="bibr" target="#b45">[45]</ref> introduced a bootstrapping method for noisy label learning. <ref type="bibr" target="#b55">[55]</ref> proposed an extra noise layer into the network adapting the network outputs to match the noisy label distribution. Network regularization: Regularization is a typical approach in supervised neural network training to avoid overfitting. Besides the standard weight decay, typical regularization techniques include label smoothing <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b32">32]</ref>, network output regularization <ref type="bibr" target="#b43">[43]</ref>, knowledge distillation <ref type="bibr" target="#b23">[23]</ref>. Yet few principled research have considered regularized self-training within the context of SSL/UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Continuous class-balanced self-training</head><p>In this section, we review the class-balanced self-training algorithm in <ref type="bibr" target="#b68">[68]</ref> and reformulate it under a continuous framework. Specifically, for an UDA problem, we have access to the labeled source samples (x s , y s ) from source do-main {X S , Y S }, and target samples x t from unlabeled target domain data X T . Any target labelŷ t = (ŷ</p><formula xml:id="formula_0">(1) t , ...,ŷ (K) t )</formula><p>fromŶ T is unknown. K is the total number of classes. We define the network weights as w and p(k|x; w) as the classifier's softmax probability for class k.</p><p>CBST is a self-training framework that performs joint network learning and pseudo-label estimation under a unified loss minimization problem. The pseudo-labels are treated as discrete learnable latent variables being either one-hot or all-zero. Here, we first relax the pseudo-label variables to continuous domain, as shown in Eq. <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_1">min w,Ŷ T L CB (w,Ŷ) = − s∈S K k=1 y (k) s log p(k|x s ; w) − t∈T K k=1ŷ (k) t log p(k|x t ; w) λ k s.t.ŷ t ∈ ∆ K−1 ∪ {0}, ∀t<label>(1)</label></formula><p>The feasible set is the union of {0} and a probability simplex ∆ K−1 . The continuous CBST is solved by alternating optimization based on the following a), b) steps: a) Pseudo-label generation Fix w and solve:</p><formula xml:id="formula_2">min Y T − t∈T K k=1ŷ (k) t log p(k|x t ; w) λ k s.t.ŷ t ∈ ∆ K−1 ∪ {0}, ∀t<label>(2)</label></formula><p>b) Network retraining FixŶ T and solve:</p><formula xml:id="formula_3">min w − s∈S K k=1 y (k) s log p(k|x s ; w) − t∈T K k=1ŷ (k) t log p(k|x t ; w)<label>(3)</label></formula><p>We define going through step a) and b) once as one "selftraining round". For solving step a), there is a global optimizer for arbitraryŷ t = (ŷ</p><formula xml:id="formula_4">(1) t , ...,ŷ (K) t ) as follows. y (k) * t =          1, if k = arg max c { p(c|x t ; w) λ c } and p(k|x t ; w) &gt; λ k 0, otherwise<label>(4)</label></formula><p>For solving step b), one can use typical gradient-based methods such as mini-batch gradient descent. Intuitively, solving a) by (4) is actually conducting pseudo-label learning and selection simultaneously. Note thatŷ * t in (4) not only can be one-hot, but also can be a zero vector 0. For each target sample (x t ,ŷ * t ), ifŷ * t is an one-hot, the sample is selected for model retraining. Ifŷ * t = 0, this sample is not chosen. Specifically, λ k is a parameter controlling sample selection. If a sample's predication is relatively confident with p(k * |x t ; w) &gt; λ k * , it is selected and labeled as class k * = arg max k { p(k|xt;w) λ k }. The less confident ones with p(k * |x t ; w) ≤ λ k * are not selected.</p><p>λ k are critical parameters to control pseudo-label learning and selection. The same class-balanced λ k strategy introduced in <ref type="bibr" target="#b68">[68]</ref> is adopted for all self-training methods in this work. λ k for each class k is determined by a single portion parameter p which indicts how many samples we want to select in target domain. Specifically, we define the confidence for a sample as the max of its output softmax probabilities. For each class k, λ k is determined by the confidence value selecting the most confident p portion of class k predictions in the entire target set. We emphasize that only one parameter p is used to determine all λ k 's. Practically, we gradually increase p to incorporate more pseudo-labels for each additional round. For detailed algorithm, we recommend to read Algorithm 2 in <ref type="bibr" target="#b68">[68]</ref>. Remark: The only difference between CBST and continuous CBST lies in the feasible set where continuous CBST has a probability simplex while CBST has a set of onehot vectors. Although the feasible set relaxization does not change the solutions of CBST and the pseudo-labels are still one-hot vectors, continuous CBST allows generating soft pseudo-labels if specific regularizers are introduced into pseudo-label generation. Thus it serves as the basis for our proposed label regularized self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Confidence regularized self-training</head><p>As mentioned in Section 1, we leverage confidence regularization to prevent the over-minimization of entropy that could lead to degraded performance in self-training. Below, we introduce the general definition of CRST:</p><formula xml:id="formula_5">min w,Ŷ T L CR (w,Ŷ T ) = L CB (w,Ŷ T ) + αR C (w,Ŷ T ) = − s∈S K k=1 y (k) s log p(k|x s ; w) − t∈T K k=1ŷ (k) t log p(k|x t ; w) λ k − αr c (w,ŷ t ) s.t.ŷ t ∈ ∆ (K−1) ∪ {0}, ∀t<label>(5)</label></formula><p>R C (w,Ŷ T ) = t∈T r c (w,ŷ t ) is the confidence regularizer and α ≥ 0 is the weight coefficient. Similar to CBST, the optimization algorithm of CRST can be formulated as alternatively taking step a) pseudo-label generation and step b) network retraining. In this paper, we introduce two types of CRST frameworks: label regularized self-training and model regularized self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Label regularization</head><p>The label regularizer has a general form of R C (Ŷ T ) = t∈T r c (ŷ t ) and only depends on pseudo-labels {ŷ t }. With fixed w, the pseudo-label generation in step a) of CRST-LR is defined as follows:</p><formula xml:id="formula_6">min Y T − t∈T K k=1ŷ (k) t log p(k|x t ; w) λ k − αr c (ŷ t ) s.t.ŷ t ∈ ∆ (K−1) ∪ {0}, ∀t<label>(6)</label></formula><p>The global minimizer of (6) can be found via a twostage optimization given the special structure of the feasible space. The first stage involves minimizing (6) within ∆ (K−1) only, which givesŷ † t . The second stage is to select betweenŷ † t or 0 by checking which leads to a lower cost:</p><formula xml:id="formula_7">y * t = ŷ † t , if C(ŷ † t ) &lt; C(0) 0 , otherwise<label>(7)</label></formula><p>where C(ŷ t ) is the cost of a single sample t in <ref type="formula" target="#formula_6">(6)</ref>:</p><formula xml:id="formula_8">C(ŷ t ) = −ŷ (k) t K k=1 log p(k|x t ; w) λ k + αr c (ŷ t ) (8)</formula><p>Note that the above regularized term prefers selecting pseudo-labels with certain smoothness rather than sparse ones. In addition, CRST-LR and CBST share the same network retraining strategy in step b). Specifically, we introduce a negative entropy label regularizer (LRENT) in <ref type="table">Table 1</ref> with its definition and the corresponding solution ofŷ † t . For clarity, we write p(k|x t ; w) as p(k|x t ) for short.ŷ † t can be obtained via solving with a Lagrangian multiplier (KKT conditions) <ref type="bibr" target="#b2">[3]</ref>. The detailed derivations are shown in Section A of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model regularization</head><p>The model regularizer has a general form of R C (w) = t∈T r c (p(x t ; w)) where p(x t ; w) is the network softmax output probabilites. Compared to CBST, CRST-MR has the same hard pseudo-label generation process. But in network retraining of step b), CRST-MR uses a cross-entropy loss regularized by an output smoothness encouraging term. We define the optimization problem in step b) as follows:</p><formula xml:id="formula_9">min w − s∈S K k=1 y (k) s log p(k|x s ; w) − t∈T [ K k=1ŷ (k) t log p(k|x t ; w) − αr c (p(x t ; w))]<label>(9)</label></formula><p>Specifically, we introduce three model regularizers in <ref type="table">Table 1</ref> based on L 2 , negative entropy and KLD between uniform distribution u and softmax output. The gradients w.r.t. softmax logits z i are also provided. H(p) is the entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularizer</head><p>Pseudo-label solution (LR)/Gradient (MR) <ref type="table">Table 1</ref>: List of proposed regularizers with corresponding pseudo-label solution or gradients w.r.t. softmax logit z i .</p><formula xml:id="formula_10">LRENT K k=1ŷ (k) t log (ŷ (k) t )ŷ (i) † t = ( p(i|xt) λk ) 1 α K k=1 ( p(k|xt) λk ) 1 α MRL2 K k=1 p(k|x t ) 2 2 K k=1 p 2 (k|x t )[δ ki − p(i|x t )], δ ki = 1[k = i] MRENT K k=1 p(k|x t ) log p(k|x t ) p(i|x t )[log p(i|x t ) + H(p(x t ))] MRKLD − K k=1 1 K log p(k|x t ) p(i|x t ) − 1 K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Theoretical properties 5.1. A probabilistic view of CRST</head><p>There exists an inherent connection between the CRST and some probabilistic models. Specifically, the CRST selftraining algorithm can be interpreted as an instance of classification expectation maximization <ref type="bibr" target="#b0">[1]</ref>: Proposition 1. CRST can be modeled as a regularized classification maximum likelihood (RCML) problem optimized via classification expectation maximization.</p><p>Proof. Please refer to Section B.1 of the Appendix. Proposition 2. Given pre-determined λ k , CRST is convergent under certain conditions. Proof. Please refer to Section B.2 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Soft pseudo-label in LRENT</head><p>There is an intrinsic connection between the soft pseudolabel of LRENT (given in <ref type="table">Table 1</ref>) and softmax with temperature. Softmax with temperature [23] is a common approach in neural network for scaling softmax probabilities with applications in knowledge distillation <ref type="bibr" target="#b23">[23]</ref>, model calibration <ref type="bibr" target="#b20">[20]</ref>, etc. Typically, networks produce categorical probabilities by a softmax activation layer to convert the logit z i for each class into a probability p(i). And the softmax with temperature introduces a positive temperature α to scale its smoothness as follows.</p><formula xml:id="formula_11">p(i) = e z i α k=1,...,K e z k α<label>(10)</label></formula><p>For high temperature (α → ∞), the new distribution is softened as a uniform distribution that has the highest entropy and uncertainty. For temperature α = 1, we recover the original softmax probabilities. For low temperature (α → 0), the distribution collapses to a sparse one-hot vector with all probability on the class with the most original softmax probability. Now we draw the connection of soft pseudo-label in LRENT to softmax with temperature: Proposition 3. If λ k are equal for all k, the soft pseudolabel of LRENT given in <ref type="table">Table 1</ref> is exactly the same as softmax with temperature.</p><p>Proof.</p><formula xml:id="formula_12">y (i) * t = ( p(i|xt) λ k ) 1 α k ( p(k|xt) λ k ) 1 α = p(k|x t ) 1 α k p(k|x t ) 1 α = [ e z i q e zq ] 1 α k [ e z k q e zq ] 1 α = (e zi ) 1 α k (e z k ) 1 α = e z i α k e z k α</formula><p>The soft pseudo-label of LRENT can be regarded as a generalized softmax with temperature. In self-training, if selected properly, λ k can help to generate class-balanced soft pseudo-labels. Proof. Please refer to Section B.3 of the Appendix.</p><formula xml:id="formula_13">Proposition 5. D KL (p(x t )||u) KLD model regularizer (the reverse of the proposed D KL (u||p(x t )) KLD regular- izer) is equivalent to entropy model regularizer −H(p(x t )),</formula><p>where u is the uniform distribution.</p><p>Proof. Please refer to Section B.4 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we conduct comprehensive evaluation on different domain adaptation tasks. Adaptation for image classification: We consider two adaptation benchmarks: 1) VisDA17 <ref type="bibr" target="#b42">[42]</ref> and 2) Office-31 <ref type="bibr" target="#b48">[48]</ref>. VisDA17 contains 152, 409 2D synthetic images of 12 classes in the source training set and 55, 400 real images from MS-COCO <ref type="bibr" target="#b30">[30]</ref> as the target domain validation set. Office-31 is a small-scale dataset containing images of 31 classes from three domains -Amazon (A), Webcam (W) and DSLR (D). Each domain contains 2, 817, 795 and 498 images respectively. We follow the standard protocol in <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b53">53]</ref> and evaluate on six transfer tasks A → W ,</p><formula xml:id="formula_14">D → W , W → D, A → D, D → A, and W → A.</formula><p>Adaptation for semantic segmentation: We consider two popular synthetic-to-real adaptation scenarios: 1) GTA5 <ref type="bibr" target="#b46">[46]</ref> to Cityscapes <ref type="bibr" target="#b10">[11]</ref>, and 2) SYNTHIA <ref type="bibr" target="#b47">[47]</ref> to Cityscapes. The GTA5 dataset includes 24, 966 images rendered by GTA5 game engine. For SYNTHIA, we choose SYNTHIA-RAND-CITYSCAPES which includes 9, 400 labeled images. Following the standard protocols <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b60">60]</ref>, we adapt the model to the Cityscapes training set and evaluate the performance on the validation set.</p><p>To comprehensively demonstrate the improvement of CRST, we report the performance of CRST with all regularizers and compare with CBST in each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>Image classification: For VisDA17/Office-31, we implement CBST/CRSTs using PyTorch <ref type="bibr" target="#b41">[41]</ref> and choose ResNet-101/ResNet-50 <ref type="bibr" target="#b22">[22]</ref> as backbones. For fair comparison, we compare to other works with the same backbone networks. Both backbones are pre-trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>, and then fine-tuned on source domain using SGD, with learning rate 1 × 10 −3 , weight decay 5 × 10 −4 , momentum 0.9 and batch size 32. For self-training, we apply the same training strategy but a different learning rate 1 × 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation:</head><p>For semantic segmentation, we further consider DeepLabv2 <ref type="bibr" target="#b5">[6]</ref> as a backbone besides the ResNet-38 backbone in <ref type="bibr" target="#b68">[68]</ref>. For experiments with DeepLabv2, we implement CBST/CRSTs using PyTorch, while following the MXNet <ref type="bibr" target="#b6">[7]</ref> implementation of <ref type="bibr" target="#b68">[68]</ref> for experiments with ResNet-38. DeepLabv2 is pre-trained on ImageNet and fine-tuned on source domain using SGD, with learning rate 2.5 × 10 −4 , weight decay 5 × 10 −4 , momentum 0.9, batch size 2, patch size 512 × 1024, multiscale training augmentation (0.5 − 1.5) and horizontal flipping. In self-training, we apply SGD with learning rate of 5 × 10 −5 . For fair comparison, we unify the total number of self-training rounds to be 3, each with 2 re-training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Domain adaptation for image classification</head><p>VisDA17: We present the results on VisDA17 in <ref type="table" target="#tab_1">Table 2</ref> in terms of per-class accuracy and mean accuracy. For each proposed approach, we report the averages and standard deviations of the evaluation results over 5 runs. Note that both MRKLD and LRENT outperform the non-regularized CBST, whereas MRL2 and MRENT show slightly worse results. Among CRSTs with single regularizer, MRKLD achieves the best performance with considerable improvement. The combination of MRKLD and LRENT further outperforms single regularizers and other recently proposed methods. The result even outperforms certain methods with stronger backbones (ResNet-152) <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b53">53]</ref>. Office-31: We compare the performance of different methods on Office-31 with the same backbone ResNet-50 in <ref type="table" target="#tab_2">Table 3</ref>. All CRSTs achieve similar results that outperform the baseline CBST. In addition, MRKLD+LRENT again outperforms single regularizers, achieving comparable or better performance compared with other recent methods.</p><p>6.3. Domain adaptation for semantic segmentation GTA5 → Cityscapes:   DeepLabv2 backbone, one could see that MRKLD achieves the best result outperforming previous state-of-the-art. In addition, <ref type="figure" target="#fig_5">Fig. 10</ref> visualizes the adapted prediction results obtained by CBST and CRSTs on Cityscapes validation set. <ref type="figure" target="#fig_5">Fig. 11</ref> further compares the pseudo-label maps in the second round of self-training. On a wide ResNet-38 backbone, all CRSTs outperform the baseline CBST and we achieve the state-of-the-art system-level performance with the spatial priors (SP) and multi-scale testing (MST) from <ref type="bibr" target="#b68">[68]</ref>. SYNTHIA → Cityscapes: <ref type="table" target="#tab_6">Table 5</ref> shows the adaptation results where CRSTs again show the performance on par with or better than the baseline CBST. In particular, MRKLD maintains the best performance among all regularizers and outperforms the previous state-of-the-art [68].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Parameter analysis</head><p>p is an important parameter controling the pseudo-label generation and selection sensitivity. We adopt the same p policy as <ref type="bibr" target="#b68">[68]</ref> where we start p from 20%, and empirically add 5% to p in each additional self-training round. We conduct a sensitivity analysis for portion p similar to <ref type="bibr" target="#b68">[68]</ref>, where we consider the starting portion p 0 and the incremental portion ∆p on a difficult task of Office-31: W → A. <ref type="table" target="#tab_7">Table 6</ref> shows that CRSTs are not sensitive to p 0 and ∆p.</p><p>In CRST, the coefficient α is an important parameter that balances the weight between self-training loss and confidence regularizer. In all the experiments, we unify α to be 0.025, 0.1, 0.1, 0.25 for MRL2, MRENT, MRKLD and LRENT, respectively. Note that various regularizers have different α due to their intrinsic differences. We also present the sensitivity analysis of α on W → A in <ref type="table" target="#tab_8">Table 7</ref>. We can see all CRSTs are not sensitive to α in certain intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">How does confidence regularization work?</head><p>Confidence regularization smooths the output by lowering the confidence (the max of output softmax) and raising the probability level of other classes. Such smoothing helps to reduce the confidence on false positives (FP), although the confidence of certain true positives (TP) may also decrease. To see the change w/wo CR, we compare CBST vs MRKLD/LRENT (DeepLabv2) on GTA5 → Cityscapes, by presenting their per-class mean confidence of TP (C T P ), mean confidence of FP (C F P ) and the C T P /C F P ratios at the end of first round in <ref type="table">Table 8</ref>. For both TP and FP, the confidence of MRKLD/LRENT are lower than CBST, but either MRKLD or LRENT outperforms CBST on almost all per-class ratios and mean ratios. This intuitively illustrates how confidence regularization benefits self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">MR versus LR</head><p>We analyze MR/LR intuitively and theoretically to give suggestions for practical choice of confidence regularizers. Complexity analysis: All model regularizers only introduce negligible extra costs for the gradient computation. Label regularizers, however, requires the storage of datasetlevel soft pseudo-labels. This does not present an issue in image classification but may introduce extra I/O costs in segmentation, where labels are often too large to be stored in memory and need to be written to disk. Loss curves: To further illustrate the different properties of regularizers, we visualize how they influence the original loss surfaces by reducing the problem into binary classification with a single sample. We assume a cross-entropy loss −y log p − (1 − y) log(1 − p) plus an MR/LR weighted by α. For MRs, we assume y = 1 and illustrate the regularized loss curves versus p in <ref type="figure" target="#fig_0">Fig. 4</ref>. For all MRs, p * becomes smoother when α increases. We notice that MRKLD serves as a better barrier to prevent sharp outputs than other MRs DeepLabv2     by having steeper gradient near p = 1. This accords with our observation that MRKLD overall works the best. For LRENT, we assume p = 0.9 and illustrate the regularized loss curves versus y at different α in <ref type="figure" target="#fig_0">Fig. 4</ref>. Again, y * becomes smoother when α increases.</p><formula xml:id="formula_15">- - - - - - - - - - - - - - - - - - - 29.2 FCAN [67] - - - - - - - - - - - - - - - - - - -</formula><p>Class ranking: Based on the closed-form solution of LR in <ref type="table">Table 1</ref>, we can prove that LR preserves the confidence ranking order between classes. On the other hand, given one-hot labels, MRs tend to discard such order information by giving equal confidences to negative classes. Tak-   <ref type="table">Table 8</ref>: Comparison of C T P , C F P and C T P /C F P on GTA5 → Cityscapes.  ing MRKLD as example: using Lagrangian multiplier, we can prove the closed-form global minimizer for regularized cross-entropy loss as p * (k) = (y (k) + α K )/(1 + α), where k = 1, ..., K is class index. With y being one-hot, the global minimizer is uniformly smoothed on negative classes. Similar property can be also proved for MRENT/MRL2. <ref type="figure" target="#fig_4">Fig. 5</ref>, where we assume p = [0.2, 0.1, 0.55, 0.15] for LRENT and y (2) = 1 for MRKLD. One can see, LRENT sharpens the input p when α ∈ [0, 1] (one-hot when α = 0), while smooths p when α &gt; 1. In all cases, the inter-class confidence orders are always preserved, while the same property does not hold for MRKLD. MR+LR: The combination of MR and LR can take advantages of both regularizers and achieve better performance compared to single regularizer, demonstrated in VisDA17 and Office-31. However, it will also introduce extra cost to validate both hyperparameters for MR and LR. Practical suggestions: Overall, we recommend CRST-MRKLD most based on the above analysis and its better performance. Moreover, combining MR and LR may also benefit self-training at the cost of slight extra tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We illustrate two examples of LRENT and MRKLD in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>In this paper, we introduce a confidence regularized selftraining framework formulated as regularized self-training loss minimization. Model regularization and label regularization are considered with a family of proposed confidence regularizers. We investigate theoretical properties of CRST, including its probabilistic explanation and connection to softmax with temperature. Comprehensive experiments demonstrate the effectiveness of CRST with stateof-the-art performance. We also systematically discuss the pros and cons of the proposed regularizers and made practical suggestions. We believe this work can inspire future research on novel designs of regularizations as desired inductive biases to benefit many UDA/SSL problems.</p><formula xml:id="formula_16">K k=1 ( p(k|xt) λ k ) 1 α</formula><p>It is easy to see that the optimization in <ref type="formula" target="#formula_1">(11)</ref> is a convex problem. Therefore, the global optimum can be found with a Lagrangian multiplier <ref type="bibr" target="#b2">[3]</ref> defined as follows:</p><formula xml:id="formula_17">L (ŷ, β) = K k=1 −ŷ (k) t log p(k|x t ; w) λ k + α K k=1ŷ (k) t (log(ŷ (k) t ) − 1) + β( K k=1ŷ (k) t − 1)</formula><p>Setting the corresponding gradients equals to 0 gives the global optimum (k = 1, ..., K). Classification maximum likelihood (CML) was initially proposed to model clustering tasks, and can be optimized via classification expectation maximization (CEM). Compared with traditional expectation maximization (EM) that has an "expectation" (E) step and a "maximization" (M) step, CEM has an additional "classification" (C) step (between E and M steps) that assigns a sample to the cluster with maximal posterior probability. In <ref type="bibr" target="#b0">[1]</ref>, CML is generalized to discriminant semi-supervised learning with both labeled and unlabeled data defined as follows:</p><formula xml:id="formula_18">     ∂L ∂ŷ (i) † t = − log p(k|xt;w) λi + α logŷ (i) † t + β = 0; K k=1ŷ (k) † t = 1 ⇔     ŷ (i) † t = exp( −β α )( p(i|xt;w) λi ) 1 α ; K i=1ŷ (i) † t = 1 ⇔     ŷ (i) † t = exp( −β α )( p(i|xt;w) λi ) 1 α ; K i=1 exp( −β α )( p(k|xt;w) λ k ) 1 α = 1 ⇔     ŷ (i) † t = exp( −β α )( p(i|xt;w) λi ) 1 α ; exp( −β α ) = 1 K k=1 ( p(k|x t ;w) λ k ) 1 α ⇔         ŷ (i) † t = ( p(i|x t ;w) λ i ) 1 α K k=1 ( p(k|x t ;w) λ k ) 1 α ; β = α log</formula><formula xml:id="formula_19">log L C = logL C + i∈S,T log p(x i )</formula><p>where:</p><formula xml:id="formula_20">logL C = s∈S K k=1 y (k) s log p(k|x s ; w) + t∈T K k=1ŷ (k) t log p(k|x t ; w)</formula><p>Note thatŷ t ∈ {0, 1} K , ∀t. p(k|x t ; w) is the posterior probability modeled by classifiers such as logistic classifier and neural network and w is the learnable weight. <ref type="bibr" target="#b0">[1]</ref> uses a discriminant classifier which makes no assumptions about the data distribution p(x t ). Thus maximizing (B.1) is equal to maximizing (B.1). Below we draw the connection of the CRST self-training algorithm to CEM. We first show that CRST can be rewritten as the following regularized classification maximum likelihood model: </p><formula xml:id="formula_21">= logL C + R C s.t.ŷ t ∈ ∆ (K−1) ∪ {0}, ∀t</formula><p>where the above problem contains an additional regularizer term (R C ) compared with CML, defined as:</p><formula xml:id="formula_22">R C = − t∈T K k=1ŷ (k) t log λ k + αr c (w,ŷ t )</formula><p>In addition, the corresponding alternative self-training optimization can be written as the following CEM process: E-Step: Given the model weight w, estimate the posterior probability p(x t ; w), ∀t. C-Step: Fix w and solve the following problem forŶ T : </p><formula xml:id="formula_23">max Y T t∈T K k=1ŷ (k) t log p(k|x t ; w) − αr c (w,ŷ t ) s.t.ŷ t ∈ ∆ (K−1) ∪ {0},</formula><formula xml:id="formula_24">+ t∈T K k=1ŷ (k) t log p(k|x t ; w) − αr c (w,ŷ t )</formula><p>We have thus shown that the CRST self-training algorithm is an instance of CEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Proposition 2</head><p>As a brief recap, the general form of CRST in (5) can be optimized via the following two steps: a) Pseudo-label learning Fix w and solve:</p><formula xml:id="formula_25">min Y T t∈T K k=1 −ŷ (k) t log p(k|x t ; w) λ k + αr c (w,ŷ t ) s.t.ŷ t ∈ ∆ (K−1) ∪ {0}, ∀t<label>(12)</label></formula><p>which leads to the following solver for eachŷ t :</p><formula xml:id="formula_26">y * t = ŷ † t , if C(ŷ † t ) &lt; C(0) 0 , otherwise<label>(13)</label></formula><p>where y † t is the minimizer of (12) with the feasible set being ∆ K−1 only, and C(ŷ t ) is defined as:</p><formula xml:id="formula_27">C(ŷ t ) = −ŷ (k) t K k=1 log p(k|x t ; w) λ k + αr c (w,ŷ t )</formula><p>b) Network retraining FixŶ T and solve the following optimization by gradient descent:</p><formula xml:id="formula_28">min w − s∈S K k=1 y (k) s log(p(k|x s ; w)) − t∈T [ K k=1ŷ (k) t log(p(k|x t ; w)) − αr c (w,ŷ t )]<label>(14)</label></formula><p>We assume α ≥ 0, and r c (w,ŷ t ) is convex w.r.t. w and y t given the listed regularizers in <ref type="table">Table 1</ref>. Note that the definition and optimization of continuous CBST is simply a special case of CRST with α = 0. Therefore, the convergence of CRST also indicates the convergence of CBST. With the above preliminaries, we have:</p><p>Step a) is non-increasing: (13) is obtained by decomposing (12) into two subproblems with feasible sets being ∆ K−1 and 0, respectively. The former is a convex problems which gives a globally optimal solution, while (13) is the result of comparing this solution against 0 by taking the one with a smaller cost. As a result, <ref type="bibr" target="#b12">(13)</ref> is also a global minimizer and (12) is guaranteed to be non-increasing.</p><p>Step b) is non-increasing: One may use gradient descent to minimize the loss in <ref type="bibr" target="#b13">(14)</ref>. With a proper learning rate, the loss is guaranteed to decrease monotonically. In practice, network re-training is often done with mini-batch gradient descent instead of gradient descent. This may not strictly guarantee the monotonic decrease of the loss, but will almost certainly converge to a lower one.</p><p>One can prove that the self-training loss in (5) is lower bounded. Therefore, the optimization of (5) by alternatively taking step a) and b) is convergent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Confusion matrix</head><p>In <ref type="figure">Fig. 8</ref>, we illustrate the normalized confusion matrices of the source model, CBST and MRKLD+LRENT on VisDA17. One can see, both CBST and MRKLD+LRENT show more diagonalized confusion matrices than source model, and MRKLD+LRENT shows less mistakes. Specifically, the confusions between pairwise different classes such as "person vs. horse" and "motor vs. bike" have be reduced by confidence regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Distributions of softmax probability entries</head><p>Following the analysis approach in <ref type="bibr" target="#b43">[43]</ref>, we present the distributions of predicted softmax probability entries in the target domain for different models. Specifically, we consider the ResNet-38 backbone on GTA5 → Cityscapes, with   <ref type="figure">Figure 8</ref>: Confusion matrices with normalization for CBST and CRSTs. the distributions shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. One could see that confidence regularization promote softer distributions by significantly reducing the proportion of highly confident entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Segmentation visualization</head><p>For qualitative evaluation, we visualize the segmentation predictions obtained by different models in <ref type="figure" target="#fig_5">Fig. 10</ref>. Specifically, predictions are made on sampled Cityscapes validation images by GTA5 → Cityscapes models. In <ref type="figure" target="#fig_5">Fig. 11</ref>, we also visualize the pseudo-labels on sampled Cityscapes training images at the beginning second self-training round. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 4 .</head><label>4</label><figDesc>KLD model confidence regulared selftraining is equivalent to self-training with pseudo-label uniformly smoothed by = (Kα − α)/(K + Kα), where α is the regularizer weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Adaptation results on GTA5 → Cityscapes. Rows correspond to sample images in Cityscapes. From left to right, columns correspond to original images, ground truth, and predication results of CBST, MRL2, MRENT, MRKLD, LRENT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Pseudo-labels in GTA5 → Cityscapes. Rows correspond to sample images in Cityscapes. From left to right, columns correspond to original images, ground truth, and pseudo-labels of CBST, MRL2, MRENT, MRKLD, LRENT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Loss curves regularized by different regularizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Minimizers of LRENT and MRKLD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Proof of Proposition 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>t</head><label></label><figDesc>log λ k + αr c (w,ŷ t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Mean accuracy versus number of epochs. Feature visualization for target domain of VisDA17. From left to right: Source model, CBST, MRKLD+LRENT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Histograms of softmax probability entries in target domain of GTA5 → Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Adaptation results on GTA5 → Cityscapes. Rows correspond to sample images in Cityscapes. From top to bottom, rows correspond to original images, ground truth, and predication results of CBST, MRL2, MRENT, MRKLD, LRENT. Adaptation results on GTA5 → Cityscapes. Rows correspond to sample images in Cityscapes. From top to bottom, rows correspond to original images, ground truth, and pseudo-label maps of CBST, MRL2, MRENT, MRKLD, LRENT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4</head><label>4</label><figDesc>shows the adaptation performance of CRSTs and other comparing methods. On a 2±2.4 78.8±1.0 56.5±2.2 55.4±3.6 85.1±1.4 79.2±10.3 83.8±0.4 77.7±4.0 82.8±2.8 88.8±3.2 69.0±2.9 72.0±3.8 76.4±0.9 MRL2 87.0±2.9 79.5±1.9 57.1±3.2 54.7±2.9 85.5±1.1 78.1±11.7 83.0±1.5 77.7±3.7 82.4±1.7 88.6±2.7 69.1±2.2 71.8±3.0 76.2±1.0 MRENT 87.1±2.7 78.3±0.7 56.1±4.0 54.4±2.7 84.4±2.3 79.9±10.6 83.7±1.1 77.9±4.4 82.7±2.4 87.4±2.8 70.0±1.4 72.8±3.3 76.2±0.8 MRKLD 87.3±2.5 79.4±1.9 60.5±2.4 59.7±2.5 87.6±1.4 82.4±4.4 86.5±1.1 78.4±2.6 84.6±1.7 86.4±2.8 72.5±2.4 69.8±2.5 77.9±0.5 LRENT 87.7±2.4 78.7±0.8 57.3±3.3 54.5±4.0 84.8±1.7 79.7±10.3 84.2±1.4 77.4±3.7 83.1±1.5 88.3±2.6 70.9±2.1 72.6±2.4 76.6±0.9 MRKLD+LRENT 88.0±0.6 79.2±2.2 61.0±3.1 60.0±1.0 87.5±1.2 81.4±5.6 86.3±1.5 78.8±2.1 85.6±0.9 86.6±2.5 73.9±1.3 68.8±2.3 78.1±0.2</figDesc><table><row><cell>Method</cell><cell>Aero</cell><cell>Bike</cell><cell>Bus</cell><cell>Car</cell><cell>Horse</cell><cell>Knife</cell><cell>Motor</cell><cell>Person</cell><cell>Plant</cell><cell>Skateboard</cell><cell>Train</cell><cell>Truck</cell><cell>Mean</cell></row><row><cell>Source [50]</cell><cell>55.1</cell><cell>53.3</cell><cell>61.9</cell><cell>59.1</cell><cell>80.6</cell><cell>17.9</cell><cell>79.7</cell><cell>31.2</cell><cell>81.0</cell><cell>26.5</cell><cell>73.5</cell><cell>8.5</cell><cell>52.4</cell></row><row><cell>MMD [33]</cell><cell>87.1</cell><cell>63.0</cell><cell>76.5</cell><cell>42.0</cell><cell>90.3</cell><cell>42.9</cell><cell>85.9</cell><cell>53.1</cell><cell>49.7</cell><cell>36.3</cell><cell>85.8</cell><cell>20.7</cell><cell>61.1</cell></row><row><cell>DANN [16]</cell><cell>81.9</cell><cell>77.7</cell><cell>82.8</cell><cell>44.3</cell><cell>81.2</cell><cell>29.5</cell><cell>65.1</cell><cell>28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>ENT [19]</cell><cell>80.3</cell><cell>75.5</cell><cell>75.8</cell><cell>48.3</cell><cell>77.9</cell><cell>27.3</cell><cell>69.7</cell><cell>40.2</cell><cell>46.5</cell><cell>46.6</cell><cell>79.3</cell><cell>16.0</cell><cell>57.0</cell></row><row><cell>MCD [51]</cell><cell>87.0</cell><cell>60.9</cell><cell>83.7</cell><cell>64.0</cell><cell>88.9</cell><cell>79.6</cell><cell>84.7</cell><cell>76.9</cell><cell>88.6</cell><cell>40.3</cell><cell>83.0</cell><cell>25.8</cell><cell>71.9</cell></row><row><cell>ADR [50]</cell><cell>87.8</cell><cell>79.5</cell><cell>83.7</cell><cell>65.3</cell><cell>92.3</cell><cell>61.8</cell><cell>88.9</cell><cell>73.2</cell><cell>87.8</cell><cell>60.0</cell><cell>85.5</cell><cell>32.3</cell><cell>74.8</cell></row><row><cell>SimNet-Res152 [44]</cell><cell>94.3</cell><cell>82.3</cell><cell>73.5</cell><cell>47.2</cell><cell>87.9</cell><cell>49.2</cell><cell>75.1</cell><cell>79.7</cell><cell>85.3</cell><cell>68.5</cell><cell>81.1</cell><cell>50.3</cell><cell>72.9</cell></row><row><cell>GTA-Res152 [53]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.1</cell></row><row><cell>Source-Res101</cell><cell>68.7</cell><cell>36.7</cell><cell>61.3</cell><cell>70.4</cell><cell>67.9</cell><cell>5.9</cell><cell>82.6</cell><cell>25.5</cell><cell>75.6</cell><cell>29.4</cell><cell>83.8</cell><cell>10.9</cell><cell>51.6</cell></row><row><cell>CBST</cell><cell>87.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on VisDA17.</figDesc><table><row><cell>Method</cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell>Mean</cell></row><row><cell>ResNet-50 [22]</cell><cell cols="6">68.4±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3</cell><cell>76.1</cell></row><row><cell>DAN [33]</cell><cell cols="6">80.5±0.4 97.1±0.2 99.6±0.1 78.6±0.2 63.6±0.3 62.8±0.2</cell><cell>80.4</cell></row><row><cell>RTN [35]</cell><cell cols="6">84.5±0.2 96.8±0.1 99.4±0.1 77.5±0.3 66.2±0.2 64.8±0.3</cell><cell>81.6</cell></row><row><cell>DANN [16]</cell><cell cols="6">82.0±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5</cell><cell>82.2</cell></row><row><cell>ADDA [61]</cell><cell cols="6">86.2±0.5 96.2±0.3 98.4±0.3 77.8±0.3 69.5±0.4 68.9±0.5</cell><cell>82.9</cell></row><row><cell>JAN [36]</cell><cell cols="6">85.4±0.3 97.4±0.2 99.8±0.2 84.7±0.3 68.6±0.3 70.0±0.4</cell><cell>84.3</cell></row><row><cell>GTA [53]</cell><cell cols="6">89.5±0.5 97.9±0.3 99.8±0.4 87.7±0.5 72.8±0.3 71.4±0.4</cell><cell>86.5</cell></row><row><cell>CBST</cell><cell cols="6">87.8±0.8 98.5±0.1 100±0.0 86.5±1.0 71.2±0.4 70.9±0.7</cell><cell>85.8</cell></row><row><cell>MRL2</cell><cell cols="6">88.4±0.2 98.6±0.1 100±0.0 87.7±0.9 71.8±0.2 72.1±0.2</cell><cell>86.4</cell></row><row><cell>MRENT</cell><cell cols="6">88.0±0.4 98.6±0.1 100±0.0 87.4±0.8 72.7±0.2 71.0±0.4</cell><cell>86.4</cell></row><row><cell>MRKLD</cell><cell cols="6">88.4±0.9 98.7±0.1 100±0.0 88.0±0.9 71.7±0.8 70.9±0.4</cell><cell>86.3</cell></row><row><cell>LRENT</cell><cell cols="6">88.6±0.4 98.7±0.1 100±0.0 89.0±0.8 72.0±0.6 71.0±0.3</cell><cell>86.6</cell></row><row><cell cols="7">MRKLD+LRENT 89.4±0.7 98.9±0.4 100±0.0 88.7±0.8 72.6±0.7 70.9±0.5</cell><cell>86.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Experimental results on Office-31.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on GTA5 → Cityscapes.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="5">Road SW Build Wall* Fence* Pole* TL</cell><cell>TS Veg. Sky</cell><cell cols="5">PR Rider Car Bus Motor Bike mIoU mIoU*</cell></row><row><cell>Source MCD [51]</cell><cell>DRN-105</cell><cell>14.9 11.4 58.7 84.8 43.6 79.0</cell><cell>1.9 3.9</cell><cell>0.0 0.2</cell><cell>24.1 29.1</cell><cell>1.2 7.2</cell><cell cols="2">6.0 68.8 76.0 54.3 5.5 83.8 83.1 51.0 11.7 79.9 27.2 7.1 34.2 15.0</cell><cell>0.8 6.2</cell><cell>0.0 0.0</cell><cell>23.4 37.3</cell><cell>26.8 43.5</cell></row><row><cell>Source AdaptSegNet [60]</cell><cell>DeepLabv2</cell><cell>55.6 23.8 74.6 84.3 42.7 77.5</cell><cell>− −</cell><cell>− −</cell><cell>− −</cell><cell cols="3">6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2</cell><cell>13.7 18.9</cell><cell>25.0 32.3</cell><cell>− −</cell><cell>38.6 46.7</cell></row><row><cell>AdvEnt [63]</cell><cell>DeepLabv2</cell><cell>85.6 42.2 79.7</cell><cell>8.7</cell><cell>0.4</cell><cell>25.9</cell><cell>5.4</cell><cell cols="2">8.1 80.4 84.1 57.9 23.8 73.3 36.4</cell><cell>14.2</cell><cell>33.0</cell><cell>41.2</cell><cell>48.0</cell></row><row><cell>Source CBST [68]</cell><cell>ResNet-38</cell><cell>32.6 21.5 46.5 53.6 23.7 75.0</cell><cell>4.8 12.5</cell><cell>0.1 0.3</cell><cell cols="4">26.5 14.8 13.1 70.8 60.3 56.6 36.4 23.5 26.3 84.8 74.7 67.2 17.5 84.5 28.4 3.5 74.1 20.4</cell><cell>8.9 15.2</cell><cell>13.1 55.8</cell><cell>29.2 42.5</cell><cell>33.6 48.4</cell></row><row><cell>Source</cell><cell></cell><cell>64.3 21.3 73.1</cell><cell>2.4</cell><cell>1.1</cell><cell>31.4</cell><cell cols="3">7.0 27.7 63.1 67.6 42.2 19.9 73.1 15.3</cell><cell>10.5</cell><cell>38.9</cell><cell>34.9</cell><cell>40.3</cell></row><row><cell>CBST</cell><cell></cell><cell>68.0 29.9 76.3</cell><cell>10.8</cell><cell>1.4</cell><cell cols="4">33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5</cell><cell>18.8</cell><cell>39.8</cell><cell>42.6</cell><cell>48.9</cell></row><row><cell>MRL2 MRENT</cell><cell>DeepLabv2</cell><cell>63.4 27.1 76.4 69.6 32.6 75.8</cell><cell>14.2 12.2</cell><cell>1.4 1.8</cell><cell cols="4">35.2 23.6 29.4 78.5 77.8 61.4 29.5 82.2 22.8 35.3 23.3 29.5 77.7 78.9 60.0 28.5 81.5 25.9</cell><cell>18.9 19.6</cell><cell>42.3 41.8</cell><cell>42.8 43.4</cell><cell>48.7 49.6</cell></row><row><cell>MRKLD</cell><cell></cell><cell>67.7 32.2 73.9</cell><cell>10.7</cell><cell>1.6</cell><cell cols="4">37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0</cell><cell>19.4</cell><cell>45.3</cell><cell>43.8</cell><cell>50.1</cell></row><row><cell>LRENT</cell><cell></cell><cell>65.6 30.3 74.6</cell><cell>13.8</cell><cell>1.5</cell><cell cols="4">35.8 23.1 29.1 77.0 77.5 60.1 28.5 82.2 22.6</cell><cell>20.1</cell><cell>41.9</cell><cell>42.7</cell><cell>48.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on SYNTHIA → Cityscapes.</figDesc><table><row><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic lgt</cell><cell>traffic sgn</cell><cell>vegetation</cell><cell>ignored</cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Sensitivity analysis of portion p 0 and portion step ∆p. Accuracy 71.5±0.8 72.1±0.2 71.7±1.1 71.0±0.8 71.0±0.4 70.9±1.0 70.9±0.6 70.9±0.4 70.6±0.7 71.2±1.2 71.0±0.3 70.8±0.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">W → A (Office-31)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MRL2</cell><cell></cell><cell></cell><cell>MRENT</cell><cell></cell><cell></cell><cell>MRKLD</cell><cell></cell><cell></cell><cell>LRENT</cell><cell></cell></row><row><cell>α</cell><cell>0.01</cell><cell>0.025</cell><cell>0.05</cell><cell>0.075</cell><cell>0.1</cell><cell>0.125</cell><cell>0.075</cell><cell>0.1</cell><cell>0.125</cell><cell>0.1</cell><cell>0.25</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Sensitivity analysis of regularizer weight α.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Road SW Build Wall Fence Pole TL</cell><cell cols="3">TS Veg. Terrain Sky</cell><cell cols="5">PR Rider Car Truck Bus Train Motor Bike mean</cell></row><row><cell></cell><cell>CT P (%)</cell><cell>96.2 86.0 94.6 83.8</cell><cell>84.9</cell><cell cols="2">84.5 80.4 78.0 93.9</cell><cell>87.9</cell><cell cols="2">94.5 90.4 81.4 95.4</cell><cell>88.4</cell><cell cols="2">85.9 59.5</cell><cell>78.5</cell><cell>80.6</cell><cell>85.5</cell></row><row><cell>CBST</cell><cell>CF P (%)</cell><cell>72.2 74.1 69.8 71.7</cell><cell>76.7</cell><cell cols="2">73.7 72.9 76.5 71.9</cell><cell>71.2</cell><cell cols="2">68.5 67.2 69.1 66.1</cell><cell>76.9</cell><cell cols="2">65.5 76.7</cell><cell>67.2</cell><cell>73.0</cell><cell>71.6</cell></row><row><cell></cell><cell>CT P /CF P</cell><cell>1.33 1.16 1.36 1.17</cell><cell>1.11</cell><cell cols="2">1.15 1.10 1.02 1.31</cell><cell>1.23</cell><cell cols="2">1.38 1.35 1.18 1.44</cell><cell>1.15</cell><cell cols="2">1.31 0.78</cell><cell>1.17</cell><cell>1.10</cell><cell>1.19</cell></row><row><cell></cell><cell>CT P (%)</cell><cell>94.7 82.8 92.4 81.7</cell><cell>77.8</cell><cell cols="2">84.4 77.0 76.4 93.4</cell><cell>86.5</cell><cell cols="2">94.4 88.8 79.7 93.9</cell><cell>87.0</cell><cell cols="2">84.9 71.9</cell><cell>77.6</cell><cell>79.2</cell><cell>84.5</cell></row><row><cell>MRKLD</cell><cell>CF P (%)</cell><cell>67.7 70.3 65.4 68.5</cell><cell>69.2</cell><cell cols="2">66.7 69.4 71.3 66.7</cell><cell>68.8</cell><cell cols="2">66.7 60.0 65.5 63.0</cell><cell>74.6</cell><cell cols="2">63.6 70.2</cell><cell>59.3</cell><cell>53.2</cell><cell>66.3</cell></row><row><cell></cell><cell>CT P /CF P</cell><cell>1.40 1.18 1.41 1.19</cell><cell>1.12</cell><cell cols="2">1.27 1.11 1.07 1.40</cell><cell>1.26</cell><cell cols="2">1.42 1.48 1.22 1.49</cell><cell>1.17</cell><cell cols="2">1.34 1.02</cell><cell>1.31</cell><cell>1.49</cell><cell>1.27</cell></row><row><cell></cell><cell>CT P (%)</cell><cell>95.9 84.4 94.0 80.7</cell><cell>75.3</cell><cell cols="2">84.8 77.8 78.3 93.9</cell><cell>86.3</cell><cell cols="2">94.5 89.2 79.3 95.3</cell><cell>89.3</cell><cell cols="2">80.5 76.4</cell><cell>86.4</cell><cell>78.8</cell><cell>85.3</cell></row><row><cell>LRENT</cell><cell>CF P (%)</cell><cell>69.5 72.1 68.0 67.8</cell><cell>71.3</cell><cell cols="2">69.7 71.5 75.4 69.5</cell><cell>69.9</cell><cell cols="2">70.1 64.1 67.6 67.3</cell><cell>77.7</cell><cell cols="2">70.3 63.4</cell><cell>58.6</cell><cell>55.2</cell><cell>68.4</cell></row><row><cell></cell><cell>CT P /CF P</cell><cell>1.38 1.17 1.38 1.19</cell><cell>1.06</cell><cell cols="2">1.22 1.09 1.04 1.35</cell><cell>1.23</cell><cell cols="2">1.35 1.39 1.17 1.42</cell><cell>1.15</cell><cell>1.15</cell><cell>1.2</cell><cell>1.47</cell><cell>1.43</cell><cell>1.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>∀t M-Step: FixŶ T and use gradient ascent to solve the following problem for w.</figDesc><table><row><cell></cell><cell></cell><cell>K</cell><cell></cell></row><row><cell>max w</cell><cell>s∈S</cell><cell>k=1</cell><cell>y (k) s log p(k|x s ; w)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we present the additional details and results that are not covered by the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of soft pseudo-label in LRENT</head><p>For entropy label regularizer, the soft pseudo-label learning problem is defined as follows.</p><p>where the solution is given as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Proof of Proposition 4</head><p>As mentioned in <ref type="bibr" target="#b57">[57]</ref>, uniformly smoothed pseudo-label</p><p>And the self-training with uniformaly smoothed pseudolabels is defined as follows.</p><p>whereŷ t follows (15).</p><p>In KLD model regularized self-training, the model retraining needs to optimize the following problem:</p><p>whereŷ t , ∀t are the fixed pseudo-labels and α is the regularizer weight. Here, we show the equivalence of the above two problems with the following proof:</p><p>Replacingŷ t with a one-hot completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Proof of Proposition 5</head><p>In MRENT, the model retraining needs to optimize the following problem:</p><p>We will show the above problem is equivalent to the model retraining in the reverse KLD model regularized selftraining, which is defined as follows.</p><p>To prove the above equivalence, we have the following.</p><p>In <ref type="bibr" target="#b20">(20)</ref>, K is a constant. Thus one can prove that the minimization in <ref type="bibr" target="#b18">(18)</ref> is equivalent to the minimization in <ref type="bibr" target="#b19">(19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional details on experiments C.1. Accuracy curves</head><p>To show the learning behaviors on VisDA17, we plot the curves of mean accuracy (averaged over 5 runs) versus epochs for CBST and CRSTs in <ref type="figure">Fig. 6</ref>. One can see, the proposed self-training methods are generally stable with only slight fluctuations after 10 epochs. Among all comparing methods, MRKLD+LRENT gives the best performance and shows consistent improvement over the CBST baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Feature visualization</head><p>We also visualize the feature embeddings of the source model, CBST and MRKLD+LRENT features on VisDA17, and show them in <ref type="figure">Fig. 7</ref>. Both CBST and MRKLD+LRENT obtain improved class-wise feature alignment than the source model. MRKLD+LRENT shows slightly more accurate feature alignment due to the improved performance from confidence regularization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Massih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessam</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<title level="m">Label refinery: Improving imagenet classification through label progression</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open set domain adaptation for image and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Pau Panareda Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain stylization: A fast covariance matching framework towards domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<imprint>
			<pubPlace>Hugo Larochelle, François Laviolette, Mario</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DLOW: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.2" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via calibrating uncertainties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lezi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation: A deep max-margin Gaussian process approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature-level frankenstein: Eliminating variations for discriminative recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Site</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingsheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bvk</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conservative wasserstein training for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visda: A synthetic-to-real benchmark for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simultaneous edge alignment and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Srikumar Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
