<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
						</author>
						<title level="a" type="main">Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose Flowtron: an autoregressive flow-based generative network for textto-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive melspectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pretrained models will be made publicly available at https://github.com/NVIDIA/flowtron.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current speech synthesis methods do not give the user enough control over how speech actually sounds. Automatically converting text to audio that successfully communicates the text was achieved a long time ago <ref type="bibr" target="#b23">(Umeda et al., 1968;</ref><ref type="bibr" target="#b3">Badham et al., 1983)</ref>. However, communicating only the text information leaves out all of the acoustic properties of the voice that convey much of the meaning and human expressiveness. Nearly all the research into speech synthesis since the 1960s has focused on adding that non-textual information to synthesized speech. But in spite of this, the typical speech synthesis problem is formulated as a text to speech problem in which the user inputs only text.</p><p>Taming the non-textual information in speech is difficult 1 NVIDIA Applied Deep Learning Research (ADLR). Correspondence to: <ref type="bibr">Rafael Valle &lt;rafaelvalle@nvidia.com&gt;.</ref> because the non-textual is unlabeled. A voice actor may speak the same text with different emphasis or emotion based on context, but it is unclear how to label a particular reading. Without labels for the non-textual information, models have fallen back to unsupervised learning. Recent models have achieved nearly human-level quality, despite treating the non-textual information as a black box. The model's only goal is to match the patterns in the training data <ref type="bibr" target="#b21">(Shen et al., 2017;</ref><ref type="bibr" target="#b2">Arik et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b18">Ping et al., 2017)</ref>. Despite these models' excellent ability to recreate the non-textual information in the training set, the user has no insight into or control over the non-textual information.</p><p>It is possible to formulate an unsupervised learning problem in such a way that the user can gain insights into the structure of a data set. One way is to formulate the problem such that the data is assumed to have a representation in some latent space, and have the model learn that representation. This latent space can then be investigated and manipulated to give the user more control over the generative model's output. Such approaches have been popular in image generation for some time now, allowing users to interpolate smoothly between images and to identify portions of the latent space that correlate with various features <ref type="bibr" target="#b20">(Radford et al., 2015;</ref><ref type="bibr" target="#b13">Kingma &amp; Dhariwal, 2018)</ref>.</p><p>In audio, however, approaches have focused on embeddings that remove a large amount of information and are obtained from assumptions about what is interesting. Recent approaches that utilize deep learning for expressive speech synthesis combine text and a learned latent embedding for prosody or global style <ref type="bibr" target="#b22">Skerry-Ryan et al., 2018)</ref>. A variation of this approach is proposed by <ref type="bibr" target="#b10">(Hsu et al., 2018)</ref>, wherein a Gaussian mixture model (GMM) encoding the audio is added to Tacotron to learn a latent embedding. These approaches control the nontextual information by learning a bank of embeddings or by providing the target output as an input to the model and compressing it. However, these approaches require making assumptions about the dimensionality of the embeddings before hand and are not guaranteed to contain all the nontextual information it takes to reconstruct speech, including the risk of having dummy dimensions or not enough capacity, as the appendix sections in <ref type="bibr" target="#b22">Skerry-Ryan et al., 2018;</ref><ref type="bibr" target="#b10">Hsu et al., 2018)</ref> confirm. They also require finding an encoder and embedding that prevents the model from simply learning a complex identity function that ignores other inputs. Furthermore, these approaches focus on fixed-length embeddings under the assumption that variable-length embeddings are not robust to text and speaker perturbations. Finally, most of these approaches do not give the user control over the degree of variability in the synthesized speech.</p><p>In this paper we propose Flowtron: an autoregressive flowbased generative network for mel-spectrogram synthesis with control over acoustics and speech. Flowtron learns an invertible function that maps a distribution over melspectrograms to a latent z space parameterized by a spherical Gaussian. With this formalization, we can generate samples containing specific speech charateristics manifested in mel-space by finding and sampling the corresponding region in z-space. In the basic approach, we generate samples by sampling a zero mean spherical Gaussian prior and control the amount of variation by adjusting its variance. Despite its simplicity, this approach offers more speech variation and control than Tacotron.</p><p>In Flowtron, we can access specific regions of melspectrogram space by sampling a posterior distribution conditioned on prior evidence from existing samples <ref type="bibr" target="#b13">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b8">Gambardella et al., 2019)</ref>. This approach allows us to make a monotonous speaker more expressive by computing the region in z-space associated with expressive speech as it is manifested in the prior evidence. Finally, our formulation also allows us to impose a structure to the z-space and parametrize it with a Gaussian mixture, for example. In this approach related to <ref type="bibr" target="#b10">(Hsu et al., 2018)</ref>, speech charateristics in mel-spectrogram space can be associated with individual components. Hence, it is possible to generate samples with specific speech characteristics by selecting a component or a mixture thereof 1 .</p><p>Although VAEs and GANs <ref type="bibr" target="#b10">(Hsu et al., 2018;</ref><ref type="bibr" target="#b4">Bińkowski et al., 2019;</ref><ref type="bibr" target="#b0">Akuzawa et al., 2018)</ref> based models also provide a latent prior that can be easily manipulated, in Flowtron this comes at no cost in speech quality nor optimization challenges. We find that Flowtron is able to generalize and produce sharp mel-spectrograms by simply maximizing the likelihood of the data while not requiring any additional Prenet or Postnet layer , nor compound loss functions required by most state of the art models like <ref type="bibr" target="#b21">(Shen et al., 2017;</ref><ref type="bibr" target="#b2">Arik et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b18">Ping et al., 2017;</ref><ref type="bibr" target="#b22">Skerry-Ryan et al., 2018;</ref><ref type="bibr" target="#b28">Wang et al., 2018;</ref><ref type="bibr" target="#b4">Bińkowski et al., 2019)</ref>.</p><p>Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. It 1 What is relevant statistically might not be perceptually.</p><p>learns an invertible mapping of the a latent space that can be manipulated to control many aspects of speech synthesis. Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples, and style transfer between seen and unseen speakers with similar and different sentences. To our knowledge, this work is the first to show evidence that normalizing flow models can also be used for text-to-speech synthesis. We hope this will further stimulate developments in normalizing flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Earlier approaches to text-to-speech synthesis that achieve human like results focus on synthesizing acoustic features from text, treating the non-textual information as a black box. <ref type="bibr" target="#b21">(Shen et al., 2017;</ref><ref type="bibr" target="#b2">Arik et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b18">Ping et al., 2017)</ref>. Approaches like <ref type="bibr" target="#b21">Shen et al., 2017)</ref> require adding a critical Prenet layer to help with convergence and improve generalization . Furthermore, such models require an additional Postnet residual layer and modified loss to produce "better resolved harmonics and high frequency formant structures, which reduces synthesis artifacts."</p><p>One approach to dealing with this lack of labels for underlying non-textual information is to look for hand engineered statistics based on the audio that we believe are correlated with this underlying information. This is the approach taken by models like <ref type="bibr" target="#b16">(Nishimura et al., 2016;</ref><ref type="bibr" target="#b15">Lee et al., 2019)</ref>, wherein utterances are conditioned on audio statistics that can be calculated directly from the training data such as F 0 (fundamental frequency). However, in order to use such models, the statistics we hope to approximate must be decided upon a-priori, and the target value of these statistics must be determined before synthesis.</p><p>Another approach to dealing with the issue of unlabeled non-textual information is to learn a latent embedding for prosody or global style. This is the approach taken by models like <ref type="bibr" target="#b28">Wang et al., 2018)</ref>, wherein in a bank of embeddings or a latent embedding space of prosody is learned from unlabelled data. While these approaches have shown promise, manipulating such latent variables only offers a coarse control over expressive characteristics of speech.</p><p>A mixed approach consists of combining engineered statistics with latent embeddings learned in an unsupervised fashion. This is the approach taken by models like Mellotron <ref type="bibr" target="#b25">(Valle et al., 2019b)</ref>. In Mellotron, utterances are conditioned on both audio statistics and a latent embedding of acoustic features derived from a reference acoustic representation. Despite its advantages, this approach still requires determining these statistics before synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Flowtron</head><p>Flowtron is an autoregressive generative model that generates a sequence of mel spectrogram frames p(x) by producing each mel-spectrogram frame based on previous melspectrogram frames p(x) = p(x t |x 1:t−1 ). Our setup uses a neural network as a generative model by sampling from a simple distribution p(z). We consider two simple distributions with the same number of dimensions as our desired mel-spectrogram: a zero-mean spherical Gaussian and a mixture of spherical Gaussians with fixed or learnable parameters.</p><formula xml:id="formula_0">z ∼ N (z; 0, I) (1) z ∼ kφ k N (z;μ k ,Σ k )<label>(2)</label></formula><p>These samples are put through a series of invertible, parametrized transformations f , in our case affine transformations that transform p(z) into p(x).</p><formula xml:id="formula_1">x = f 0 • f 1 • . . . f k (z)<label>(3)</label></formula><p>As it is illustrated in <ref type="bibr" target="#b14">(Kingma et al., 2016)</ref>, in autoregressive normalizing flows the t-th variable z t only depends on previous timesteps z 1:t−1 :</p><formula xml:id="formula_2">z t = f k (z 1:t−1 )<label>(4)</label></formula><p>By using parametrized affine transformations for f and due to the autoregressive structure, the Jacobian determinant of each of the transformations f is lower triangular, hence easy to compute. With this setup we can train Flowtron by maximizing the log-likelihood of the data, which can be done using the change of variables:</p><formula xml:id="formula_3">log p θ (x) = log p θ (z) + k i=1 log | det(J (f −1 i (x)))| (5) z = f −1 k • f −1 k−1 • . . . f −1 0 (x)<label>(6)</label></formula><p>For the forward pass through the network, we take the melspectrograms as vectors and process them through several "steps of flow conditioned on the text and speaker ids. A step of flow here consists of an affine coupling layer, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Affine Coupling Layer</head><p>Invertible neural networks are typically constructed using coupling layers <ref type="bibr" target="#b6">(Dinh et al., 2014;</ref><ref type="bibr" target="#b13">Kingma &amp; Dhariwal, 2018)</ref>. In our case, we use an affine coupling layer <ref type="bibr" target="#b7">(Dinh et al., 2016)</ref>. Every input x t−1 produces scale and bias terms, s and b respectively, that affine-transform the succeeding input x t :</p><formula xml:id="formula_4">(log s t , b t ) = N N (x 1:t−1 , text, speaker) (7) x t = s t x t + b t<label>(8)</label></formula><p>Here N N () can be any autoregressive causal transformation. This can be achieved by time-wise concatenation of a 0valued vector to the input provided to N N (). The affine coupling layer preserves invertibility for the overall network, even though N N () does not need to be invertible. This follows because the first input of N N () is a constant and due to the autoregressive nature of the model the scaling and translation terms s t and b t only depend on x 1:t−1 and the fixed text and speaker vectors. Accordingly, when inverting the network, we can compute s t and b t from the preceding input x 1:t−1 , and then invert x t to compute x t , by simply recomputing N N (x 1:t−1 , text, speaker).</p><p>With an affine coupling layer, only the s t term changes the volume of the mapping and adds a change of variables term to the loss. This term also serves to penalize the model for non-invertible affine mappings.</p><formula xml:id="formula_5">log | det(J (f −1 coupling (x)))| = log |s|<label>(9)</label></formula><p>With this setup, it is also possible to revert the ordering of the input x without loss of generality. Hence, we choose to revert the order of the input at every even step of flow and to maintain the original order on odd steps of flow. This allows the model to learn dependencies both forward and backwards in time while remaining causal and invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model architecture</head><p>Our text encoder modifies Tacotron's by replacing batchnorm with instance-norm. Our decoder and N N architecture, depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, removes the essential Prenet and Postnet layers from Tacotron. We use the content-based tanh attention described in <ref type="bibr" target="#b26">(Vinyals et al., 2015)</ref>. We use the Mel Encoder described in <ref type="bibr" target="#b10">(Hsu et al., 2018)</ref> for Flowtron models that predict the parameters of the Gaussian mixture.</p><p>Unlike <ref type="bibr" target="#b9">Gibiansky et al., 2017)</ref>, where site specific speaker embeddings are used, we use a single speaker embedding that is channel-wise concatenated with the encoder outputs at every token. We use a fixed dummy speaker embedding for models not conditioned on speaker id. Finally, we add a dense layer with a sigmoid output the flow step closest to z. This provides the model with a gating mechanism as early as possible during inference to avoid extra computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>Once the network is trained, doing inference is simply a matter of randomly sampling z values from a spherical Gaussian, or Gaussian Mixture, and running them through the network, reverting the order of the input when necessary.</p><p>During training we used σ 2 = 1. The parameters of the Gaussian mixture are either fixed or predicted by Flowtron. In section 4.3 we explore the effects of different values for σ 2 . In general, we found that sampling z values from a Gaussian with a lower standard deviation from that assumed during training resulted in mel-spectrograms that sounded better, as found in <ref type="bibr" target="#b13">(Kingma &amp; Dhariwal, 2018)</ref>, and earlier work on likelihood-based generative models <ref type="bibr" target="#b17">(Parmar et al., 2018)</ref>. During inference we sampled z values from a Gaussian with σ 2 = 0.5, unless otherwise specified. The text and speaker embeddings are included at each of the coupling layers as before, but now the affine transforms are inverted in time, and these inverses are also guaranteed by the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes our training setup and provides quantitative and qualitative results. Our quantitative results show that Flowtron has mean opinion scores (MOS) that are comparable to that of state of the art models for text to melspectrogram synthesis such as Tacotron 2. Our qualitative results display many features that are not possible or not efficient with Tacotron and Tacotron 2 GST. These features include control of the amount of variation in speech, interpolation between samples and style transfer between seen and unseen speakers during training.</p><p>We decode all mel-spectrograms into waveforms by using a single pre-trained WaveGlow <ref type="bibr" target="#b19">(Prenger et al., 2019)</ref> model trained on a single speaker and available on github <ref type="bibr" target="#b24">(Valle et al., 2019a)</ref>. During inference we used σ 2 = 0.7. In consonance with <ref type="bibr" target="#b25">(Valle et al., 2019b)</ref>, our results suggests that WaveGlow can be used as an universal decoder.</p><p>Although we provide images to illustrate our results, they can best be appreciated by listening. Hence, we ask the readers to visit our website 2 to listen to Flowtron samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training setup</head><p>We train our Flowtron, Tacotron 2 and Tacotron 2 GST models using a dataset that combines the LJSpeech (LJS) dataset <ref type="bibr" target="#b11">(Ito et al., 2017)</ref> with two proprietary single speaker datasets with 20 and 10 hours each (Sally and Helen). We will refer to this combined dataset as LSH. We also train a Flowtron model on the train-clean-100 subset of LibriTTS <ref type="bibr" target="#b30">(Zen et al., 2019)</ref> with 123 speakers and 25 minutes on average per speaker. Speakers with less than 5 minutes of data and files that are larger than 10 seconds are filtered out. For each dataset we use at least 180 randomly chosen samples for the validation set and the remainder for the training set.</p><p>The models are trained on uniformly sampled normalized text and ARPAbet encodings obtained from the CMU Pronouncing Dictionary <ref type="bibr" target="#b29">(Weide, 1998)</ref>. We do not perform any data augmentation. We adapt the public Tacotron 2 and Tacotron 2 GST repos to include speaker embeddings as described in Section 3.</p><p>We use a sampling rate of 22050 Hz and mel-spectrograms with 80 bins using librosa mel filter defaults. We apply the STFT with a FFT size of 1024, window size of 1024 samples and hop size of 256 samples (∼ 12ms).</p><p>We use the ADAM <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2014)</ref> optimizer with default parameters, 1e-4 learning rate and 1e-6 weight decay for Flowtron and 1e-3 learning rate and 1e-5 weight decay for the other models, following guidelines in . We anneal the learning rate once the generalization error starts to plateau and stop training once the the generalization error stops significantly decreasing or starts increasing. The Flowtron models with 2 steps of flow were trained on the LSH dataset for approximately 1000 epochs and then fine-tuned on LibriTTS for 500 epochs. Tacotron 2 and Tacotron 2 GST are trained for approximately 500 epochs. Each model is trained on a single NVIDIA DGX-1 with 8 GPUs.</p><p>We find it faster to first learn to attend on a Flowtron model with a single step of flow and large amounts of data than multiple steps of flow and less data. After the model has learned to attend, we transfer its parameters to models with more steps of flow and speakers with less data. Thus, we first train Flowtron model with a single step of flow on the LSH dataset with many hours per speaker. Then we fine tune this model to Flowtron models with more steps of flow. Finally, these models are fine tuned on LibriTTS with an optional new speaker embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Mean Opinion Score comparison</head><p>We provide results that compare mean opinion scores (MOS) from real data from the LJS dataset, samples from a Flowtron with 2 steps of flow and samples from our implementation of Tacotron 2, both trained on LSH. Although the models evaluated are multi-speaker, we only compute mean opinion scores on LJS. In addition, we use the mean opinion scores provided in <ref type="bibr" target="#b19">(Prenger et al., 2019)</ref> for ground truth data from the LJS dataset.</p><p>We crowd-sourced mean opinion score (MOS) tests on Amazon Mechanical Turk. Raters first had to pass a hearing test to be eligible. Then they listened to an utterance, after which they rated pleasantness on a five-point scale. We used 30 volume normalized utterances from all speakers disjoint from the training set for evaluation, and randomly chose the utterances for each subject.</p><p>The mean opinion scores are shown in <ref type="table">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sampling the prior</head><p>The simplest approach to generate samples with Flowtron is to sample from a prior distribution z ∼ N (0, σ 2 ) and adjust σ 2 to control amount of variation. Whereas σ 2 = 0 completely removes variation and produces outputs based on the model bias, increasing the value of σ 2 will increase the amount of variation in speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">SPEECH VARIATION</head><p>To showcase the amount of variation and control thereof in Flowtron, we synthesize 10 mel-spectrograms and sample the Gaussian prior with σ 2 ∈ {0.0, 0.5, 1.0}. All samples are generated conditioned on a fixed speaker Sally and text "How much variation is there?" to illustrate the relationship between σ 2 and variability.</p><p>Our results show that despite all the variability added by increasing σ 2 , all the samples synthesized with Flowtron still produce high quality speech.</p><p>Figure 2 also shows that unlike most SOTA models <ref type="bibr" target="#b21">(Shen et al., 2017;</ref><ref type="bibr" target="#b2">Arik et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b18">Ping et al., 2017;</ref><ref type="bibr" target="#b22">Skerry-Ryan et al., 2018;</ref><ref type="bibr" target="#b28">Wang et al., 2018;</ref><ref type="bibr" target="#b4">Bińkowski et al., 2019)</ref>, Flowtron generates sharp harmonics and well resolved formants without a compound loss nor Prenet or Postnet layers.</p><p>(a) σ 2 = 0 (b) σ 2 = 0.5 (c) σ 2 = 1 <ref type="figure">Figure 2</ref>: Mel-spectrograms generated with Flowtron using different σ 2 . This parameter can be adjusted to control mel-spectrogram variability during inference. Now we show that adjusting σ 2 is a simple and valuable approach that provides more variation and control than Tacotron, without sacrificing speech quality. For this, we synthesize 10 samples with Tacotron 2 using different values for the Prenet dropout probability p ∈ {0.45, 0.5, 0.55}. We scale the outputs of the dropout output such that the mean of the output remains equal to the mean with p = 0.5, the value used during training. Although we also provide samples computed on values of p ∈ [0, 1] in our supplemental material, we do not include them in our results because they are unintelligible.</p><p>In <ref type="figure">Figure 3</ref> below we provide scatter plots from sample duration in seconds. Our results show that whereas σ 2 = 0 produces samples with no variation in duration, larger values of σ 2 produces samples with more variation in duration. Humans manipulate word and sentence length to express themselves, hence this is valuable. <ref type="figure">Figure 3</ref>: Sample duration in seconds given parameters σ 2 and p. These results show that Flowtron provides more variation in sample duration than Tacotron 2.</p><p>In <ref type="figure" target="#fig_1">Figure 4</ref> we provide scatter plots of F 0 contours extracted with the YIN algorithm <ref type="bibr" target="#b5">(De Cheveigné &amp; Kawahara, 2002)</ref>, with minimum F 0 , maximum F 0 and harmonicity threshold equal to 80 Hz, 400 Hz and 0.3 respectively. Our results show a behavior similar to the previous sample duration analysis. As expected, σ 2 = 0 provides no variation in F 0 contour 3 , while increasing the value of σ 2 will increase the amount of variation in F 0 contours.</p><p>Our results in <ref type="figure" target="#fig_1">Figure 4</ref> also show that the samples produced with Flowtron are considerably less monotonous than the samples produced with Tacotron 2. Whereas increasing σ 2 considerably increases variation in F 0 , modifying p barely produces any variation. This is valuable because expressive speech is associated with non-monotonic F 0 contours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">INTERPOLATION BETWEEN SAMPLES</head><p>With Flowtron we can perform interpolation in z-space to achieve interpolation in mel-spectrogram space. For this experiment we evaluate Flowtron models with and without speaker embeddings. For the experiment with speaker embeddings we choose the Sally speaker and the phrase "It is well known that deep generative models have a rich latent space.". We generate mel-spectrograms by sampling z ∼ N (0, 0.8) twice and interpolating between them over 100 steps.</p><p>3 Variations in σ 2 = 0 are due to different z for WaveGlow. For the experiment without speaker embeddings we interpolate between Sally and Helen using the phrase "We are testing this model.". First, we perform inference by sampling z ∼ N (0, 0.5) until we find two z values, z h and z s , that produce mel-spectrograms with Helen's and Sally's voice respectively. We then generate samples by performing inference while linearly interpolating between z h and z s .</p><p>Our same speaker interpolation samples show that Flowtron is able to interpolate between multiple samples while producing correct alignment maps. In addition, our different speaker interpolation samples show that Flowtron is able to blurry the boundaries between two speakers, creating a speaker that combines the characteristics of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sampling the posterior</head><p>In this approach we generate samples with Flowtron by sampling a posterior distribution conditioned on prior evidence containing speech characteristics of interest, as described in <ref type="bibr" target="#b8">(Gambardella et al., 2019;</ref><ref type="bibr" target="#b13">Kingma &amp; Dhariwal, 2018)</ref>. In this experiment, we collect prior evidence z e by performing a forward pass with the speaker id to be used during inference 4 , observed mel-spectrogram and text from a set of samples with characteristics of interest. If necessary, we time-concatenate each z e with itself to fulfill minimum length requirements defined according to the text length to be said during inference.</p><p>Tacotron 2 GST  has an equivalent posterior sampling approach, in which during inference the model is conditioned on a weighted sum of global style tokens (posterior) queried through an embedding of existing audio samples (prior). For Tacotron 2 GST, we evaluate two approaches: in one we use a single sample to query a style token in the other we use an average style token computed over multiple samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">SEEN SPEAKER WITHOUT ALIGNMENTS</head><p>In this experiment we compare Sally samples from Flowtron and Tacotron 2 GST generated by conditioning on the posterior computed over 30 Helen samples with the highest variance in fundamental frequency. The goal is to make a monotonic speaker sound expressive. Our experiments show that by sampling from the posterior or interpolating between the posterior and a standard Gaussian prior, Flowtron is able to make a monotonic speaker gradually sound more expressive. On the other hand, Tacotron 2 GST is barely able to alter characteristics of the monotonic speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">SEEN SPEAKER WITH ALIGNMENTS</head><p>We use a Flowtron model with speaker embeddings to illustrate Flowtron's ability to learn and transfer acoustic characteristics that are hard to express algorithmically but easy to perceive acoustically, we select a female speaker from LibriTTS with a distinguished nasal voice and oscillation in F 0 as our source speaker and transfer her style to a male speaker, also from LibriTTS, with acoustic characteristics that sound different from the female speaker. Unlike the previous experiment, this time the text and the alignment maps are transferred from the female to the male speaker. <ref type="figure">Figure 5</ref> is an attempt to visualize the transfer of these acoustic qualities we described. It shows that after the transfer, the lower partials of the male speaker oscillate more and become more similar to the female speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">UNSEEN SPEAKER STYLE</head><p>We compare samples generated with Flowtron and Tacotron 2 GST with speaker embeddings in which we modify a speaker's style by using data from the same speaker but from a style not seen during training. Whereas Sally's data used during training consists of news article readings, the evaluation samples contain Sally's interpretation of the somber and vampiresque novel Born of Darkness. <ref type="bibr">4</ref> To remove this speaker's information from ze  <ref type="figure">Figure 5</ref>: Mel-spectrograms from a female speaker, male speaker and a sample where we transfer the acoustic characteristics from the female speaker to the male speaker. It shows that the transferred sample is more similar to the female speaker than the male speaker.</p><p>Our samples show that Tacotron 2 GST fails to emulate the somber style from Born of Darkness's data. We show that Flowtron succeeds in transferring not only to the somber style in the evaluation data, but also the long pauses associated with the narrative style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">UNSEEN SPEAKER</head><p>In this experiment we compare Flowtron and Tacotron 2 GST samples in which we transfer the speaking style of a speaker not seen during training. Both models use speaker embeddings.</p><p>For these experiments, we consider two speakers. The first comes from speaker ID 03 from RAVDESS, a dataset with emotion labels. We focus on the label "surprised". The second speaker is Richard Feynman, using a set of 10 audio samples collected from the web.</p><p>For each experiment, we use the Sally speaker and the sentences "Humans are walking on the street?" and "Surely you are joking mister Feynman.", which do not exist in RAVDESS nor in the audio samples from Richard Feynman.</p><p>The samples generated with Tacotron 2 GST are not able to emulate the surprised style from RAVDESS nor Feynman's prosody and acoustic characteristics. Flowtron, on the other hand, is able to make Sally sound surprised, which is drastically different from the monotonous baseline. Likewise, Flowtron is able to pick up on the prosody and articulation details particular to Feynman's speaking style, and transfer them to Sally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Sampling the Gaussian Mixture</head><p>In this last section we showcase visualizations and samples from Flowtron Gaussian Mixture (GM). First we investigate how different mixture components and speakers are correlated. Then we provide sound examples in which we modulate speech characteristics by translating one of the the dimensions of an individual component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">VISUALIZING ASSIGNMENTS</head><p>For the first experiment, we train a Flowtrom Gaussian Mixture on LSH with 2 steps of flow, speaker embeddings and fixed mean and covariance (Flowtron GM-A). We obtain mixture component assignments per mel-spectrogram by performing a forward pass and averaging the component assignment over time and samples. <ref type="figure" target="#fig_3">Figure 6</ref> shows that whereas most speakers are equally assigned to all components, component 7 is almost exclusively assigned to Helen's data. In the second experiment, we train a Flowtron Gaussian Mixture on LibriTTS with 1 step of flow, without speaker embeddings and predicted mean and covariance (Flowtron GM-B). <ref type="figure" target="#fig_4">Figure 7</ref> shows that Flowtron GM assigns more probability to component 7 when the speaker is male than when it's female. Conversely, the model assigns more probability to component 6 when the speaker is female than when it's male. Components 7 and 8 are assigned different probabilities according to gender, suggesting that the information stored in the components is gender dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">TRANSLATING DIMENSIONS</head><p>In this subsection, we use the model Flowtron GM-A described previously. We focus on selecting a single mixture component and translating one of its dimensions by adding an offset.</p><p>The samples in our supplementary material show that we are able to modulate specific speech characteristics like pitch and word duration. Although the samples generated by translating one the dimensions associated with pitch height have different pitch contours, they have the same duration. Similarly, our samples show that translating the dimension associated with length of the first word does not modulate the pitch of the first word. This provides evidence that we can modulate these attributes by manipulating these dimensions and that the model is able to learn a disentangled representation of these speech attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper we propose a new text to mel-spectrogram synthesis model based on autoregressive flows that is optimized by maximizing the likelihood and allows for control of speech variation and style transfer. Our results show that samples generated with FlowTron achieve mean opinion scores that are similar to samples generated with state-ofthe-art text-to-speech synthesis models. In addition, we demonstrate that at no extra cost and without a compound loss term, our model learns a latent space that stores nontextual information. Our experiments show that FlowTron gives the user the possibility to transfer charactersitics from a source sample or speaker to a target speaker, for example making a monotonic speaker sound more expressive.</p><p>Our results show that despite all the variability added by increasing σ 2 , the samples synthesized with FlowTron still produce high quality speech. Our results show that FlowTron learns a latent space over non-textual features that can be investigated and manipulated to give the user more control over the generative models output. We provide many examples that showcase this including increasing variation in mel-spectrograms in a controllable manner, transferring the style from speakers seen and unseen during training to another speaker using sentences with similar or different text, and making a monotonic speaker sound more expressive.</p><p>Flowtron produces expressive speech without labeled data or ever seeing expressive data. It pushes text-to-speech synthesis beyond the expressive limits of personal assistants. It opens new avenues for speech synthesis in human-computer interaction and the arts, where realism and expressivity are of utmost importance. To our knowledge, this work is the first to demonstrate the advantages of using normalizing flow models in text to mel-spectrogram synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowtron network. Text and speaker embeddings are channel-wise concatenated. A 0-valued vector is concatenated with x in the time dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>F 0 contours obtained from samples generated by Flowtron and Tacotron 2 with different values for σ 2 and p. Flowtron provides more expressivity than Tacotron 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Component assignments for Flowtron GM-A. Unlike LJS and Sally, Helen is almost exclusively assigned to component 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Component assignments for Flowtron GM-B.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://nv-adlr.github.io/Flowtron</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Expressive speech synthesis via modeling expressions with variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Akuzawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02135</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep voice 2: Multi-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08947</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07825</idno>
		<title level="m">Deep voice: Real-time neural text-to-speech</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Badham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lasker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wargames</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11646</idno>
		<title level="m">High fidelity speech synthesis with adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yin, a fundamental frequency estimator for speech and music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Cheveigné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Transflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13270</idno>
		<title level="m">Repurposing flow models without retraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep voice 2: Multispeaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical generative modeling for controllable speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07217</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The LJ speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarially trained end-to-end korean singing voice synthesis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01919</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Singing voice synthesis based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-1027</idno>
		<idno>doi: 10.21437/ Interspeech.2016-1027</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2016-1027" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2478" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">D. Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05884</idno>
		<title level="m">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards end-to-end prosody transfer for expressive speech synthesis with tacotron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synthesis of fairy tales using an analog vocal tract</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Umeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Omura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Congress on Acoustics</title>
		<meeting>6th International Congress on Acoustics</meeting>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="page" from="159" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mellotron github repo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/mellotron" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mellotron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11997</idno>
		<title level="m">Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10135</idno>
		<title level="m">A fully end-to-end text-to-speech synthesis model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09017</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The cmu pronouncing dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Weide</surname></persName>
		</author>
		<ptr target="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Libritts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02882</idno>
		<title level="m">A corpus derived from librispeech for text-to-speech</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
