<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<email>zhedong.zheng@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Artifi-cial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Syd-ney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Artifi-cial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Syd-ney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Artifi-cial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Syd-ney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Artifi-cial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Syd-ney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Unsupervised Domain Adaptation · Domain Adaptive Semantic Segmentation · Image Segmentation · Uncertainty Estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled targetdomain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process.</p><p>To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 → Cityscapes and SYNTHIA → Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes → Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have been widely adopted in the field of semantic segmentation, yielding the state-ofthe-art performance <ref type="bibr" target="#b21">[Liang et al., 2017</ref><ref type="bibr" target="#b41">, Wei et al., 2018</ref>. However, recent works show that DNNs are limited in the scalability to the unseen environments, e.g., the testing data collected in rainy days <ref type="bibr" target="#b12">[Hendrycks and</ref><ref type="bibr">Dietterich, 2019, Wu et al., 2019]</ref>. One straightforward idea is to annotate more training data of the target environment and then retrain the segmentation model. However, semantic segmentation task usually demands dense annotations and it is unaffordable to manually annotate the pixel-wise label for collected data in new environments. To address the challenge, the researchers, therefore, resort to unsupervised semantic segmentation adaption, which takes one step closer to realworld practice. In unsupervised semantic segmentation adaptation, two datasets collected in different environments are considered: a labeled source-domain dataset where category labels are provided for every pixel, and an unlabeled targetdomain dataset where only provides the collected data without annotations. Compared with the annotated data in the target domain, the unlabeled data is usually easy to collect. Semantic segmentation adaptation aims at leveraging the labeled source-domain data as well as the unlabeled targetdomain data to adapt the well-trained model to the target environment.</p><p>The main challenge of semantic segmentation adaption is the discrepancy of data distribution between the source domain and the target domain. There are two lines of methods for semantic segmentation adaptation. On one hand, sev-eral existing works focus on the domain alignment by minimizing the distribution discrepancy in different levels, such as pixel level <ref type="bibr" target="#b42">[Wu et al., 2018</ref><ref type="bibr" target="#b43">, Wu et al., 2019</ref><ref type="bibr" target="#b13">, Hoffman et al., 2018</ref>, feature level <ref type="bibr" target="#b46">, Yue et al., 2019</ref><ref type="bibr" target="#b22">, Luo et al., 2019a</ref><ref type="bibr" target="#b48">, Zhang et al., 2019b</ref> and semantic level <ref type="bibr" target="#b34">[Tsai et al., 2018</ref><ref type="bibr" target="#b35">, Tsai et al., 2019</ref>. Despite great success, this line of work is sub-optimal. Because the alignment objective drives the model to learn the shared knowledge between domains but ignores the domainspecific knowledge. The domain-specific knowledge is one of the keys to the final target, i.e., the model adapted to the target domain. On the other hand, some researchers focus on learning the domain-specific knowledge of the target domain by fully exploiting the unlabeled target-domain data <ref type="bibr" target="#b54">[Zou et al., 2018</ref><ref type="bibr" target="#b9">, Han et al., 2019</ref>. Specifically, this line of methods usually adopts the two-stage pipeline, which is similar to the traditional semi-supervised framework <ref type="bibr">[Lee, 2013]</ref>. The first step is to predict pseudo labels by the knowledge learned from the labeled data, e.g., the model trained on the source domain. The second step is to minimize the cross-entropy loss on the pseudo labels of the unlabeled target-domain data. In the training process, pseudo labels are usually regarded as accurate annotations to optimize the model. However, one inherent problem exists in the pseudo label based scene adaptation approaches. Pseudo labels usually suffer from the noise caused by the model trained on different data distribution (see <ref type="figure" target="#fig_0">Figure 1</ref>). The noisy label could compromise the subsequent learning. Although some existing works <ref type="bibr" target="#b54">[Zou et al., 2018</ref> have proposed to manually set the threshold to neglect the low-confidence pseudo labels, it is still challenging in several aspects: First, the value of the threshold is hard to be determined for different target domain. It depends on the similarity of the source domain and target domain, which is hard to estimate in advance. Second, the value of the threshold is also hard to be determined for different categories. For example, the objectives, such as traffic signs, have rarely appeared in the source domain. The overall confidence score for the rare category is relatively low. The high threshold may ignore the information of rare categories. Third, the threshold is also related to the location of the pixel. For example, the pixel in the center of objectives, such as cars, is relatively easy to predict, while the pixel on the objective edge usually faces ambiguous predictions. It reflects that the threshold should not only consider the confidence score but also the location of the pixel. In summary, every pixel in the segmentation map needs to be treated differently. The fixed threshold is hard to match the demand.</p><p>To address the mentioned challenges, we propose one simple and effective method for semantic segmentation adaption via modeling uncertainty, which could provide the pixelwise threshold for the input image automatically. Without introducing extra parameters or modules, we formulate the uncertainty as the prediction variance. The prediction variance reflects the model uncertainty towards the prediction in a bootstrapping manner. Meanwhile, we explicitly involve the variance into the optimization objective, called variance regularization, which works as an automatic threshold and is compatible with the standard cross-entropy loss. The automatic threshold rectifies the learning from noisy labels and ensures the training in a coherent manner. Therefore, the proposed method could effectively exploit the domainspecific information offered by pseudo labels and takes advantage of the unlabeled target-domain data.</p><p>In a nutshell, our contributions are as follows:</p><p>-To our knowledge, we are among the first attempts to exploit the uncertainty estimation and enable the automatic threshold to learn from noisy pseudo labels. This is in contrast to most existing domain adaptation methods that directly utilize noisy pseudo labels or manually set the confidence threshold. -Without introducing extra parameters or modules, we formulate the uncertainty as the prediction variance. Specifically, we introduce a new regularization term, variance regularization, which is compatible with the standard crossentropy loss. The variance regularization works as the automatic threshold, and rectifies the learning from noisy pseudo labels. -We verify the proposed method on two synthetic-to-real benchmarks and one cross-city benchmark. The proposed method has achieved significant improvements over the conventional pseudo label learning, yielding competitive performance to existing methods.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation Adaptation</head><p>The main challenge in unsupervised domain adaptation is different data distribution between the source domain and the target domain <ref type="bibr" target="#b3">[Fu et al., 2015</ref><ref type="bibr" target="#b19">, Li et al., 2020b</ref><ref type="bibr" target="#b20">, Li et al., 2020c</ref><ref type="bibr" target="#b15">, Kang et al., 2020</ref>. To deal with the challenge, some pioneering works <ref type="bibr" target="#b13">[Hoffman et al., 2018</ref><ref type="bibr" target="#b42">, Wu et al., 2018</ref> propose to transfer the visual style of the source-domain data to the target domain. In this way, the model could be trained on the labeled data with the target style. Similarly, some recent works leverage Adversarial Domain Adaptation <ref type="bibr" target="#b36">[Tzeng et al., 2015</ref><ref type="bibr" target="#b6">, Ganin and Lempitsky, 2015</ref><ref type="bibr" target="#b23">,Luo et al., 2020</ref> to transfer the source-domain images or features to multiple domains and intend to learn the domain-invariant features <ref type="bibr" target="#b43">[Wu et al., 2019</ref><ref type="bibr" target="#b46">,Yue et al., 2019</ref>. Furthermore, some works focus on the alignment among the middle activation of neural networks. <ref type="bibr" target="#b22">Luo et al. [Luo et al., 2019a</ref><ref type="bibr" target="#b24">, Luo et al., 2019b</ref> utilize the attention mechanism to  <ref type="bibr" target="#b2">[Cordts et al., 2016]</ref>. We leverage the widely-used baseline model <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref> to generate pseudo labels. Despite the large area of correct prediction, the pseudo labels still suffer from the data distribution biases, and inevitably contains incorrect predictions. (Best viewed in color) refine the feature alignment. Instead of modifying the visual appearance, the alignment between the high-level semantic features also attracts a lot of attention. <ref type="bibr" target="#b34">Tsai et al. [Tsai et al., 2018</ref><ref type="bibr" target="#b35">,Tsai et al., 2019</ref> propose to utilize the discriminator to demand the similar semantic outputs between two domains. In summary, this line of methods focuses on the alignment, learning the shared knowledge between the source and target domains. However, the domain-specific information is usually ignored, which is one of the keys to the adaptation in the target environment. Therefore, in this paper, we resort to another line of methods, which is based on pseudo label learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo label learning</head><p>Another line of semantic segmentation adaptation approaches utilizes the pseudo label to adapt the model to target domain <ref type="bibr" target="#b54">[Zou et al., 2018</ref><ref type="bibr" target="#b52">, Zheng and Yang, 2020</ref>. The main idea is close to the conventional semi-supervised learning approach, entropy minimization, which is first proposed to leverage the unlabeled data <ref type="bibr" target="#b7">[Grandvalet and Bengio, 2005]</ref>. Entropy minimization encourages the model to give the prediction with a higher confidence score. In practice, <ref type="bibr" target="#b29">Reed et al. [Reed et al., 2014]</ref> propose bootstrapping via entropy minimization and show the effectiveness on the object detection and emotion recognition. Furthermore, Lee et al. <ref type="bibr">[Lee, 2013]</ref> exploit the trained model to predict pseudo labels for the unlabeled data, and then fine-tune the model as supervised learning methods to fully leverage the unla-beled data. Recently, Pan et al. <ref type="bibr" target="#b27">[Pan et al., 2019]</ref> utilize the pseudo label learning to minimize the distribution of target-domain data with the source-domain prototypes. For unsupervised semantic segmentation, Zou et al. <ref type="bibr" target="#b54">,Zou et al., 2018</ref> introduce the pseudo label strategy to the semantic segmentation adaptation and provide one comprehensive analysis on the regularization terms. In a similar spirit, Zheng et al. <ref type="bibr" target="#b52">[Zheng and Yang, 2020]</ref> also apply the pseudo label to learn the domain-specific features, yielding competitive results. However, one inherent weakness of the pseudo label learning is that the pseudo label usually contains noisy predictions. Despite the fact that most pseudo labels are correct, wrong labels also exist, which could compromise the subsequent training. If the model is fine-tuned on the noisy label, the error would also be transferred to the adapted model. Different from existing works, we do not treat the pseudo labels equally and intend to rectify the learning from noisy labels. The proposed method explicitly predict the uncertainty of pseudo labels, when fine-tuning the model. The uncertainty could be regarded as an automatic threshold to adjust the learning from noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Co-training</head><p>Co-training is a semi-supervised learning method, which demands two classifiers to learn complementary information <ref type="bibr" target="#b0">[Blum and Mitchell, 1998</ref>]. Some domain adaptation works also explore a similar learning strategy. <ref type="bibr" target="#b32">[Saito et al., 2018</ref><ref type="bibr" target="#b24">, Luo et al., 2019b</ref> explicitly maximizes the discrepancy of two classifiers by introducing one extra loss, i.e., the L adv in <ref type="bibr" target="#b32">[Saito et al., 2018]</ref> and the L weight in <ref type="bibr" target="#b24">[Luo et al., 2019b]</ref>, to obtain complementary classifiers. <ref type="bibr" target="#b32">[Saito et al., 2018]</ref> minimizes the feature discrepancy via adversarial training. Similarly, <ref type="bibr" target="#b24">[Luo et al., 2019b]</ref> apply the classifier discrepancy on the discriminator loss to stabilize the training. In contrast, the proposed method enables the classifier discrepancy in nature, since we deploy two classifiers on different intermediate layers. We do not introduce such loss to encourage the classifier discrepancy. Otherwise, every pseudo label will be high-uncertainty. For instance, if the two classifiers output one identical category prediction, we will not punish the network. In contrast, <ref type="bibr" target="#b32">[Saito et al., 2018]</ref> will punish the classifiers for enabling adversarial training. Besides, <ref type="bibr" target="#b32">[Saito et al., 2018</ref><ref type="bibr" target="#b24">, Luo et al., 2019b</ref> still use conventional segmentation loss and do not deal with noisy labels, when the proposed method uses the classifier discrepancy to rectify the pseudo label learning on segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Uncertainty Estimation</head><p>To address the noise, existing works have explored the uncertainty estimation from different aspects, such as the input data, the annotation and the model weights. In this work, we focus on the annotation uncertainty. Our target is to learn a model that could predict whether the annotation is correct, and learn from noisy pseudo labels. Among existing works, Bayesian networks are widely used to predict the uncertainty of weights in the network <ref type="bibr" target="#b26">[Nielsen and Jensen, 2009]</ref>. In a similar spirit, Kendall et al. <ref type="bibr" target="#b16">[Kendall and Gal, 2017]</ref> apply the Bayesian theory to the prediction of computer vision tasks, and intend to provide not only the prediction results but also the confidence of the prediction. Further, Yu et al.  explicitly model the uncertainty via an extra auxiliary branch, and involve the random noise into training. The model could explicitly estimate the feature mean as well as the prediction variance. Inspired by the above-mentioned works, we propose to leverage the prediction variance to formulate the uncertainty. There are two fundamental differences between previous works and ours:</p><p>(1) We do not introduce extra modules or parameters to simulate the noise. Instead, we leverage the prediction discrepancy within the segmentation model. <ref type="formula">(2)</ref> We explicitly involve the uncertainty into the training target and adopt the adaptive method to learn the pixel-wise uncertainty map automatically. The proposed method does not need manually setting the threshold to enforce the pseudo label learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In Section 3.1, we first provide the problem definition and denotations. We then revisit the conventional domain adap-tion method based on the pseudo label and discuss the limitation of the pseudo label learning (see Section 3.2). To deal with the mentioned limitations, we propose to leverage the uncertainty estimation. In particular, we formulate the uncertainty as the prediction variance and provide one brief definition in Section 3.3, followed by the proposed variance regularization, which is compatible with the standard crossentropy loss in Section 3.4. Besides, the implementation details are provided in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Given the labeled dataset X s = {x i s } M i=1 from the source domain and the unlabeled dataset X t = {x j t } N j=1 from the target domain, semantic segmentation adaptation intends to learn the projection function F , which maps the input image X to the semantic segmentation Y . M and N denote the number of the labeled data and the unlabeled data. The source-domain semantic segmentation label</p><formula xml:id="formula_0">Y s = {y i s } M i=1</formula><p>is provided for every labeled data of the source domain X s , while the target-domain label Y t = {y j t } N j=1 remains unknown during the training. The aim of unsupervised domain adaptation is to estimate the model parameter θ t , which could minimize the prediction bias on the target-domain inputs:</p><formula xml:id="formula_1">Bias(p t ) = E[F (x j t |θ t ) − p j t ],<label>(1)</label></formula><p>where p t is the ground-truth class probability of target data. Ideally, p j t is one-hot vector and the maximum value of p j t is 1. The ground-truth label y j t = arg max p j t . In contrast, F (x j t |θ t ) is the predicted probability distribution of x j t . When we minimize the prediction bias in Equation 1, the discrepancy between predicted results and the ground-truth probability is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo Label Learning Revisit</head><p>Pseudo label learning is to leverage the pseudo label to learn from the unlabeled data. The common practice contains two stages. The first stage is to generate the pseudo label for the unlabeled target-domain training data. The pseudo labels could be obtained via the model trained on source-domain data:ŷ j t = arg max F (x j t |θ s ). We note that θ s is the model parameters learned from the source-domain training data. Therefore, the pseudo labelsŷ t , are not accurate in nature due to different data distribution between X s and X t . We denotep j <ref type="figure">Fig. 2</ref>: Illustration of the two-classifier model based on <ref type="bibr">Deeplab-v2 [Chen et al., 2017]</ref>, which adopts ResNet-101 <ref type="bibr" target="#b10">[He et al., 2016a]</ref> as backbone. We follow the previous works <ref type="bibr" target="#b51">[Zhao et al., 2017</ref><ref type="bibr" target="#b34">, Tsai et al., 2018</ref><ref type="bibr" target="#b35">, Tsai et al., 2019</ref><ref type="bibr" target="#b22">, Luo et al., 2019a</ref><ref type="bibr" target="#b24">, Luo et al., 2019b</ref><ref type="bibr" target="#b52">, Zheng and Yang, 2020</ref> to add an auxiliary classifier with the similar structure as the primary classifier. The auxiliary classifier takes the activation of the shallow layer res4b22 as the input, while the primary classifier leverages that of res5c. The ASPP module denotes Atrous Spatial Pyramid Pooling layer <ref type="bibr" target="#b1">[Chen et al., 2017]</ref>, and the fc layer denotes the fully-connected layer. The original goal of two-classifier model is to evade the problem of gradient vanishing and help the training. In this work, we take one step further to leverage the prediction discrepancy of two classifiers as the uncertainty estimation.</p><p>The first term is the difference between the prediction and the pseudo label, while the second term is the error between the pseudo label and the ground-truth label. When fine-tuning the model in the second stage, we fix the pseudo label. Therefore, the second term is one constant. Existing methods usually optimize the first term as the pretext task. It equals to considering the pseudo labelsp t as true labels. Existing methods train the model parameter θ t to minimize the bias between the prediction and pseudo labels. In practice, the crossentropy loss is usually adopted <ref type="bibr" target="#b54">[Zou et al., 2018</ref><ref type="bibr" target="#b52">, Zheng and Yang, 2020</ref>. The objective could be formulated as:</p><formula xml:id="formula_2">L ce = E[−p j t log F (x j t |θ t )].<label>(3)</label></formula><p>Discussion. There are two advantages of pseudo label learning : First, the model is only trained on the target-domain data. The training data distribution is close to the testing data distribution, minoring the input distribution discrepancy. Second, despite the domain discrepancy, most pseudo labels are correct. Theoretically, the fine-tuned model could arrive the competitive performance with the fully-supervised model. However, one inherent problem exists that the pseudo label inevitably contains noise. The wrong annotations are transferred from the source model to the final model. Noisy pseudo label could largely compromise the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty Estimation</head><p>To address the label noise, we model the uncertainty of the pseudo label via the prediction variance. Intuitively, we could formulate the variance of the prediction as:</p><formula xml:id="formula_3">V ar(p t ) = E[(F (x j t |θ t ) − p j t ) 2 ].<label>(4)</label></formula><p>Since p t remains unknown, one naive way is to utilize the pseudo labelp t to replace the p t . The variance could be approximated as:</p><formula xml:id="formula_4">V ar(p t ) ≈ E[(F (x j t |θ t ) −p j t ) 2 ].<label>(5)</label></formula><p>However, in Equation 2, we have pushed F (x j t |θ t ) top t . When optimizing the prediction bias, the variance in Equation 5 will also be minimized. It could not reflect the real prediction variance during training. In this paper, therefore, we adopt another approximation as:</p><formula xml:id="formula_5">V ar(p t ) ≈ E[(F (x j t |θ t ) − F aux (x j t |θ t )) 2 ],<label>(6)</label></formula><p>where F aux (x t |θ t ) denotes the auxiliary classifier output of the segmentation model. As shown in <ref type="figure">Figure 2</ref>, we adopt the widely-used two-classifier model, which contains one primary classifier as well as one auxiliary classifier. We note that the extra auxiliary classifier could be viewed as a free lunch since most segmentation models, including PSPNet <ref type="bibr" target="#b51">[Zhao et al., 2017]</ref> and the modified DeepLab-v2 in <ref type="bibr" target="#b34">[Tsai et al., 2018</ref><ref type="bibr" target="#b35">, Tsai et al., 2019</ref><ref type="bibr" target="#b22">, Luo et al., 2019a</ref><ref type="bibr" target="#b52">, Zheng and Yang, 2020</ref>, contain the auxiliary classifier to solve the gradient vanish problem <ref type="bibr" target="#b11">[He et al., 2016b]</ref> and help the training. In this paper, we further leverage the auxiliary classifier to estimate the variance. In practice, we utilize the KLdivergence of two classifier predictions as the variance:</p><formula xml:id="formula_6">D kl = E[F (x j t |θ t ) log( F (x j t |θ t ) F aux (x j t |θ t ) )],<label>(7)</label></formula><p>If two classifiers provide two different class predictions, the approximated variance will obtain one large value. It reflects the uncertainty of the model on the prediction. Besides, it is worthy to note that the proposed variance in Equation 7 is independent with the pseudo labelp t . Discussion: What leads to the discrepancy of the primary classifier and the auxiliary classifier? First of all, the main reason is different receptive fields. As shown in <ref type="figure">Figure 2</ref>, the auxiliary classifier is located at the relatively shallow layer, when the primary classifier learns from the deeper layer. The input activation is different between two classifiers, leading to the prediction difference. Second, the two classifiers have not been trained on the target-domain data. Therefore, both classifiers may have different biases to the target-domain data. Third, we apply the dropout function <ref type="bibr" target="#b33">[Srivastava et al., 2014]</ref> to two classifiers, which also could lead to the different prediction during training. The prediction discrepancy helps us to estimate the uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Procedure of the Proposed Method</head><p>Require: The target domain dataset X t = {x j t } N j=1 ; The generated pseudo labelŶ t = {ŷ j t } N j=1 ; Require: The source-domain parameter θ s ; The iteration number T .</p><p>1: Initialize θ t = θ s ; 2: for iteration = 1 to T do 3: Input x j t to F (·|θ t ), extract the prediction of two classifiers, calculate the prediction variance according to Equation 7:</p><formula xml:id="formula_7">D kl = E[F (x j t |θ t ) log( F (x j t |θ t ) F aux (x j t |θ t ) )].<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>We fix the prediction variance, and calculate the original crossentropy loss according to Equation 2, wherep j t is the one-hot vector of the pseudo labelŷ j t :</p><formula xml:id="formula_8">L ce = E[−p j t log F (x j t |θ t )].<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>We combine the prediction variance with the conventional objective to obtain the rectified objective. Update the θ t according to Equation <ref type="formula" target="#formula_1">12</ref>:</p><formula xml:id="formula_9">L rect = E[exp{−D kl }L ce + D kl ]<label>(10)</label></formula><p>6: end for 7: return θ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variance Regularization</head><p>In this paper, we propose the variance regularization term to rectify the learning from noisy labels. It leverages the approximated variance introduced in Section 3.3. The rectified objective could be formulated as:</p><formula xml:id="formula_10">L rect = E[ 1 V ar(p t ) Bias(p t ) + V ar(p t )]<label>(11)</label></formula><p>It is worthy to note that we do not intend to minimize the prediction bias under all conditions. If the prediction variance has received one large value, we will not punish the prediction bias Bias(p t ). Meanwhile, to prevent that the model predicts the large variance all the time, as a trade-off, we introduce the regularization term via adding V ar(p t ).</p><p>Besides, since V ar(p t ) could be zero, it may lead to the problem of dividing by zero. To stabilize the training, we adopt the policy in <ref type="bibr" target="#b16">[Kendall and Gal, 2017</ref>] that replace 1/V ar as exp(−V ar). Therefore, the loss term could be rewritten with the approximated terms as:</p><formula xml:id="formula_11">L rect = E[exp{−D kl }L ce + D kl ].<label>(12)</label></formula><p>The training procedure of the proposed method is summarized in Algorithm 1. In practice, we utilize the parameter θ s learned in the source-domain dataset to initialize the θ t .</p><p>In every iteration, we calculate the prediction variance as well as the cross-entropy loss for the given inputs. We utilize the L rect to update the θ t . Instead, we leverage the prediction variance of the model itself. Second, the proposed variance regularization has good scalability. If the variance equals to zero, the optimization loss degrades to the objective of the conventional pseudo learning and the model will focus on minimizing the prediction bias only. In contrast, when the value of variance is high, the model is prone to neglect the bias and skip ambiguous pseudo labels; Third, the proposed variance regularization has the same shape of the prediction, and could works as the pixel-wise threshold of the pseudo label. As shown in <ref type="figure">Figure 3</ref>, we could observe that the noise usually exists in the area with high variance. The proposed rectified loss assigns different thresholds to different areas. For example, for the location with coherent predictions, the variance regularization drives the model trust pseudo labels. For the area with ambiguous predictions, the variance regularization drives the model to neglect pseudo labels. Different from existing works that set the unified threshold for all training samples, the proposed pseudo label could provide more accurate and adaptive threshold for every pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation</head><p>Network Architecture. In this work, we utilize the widely-  Zheng and Yang, 2020] to add one auxiliary classifier. The auxiliary classifier has similar structure with the primary classifier, including one Atrous Spatial Pyramid Pooling (ASPP) module <ref type="bibr" target="#b1">[Chen et al., 2017]</ref> and one fully-connected layer. The auxiliary classifier is added after the res4b22 layer. We also insert the dropout layer <ref type="bibr" target="#b33">[Srivastava et al., 2014]</ref> before the fully-connected layer, and the dropout rate is 0.1. Pseudo Label. To verify the effectiveness of the proposed method, we deploy two existing methods, i.e., AdaptSegNet <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref> and MRNet <ref type="bibr" target="#b52">[Zheng and Yang, 2020]</ref>, to generate the pseudo labels of the target-domain dataset.</p><p>-AdaptSegNet <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref> is one widely-adopted baseline model, which utilize the adversarial training to align the semantic outputs. -MRNet <ref type="bibr" target="#b52">[Zheng and Yang, 2020]</ref> is one recent work, which leverages the memory module to regularize the model training, especially for the target-domain data.</p><p>Specifically, MRNet arrives superior performance to Adapt-SegNet in terms of mIoU on three benchmarks. Therefore, if not specific, we adopt the pseudo label generated by the stronger baseline, i.e., MRNet. It is worth mentioning that we do not use source-domain training data. In practice, we fine-tune the model only on the target-domain training data with pseudo labels.</p><p>Training Details. The input image is resized to 1280 × 640 with scale jittering from [0.8, 1.2], and then we randomly crop 512 × 256 for training. Horizontal flipping is applied with the possibility of 50%. We train the model with minibatch size of 9, and the parameters of batch normalization layers are also fine-tuned. The learning rate is set to 0.0001. Following <ref type="bibr" target="#b51">[Zhao et al., 2017</ref><ref type="bibr" target="#b47">, Zhang et al., 2019a</ref><ref type="bibr" target="#b49">, Zhang et al., 2020</ref>, we deploy the ploy learning rate policy by multiplying the factor (1 − iter total−iter ) 0.9 . The total iteration is set as 100k iterations and we adopt the early-stop strategy. We stop the training after 50k iterations. When inference, we follow <ref type="bibr" target="#b52">[Zheng and Yang, 2020]</ref> to combine the output of both classifier as the final result. Output = arg max(F (x j t |θ t )+ 0.5F aux (x j t |θ t )). Our implementation is based on Pytorch <ref type="bibr" target="#b28">[Paszke et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>Datasets. To simplify, we denote the test setting as A → B, where A represents the labeled source domain and B denotes the unlabeled target domain. We evaluate the proposed method on two widely-used synthetic-to-real benchmarks: i.e., GTA5 <ref type="bibr" target="#b30">[Richter et al., 2016]</ref>→Cityscapes <ref type="bibr" target="#b2">[Cordts et al., 2016]</ref> and SYNTHIA5 <ref type="bibr" target="#b31">[Ros et al., 2016]</ref>→Cityscapes <ref type="bibr" target="#b2">[Cordts et al., 2016]</ref>. Both source dataset, i.e., GTA5 and SYNTHIA are the synthetic datasets, and the corresponding annotation is easy to obtain. Specifically, the GTA5 dataset is collected from a video game, which contains 24, 966 images for training. The SYNTHIA dataset is rendered from a virtual city and comes with pixel-level segmentation annotations, containing 9, 400 training images. The realistic dataset, Cityscapes, collect street-view scenes from 50 different cities, which contains 2, 975 training images and 500 images for validation. Besides, we also evaluate the performance on the crosscity benchmark, i.e., Cityscapes <ref type="bibr" target="#b2">[Cordts et al., 2016]</ref>→Oxford RobotCar <ref type="bibr" target="#b25">[Maddern et al., 2017]</ref>. We utilize the annotation of Cityscapes training images in this setting. The Oxford RobotCar dataset serves as the unlabeled target domain, containing 894 training images and 271 validation images. We note that this setting is challenging in different weather conditions. Oxford RobotCar is collected in the rainy days, while the Cityscapes dataset is mostly collected in the sunny days. The differences between datasets are listed in <ref type="table" target="#tab_2">Table 1</ref>. Evaluation Metric. We report pre-class IoU and mean IoU over all classes. For SYNTHIA → Cityscapes, due the limited annotated classes in the source dataset, we report the results based on 13 categories as well as 16 categories with three small-scale categories. For Cityscapes → Oxford Robot-Car, we follow the setting in <ref type="bibr" target="#b35">[Tsai et al., 2019]</ref> and report 9 pre-class IoU as well as the mIoU accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with state-of-the-art methods</head><p>Synthetic-to-real. We compare the proposed method with other recent semantic segmentation adaptation methods that have reported the results or can be re-implemented by us on three benchmarks. For a fair comparison, we mainly compare the results based on the same network structure, i.e., DeepLabv2. The competitive methods cover a wide range of approaches and could be roughly categorised according to the usage of pseudo label: AdaptSegNet <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref>, SIBAN <ref type="bibr" target="#b22">[Luo et al., 2019a]</ref>, CLAN <ref type="bibr" target="#b24">[Luo et al., 2019b]</ref>, APODA  and PatchAlign <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref> do not leverage the pseudo labels and focus on aligning the distribution between the source domain and the target domain; CBST <ref type="bibr" target="#b54">[Zou et al., 2018]</ref>, MRKLD , and our implemented MRNet+Pseudo are based on the pseudo  <ref type="bibr" target="#b50">[Zhang et al., 2018]</ref> -    label learning to fully exploit the unlabeled target-domain data.</p><formula xml:id="formula_13">- - - - - - - - - - - - - - - - - - - 29.2 FCAN</formula><formula xml:id="formula_14">- - - - - - - - - - - - - - - - - -</formula><p>First of all, we consider the widely-used GTA5 → Cityscapes benchmark. <ref type="table" target="#tab_5">Table 2</ref> shows that: (1) The proposed method arrives the state-of-the-art results 50.3% mIoU, which surpasses other methods. Besides, the proposed method also yields the competitive performance in terms of the pre-class IoU.</p><p>(2) Comparing to our baseline, i.e., MRNet+Pseudo (48.3% mIoU), which adopts the conventional pseudo learning, the proposed method (50.3% mIoU) gains +2.0% mIoU improvement. It verifies the effectiveness of the proposed method in rectifying the learning from the noisy pseudo label. The variance regularization plays an important role in achieving this result; (3) Meanwhile, we could observe that the proposed method outperforms the source-domain model, i.e., MRNet (45.5% mIoU), which provides the pseudo label, 4.8 mIoU. It verifies the effectiveness of the pseudo label learning that push the model to be confident about the prediction. If most pseudo labels are correct, the pseudo label learning could effectively boost the target-domain performance. (4) The proposed method also surpasses the other domain alignment method by a relatively large margin. For example, the modified AdaptSegNet, i.e., PatchAlign <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref>, leverages the patch-level information, yielding 46.5%, which is inferior to ours. (5) Without using the prior knowledge, the proposed method is also superior to other pseudo label learning works, i.e., CBST <ref type="bibr" target="#b54">[Zou et al., 2018]</ref> and MRKLD . CBST <ref type="bibr" target="#b54">[Zou et al., 2018]</ref> introduces the location knowledge, e.g., sky is always in the upper bound of the image. In this work, we do not apply such prior knowledge, but we note that the prior knowledge is compatible with our method.</p><p>We observe a similar result on SYNTHIA → Cityscapes (see <ref type="table" target="#tab_6">Table 3</ref>). Following the setting in <ref type="bibr" target="#b54">[Zou et al., 2018</ref>, we include the mIoU results of 13 categories as well as 16 categories, which also calculate IoU of other three small-scale objectives, i.e., Wall, Fence and Pole. The proposed method has achieved 47.9 mIoU of 16 categories and 54.9 mIoU * of 13 categories. Comparing to the baseline, MRNet+Pseudo, we yield +1.4% mIoU and +1.1% mIoU * improvement. Meanwhile, the proposed method also outperforms the second best method, i.e., <ref type="bibr">APODA [Yang et al., 2020]</ref>, 1.8% mIoU * . Cross-city. We further evaluate the proposed method on the cross-city benchmark, i.e., Cityscapes → Oxford RobotCar. Both of the source-domain and target-domain datasets are collected in the real-world scenario. We follow the settings in <ref type="bibr" target="#b35">[Tsai et al., 2019]</ref> to report IoU of the shared 9 categories between the two datasets. As shown in <ref type="table" target="#tab_7">Table 4</ref>, the proposed method arrives 74.4% mIoU. Comparing to the baseline, i.e., MRNet+Pseudo (73.9%), the improvement (+0.5%) on the cross-city benchmark is relatively limited. Therefore, the baseline, MRNet+Pseudo, also could obtain competitive results by directly utilizing all pseudo labels. Besides, it is worthy to note that the proposed method has arrived the 6 of 9 best pre-class IoU accuracy, and achieved +5.7% on the class of traffic sign, which is a small-scale objective. Visualization. As shown in <ref type="figure">Figure 4</ref>, we provide the qualitative results of the semantic segmentation adaptation on all three benchmarks. Comparing to the source model, the pseudo label learning could significantly improve the performance. Besides, in contrast with the baseline method with conventional pseudo label learning, we observe that the proposed variance regularization has better scalability to smallscale objectives, such as traffic signs and poles. It is because that the noisy pseudo label usually contains the error of predicting the rare category to the common category, i.e., largescale objectives. The proposed method rectifies the learning from such mistakes, yielding more reasonable segmentation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Evaluations</head><p>Variance Regularization vs. Handcrafted Threshold. The proposed variance regularization is free from setting the threshold. To verify the effectiveness of the variance regularization, we also compare the conventional pseudo label learning with different thresholds. As shown in <ref type="table" target="#tab_9">Table 5</ref>, the proposed regularization arrives the superior performance to the hand-crafted threshold. It is due to that the variance regularization could be viewed as a dynamic threshold, providing different thresholds for different pixels in the same image. For the coherent predictions, the model is prone to learning the pseudo label and maximizing the impact of such labels. For the incoherent results, the model is prone to neglecting the pseudo label automatically and minimizing the negative effect of noisy labels. The best handcrafted threshold is to neglect the label with the prediction score ≤ 0.90, yielding 48.4% mIoU. In contrast, the proposed method achieves 50.3% mIoU with +1.9% increment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Threshold mIoU MRNet <ref type="bibr" target="#b52">[Zheng and Yang, 2020</ref>    Could the proposed method work on the pseudo label generated by other models (e.g., with more noise)? To verify the scalability of the proposed method, we adopt the AdaptSegNet <ref type="bibr" target="#b34">[Tsai et al., 2018]</ref> to generate pseudo labels. AdaptSegNet is inferior to MRNet in terms of the mIoU on GTA5 → Cityscapes. As shown in <ref type="table" target="#tab_10">Table 6</ref>, the proposed method still could learn from the label generated by Adapt-SegNet, improving the performance from 42.4% to 47.4%. Meanwhile, the proposed method is also superior to the baseline method with the conventional pseudo learning (46.8% mIoU).</p><p>Training Convergence. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, the conventional pseudo label learning (orange line) is prone to overfit all pseudo labels, including the noisy label. Therefore, the training loss converges to zero. In contrast, the proposed  </p><formula xml:id="formula_15">E[(F (x j t |θ t ) − F aux (x j t |θ t )) 2 ] 49.6 E[F aux (x j t |θ t ) log( F aux (x j t |θ t ) F (x j t |θ t ) )] 49.4 E[F (x j t |θ t ) log( F (x j t |θ t ) F aux (x j t |θ t ) )]</formula><p>50.3 <ref type="table">Table 9</ref>: Ablation study of distance functions on GTA5 → Cityscapes.  method (blue line) also converges, but does not force the loss to be zero. It is because that we provide the variance regularization term, which could punish the wrong prediction for the uncertain pseudo labels with flexibility.</p><p>Effect of Dropout. The proposed method is not very sensitive to the dropout rate. As shown in <ref type="table" target="#tab_11">Table 7</ref>, we could observe two points: 1) The dropout function is not the main reason for variance of the predictions. Without dropout function (p = 0), the proposed method still could achieve 49.6% mIoU, which is better than the conventional pseudo label <ref type="figure">Fig. 5</ref>: Qualitative results of the discrepancy between the prediction variance and the prediction confidence. We could observe that the prediction variance used in this work has more overlaps with the ambiguous predictions, while the prediction confidence usually focuses on the edge of the two different classes. (Best viewed in color). learning. 2) With a propose dropout rate, the proposed method could generally achieve better results around 50% mIoU.</p><p>Uncertainty of High-confidence Predictions. We analyze the variance of high-confidence predictions on Cityscape. Specifically, we calculate the average uncertainty of rightassigned and wrong-assigned prediction with a confidence score&gt; 0.95. Here we use the metric exp{−D kl } in Equation 12 to report the variance value. The high value means low uncertainty. The average variance of right-assigned highconfidence labels is 0.9901, when the average variance of wrong-assigned high-confidence labels is 0.9332. We could see one significant variance gap between the right-assigned labels and wrong-assigned labels, even if they all achieve a high confidence score. The result verifies that the variance value could reflect the difference between wrong-assigned labels and right-assigned labels.</p><p>Comparison with Monte Carlo Dropout. Monte Carlo Dropout (MC-Dropout) <ref type="bibr" target="#b4">[Gal and Ghahramani, 2016]</ref> activates the dropout function when inference to obtain various predictions. Here we compare the ability of representing the uncertainty of the proposed method and MC-Dropout. For a fair comparison, we just replace the prediction of the aux classifier with the main classifier F drop with MC dropout rate of {0.5, 0.7, 0.9}.</p><formula xml:id="formula_16">D mc = E[F (x j t |θ t ) log( F (x j t |θ t ) F drop (x j t |θ t ) )].<label>(13)</label></formula><p>Since the prediction score could not reflect the ground-truth uncertainty, we introduce one new metric called uncertainty gap as indicator. Uncertainty gap is the variance difference of right predictions and wrong predictions. Generally, we hope that the right prediction obtains low uncertainty value, while the wrong prediction obtains high uncertainty value. In practice, we use the exp(−D) to keep the value in [0,1]. As shown in <ref type="table" target="#tab_13">Table 8</ref>, the proposed method obtains 0.1357 variance gap, which is competitive to MC-dropout with 0.9 drop rate. The proposed method is also complementary to MC-dropout. The proposed method with MC-dropout could further boost the uncertainty gap. Meanwhile, it is worth noting that the proposed method directly leverages the variance of both main and auxiliary classifiers without multiple inferences, which can largely save the test time.</p><p>Effect of Distance Functions. In fact, KL-divergence is an alternative option for variance calculation. We could swap the main and aux classifiers to calculate the distance or use mean-square error (MSE). Here we add one experiment to compare common distance functions (see <ref type="table">Table 9</ref>). First, we could observe that the model is not very sensitive to the distance metric, since the performances are close. Second, the KL-divergence used in Method is slightly better than swapping the predictions and MSE distance.</p><p>Effect of Inference Weighting. Inference weighting is one practical trick to combine the predictions of both main and auxiliary classifiers. Generally, the main classifier could achieve better performance, so we give the prediction of the main classifier a larger weight of α = 1 and assign β = 0.5 to the prediction of auxiliary classifier. Output = arg max(αF (x j t |θ t )+ βF aux (x j t |θ t )). This trick could slightly improve the final performance. Here we provide the ablation study on the sensitivity of inference weighting in <ref type="table" target="#tab_2">Table 10</ref>. If we only deploy the main classifier (α = 1, β = 0), the model could achieve 49.3% mIoU accuracy. When we combine the prediction of two classifiers, the performance could be improved about 1.0% mIoU. Uncertainty Visualization. As a by-product, we also could estimate the prediction uncertainty when inference. We provide the visualization results to show the difference between the uncertainty estimation and the confidence score. As shown in <ref type="figure">Figure 5</ref>, we observe that the model is prone to provide the low confidence score of the boundary pixels, which does not provide the effective cue to the ambiguous prediction. Instead, the proposed prediction variance reflects the label uncertainty, and the highlight area in prediction variance map has lots of overlaps with the wrong prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We identify the challenge of pseudo label learning in adaptive semantic segmentation and present a simple and effective method to estimate the prediction uncertainty during training. We also involve the uncertainty into the optimization objective as the variance regularization to rectify the training. The regularization helps the model learn from the noisy label, without introducing extra parameters or modules. As a result, we achieve the competitive performance on three benchmarks, including two synthetic-to-real benchmarks and one cross-city benchmark. In the future, we will continue to investigate the usage of uncertainty and the applications to other related tasks, e.g., medical imaging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Samples of the noisy pseudo labels on Cityscapes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>The training loss of the proposed method and the pseudo label learning. The pseudo label learning is prone to over-fit all pseudo label, and the training loss converges to zero. In contrast, the proposed method would converge to one non-zero constant while training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: List of categories and number of images in four</cell></row><row><cell>datasets, i.e., GTA5 [Richter et al., 2016], SYNTHIA [Ros</cell></row><row><cell>et al., 2016], Cityscapes [Cordts et al., 2016] and Oxford</cell></row><row><cell>RobotCar [Maddern et al., 2017].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on GTA5 → Cityscapes. We present pre-class IoU and mIoU. The best accuracy in every column is in bold.</figDesc><table><row><cell>Method</cell><cell cols="5">Road SW Build Wall* Fence* Pole* TL</cell><cell>TS Veg. Sky</cell><cell cols="5">PR Rider Car Bus Motor Bike mIoU* mIoU</cell></row><row><cell>Source</cell><cell>55.6 23.8 74.6</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3</cell><cell>13.7</cell><cell>25.0</cell><cell>38.6</cell><cell>−</cell></row><row><cell>SIBAN [Luo et al., 2019a]</cell><cell>82.5 24.0 79.4</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">16.5 12.7 79.2 82.8 58.3 18.0 79.3 25.3</cell><cell>17.6</cell><cell>25.9</cell><cell>46.3</cell><cell>−</cell></row><row><cell>PatchAlign [Tsai et al., 2019]</cell><cell>82.4 38.0 78.6</cell><cell>8.7</cell><cell>0.6</cell><cell>26.0</cell><cell cols="3">3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6</cell><cell>19.3</cell><cell>31.7</cell><cell>46.5</cell><cell>40.0</cell></row><row><cell cols="2">AdaptSegNet [Tsai et al., 2018] 84.3 42.7 77.5</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>4.7</cell><cell cols="2">7.0 77.9 82.5 54.3 21.0 72.3 32.2</cell><cell>18.9</cell><cell>32.3</cell><cell>46.7</cell><cell>−</cell></row><row><cell>CLAN [Luo et al., 2019b]</cell><cell>81.3 37.0 80.1</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9</cell><cell>22.6</cell><cell>30.7</cell><cell>47.8</cell><cell>−</cell></row><row><cell>CCM [Li et al., 2020a]</cell><cell>79.6 36.4 80.6</cell><cell>13.3</cell><cell>0.3</cell><cell cols="4">25.5 22.4 14.9 81.8 77.4 56.8 25.9 80.7 45.3</cell><cell>29.9</cell><cell>52.0</cell><cell>52.9</cell><cell>45.2</cell></row><row><cell>APODA [Yang et al., 2020]</cell><cell>86.4 41.3 79.3</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="3">22.6 17.3 80.3 81.6 56.9 21.0 84.1 49.1</cell><cell>24.6</cell><cell>45.7</cell><cell>53.1</cell><cell>−</cell></row><row><cell>AdvEnt [Vu et al., 2019]</cell><cell>85.6 42.2 79.7</cell><cell>8.7</cell><cell>0.4</cell><cell>25.9</cell><cell>5.4</cell><cell cols="2">8.1 80.4 84.1 57.9 23.8 73.3 36.4</cell><cell>14.2</cell><cell>33.0</cell><cell>48.0</cell><cell>41.2</cell></row><row><cell>Source</cell><cell>64.3 21.3 73.1</cell><cell>2.4</cell><cell>1.1</cell><cell>31.4</cell><cell cols="3">7.0 27.7 63.1 67.6 42.2 19.9 73.1 15.3</cell><cell>10.5</cell><cell>38.9</cell><cell>40.3</cell><cell>34.9</cell></row><row><cell>CBST [Zou et al., 2018]</cell><cell>68.0 29.9 76.3</cell><cell>10.8</cell><cell>1.4</cell><cell cols="4">33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5</cell><cell>18.8</cell><cell>39.8</cell><cell>48.9</cell><cell>42.6</cell></row><row><cell>MRKLD [Zou et al., 2019]</cell><cell>67.7 32.2 73.9</cell><cell>10.7</cell><cell>1.6</cell><cell cols="4">37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0</cell><cell>19.4</cell><cell>45.3</cell><cell>50.1</cell><cell>43.8</cell></row><row><cell>Source</cell><cell>44.0 19.3 70.9</cell><cell>8.7</cell><cell>0.8</cell><cell cols="4">28.2 16.1 16.7 79.8 81.4 57.8 19.2 46.9 17.2</cell><cell>12.0</cell><cell>43.8</cell><cell>40.4</cell><cell>35.2</cell></row><row><cell cols="2">MRNet [Zheng and Yang, 2020] 82.0 36.5 80.4</cell><cell>4.2</cell><cell>0.4</cell><cell cols="4">33.7 18.0 13.4 81.1 80.8 61.3 21.7 84.4 32.4</cell><cell>14.8</cell><cell>45.7</cell><cell>50.2</cell><cell>43.2</cell></row><row><cell>MRNet+Pseudo</cell><cell>83.1 38.2 81.7</cell><cell>9.3</cell><cell>1.0</cell><cell cols="4">35.1 30.3 19.9 82.0 80.1 62.8 21.1 84.4 37.8</cell><cell>24.5</cell><cell>53.3</cell><cell>53.8</cell><cell>46.5</cell></row><row><cell>MRNet+Ours</cell><cell>87.6 41.9 83.1</cell><cell>14.7</cell><cell>1.7</cell><cell cols="4">36.2 31.3 19.9 81.6 80.6 63.0 21.8 86.2 40.7</cell><cell>23.6</cell><cell>53.1</cell><cell>54.9</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on SYNTHIA → Cityscapes. We present pre-class IoU, mIoU and mIoU*. mIoU and mIoU* are averaged over 16 and 13 categories, respectively. The best accuracy in every column is in bold.</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>light</cell><cell>sign</cell><cell>sky</cell><cell>person</cell><cell>automobile</cell><cell>two-wheel</cell><cell>mIoU</cell></row><row><cell>Source</cell><cell cols="10">79.2 49.3 73.1 55.6 37.3 36.1 54.0 81.3 49.7 61.9</cell></row><row><cell cols="11">AdaptSegNet [Tsai et al., 2018] 95.1 64.0 75.7 61.3 35.5 63.9 58.1 84.6 57.0 69.5</cell></row><row><cell>PatchAlign [Tsai et al., 2019]</cell><cell cols="10">94.4 63.5 82.0 61.3 36.0 76.4 61.0 86.5 58.6 72.0</cell></row><row><cell cols="11">MRNet [Zheng and Yang, 2020] 95.9 73.5 86.2 69.3 31.9 87.3 57.9 88.8 61.5 72.5</cell></row><row><cell>MRNet+Pseudo</cell><cell cols="10">95.1 72.5 87.0 72.2 37.4 87.9 63.4 90.5 58.9 73.9</cell></row><row><cell>MRNet+Ours</cell><cell cols="10">95.9 73.7 87.4 72.8 43.1 88.6 61.7 89.6 57.0 74.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on the cross-city benchmark: Cityscapes → Oxford RobotCar. The best accuracy in every column is in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Variance Regularization vs. Handcrafted Threshold.</figDesc><table><row><cell cols="3">The proposed method is free from hand-crafted threshold.</cell></row><row><cell cols="3">'&gt; k' denotes that we only utilize the label confidence &gt; k</cell></row><row><cell cols="3">to train the model. We report the mIoU accuracy on GTA5</cell></row><row><cell>→ Cityscapes.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Pseudo Label</cell><cell>mIoU</cell></row><row><cell>AdaptSegNet [Tsai et al., 2018]</cell><cell>-</cell><cell>42.4</cell></row><row><cell>Pseudo Learning</cell><cell>AdaptSegNet</cell><cell>46.8</cell></row><row><cell>Ours</cell><cell>AdaptSegNet</cell><cell>47.4</cell></row><row><cell>MRNet [Zheng and Yang, 2020]</cell><cell>-</cell><cell>45.5</cell></row><row><cell>Pseudo Learning</cell><cell>MRNet</cell><cell>48.3</cell></row><row><cell>Ours</cell><cell>MRNet</cell><cell>50.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of the impact of different pseudo labels. The model name in the 'Pseudo Label' column denotes that we deploy the pseudo label generated by the corresponding model.</figDesc><table><row><cell>Dropout Rate</cell><cell>mIoU</cell></row><row><cell>Pseudo Learning</cell><cell>48.3</cell></row><row><cell>droprate = 0</cell><cell>49.6</cell></row><row><cell>droprate = 0.1</cell><cell>50.3</cell></row><row><cell>droprate = 0.3</cell><cell>50.1</cell></row><row><cell>droprate = 0.5</cell><cell>50.1</cell></row><row><cell>droprate = 0.7</cell><cell>50.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of dropout rate on GTA5 → Cityscapes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Fig. 4: Qualitative results of semantic segmentation adaptation on GTA5 → Cityscapes, SYNTHIA → Cityscapes and Cityscapes → Oxford RobotCar. We show the original target image, the ground-truth segmentation, the output of the source model, i.e., MRNet, and the baseline, i.e., MRNet+Pseudo. Our results are in the right column. (Best viewed in color).</figDesc><table><row><cell>Methods</cell><cell>Right-prediction</cell><cell>Wrong-prediction</cell><cell>Uncertainty</cell></row><row><cell></cell><cell>Certainty</cell><cell>Certainty</cell><cell>Gap</cell></row><row><cell>MC-dropout 0.5</cell><cell>0.9945</cell><cell>0.9733</cell><cell>0.0212</cell></row><row><cell>MC-dropout 0.7</cell><cell>0.9870</cell><cell>0.9396</cell><cell>0.0474</cell></row><row><cell>MC-dropout 0.9</cell><cell>0.9486</cell><cell>0.8118</cell><cell>0.1368</cell></row><row><cell>Ours</cell><cell>0.9767</cell><cell>0.8410</cell><cell>0.1357</cell></row><row><cell>Ours + dropout 0.5</cell><cell>0.9673</cell><cell>0.8065</cell><cell>0.1608</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparison with Monte Carlo Dropout.</figDesc><table><row><cell>Distance Functions</cell><cell>mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Sensitivity of inference weighting.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t as the one-hot vector ofŷ j t . If the class index c equals toŷ j t ,p j t (c) = 1 elsep j t (c) = 0. The second stage of pseudo learning is to minimize the prediction bias. We could formulate the bias as the similar style of Equation 1:Bias(p t ) = E[F (x j t |θ t ) −p j t ] + E[p j t − p j t ].(2)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">;</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transductive multi-view zero-shot learning. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2332" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani ;</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lempitsky ;</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio ;</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via calibrating uncertainties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietterich ;</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain transfer through deep activation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pixel-level cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<editor>Lee, 2013. Lee, D.-H.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Consistent structural relation learning for zero-shot segmentationg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta parsing networks: Towards generalized few-shot scene parsing with adaptive metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial style mining for one-shot unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maddern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bayesian networks and decision graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jensen ;</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ros</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class-specific reconstruction transfer learning for visual recognition across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2424" to="2438" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ace: adapting to changing environments for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust person re-identification by modelling feature uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Manifold criterion guided transfer learning via intermediate domain generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3759" to="3773" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
