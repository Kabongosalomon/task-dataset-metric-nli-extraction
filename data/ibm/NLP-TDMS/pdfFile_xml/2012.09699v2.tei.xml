<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generalization of Transformer Networks to Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xbresson@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generalization of Transformer Networks to Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a tremendous success in the field of natural language processing (NLP) since the development of Transformers <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref> which are currently the best performing neural network architectures for handling long-term sequential datasets such as sentences in NLP. This is achieved by the use of attention mechanism (Bahdanau, Cho, and Bengio 2014) where a word in a sentence attends to each other word and combines the received information to generate its abstract feature representations. From a perspective of message-passing paradigm (Gilmer AAAI'21 Workshop on Deep Learning on Graphs: Methods and Applications (DLG-AAAI'21). Copyright Â© 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/graphdeeplearning/graphtransformer. <ref type="bibr">et al. 2017)</ref> in graph neural networks (GNNs), this process of learning word feature representations by combining feature information from other words in a sentence can alternatively be viewed as a case of a GNN applied on a fully connected graph of words <ref type="bibr" target="#b15">(Joshi 2020)</ref>. Transformers based models have led to state-of-the-art performance on several NLP applications <ref type="bibr" target="#b9">(Devlin et al. 2018;</ref><ref type="bibr" target="#b26">Radford et al. 2018;</ref><ref type="bibr" target="#b5">Brown et al. 2020</ref>). On the other hand, graph neural networks (GNNs) are shown to be the most effective neural network architectures on graph datasets and have achieved significant success on a wide range of applications, such as in knowledge graphs <ref type="bibr" target="#b28">(Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b6">Chami et al. 2020)</ref>, in social sciences <ref type="bibr" target="#b21">(Monti et al. 2019)</ref>, in physics <ref type="bibr" target="#b7">(Cranmer et al. 2019;</ref><ref type="bibr" target="#b27">Sanchez-Gonzalez et al. 2020)</ref>, etc.</p><p>In particular, GNNs exploit the given arbitrary graph structure while learning the feature representations for nodes and edges and eventually the learned representations are used for downstream tasks. In this work, we explore inductive biases at the convergence of these two active research areas in deep learning towards presenting an improved version of Graph Transformer (see <ref type="figure">Figure 1</ref>) which extends the key design components of the NLP transformers to arbitrary graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>As a preliminary, we highlight the most recent research works which attempt to develop graph transformers <ref type="bibr" target="#b18">(Li et al. 2019;</ref><ref type="bibr" target="#b23">Nguyen, Nguyen, and Phung 2019;</ref><ref type="bibr" target="#b39">Zhang et al. 2020)</ref> with few focused on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. <ref type="bibr" target="#b38">(Yun et al. 2019;</ref><ref type="bibr" target="#b36">Xu, Joshi, and Bresson 2019;</ref><ref type="bibr" target="#b12">Hu et al. 2020;</ref><ref type="bibr" target="#b40">Zhou et al. 2020)</ref>. The model proposed in <ref type="bibr" target="#b18">Li et al. (2019)</ref> employs attention to all graph nodes instead of a node's local neighbors for the purpose of capturing global information. This limits the efficient exploitation of sparsity which we show is a good inductive bias for learning on graph datasets. For the purpose of global information, we argue that there are other ways to incorporate the same instead of letting go sparsity and local contexts. For example, the use of graph-specific positional features <ref type="bibr" target="#b39">(Zhang et al. 2020)</ref>, or node Laplacian position eigenvectors <ref type="bibr" target="#b3">(Belkin and Niyogi 2003;</ref><ref type="bibr" target="#b10">Dwivedi et al. 2020)</ref>, or relative learnable positional information <ref type="bibr" target="#b37">(You, Ying, and Leskovec 2019)</ref>, virtual nodes <ref type="bibr" target="#b19">(Li et al. 2015)</ref>, etc. <ref type="bibr" target="#b39">Zhang et al. (2020)</ref> propose Graph-BERT with an emphasis on pre- training and parallelized learning using a subgraph batching scheme that creates fixed-size linkless subgraphs to be passed to the model instead of the original graph. Graph-BERT employs a combination of several positional encoding schemes to capture absolute node structural and relative node positional information. Since the original graph is not used directly in Graph-BERT and the subgraphs do not have edges between the nodes (i.e., linkless), the proposed combination of positional encodings attempts at retaining the original graph structure information in the nodes. We perform detailed analysis of Graph-BERT positional encoding schemes, along with experimental comparison with the model we present in this paper in Section 4.1. <ref type="bibr" target="#b38">Yun et al. (2019)</ref> developed Graph Transformer Networks (GTN) to learn on heterogeneous graphs with a target to transform a given heterogeneous graph into a meta-path based graph and then perform convolution. Notably, their focus behind the use of attention framework is for inter-preting the generated meta-paths. There is another transformer based approach developed for heterogeneous information networks, namely Heterogeneous Graph Transformer (HGT) by <ref type="bibr" target="#b12">Hu et al. (2020)</ref>. Apart from its ability of handling arbitrary number of node and edge types, HGT also captures the dynamics of information flow in the heterogeneous graphs in the form of relative temporal positional encoding which is based on the timestamp differences of the central node and the message-passing nodes. Furthermore, <ref type="bibr" target="#b40">Zhou et al. (2020)</ref> proposed a transformer based generative model which generates temporal graphs by directly learning from dynamic information in networks. The architecture presented in <ref type="bibr" target="#b23">Nguyen, Nguyen, and Phung (2019)</ref> somewhat proceeds along our goal to develop graph transformer for arbitrary homogeneous graphs with a coordinate embedding based positional encoding scheme. However, their experiments show that the coordinate embeddings are not universal in performance and only helps in a couple of unsupervised learning experiments among all evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>Overall, we find that the most fruitful ideas from the transformers literature in NLP can be applied in a more efficient way and posit that sparsity and positional encodings are two key aspects in the development of a Graph Transformer. As opposed to designing a best performing model for specific graph tasks, our work attempts for a generic, competitive transformer model which draws ideas together from the domains of NLP and GNNs. For an overview, this paper brings the following contributions:</p><p>â¢ We put forward a generalization of transformer networks to homogeneous graphs of arbitrary structure, namely Graph Transformer, and an extended version of Graph Transformer with edge features that allows the usage of explicit domain information as edge features.</p><p>â¢ Our method includes an elegant way to fuse node positional features using Laplacian eigenvectors for graph datasets, inspired from the heavy usage of positional encodings in NLP transformer models and recent research on node positional features in GNNs. The comparison with literature shows Laplacian eigenvectors to be wellplaced than any existing approaches to encode node positional information for arbitrary homogeneous graphs.</p><p>â¢ Our experiments demonstrate that the proposed model surpasses baseline isotropic and anisotropic GNNs. The architecture simultaneously emerges as a better attention based GNN baseline as well as a simple and effective Transformer network baseline for graph datasets for future research at the intersection of attention and graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Architecture</head><p>As stated earlier, we take into account two key aspects to develop Graph Transformers -sparsity and positional encodings which should ideally be used in the best possible way for learning on graph datasets. We first discuss the motivations behind these using a transition from NLP to graphs, and then introduce the architecture proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">On Graph Sparsity</head><p>In NLP transformers, a sentence is treated as a fully connected graph and this choice can be justified for two reasons -a) First, it is difficult to find meaningful sparse interactions or connections among the words in a sentence. For instance, the dependency of a word in a sentence on another word can vary with context, perspective of a user and specific application. There can be numerous plausible ground truth connections among words in a sentence and therefore, text datasets of sentences do not have explicit word interactions available. It thereby makes sense to have each word attending to each other word in a sentence, as followed by the Transformer architecture <ref type="bibr" target="#b30">(Vaswani et al. 2017</ref>). -b) Next, the so-called graph considered in an NLP transformer often has less than tens or hundreds of nodes (i.e. sentences are often less than tens or hundreds of words). This makes for computationally feasibility and large transformer models can be trained on such fully connected graphs of words. In case of actual graph datasets, graphs have arbitrary connectivity structure available depending on the domain and target of application, and have node sizes in ranges of up to millions, or billions. The available structure presents us with a rich source of information to exploit as an inductive bias in a neural network, whereas the node sizes practically makes it impossible to have a fully connected graph for such datasets. On these accounts, it is ideal and practical to have a Graph Transformer where a node attends to local node neighbors, same as in GNNs <ref type="bibr" target="#b8">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b16">Kipf and Welling 2017;</ref><ref type="bibr" target="#b20">Monti et al. 2017;</ref><ref type="bibr" target="#b11">Gilmer et al. 2017;</ref><ref type="bibr" target="#b31">VeliÄkoviÄ et al. 2018;</ref><ref type="bibr" target="#b4">Bresson and Laurent 2017;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">On Positional Encodings</head><p>In NLP, transformer based models are, in most cases, supplied with a positional encoding for each word. This is critical to ensure unique representation for each word, and eventually preserve distance information. For graphs, the design of unique node positions is challenging as there are symmetries which prevent canonical node positional information <ref type="bibr" target="#b22">(Murphy et al. 2019)</ref>. In fact, most of the GNNs which are trained on graph datasets learn structural node information that are invariant to the node position <ref type="bibr" target="#b29">(Srinivasan and Ribeiro 2020)</ref>. This is a critical reason why simple attention based models, such as GAT <ref type="bibr" target="#b31">(VeliÄkoviÄ et al. 2018)</ref>, where the attention is a function of local neighborhood connectivity, instead full-graph connectivity, do not seem to achieve competitive performance on graph datasets. The issue of positional embeddings has been explored in recent GNN works <ref type="bibr" target="#b22">(Murphy et al. 2019;</ref><ref type="bibr" target="#b37">You, Ying, and Leskovec 2019;</ref><ref type="bibr" target="#b29">Srinivasan and Ribeiro 2020;</ref><ref type="bibr" target="#b10">Dwivedi et al. 2020;</ref><ref type="bibr" target="#b17">Li et al. 2020)</ref> with a goal to learn both structural and positional features. In particular, <ref type="bibr" target="#b10">Dwivedi et al. (2020)</ref> make the use of available graph structure to pre-compute Laplacian eigenvectors <ref type="bibr" target="#b3">(Belkin and Niyogi 2003)</ref> and use them as node positional information. Since Laplacian PEs are generalization of the PE used in the original transformers <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref> to graphs and these better help encode distance-aware information (i.e., nearby nodes have similar positional features and farther nodes have dissimilar positional features), we use Laplacian eigenvectors as PE in Graph Transformer. Although these eigenvectors have multiplicity occuring due to the arbitrary sign of eigenvectors, we randomly flip the sign of the eigenvectors during training, following <ref type="bibr" target="#b10">Dwivedi et al. (2020)</ref>.We pre-compute the Laplacian eigenvectors of all graphs in the dataset. Eigenvectors are defined via the factorization of the graph Laplacian matrix;</p><formula xml:id="formula_0">â = I â D â1/2 AD â1/2 = U T ÎU,<label>(1)</label></formula><p>where A is the n Ã n adjacency matrix, D is the degree matrix, and Î, U correspond to the eigenvalues and eigenvectors respectively. We use the k smallest non-trivial eigenvectors of a node as its positional encoding and denote by Î» i for node i. Finally, we refer to Section 4.1 for a comparison of Laplacian PE with existing Graph-BERT PEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Transformer Architecture</head><p>We now introduce the Graph Transformer Layer and Graph Transformer Layer with edge features. The layer architecture is illustrated in <ref type="figure">Figure 1</ref>. The first model is designed for graphs which do not have explicit edge attributes, whereas the second model maintains a designated edge feature pipeline to incorporate the available edge information and maintain their abstract representations at every layer.</p><p>Input First of all, we prepare the input node and edge embeddings to be passed to the Graph Transformer Layer. For a graph G with node features Î± i â R dnÃ1 for each node i and edge features Î² ij â R deÃ1 for each edge between node i and node j, the input node features Î± i and edge features Î² ij are passed via a linear projection to embed these to ddimensional hidden features h 0 i and e 0 ij .</p><formula xml:id="formula_1">h 0 i = A 0 Î± i + a 0 ; e 0 ij = B 0 Î² ij + b 0 ,<label>(2)</label></formula><p>where A 0 â R dÃdn , B 0 â R dÃde and a 0 , b 0 â R d are the parameters of the linear projection layers. We now embed the pre-computed node positional encodings of dim k via a linear projection and add to the node featuresÄ¥ 0 i . Graph Transformer Layer The Graph Transformer is closely the same transformer architecture initially proposed in <ref type="bibr" target="#b30">(Vaswani et al. 2017</ref>), see <ref type="figure">Figure 1</ref> (Left). We now proceed to define the node update equations for a layer .</p><formula xml:id="formula_2">Î» 0 i = C 0 Î» i + c 0 ; h 0 i =Ä¥ 0 i + Î» 0 i ,<label>(3)</label></formula><formula xml:id="formula_3">h +1 i = O h H k=1 jâNi w k, ij V k, h j ,<label>(4)</label></formula><p>where, w k,</p><formula xml:id="formula_4">ij = softmax j Q k, h i Â· K k, h j â d k ,<label>(5)</label></formula><p>and Q k, , K k, , V k, â R d k Ãd , O h â R dÃd , k = 1 to H denotes the number of attention heads, and denotes concatenation. For numerical stability, the outputs after taking exponents of the terms inside softmax is clamped to a value between â5 to +5. The attention outputsÄ¥ +1 i are then passed to a Feed Forward Network (FFN) preceded and succeeded by residual connections and normalization layers, as:</p><formula xml:id="formula_5">h +1 i = Norm h i +Ä¥ +1 i ,<label>(6)</label></formula><formula xml:id="formula_6">h +1 i = W 2 ReLU(W 1Ä¥ +1 i ),<label>(7)</label></formula><formula xml:id="formula_7">h +1 i = Norm Ä¥ +1 i +Ä¥ +1 i ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">W 1 , â R 2dÃd , W 2 , â R dÃ2d ,Ä¥ +1 i ,Ä¥ +1</formula><p>i denote intermediate representations, and Norm can either be Layer-Norm <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton 2016)</ref> or BatchNorm <ref type="bibr" target="#b13">(Ioffe and Szegedy 2015)</ref>. The bias terms are omitted for clarity of presentation.</p><p>Graph Transformer Layer with edge features The Graph Transformer with edge features is designed for better utilization of rich feature information available in several graph datasets in the form of edge attributes. See <ref type="figure">Figure 1</ref> (Right) for a reference to the building block of a layer. Since our objective remains to better use the edge features which are pairwise scores corresponding to a node pair, we tie these available edge features to implicit edge scores computed by pairwise attention. In other words, say an intermediate attention score before softmax,Åµ ij , is computed when a node i attends to node j after the multiplication of query and key feature projections, see the expression inside the brackets in Equation 5. Let us treat this scoreÅµ ij as implicit information about the edge &lt; i, j &gt;. We now try to inject the available edge information for the edge &lt; i, j &gt; and improve the already computed implicit attention scoreÅµ ij . It is done by simply multiplying the two valuesÅµ ij and e ij , see Equation 12. This kind of information injection is not seen to be explored much, or applied in NLP Transformers as there is usually no available feature information between two words. However, in graph datasets such as molecular graphs, or social media graphs, there is often some feature information available on the edge interactions and it becomes natural to design an architecture to use this information while learning. For the edges, we also maintain a designated nodesymmetric edge feature representation pipeline for propagating edge attributes from one layer to another, see <ref type="figure">Figure 1</ref>. We now proceed to define the layer update equations for a layer .Ä¥</p><formula xml:id="formula_9">+1 i = O h H k=1 jâNi w k, ij V k, h j ,<label>(9)</label></formula><formula xml:id="formula_10">e +1 ij = O e H k=1 Åµ k, ij , where,<label>(10)</label></formula><formula xml:id="formula_11">w k, ij = softmax j (Åµ k, ij ),<label>(11)</label></formula><formula xml:id="formula_12">w k, ij = Q k, h i Â· K k, h j â d k Â· E k, e ij ,<label>(12)</label></formula><p>and</p><formula xml:id="formula_13">Q k, , K k, , V k, , E k, â R d k Ãd , O h , O e â R dÃd , k = 1</formula><p>to H denotes the number of attention head, and denotes concatenation. For numerical stability, the outputs after taking exponents of the terms inside softmax is clamped to a value between â5 to +5. The outputsÄ¥ +1 i andÃª +1 ij are then passed to separate Feed Forward Networks preceded and succeeded by residual connections and normalization layers, as:Ä¥</p><formula xml:id="formula_14">+1 i = Norm h i +Ä¥ +1 i ,<label>(13)</label></formula><formula xml:id="formula_15">h +1 i = W h,2 ReLU(W h,1Ä¥ +1 i ),<label>(14)</label></formula><formula xml:id="formula_16">h +1 i = Norm Ä¥ +1 i +Ä¥ +1 i ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_17">W h,1 , â R 2dÃd , W h,2 , â R dÃ2d ,Ä¥ +1 i ,Ä¥ +1 i denote intermediate representations, e +1 ij = Norm e ij +Ãª +1 ij , (16) e +1 ij = W e,2 ReLU(W e,1Ãª +1 ij ),<label>(17)</label></formula><formula xml:id="formula_18">e +1 ij = Norm Ãª +1 ij +Ãª +1 ij ,<label>(18)</label></formula><p>where</p><formula xml:id="formula_19">W e,1 , â R 2dÃd , W e,2 , â R dÃ2d ,Ãª +1 ij ,Ãª +1 ij denote intermediate representations.</formula><p>Task based MLP Layers The node representations obtained at the final layer of Graph Transformer are passed to a task based MLP network for computing task-dependent outputs, which are then fed to a loss function to train the parameters of the model. The formal definitions of the task based layers that we use can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Numerical Experiments</head><p>We evaluate the performance of proposed Graph Transformer on three benchmark graph datasets-ZINC <ref type="bibr" target="#b14">(Irwin et al. 2012)</ref>, PATTERN and CLUSTER (Abbe 2017) from a recent GNN benchmark <ref type="bibr" target="#b10">(Dwivedi et al. 2020)</ref>.</p><p>ZINC, Graph Regression ZINC <ref type="bibr" target="#b14">(Irwin et al. 2012</ref>) is a molecular dataset with the task of graph property regression for constrained solubility. Each ZINC molecule is represented as a graph of atoms as nodes and bonds as edges. Since this dataset have rich feature information in terms of bonds as edge attributes, we use the 'Graph Transformer with edge features' for this task. We use the 12K subset of the data as in <ref type="bibr" target="#b10">Dwivedi et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PATTERN, Node</head><p>Classification PATTERN is a node classification dataset generated using the Stochastic Block Models (SBM) (Abbe 2017). The task is classify the nodes into 2 communities. PATTERN graphs do not have explicit edge features and hence we use the simple 'Graph Transformer' for this task. The size of this dataset is 14K graphs.</p><p>CLUSTER, Node Classification CLUSTER is also a synthetically generated dataset using SBM model. The task is to assign a cluster label to each node. There are total 6 cluster labels. Similar to PATTERN, CLUSTER graphs do not have explicit edge features and hence we use the simple 'Graph Transformer' for this task. The size of this dataset is 12K graphs. We refer the readers to <ref type="bibr" target="#b10">(Dwivedi et al. 2020)</ref> for additional information, inlcuding preparation, of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configurations</head><p>For experiments, we follow the benchmarking protocol introduced in <ref type="bibr" target="#b10">Dwivedi et al. (2020)</ref> based on PyTorch <ref type="bibr" target="#b25">(Paszke et al. 2019)</ref> and DGL <ref type="bibr">(Wang et al. 2019)</ref>. We use 10 layers of Graph Transformer layers with each layer having 8 attention heads and arbitrary hidden dimensions such that the total number of trainable parameters is in the range of 500k. We use learning rate decay strategy to train the models where the training stops at a point when the learning rate reaches to a value of 1 Ã 10 â6 . We run each experiment with 4 different seeds and report the mean and average performance measure of the 4 runs. The results are reported in <ref type="table" target="#tab_2">Table 1 and comparison in Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussion</head><p>We now present the analysis of our experiments on the proposed Graph Transformer Architecture, see <ref type="table" target="#tab_2">Tables 1 and 2.</ref> â¢ The generalization of transformer network on graphs is best when Laplacian PE are used for node positions and Batch Normalization is selected instead of Layer Normalization. For all three benchmark datasets, the experiments score the highest performance in this setting, see <ref type="table">Table 1</ref>. â¢ The proposed architecture performs significantly better than baseline isotropic and anisotropic GNNs (GCN and GAT respectively), and helps close the gap between the original transformer and transformer for graphs. Notably, our architecture emerges as a fresh and improved attention based GNN baseline surpassing GAT (see <ref type="table" target="#tab_2">Table 2</ref>), which employs multi-headed attention inspired by the original transformer <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref> and have been often used in the literature as a baseline for attention-based GNN models. â¢ As expected, sparse graph connectivity is a critical inductive bias for datasets with arbitrary graph structure, as demonstrated by comparing sparse vs. full graph experiments. â¢ Our proposed extension of Graph Transformer with edge features reaches close to the best performing GNN, i.e., GatedGCN, on ZINC. This architecture specifically brings exciting promise to datasets where domain information along pairwise interactions can be leveraged for maximum learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to PEs used in Graph-BERT</head><p>In addition to the reasons underscored in Sections 1.1 and 2.2, we demonstrate the usefulness of Laplacian eigenvectors as a suitable candidate PE for Graph Transformer in this section, by its comparison with different PE schemes applied in Graph-BERT <ref type="bibr" target="#b39">(Zhang et al. 2020</ref>). 2 In Graph-BERT, which operates on fixed size sampled subgraphs, a node attends to every other node in a subgraph. For a given graph G = (V, E) with V nodes and E edges, a subgraph g i of size k + 1 is created for every node i in the graph, which means the original single graph G is converted to V subgraphs. For a subgraph g i corresponding to node u i , the k other nodes are the ones which have the top k intimacy scores with node u i based on a pre-computed intimacy matrix that maps every edge in the graph G to an intimacy score. While the sampling is great for parallelization and efficiency, the original graph structure is not directly used in the layers. Graph-BERT uses  a combination of node PE schemes to inform the model on node structural, positional, and distance information from original graph-i) Intimacy based relative PE, ii) Hop based relative distance encoding, and iii) Weisfeiler Lehman based absolute PE (WL-PE). The intimacy based PE and the hop based PE are variant to the sampled subgraphs, i.e., these PEs for a node in a subgraph g i depends on the node u i w.r.t which it is sampled, and cannot be directly used in other cases unless we use similar sampling strategy. The WL-PE which are absolute structural roles of nodes in the original graph computed using WL algorithm <ref type="bibr" target="#b39">(Zhang et al. 2020;</ref><ref type="bibr" target="#b24">Niepert, Ahmed, and Kutzkov 2016)</ref>, are not variant to the subgraphs and can be easily used as a generic PE mechanism. On that account, we swap Laplacian PE in our experiments for an ablation analysis and use WL-PE from Graph-BERT, see <ref type="table" target="#tab_4">Table 3</ref>. As Laplacian PE capture better structural and positional information about the nodes, which essentially is the objective behind using the three Graph-BERT PEs, they outperform the WL-PE. Besides, WL-PEs tend to overfit SBM datasets and lead to poor generalization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work presented a simple yet effective approach to generalize transformer networks on arbitrary graphs and introduced the corresponding architecture. Our experiments consistently showed that the presence of -i) Laplacian eigenvectors as node positional encodings and -ii) batch normalization, in place of layer normalization, around the transformer feed forward layers enhanced the transformer universally on all experiments. Given the simple and generic nature of our architecture and competitive performance against standard GNNs, we believe the proposed model can be used as baseline for further improvement across graph applications employing node attention. In future works, we are interested in building upon the graph transformer along aspects such as efficient training on single large graphs, applicability on heterogeneous domains, etc., and perform efficient graph representation learning keeping in account the recent innovations in graph inductive biases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where C 0 â R dÃk and c 0 â R d . Note that the Laplacian positional encodings are only added to the node features at the input layer and not during intermediate Graph Transformer layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2012.09699v2 [cs.LG] 24 Jan 2021</figDesc><table><row><cell>Add&amp;Norm</cell><cell></cell><cell>Add&amp;Norm</cell><cell>Add&amp;Norm</cell></row><row><cell>Add&amp;Norm</cell><cell></cell><cell>Add&amp;Norm</cell><cell>Add&amp;Norm</cell></row><row><cell>+</cell><cell>+</cell><cell></cell></row><row><cell>Graph Transformer Layer</cell><cell>Laplacian EigVecs as Positional Encoding</cell><cell>Graph Transformer Layer with edge features</cell></row></table><note>+ + Figure 1: Block Diagram of Graph Transformer with Laplacian Eigvectors (Î») used as positional encoding (LapPE). LapPE is added to input node embeddings before passing the features to the first layer. Left: Graph Transformer operating on node embeddings only to compute attention scores; Right: Graph Transformer with edge features with designated feature pipeline to maintain layer wise edge representations. In this extension, the available edge attributes in a graph is used to explicitly modify the corresponding pairwise attention scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Results of GraphTransformer (GT) on all datasets. Performance Measure for ZINC is MAE, for PATTERN and CLUS-TER is Acc. Results (higher is better for all except ZINC) are averaged over 4 runs with 4 different seeds. Bold: the best performing model for each dataset. We perform each experiment with given graphs (Sparse Graph) and (Full Graph) in which we create full connections among all nodes; For ZINC full graphs, edge features are discarded given our motive of the full graph experiments without any sparse structure information. GCN 0.367Â±0.011 68.498Â±0.976 71.892Â±0.334 GAT 0.384Â±0.007 70.587Â±0.447 78.271Â±0.186 GatedGCN 0.214Â±0.013 76.082Â±0.196 86.508Â±0.085 OUR RESULTS GT (Ours) 0.226Â±0.014 73.169Â±0.622 84.808Â±0.068</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sparse Graph</cell><cell></cell><cell></cell><cell>Full Graph</cell><cell></cell><cell></cell></row><row><cell cols="6">Dataset LapPE L #Param Test Perf.Â±s.d. Train Perf.Â±s.d. #Epoch</cell><cell>Epoch/Total</cell><cell cols="3">Test Perf.Â±s.d. Train Perf.Â±s.d. #Epoch</cell><cell>Epoch/Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Batch Norm: False; Layer Norm: True</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ZINC</cell><cell>x</cell><cell>10 588353 10 588929</cell><cell>0.278Â±0.018 0.284Â±0.012</cell><cell>0.027Â±0.004 0.031Â±0.006</cell><cell>274.75 263.00</cell><cell>26.87s/2.06hr 26.64s/1.98hr</cell><cell>0.741Â±0.008 0.735Â±0.006</cell><cell>0.431Â±0.013 0.442Â±0.031</cell><cell>196.75 196.75</cell><cell>37.64s/2.09hr 31.50s/1.77hr</cell></row><row><cell>CLUSTER</cell><cell>x</cell><cell>10 523146 10 524026</cell><cell>70.879Â±0.295 70.649Â±0.250</cell><cell>86.174Â±0.365 86.395Â±0.528</cell><cell>128.50 130.75</cell><cell>202.68s/7.32hr 200.55s/7.43hr</cell><cell>19.596Â±2.071 27.091Â±3.920</cell><cell>19.570Â±2.053 26.916Â±3.764</cell><cell cols="2">103.00 512.34s/15.15hr 139.50 565.13s/22.37hr</cell></row><row><cell>PATTERN</cell><cell>x</cell><cell cols="2">10 522742 73.140Â±13.633 10 522982 71.005Â±11.831</cell><cell>73.070Â±13.589 71.125Â±11.977</cell><cell cols="3">184.25 276.66s/13.75hr 50.854Â±0.111 192.50 294.91s/14.79hr 56.482Â±3.549</cell><cell>50.906Â±0.005 56.565Â±3.546</cell><cell cols="2">108.00 540.85s/16.77hr 124.50 637.55s/22.69hr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Batch Norm: True; Layer Norm: False</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ZINC</cell><cell>x</cell><cell>10 588353 10 588929</cell><cell>0.264Â±0.008 0.226Â±0.014</cell><cell>0.048Â±0.006 0.059Â±0.011</cell><cell>321.50 287.50</cell><cell>28.01s/2.52hr 27.78s/2.25hr</cell><cell>0.724Â±0.013 0.598Â±0.049</cell><cell>0.518Â±0.013 0.339Â±0.123</cell><cell>192.25 273.50</cell><cell>50.27s/2.72hr 45.26s/3.50hr</cell></row><row><cell>CLUSTER</cell><cell>x</cell><cell>10 523146 10 524026</cell><cell>72.139Â±0.405 73.169Â±0.622</cell><cell>85.857Â±0.555 86.585Â±0.905</cell><cell>121.75 126.50</cell><cell>200.85s/6.88hr 201.06s/7.20hr</cell><cell>21.092Â±0.134 27.121Â±8.471</cell><cell>21.071Â±0.037 27.192Â±8.485</cell><cell cols="2">100.25 595.24s/17.10hr 133.75 552.06s/20.72hr</cell></row><row><cell>PATTERN</cell><cell>x</cell><cell>10 522742 10 522982</cell><cell>83.949Â±0.303 84.808Â±0.068</cell><cell>83.864Â±0.489 86.559Â±0.116</cell><cell cols="3">236.50 299.54s/19.71hr 50.889Â±0.069 145.25 309.95s/12.67hr 54.941Â±3.739</cell><cell>50.873Â±0.039 54.915Â±3.769</cell><cell cols="2">104.50 621.33s/17.53hr 117.75 683.53s/22.77hr</cell></row><row><cell cols="2">Table 1: Model</cell><cell>ZINC</cell><cell>CLUSTER</cell><cell>PATTERN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">GNN BASELINE SCORES from (Dwivedi et al. 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our best performing scores (fromTable 1)on each dataset against the GNN baselines (GCN (Kipf and Welling 2017), GAT (VeliÄkoviÄ et al. 2018), Gat-edGCN(Bresson and Laurent 2017)) of 500k model parameters. Note: Only GatedGCN and GT models use the available edge attributes in ZINC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Sparse Graph Dataset PE #Param Test Perf.Â±s.d. Train Perf.Â±s.d. #Epoch Epoch/Total Batch Norm: True; Layer Norm: False; L = 10</figDesc><table><row><cell></cell><cell>x 588353</cell><cell>0.264Â±0.008</cell><cell>0.048Â±0.006</cell><cell>321.50</cell><cell>28.01s/2.52hr</cell></row><row><cell>ZINC</cell><cell>L 588929</cell><cell>0.226Â±0.014</cell><cell>0.059Â±0.011</cell><cell>287.50</cell><cell>27.78s/2.25hr</cell></row><row><cell></cell><cell>W 590721</cell><cell>0.267Â±0.012</cell><cell>0.059Â±0.010</cell><cell>263.25</cell><cell>27.04s/2.00hr</cell></row><row><cell></cell><cell>x 523146</cell><cell>72.139Â±0.405</cell><cell>85.857Â±0.555</cell><cell>121.75</cell><cell>200.85s/6.88hr</cell></row><row><cell>CLUSTER</cell><cell>L 524026</cell><cell>73.169Â±0.622</cell><cell>86.585Â±0.905</cell><cell>126.50</cell><cell>201.06s/7.20hr</cell></row><row><cell></cell><cell>W 531146</cell><cell>70.790Â±0.537</cell><cell>86.829Â±0.745</cell><cell>119.00</cell><cell>196.41s/6.69hr</cell></row><row><cell></cell><cell>x 522742</cell><cell>83.949Â±0.303</cell><cell>83.864Â±0.489</cell><cell cols="2">236.50 299.54s/19.71hr</cell></row><row><cell>PATTERN</cell><cell>L 522982</cell><cell>84.808Â±0.068</cell><cell>86.559Â±0.116</cell><cell cols="2">145.25 309.95s/12.67hr</cell></row><row><cell></cell><cell>W 530742</cell><cell>75.489Â±0.216</cell><cell>97.028Â±0.104</cell><cell>109.25</cell><cell>310.11s/9.73hr</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of GraphTransformer (GT) using different PE schemes. Notations x: No PE; L: LapPE (ours); W: WL-PE (Zhang et al. 2020). Bold: the best performing model for each dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we do not perform empirical comparison with other PEs in Graph Transformer literature except Graph-BERT, because of two reasons: i) Some existing Graph Transformer methods do not use PEs, ii) If PEs are used, they are usually specialised; for instance, Relative Temporal Encoding (RTE) for encoding dynamic information in heterogeneous graphs in<ref type="bibr" target="#b12">(Hu et al. 2020</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">experiment. The maximum training time for an experiment is limited to 24 hours.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>XB is supported by NRF Fellowship NRFF2017-10.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Task based MLP layer equations</head><p>Graph prediction layer For graph prediction task, the final layer node features of a graph is averaged to get a ddimensional graph-level feature vector y G .</p><p>The graph feature vector is then passed to a MLP to obtain the un-normalized prediction score for each class, y pred â R C for each class:</p><p>where P â R dÃC , Q â R dÃd , C is the number of task labels (classes) to be predicted. Since we perform single-target graph regression in ZINC, C = 1, and the L1-loss between the predicted and groundtruth values is minimized during training.</p><p>Node prediction layer For node prediction task, each node's feature vector is passed to a MLP for computing the un-normalized prediction scores y i,pred â R C for each class:</p><p>where P â R dÃC , Q â R dÃd . During training, the crossentropy loss weighted inversely by the class size is used.</p><p>As a note, these task based layers can be modified as per the requirements of the dataset, and or the prediction to be done. For example, the Graph Transformer edge outputs <ref type="figure">(Figure 1 (Right)</ref>) can be used for edge prediction tasks and the task based MLP layers can be defined in similar fashion as we do for node prediction. Besides, different styles of using final and/or intermediate Graph Transformer layers can be used as inputs to the task based MLP layers, such as JK Readout (Jumping Knowledge) <ref type="bibr" target="#b35">(Xu et al. 2018)</ref>, etc. used often in GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hardware Information</head><p>All experiments are run on Intel Xeon CPU E5-2690 v4 server with 4 Nvidia 1080Ti GPUs. At a given time, 4 experiments were run on the server with each single GPU running</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>RÃ©</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Low-Dimensional Hyperbolic Knowledge Graph Embeddings. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05862</idno>
		<title level="m">Learning Symbolic Physics with Graph Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ZINC: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Transformers are Graph Neural Networks. The Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distance Encoding-Design Provably More Powerful GNNs for Structural Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00142</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJei-2RcK7" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.576</idno>
		<title level="m">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fake news detection on social media using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relational Pooling for Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Universal Self-Attention Network for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>KÃ¶pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<title level="m">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09405</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the Equivalence between Node Embeddings and Structural Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>VeliÄkoviÄ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-graph transformer for free-hand sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11258</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11983" to="11993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<title level="m">Graph-Bert: Only Attention is Needed for Learning Graph Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Data-Driven Graph Generative Model for Temporal Interaction Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
