<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphMix: Improved Training of GNNs for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
							<email>†vikas.verma@aalto.fi</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">MIT</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">HEC</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphMix: Improved Training of GNNs for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>We experimentally validate this analy-sis by applying GraphMix to various architectures such as Graph Convolutional Networks, Graph Attention Networks and Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can consistently improve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and Co-author-Physics.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present GraphMix, a regularization method for Graph Neural Network based semi-supervised object classification, whereby we propose to train a fullyconnected network jointly with the graph neural network via parameter sharing and interpolation-based regularization. Further, we provide a theoretical analysis of how GraphMix improves the generalization bounds of the underlying graph neural network, without making any assumptions about the "aggregation" layer or the depth of the graph neural networks. We experimentally validate this analysis by applying GraphMix to various architectures such as Graph Convolutional Networks, Graph Attention Networks and Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can consistently improve or closely match state-ofthe-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and Co-author-Physics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the presence of graph-structured data across a wide variety of domains, such as biological networks, citation networks and social networks, there have been several attempts to design neural networks, known as graph neural networks (GNN), that can process arbitrarily structured graphs. Early work includes <ref type="bibr" target="#b15">(Gori et al., 2005;</ref><ref type="bibr" target="#b35">Scarselli et al., 2009</ref>) which propose a neural network that can directly process most types of graphs e.g., acyclic, cyclic, directed, and undirected graphs. More recent approaches include <ref type="bibr" target="#b6">(Bruna et al., 2013;</ref><ref type="bibr" target="#b18">Henaff et al., 2015;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b14">Gilmer et al., 2017;</ref><ref type="bibr" target="#b17">Hamilton et al., 2017;</ref><ref type="bibr" target="#b42">Veličković et al., 2018</ref><ref type="bibr" target="#b43">Veličković et al., , 2019</ref><ref type="bibr" target="#b34">Qu et al., 2019;</ref><ref type="bibr" target="#b29">Ma et al., 2019)</ref>, among others. Many of these approaches are designed for addressing the problem of semi-supervised learning over graph-structured data <ref type="bibr" target="#b52">(Zhou et al., 2018)</ref>. Much of these research efforts have been dedicated to developing novel architectures.</p><p>Here we instead propose an architecture-agnostic method for regularized training of GNNs for semisupervised node classification. Recently, regularization based on data-augmentation has been shown to be very effective in other types of neural networks but how to apply these techniques in GNNs is still under-explored. Our proposed method GraphMix 1 is a unified framework that draws inspiration from interpolation based data augmentation <ref type="bibr" target="#b45">Verma et al., 2019a)</ref> and self-training based data-augmentation <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b38">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b46">Verma et al., 2019b;</ref><ref type="bibr" target="#b3">Berthelot et al., 2019)</ref>. We show that with our proposed method, we can achieve state-of-the-art performance even when using simpler GNN architectures such as Graph Convolutional Networks , with no additional memory cost and with minimal additional computation cost. Further, we conduct a theoritical analysis to demonstrate the effectiveness of the proposed method over the underlying GNNs. <ref type="figure">Figure 1</ref>: The procedure for training with GraphMix . The labeled and unlabeled nodes are shown with different colors in the graph. GraphMix augments the training of a baseline Graph Neural Network (GNN) with a Fully-Connected Network (FCN). The FCN is trained by interpolating the hidden states and the corresponding labels. This leads to better features which are transferred to the GNN via sharing the linear transformation parameters W (in Equation1) of the GNN and FCN layers. Furthermore, the predictions made by the GNN for unlabeled data are used to augment the input data for the FCN. The FCN and the GNN losses are minimized jointly by alternate minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Preliminaries</head><p>Problem Setup: We are interested in the problem of semi-supervised node and edge classification using graph-structured data. We can formally define such graph-structured data as an undirected graph G = (V, A, X ), where V = V l ∪ V u is the union of labeled (V l ) and unlabeled (V u ) nodes in the graph with cardinalities n l and n u , and A is the adjacency matrix representing the edges between the nodes of V, X ∈ R (n l +nu)×d is the input node features. Each node v belongs to one out of C classes and can be labeled with a C-dimensional one-hot vector y v ∈ R C . Given the labels Y l ∈ R n l ×C of the labeled nodes V l , the task is to predict the labels Y u ∈ R nu×C of the unlabeled nodes V u .</p><p>Graph Neural Networks: Graph Neural Networks (GNN) learn the l th layer representations of a sample i by leveraging the representations of the samples N B(i) in the neighbourhood of i. This is done by using an aggregation function that takes as an input the representations of all the samples along with the graph structure and outputs the aggregated representation. The aggregation function can be defined using the Graph Convolution layer , Graph Attention Layer , or any general message passing layer <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref>. Formally, let h (l) ∈ R n×k be a matrix containing the k-dimensional representation of n nodes in the l th layer, then:</p><formula xml:id="formula_0">h (l+1) = σ(AGGREGAT E(h (l) W, A))<label>(1)</label></formula><p>where W ∈ R k×k is a linear transformation matrix, k is the dimension of (l + 1) th layer, AGGREGAT E is the aggregation function that utilizes the graph adjacency matrix A to aggregate the hidden representations of neighbouring nodes and σ is a non-linear activation function, e.g. ReLU.</p><p>Interpolation Based Regularization Techniques: Recently, interpolation-based techniques have been proposed for regularizing neural networks. We briefly describe some of these techniques here.</p><p>Mixup  trains a neural network on the convex combination of input and targets, whereas Manifold Mixup <ref type="bibr" target="#b45">(Verma et al., 2019a</ref>) trains a neural network on the convex combination of the hidden states of a randomly chosen hidden layer and the targets. While Mixup regularizes a neural network by enforcing that the model output should change linearly in between the examples in the input space, Manifold Mixup regularizes the neural network by learning better (more discriminative) hidden states.</p><formula xml:id="formula_1">Formally, suppose T θ (x) = (f • g) θ (x)</formula><p>is a neural network parametrized with θ such that g : x − → h is a function that maps input sample to hidden states, f : h − →ŷ is a function that maps hidden states to predicted output, λ is a random variable drawn from Beta(α, α) distribution, Mix λ (a, b) = λ * a + (1 − λ) * b is an interpolation function, D is the data distribution, (x, y) and (x , y ) is a pair of labeled examples sampled from distribution D and be a loss function such as cross-entropy loss, then the Manifold Mixup Loss is defined as:</p><formula xml:id="formula_2">L MM (D, T θ , α) = E (x,y)∼D E (x ,y )∼D E λ∼Beta(α,α) (f (Mix λ (g(x), g(x ))), Mix λ (y, y )). (2)</formula><p>We use above Manifold Mixup loss for training an auxiliary Fully-connected-network as described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GraphMix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Data Augmentation is one of the simplest and most efficient technique for regularizing a neural network. In the domains of computer vision, speech and natural language, there exist efficient data augmentation techniques, for example, random cropping, translation or Cutout <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref> for computer vision, <ref type="bibr" target="#b22">(Ko et al., 2015)</ref> and <ref type="bibr" target="#b32">(Park et al., 2019)</ref> for speech and <ref type="bibr" target="#b48">(Xie et al., 2017)</ref> for natural language. However, data augmentation for the graph-structured data remains under-explored. There exists some recent work along these lines but the prohibitive computation cost (see Section 5) introduced by these methods make them impractical for real-world large graph datasets. Based on these limitations, our main objective is to propose an efficient data augmentation technique for graph datasets.</p><p>Recent work based on interpolation-based data augmentation <ref type="bibr" target="#b45">Verma et al., 2019a)</ref> has seen sizable improvements in regularization performance across a number of tasks. However, these techniques are not directly applicable to graphs for an important reason: Although we can create additional nodes by interpolating the features and corresponding labels, it remains unclear how these new nodes must be connected to the original nodes via synthetic edges such that the structure of the whole graph is preserved. To alleviate this issue, we propose to train an auxiliary Fully-Connected Network (FCN) using Manifold Mixup as discussed in Section 3.2. Note that the FCN only uses the node features (not the graph structure), thus the Manifold mixup loss in Eq. 2 can be directly used for training the FCN.</p><p>Interpolation based data-augmentation techniques have an added advantage for training GNNs: A vanilla GNN learns the representation of each node by iteratively aggregating information from the neighbors of that node (Equation 1). However, this induces the problem of oversmoothing <ref type="bibr" target="#b49">Xu et al., 2018)</ref> while training GNNs with many layers. Due to this limitation, GNNs are trained only with a few layers, and thus they can only leverage the local neighbourhood of each node for learning its representations, without leveraging the representations of the nodes which are multiple hops away in the graph. This limitation can be addressed using the interpolation-based method such as Manifold Mixup: in Manifold Mixup, the representations of a randomly chosen pair of nodes is used to facilitate better representation learning; it is possible that the randomly chosen pair of nodes will not be in the local neighbourhood of each other. Based on these challenges and motivations we present our proposed approach GraphMix for training GNNs in the following Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>We first describe GraphMix at a high-level and then give a more formal description. GraphMix augments the vanilla GNN with a Fully-Connected Network (FCN). The FCN loss is computed using Manifold Mixup as discussed below and the GNN loss is computed normally. Manifold Mixup training of FCN facilitates learning more discriminative node representations <ref type="bibr" target="#b45">(Verma et al., 2019a</ref>). An important question is how these more discriminative node representations can be transferred to the GNN? One potential approach could involve maximizing the mutual information between the hidden states of the FCN and the GNN using formulations similar to those proposed by <ref type="bibr" target="#b37">Sun et al., 2020)</ref>. However, this requires optimizing additional network parameters. Instead, we propose parameter sharing between FCN and GNN to facilitate the transfer of discriminative node representations from the FCN to the GNN. It is a viable option because as mentioned in Eq 1, a GNN layer typically performs an additional operation (AGGREGAT E) on the linear transformations of node representations (which are essentially pre-activation representations of the FCN layer). Using the more discriminative representations of the nodes from FCN, as well as the graph structure, the GNN loss is computed in the usual way to further refine the node representations. In this way we can exploit the improved representations from Manifold Mixup for training GNNs.</p><p>In Section 3.3, without making any assumption about the aggregation function and the depth of the graph neural network, we show that GraphMix improves the generalization of the underlying graph neural network. This makes GraphMix applicable to various kind of architectures having different aggregation functions, such as weighed averaging in GCN <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2016)</ref>, attention based aggregation in GAT  and graph-pooling/unpooling operations in Graph U-Nets . In the aforementioned sense, GraphMix procedure is highly flexible: it can be applied to any underlying GNN as long as the underlying GNN applies parametric transformations to the node features.</p><p>Additionally, we propose to use the predicted targets from the GNN to augment the training set of the FCN. In this way, both the FCN and the GNN facilitate each other's learning process. Both the FCN loss and the GNN loss are optimized in an alternating fashion during training. At inference time, predictions are made using only the GNN.</p><p>A diagram illustrating GraphMix is presented in <ref type="figure">Figure 1</ref> and the full algorithm is presented in Appendix A.3. Further, we draw similarities and difference of GraphMix w.r.t. Co-training framework in the Appendix A.2.</p><p>So far we have presented the general design of GraphMix, now we present GraphMix more formally. Given a graph G, let (X l , Y l ) be the input features and the labels of the labeled nodes V l and let (X u ) be the input features of the unlabeled nodes V u . Let F θ and G θ be a FCN and a GNN respectively, which share the parameters θ. The FCN loss from the labeled data is computed using Eq. 2 as follows:</p><formula xml:id="formula_3">L supervised = L MM ((X l , Y l ), F θ , α)<label>(3)</label></formula><p>For unlabeled nodes V u , we compute the predictionŶ u using the GNN:</p><formula xml:id="formula_4">Y u = G θ (X u )<label>(4)</label></formula><p>We note that recent state-of-the-art semi-supervised learning methods use a teacher model to accurately predict targets for the unlabeled data. The teacher model can be realized as a temporal ensemble of the student model (the model being trained) <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016)</ref> or by using an Exponential Moving Average (EMA) of the parameters of the student model <ref type="bibr" target="#b38">(Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b46">Verma et al., 2019b)</ref>. Different from these approaches, we use the GNN as a teacher model for predicting the targets for the FCN. This is due to the fact that GNNs leverage graph structure, which in practice, allows them to make more accurate predictions than the temporal ensemble or EMA ensemble of FCN (although there is no theoretical guarantee for this).</p><p>Moreover, to improve the accuracy of the predicted targets in Eq 4, we applied the average of the model prediction on K random perturbations of an input sample along with sharpening as discussed in Appendix A.1.</p><p>Using the predicted targets for unlabeled nodes, we create a new training set (X u ,Ŷ u ). The loss from the unlabeled data for the FCN is computed as:</p><formula xml:id="formula_5">L unsupervised = L MM ((X u ,Ŷ u ), F θ , α)<label>(5)</label></formula><p>Total loss for training the FCN is given as the weighted sum of above two loss terms.</p><formula xml:id="formula_6">L FCN = L supervised + w(t) * L unsupervised (6)</formula><p>where w(t) is a sigmoid ramp-up function <ref type="bibr" target="#b38">(Tarvainen &amp; Valpola, 2017)</ref> which increases from zero to its max value γ during the course of training.</p><p>Now let us assume that the loss for an underlying GNN is L GNN = (G θ (X l ), Y l ); the overall GraphMix loss for the joint training of the FCN and GNN can be defined as the weighted sum of the GNN loss and the FCN loss:</p><formula xml:id="formula_7">L GraphMix = L GNN + λ * L FCN<label>(7)</label></formula><p>However, throughout our experiments, optimizing FCN loss and GNN loss alternatively at each training epoch achieved better test accuracy (discussed in Appendix A.12). This has an added benefit that it removes the need to tune weighing hyper-parameter λ.</p><p>For Manifold Mixup training of FCN, we apply mixup only in the hidden layer. Note that in <ref type="bibr" target="#b45">(Verma et al., 2019a)</ref>, the authors recommended applying mixing in a randomly chosen layer (which also includes the input layer) at each training update. However, we observed under-fitting when applying mixup randomly at the input layer or the hidden layer. Applying mixup only in the input layer also resulted in underfitting and did not improve test accuracy.</p><p>Memory and Computational Requirements: One of the major limitations of current GNNs, which prohibits their application to real-world large datasets, is their memory complexity. For example, the fastest implementation of GCN, which stores the entire adjacency matrix A in the memory, has the memory complexity O(|V| 2 ). Implementations with lower memory requirements are possible but they incur higher latency cost due to repeatedly loading parts of adjacency matrix in the memory. Due to these reasons, methods which have additional memory requirement in comparison to the baseline GNNs, are less appealing in practice. In GraphMix, since the parameters of the FCN and GNN are shared, there is no additional memory requirement. Furthermore, GraphMix does not add any significant computation cost over the underlying GNN, because the underlying GNN is trained in the standard way and the FCN training requires trivial additional computation cost for computing the predicted-targets (Appendix A.1) and the interpolation function ( Mix λ (a, b) in Section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>In this subsection, we study how GraphMix impacts the generalization bound of a underlying GNN. Our analysis, which is based on Rademacher complexity <ref type="bibr" target="#b0">(Bartlett &amp; Mendelson, 2002)</ref>, provides a new generalization bound for GraphMix, which shows how adding regularization to training the FCN with pseudolabels improves overall generalization. We rely on the effect of changing one sample in Manifold Mixup and the fact that the weights are shared by a GNN and the corresponding FCN in GraphMix.</p><p>Let G be a fixed graph with n total nodes. Define z i = (x i , y i ) to be the pair of the feature and the true label of the i-th node. Without loss of generality, let S = (z 1 , . . . , z m ) be the training set with the labeled nodes where m = n l and data points z 1 , . . . , z m are sampled according to an unknown distribution D to form the labeled node set S. In this subsection, we follow the previous paper on graph neural networks by assuming that all samples are i.i.d. (including replacement sample) <ref type="bibr" target="#b44">(Verma &amp; Zhang, 2019)</ref>.</p><p>Let Γ be a finite set of the hyperparameters. For every hyperparameter γ ∈ Γ, define F γ to be a distribution-dependent hypothesis space corresponding the hyperparameter γ. That is,</p><formula xml:id="formula_8">F γ = {f γ : (∃S ∈ S)[f γ = A γ (S)]}</formula><p>where A γ is an algorithm that outputs the hypothesis f γ given a dataset S, and S is the set of training datasets such that the probability of S ∈ S according to D is one.</p><formula xml:id="formula_9">For each f γ ∈ F γ , f γ (· ; G) represents GNN with the graph G and f γ (· ; G 0 ) represents FCN where G 0 is the null graph version of G (i.e., G without edges). Let R m (F γ ) be the Rademacher complexity (Bartlett &amp; Mendelson, 2002) of the set {(x, y) → (f γ (x; G), y) : f γ ∈ F γ }. Let L GNN (S, f γ ) be the L GNN with the GNN f γ (· ; G) and labeled data points S as L GNN (S, f γ ) = 1 m m i=1 (f γ (x i ; G), y i ). Let L FCN (S, f γ ) be L FCN with the FCN f γ (· ; G 0 ) and labeled data points S as L FCN (S, f γ ) = L MM (S, f γ (· ; G 0 ), α) + nu m L MM ((X S u ,Ŷ S u ), f γ (· ; G 0 ), α) where (X S u ,Ŷ S u ) is the unlabeled nodes (X u ,Ŷ u ) that corresponds to the labeled node set S. Let L GraphMix (S, f γ ) = L GNN (S, f γ ) + λL FCN (S, f γ ). Let c be the upper bound on per-sample loss as c ≥ (f γ (x i ; G), y i ) and c ≥ (f γ (x i ; G 0 ), y i ).</formula><p>For example, c = 1 for training and test error (or 0-1 loss).</p><p>Theorem 1 provides a generalization bound for GraphMix, which shows that GraphMix can improve the generalization bound of the underlying GNN under the condition of V &gt; 0 as discussed below. Theorem 1. For any δ &gt; 0, with probability at least 1 − δ, the following holds: for all γ ∈ Γ and all</p><formula xml:id="formula_10">f γ ∈ F γ , we have that E x,y∼D [ (f γ (x; G), y)] − L GraphMix (S, f γ ) ≤ R m (F γ ) + c ln(|Γ|/δ) 2m − λV , where V = E S ∼D m [inf fγ ∈Fγ L FCN (S , f γ )] − 4c ln(|Γ|/δ) 2m .</formula><p>The proof of Theorem 1 is given in the following. The generalization bound in Theorem 1 becomes a generalization bound for GNN without GraphMix when λ = 0 as desired. By comparing the generalization bound with λ = 0 (no GraphMix) and λ &gt; 0 (with GraphMix), we can see that GraphMix improves the generalization bound of underlying GNN when</p><formula xml:id="formula_11">V = E S ∼D m [inf fγ ∈Fγ L FCN (S , f γ )]− 4c ln(|Γ|/δ)/(2m) &gt; 0. Here, the first term E S ∼D m [inf fγ ∈Fγ L FCN (S , f γ )]</formula><p>increases as the hypothesis space F γ gets "smaller" or has less complexity. Thus, GraphMix improves the generalization bound of an underlying GNN when the hyperparameter search over γ ∈ Γ results in F γ of moderate complexity such that the first term is greater than 4c ln(|Γ|/δ)/(2m). The first term contains the manifold mixup loss L FCN (S , f γ ) over random training datasets S ( = S), which is expected to be larger when compared with that of the standard loss without manifold mixup.</p><p>For each fixed F γ , the generalization bound in Theorem 1 goes to zero as m</p><formula xml:id="formula_12">→ ∞ since R m (F γ ) → 0 and V → V 0 ≥ 0 as m → ∞.</formula><p>The training loss is also minimized when the trained model f γ ∈ F γ fits well to the particular given training data set S. Therefore, given a particular training dataset S, the expected loss is minimized if we conduct a hyperparameter search over γ ∈ Γ such that f γ ∈ F γ minimize the training loss for the given S but F γ has moderate complexity to not being able to minimize the manifold mixup losses for other datasets S ( = S) drawn from D. Unlike the standard data points, the data points generated during manifold mixup in GraphMix are typically not memorizable or interpolated by FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Proof of Theorem 1</head><p>In the proof, we analyse the effect of changing one sample in Manifold Mixup and GNN-generated targets by utilizing the fact that the weights are shared by a GNN and the corresponding FCN. The the fact of sharing weights also results in the generalization bound that relates the generalization in FCN via Manifold Mixup to the generalization of the GNN.</p><formula xml:id="formula_13">Proof. Let γ ∈ Γ be fixed. Define ϕ(S) = sup fγ ∈Fγ E x,y∼D [ (f γ (x; G), y)] − L GraphMix (S, f γ ).</formula><p>We first provide an upper bound on ϕ(S) by using McDiarmid's inequality. To apply McDiarmid's inequality to ϕ(D), we compute an upper bound on |ϕ(S) − ϕ(S )| where S and S be two training datasets differing by exactly one point of an arbitrary index i 0 ; i.e., S i = S i for all i = i 0 and</p><formula xml:id="formula_14">S i0 = S i0 . ϕ(S ) − ϕ(S) ≤ sup fγ ∈Fγ L GraphMix (S, f γ ) − L GraphMix (S , f γ ) = sup fγ ∈Fγ (L GNN (S, f γ ) − L GNN (S , f γ )) + λ(L FCN (S, f γ ) − L FCN (S , f γ ))</formula><p>where the first line follows the property of the supremum, sup(a) − sup(b) ≤ sup(a − b), and the second line follows the definition of L GraphMix .</p><p>For the first term,</p><formula xml:id="formula_15">L GNN (S, f γ ) − L GNN (S , f γ ) = 1 m (f γ (x i0 ; G), y i0 ) − ( (f γ (x i0 ; G), y i0 ) ≤ c m ,</formula><p>where we used the fact that given a fixed G and a fixed f γ , (f γ (x i ; G), y i ) = (f γ (x i ; G), y i ) for i = i 0 . This holds since f γ (· ; G) does not depend on S given a G and a f γ , even though f γ (x i ; G) contains the aggregation functions over the graph G.</p><p>For the second term,</p><formula xml:id="formula_16">L MM (S, f γ (· ; G 0 ), α) − L MM (S , f γ (· ; G 0 ), α) ≤ c(2m − 1) m 2 ≤ 2c m ,</formula><p>where we use the fact that L MM (S, f γ (· ; G 0 ), α) has m 2 terms and 2m − 1 terms differ for S and S , each of which is bounded by the constant c. Similarly,</p><formula xml:id="formula_17">L MM ((X S u ,Ŷ S u ), f γ (· ; G 0 ), α) − L MM ((X S u ,Ŷ S u ), f γ (· ; G 0 ), α) ≤ 2c nu , since the labelsŶ S u andŶ S u are determined by f γ (x i ; G), and f γ (x i ; G) = f γ (x i ; G)</formula><p>for i = i 0 , given a fixed G and a fixed f γ . Therefore,</p><formula xml:id="formula_18">L FCN (S, f γ ) − L FCN (S , f γ ) ≤ 4c m .</formula><p>Using these upper bounds,</p><formula xml:id="formula_19">ϕ(S ) − ϕ(S) ≤ c(1 + 4λ) m . Similarly, ϕ(S)−ϕ(S ) ≤ c(1+4λ) m .</formula><p>Thus, by McDiarmid's inequality, for any δ &gt; 0, with probability at least 1 − δ/|Γ|,</p><formula xml:id="formula_20">ϕ(S) ≤ ES[ϕ(S)] + c(1 + 4λ) ln(|Γ|/δ) 2m .</formula><p>Moreover,</p><formula xml:id="formula_21">ES[ϕ(S)] + λES inf fγ ∈Fγ L FCN (S, f γ ) ≤ ES sup fγ ∈Fγ Ex ,ȳ ∼D [ (f γ (x ; G),ȳ )] − L GNN (S, f γ ) ≤ ES ,S sup fγ ∈Fγ 1 m m i=1 ( (f γ (x i ; G),ȳ i ) − (f γ (x i ; G),ȳ i )) ≤ E ξ,D,D sup fγ ∈Fγ 1 m m i=1 ξ i ( (f γ (x i ; G),ȳ i ) − (f γ (x i ; G),ȳ i )) ≤ 2R n (Θ)</formula><p>where the second line and the last line use the subadditivity of supremum, the third line uses the Jensen's inequality and the convexity of the supremum, the fourth line follows that for each</p><formula xml:id="formula_22">ξ i ∈ {−1, +1}, the distribution of each term ξ i ( (f γ (x i ; G),ȳ i )− (f γ (x i ; G),ȳ i )) is the distribution of ( (f γ (x i ; G),ȳ i ) − (f γ (x i ; G),ȳ i )</formula><p>) sinceS andS are drawn iid with the same distribution.</p><p>Therefore, for any δ &gt; 0, with probability at least</p><formula xml:id="formula_23">1 − δ/|Γ|, all f γ ∈ F γ , E x,y∼D [ (f γ (x; G), y)] − L GraphMix (S, f γ ) ≤ R m (F γ ) + c ln(|Γ|/δ) 2m − λV.</formula><p>by taking union bounds over all γ ∈ Γ, we obtain the statement of this theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present results for GraphMix algorithm using standard benchmark datasets and the standard architecture in Section 4.1 and 4.3. We also conduct an ablation study on GraphMix in Section A.5 to understand the contribution of various components to its performance. Refer to Appendix A.4 for dataset details and A.8 for implementation and hyperparameter details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semi-supervised Node Classification</head><p>For baselines, we choose GCN , and the recent state-of-the-art graph neural networks GAT , GMNN  and Graph U-Net , as well as the recently proposed regularizers for graph neural networks. GraphMix(GCN) , GraphMix(GAT) and GraphMix(Graph U-Net) refer to the methods where underlying GNNs are GCN, GAT and Graph U-Net respectively. Refer to Appendix A.8 for implementation and hyperparameter details. <ref type="bibr" target="#b36">(Shchur et al., 2018)</ref> demonstrated that the performance of the current state-of-the-art Graph Neural Networks on the standard train/validation/test split of the popular benchmark datasets (such as Cora, Citeseer, Pubmed, etc) is significantly different from their performance on the random splits. For fair evaluation, they recommend using multiple random partitions of the datasets. Along these lines, we created 10 random splits of the Cora, Citeseer and Pubmed with the same train/ validation/test number of samples as in the standard split. We also provide the results for the standard train/validation/test split. The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. We observe that GraphMix always improves the accuracy of the underlying GNNs such as GCN, GAT and Graph-U-Net across all the dataset, with GraphMix(GCN) achieving the best results. We further present results with fewer labeled samples in Appendix A.7. We observer that the relative increase in test accuracy using GraphMix over the baseline GNN is more pronounced when the labeled samples are fewer. This makes GraphMix particularly appealing for very few labeled data problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Larger Datasets</head><p>We also provide results on three recently proposed datasets which are relatively larger than standard benchmark datasets (Cora/Citeseer/Pubmed). We use Cora-Full dataset proposed in  and Coauthor-CS and Coauthor-Physics datasets proposed in <ref type="bibr" target="#b36">(Shchur et al., 2018)</ref>.  The results are presented in <ref type="table" target="#tab_2">Table 3</ref> 2 . We observe that GraphMix(GCN) improves the results over GCN for all the three datasets with a significant margin. We note that we did minimal hyperparameter search for GraphMix(GCN) as mentioned in Appendix A.8.2. The details of the datasets is given in Appendix A.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semi-supervised Link Classification</head><p>In the semi-supervised link classification problem, the task is to predict the labels of the remaining links, given a graph and labels of a few links. Following <ref type="bibr" target="#b39">(Taskar et al., 2004)</ref>, we can formulate the link classification problem as a node classification problem, i.e., given an original graph G, we construct a dual Graph G , the node set V of the dual graph corresponds to the link set E of the original graph. The nodes in the dual graph G are connected if their corresponding links in the graph G share a node. The attributes of a node in the dual graph are defined as the index of the nodes of the corresponding link in the original graph. Using this formulation, we present results on link classification on Bit OTC and Bit Alpha benchmark datasets in the <ref type="table" target="#tab_3">Table 4</ref>. As the numbers of the positive and negative edges are strongly imbalanced, we report the F1 score. Our results show that GraphMix(GCN) improves the performance over the baseline GCN method and is comparable with the recently proposed state-of-the-art method GMNN  for both the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization of the Learned Features</head><p>In <ref type="figure">Figure 2</ref>, we present an analysis of the features learned by GraphMix for Cora dataset using the t-SNE (van der Maaten &amp; Hinton, 2008) based 2D visualization of the hidden states. We observe that GraphMix learns hidden states which are better separated and condensed than GCN and GCN(selftraining). Here, GCN(self-training) refers to training a GCN in a normal way but with additional self-prediction based targets for the unlabeled samples. We further evaluate the Soft-rank (refer to Appendix A.10) of the class-specific hidden states to demonstrate that GraphMix(GCN) makes the class-specific hidden states more concentrated than GCN and GCN(self-training), as shown in <ref type="figure">Figure  2d</ref>. Refer to Appendix A.11 for 2D representation of other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>• Semi-supervised Learning over Graph Data: There exists a long line of work for semisupervised learning over graph data <ref type="bibr" target="#b53">(Zhu &amp; Ghahramani, 2002;</ref><ref type="bibr" target="#b54">Zhu et al., 2003)</ref>. Contrary to previous methods, the recent GNN based approaches define the convolutional operators using the neighbourhood information of the nodes <ref type="bibr" target="#b42">Veličković et al., 2018)</ref>.These convolution operator based method exhibit state-of-the-results for semisupervised learning over graph data, hence much of the recent attention is dedicated to proposing architectural changes to these methods <ref type="bibr" target="#b29">Ma et al., 2019)</ref>. Unlike these methods, we propose a regularization technique that can be applied to any of these GNNs which uses a parameterized transformation on the node features.</p><p>• Data Augmentation: It is well known that the generalization of a learning algorithm can be improved by enlarging the training data size. Since labeling more samples is labour-intensive and costly, Data-augmentation has become de facto technique for enlarging the training data size. Mixing based algorithms are a particular class of data-augmentation methods in which additional training samples are generated by interpolating the samples (either in the input or hidden space) and/or their corresponding targets. Mixup , BC-learning <ref type="bibr" target="#b40">(Tokozume et al., 2017)</ref>, Manifold Mixup <ref type="bibr" target="#b45">(Verma et al., 2019a)</ref>, AMR <ref type="bibr" target="#b1">(Beckham et al., 2019)</ref> are notable examples of this class of algorithms. Unlike, these methods which have been proposed for the fixed topology datasets such as images, in this work, we propose interpolation based data-augmentation techniques for graph-structured data.</p><p>• Regularizing Graph Neural Networks: Regularizing Graph Neural Networks has drawn some attention recently. GraphSGAN  first uses an embedding method such as DeepWalk  and then trains generator-classifier networks in the adversarial learning setting to generate fake samples in the low-density region between sub-graphs. BVAT  and <ref type="bibr" target="#b12">(Feng et al., 2019)</ref> generate adversarial perturbations to the features of the graph nodes while taking graph structure into account. While these methods improve generalization in graph-structured data, they introduce significant additional computation cost: GraphScan requires computing node embedding as a preprocessing step, BVAT and <ref type="bibr" target="#b12">(Feng et al., 2019)</ref> require additional gradient computation for computing adversarial perturbations. Unlike these methods, GraphMix does not introduce any significant additional computation since it is based on interpolation-based techniques and self-generated targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We presented GraphMix, a simple and efficient regularizer for semi-supervised node classification using graph neural networks. Through extensive experiments, we demonstrated state-of-the-art performances using GraphMix on various benchmark datasets. We also presented a theoritical analysis to compare generalization bounds of GraphMix vs the underlying GNNs. Further, we conduct a systematic ablation study to understand the effect of different components in the performance of GraphMix. The strong empirical results of GraphMix suggest that in parallel to designing new architectures, exploring better regularization for graph-structured data is a promising avenue for research. A future research direction is to jointly model the node features and edges of the graph such that it can be further used for generating the synthetic interpolated nodes and their corresponding connectivity to the other nodes in the graph. This will alleviate the need to train the auxiliary FCN in GraphMix.</p><p>Algorithm 1 GraphMix : A procedure for improved training of Graph Neural Networks (GNN) 1: Input: A GCN: G θ (X, A), a FCN: F θ (X, α) which shares parameters with the GCN. Beta distribution parameter α for Manifold Mixup . Number of random perturbations K, Sharpening temperature T . maximum value of weight γ in the weighted averaging of supervised FCN loss and unsupervised FCN loss. Number of parameter updates N . w(t): rampup function for increasing the importance unsupervised loss in FCN training. (XL, YL) represents labeled samples and XU represents unlabeled samples. 2: for t = 1 to N do 3: i = random(0,1) // generate randomly 0 or 1 4:</p><p>if i=0 then 5:</p><p>Lsupervised = LMM((X l , Y l ), F θ , α) // supervised loss from FCN using the Manifold Mixup 6:</p><p>for k = 1 to K do 7:X U,k = RandomP erturbations(XU ) // Apply k th round of random perturbation to XU 8: end for 9:ȲU = 1 K k g(Y |X U,k ; θ, A) // Compute average predictions across K perturbations of XU using the GCN 10: YU = Sharpen(ȲU , T ) // Apply temperature sharpening to the average prediction 11:</p><p>Lunsupervised = LMM((Xu,Ŷu), F θ , α) // unsupervised loss from FCN using the Manifold Mixup 12: L = Lsupervised + w(t) * Lunsupervised // Total loss is the weighted sum of supervised and unsupervised FCN loss 13: else 14:</p><p>L = L(G θ (X l ), Y l ) // Loss using the vanilla GCN 15:</p><p>end if 16:</p><p>Minimize L using gradient descent based optimizer such as SGD. 17: end for other. Our method has some important differences and similarities to the Co-training framework. Similar to Co-training, we train two neural networks and the predictions from the GNN are used to enlarge the training set of the FCN. An important difference is that instead of using the predictions from the FCN to enlarge the training set for the GNN, we employ parameter sharing for passing the learned information from the FCN to the GNN. In our experiments, directly using the predictions of the FCN for GNN training resulted in reduced accuracy. This is due to the fact that the number of labeled samples for training the FCN is sufficiently low and hence the FCN does not make accurate enough predictions. Another important difference is that unlike the co-training framework, the FCN and GNN do not use completely distinct views of the data: the FCN uses feature vectors X and the GNN uses the feature vector and adjacency matrix (X , A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Algorithm</head><p>The procedure for GraphMix training is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Datasets</head><p>We use three standard benchmark citation network datasets for semi-supervised node classification, namely Cora, Citeseer and Pubmed. In all these datasets, nodes correspond to documents and edges correspond to citations. Node features correspond to the bag-of-words representation of the document. Each node belongs to one of C classes. During training, the algorithm has access to the feature vectors and edge connectivity of all the nodes but has access to the class labels of only a few of the nodes.</p><p>For semi-supervised link classification, we use two datasets Bitcoin Alpha and Bitcoin OTC from <ref type="bibr" target="#b23">(Kumar et al., 2016</ref><ref type="bibr" target="#b24">(Kumar et al., , 2018</ref>. The nodes in these datasets correspond to the bitcoin users and the edge weights between them correspond to the degree of trust between the users. Following , we treat edges with weights greater than 3 as positive instances, and edges with weights less than -3 are treated as negative ones. Given a few labeled edges, the task is to predict the labels of the remaining edges. The statistics of these datasets as well as the number of training/validation/test nodes is presented in Appendix A.4.</p><p>The statistics of standard benchmark datasets as well as the number of training/validation/test nodes is presented in <ref type="table" target="#tab_4">Table 5</ref>. The statistics of larger datasets in given in <ref type="table" target="#tab_5">Table 6</ref>. For larger datasets of Section 4.2, we took processed versions of these dataset available here 3 . We did 10 random splits of the the data into train/validation/test split. For the classes which had more than 100 samples. We choose 20 samples per class for training, 30 samples per class for validation and the remaining samples as test data. For the classes which had less than 100 samples, we chose 20% samples, per class for training, 30% samples for validation and the remaining for testing. For each split we run experiments using 100 random seeds. The statistics of these datasets is presented in Appendix <ref type="table" target="#tab_5">Table 6</ref> and The ablation results for semi-supervised node classification are presented in <ref type="table" target="#tab_6">Table 7</ref>. We did not do any hyperparameter tuning for the ablation study and used the best performing hyperparameters found for the results presented in <ref type="table" target="#tab_0">Table 1</ref>. We observe that all the components of GraphMix contribute to its performance, with Manifold Mixup training of FCN contributing possibly the most.</p><p>3 https://github.com/shchur/gnn-benchmark  GraphMix introduces four additional hyperparameters, namely the α parameter of Beta distribution used in Manifold Mixup training of the FCN, the maximum weighing coefficient γ which controls the trade-off between the supervised loss and the unsupervised loss (loss computed using the predictedtargets) of FCN, the temparature T in sharpening and the number of random perturbations K applied to the input data for the averaging of the predictions.</p><p>We conducted minimal hyperparameter seach over only α and γ and fixed the hyperparameters T and K to 0.1 and 10 respectively. The other hyperparameters were set to the best values for underlying GNN (GCN or GAT), including the learning rate, the L2 decay rate, number of units in the hidden layer etc. We observed that GraphMix is not very sensitive to the values of α and γ and similar values of these hyperparameters work well across all the benchmark datasets. Refer to Appendix A.8 and A.9 for the details about the hyperparameter values and the procedure used for the best hyperparameters selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.1 For results reported in Section 4.1</head><p>For GCN and GraphMix(GCN), we used Adam optimizer with learning rate 0.01 and L2-decay 5e-4, the number of units in the hidden layer 16 , dropout rate in the input layer and hidden layer was set to 0.5 and 0.0, respectively. For GAT and GraphMix(GAT), we used Adam optimizer with learning rate 0.005 and L2-decay 5e-4, the number of units in the hidden layer 8 , and the dropout rate in the input layer and hidden layer was searched from the values {0.2, 0.5, 0.8}. For GraphMix(GAT) : α = 1.0 works best for Cora and Citeseer and α = 0.1 works best for Pubmed. γ = 1.0 works best for Cora and Citeseer and γ = 10.0 works best for Pubmed. Input droputrate=0.5 and hidden dropout rate=0.5 work best for Cora and Citeseer and Input dropout rate=0.2 and hidden dropout rate =0.2 work best for Pubmed.</p><p>We conducted all the experiments for 2000 epochs. The value of weighing coefficient w(t) in Algorithm 1) is increased from 0 to its maximum value γ from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b38">(Tarvainen &amp; Valpola, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.2 Hyperparameter Details for Results in Section 4.2</head><p>For all the experiments we use the standard architecture mentioned in Section A.8 and used Adam optimizer with learning rate 0.001 and 64 hidden units in the hidden layer. For Coauthor-CS and Coauthor-Physics, we trained the network for 2000 epochs. For Cora-Full, we trained the network for 5000 epochs because we observed the training loss of Cora-Full dataset takes longer to converge.</p><p>For Coauthor-CS and Coauthor-Physics: We set the input layer dropout rate to 0.5 and weight-decay to 0.0005, both for GCN and GraphMix(GCN) . We did not conduct any hyperparameter search over the GraphMix hyperparameters α, λ max , temparature T and number of random permutations K applied to the input data for GraphMix(GCN) for these two datasets, and set these values to 1.0, 1.0, 0.1 and 10 respectively. For Cora-Full dataset: We found input layer dropout rate 0.2 and weight-decay 0.0 to be the best for both <ref type="bibr">GCN and GraphMix(GCN)</ref> . For GraphMix(GCN) we fixed α, temparature T and number of random permutations K to 1.0 0.1 and 10 respectively. For λ max , we did search over {1.0, 10.0, 20.0} and found that 10.0 works best.</p><p>For all the GraphMix(GCN) experiments, the value of weighing coefficient w(t) in Algorithm 1) is increased from 0 to its maximum value γ max from epoch 500 to 1000 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b38">(Tarvainen &amp; Valpola, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.3 For results reported in Section 4.3</head><p>For α of GraphMix(GCN) , we searched over the values in the set [0.0, 0.1, 0.5, 1.0] and found that 0.1 works best for both the datasets. For γ, we searched over the values in the set [0.1, 1.0, 10.0] and found that 0.1 works best for both the datasets. We conducted all the experiments for 150 epochs. The value of weighting coefficient w(t) in Algorithm 1) is increased from 0 to its maximum value γ from epoch 75 to 125 using the sigmoid ramp-up of Mean-Teacher <ref type="bibr" target="#b38">(Tarvainen &amp; Valpola, 2017)</ref>.</p><p>Both for GraphMix(GCN) and GCN, we use Adam optimizer with learning rate 0.01 and L2-decay 0.0, the number of units in the hidden layer 128 , dropout rate in the input layer was set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.4 For results reported in Section A.7</head><p>For α of GraphMix(GCN) , we searched over the values in the set [0.0, 0.1, 0.5, 1.0] and found that 0.1 works best across all the datasets. For γ, we searched over the values in the set [0.1, 1.0, 10.0] and found that 0.1 and 1.0 works best across all the datasets. Rest of the details for GraphMix(GCN) and GCN are same as Section A.8.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Hyperparameter Selection</head><p>For each configuration of hyperparameters, we run the experiments with 100 random seeds. We select the hyperparameter configuration which has the best validation accuracy averaged over these 100 trials. With this best hyperparameter configuration, for 100 random seeds, we train the model again and use the validataion set for model selection ( i.e. we report the test accuracy at the epoch which has best validation accuracy.) <ref type="figure">Figure 4</ref>: GCN train loss and validation loss for Alternate optimization vs. weighted joint optimization. lambda = X.X represents the value of λ for simultaneous optimization in Eq 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 Soft-Rank</head><p>Let H be a matrix containing the hidden states of all the samples from a particular class. The Soft-Rank of matrix H is defined by the sum of the singular values of the matrix divided by the largest singular value. A lower Soft-Rank implies fewer dimensions with substantial variability and it provides a continuous analogue to the notion of rank from matrix algebra. This provides evidence that the concentration of class-specific states observed when using GraphMix in <ref type="figure" target="#fig_2">Figure 3</ref> can be measured directly from the hidden states and is not an artifact of the T-SNE visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 Feature Visualization</head><p>We present the 2D visualization of the hidden states learned using GCN and GraphMix(GCN) for Cora, Pubmed and Citeseer datasets in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that for Cora and Citeseer, GraphMix learns substantially better hidden states than GCN. For Pubmed, we observe that although there is no clear separation between classes, "Green" and "Red" classes overlap less using the GraphMix, resulting in better hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.12 Joint Optimization vs Alternate optimization</head><p>In this Section, we discuss the effect of hyperparameter λ, that is used to compute the weighted sum of GCN and FCN losses in in Eq 7. In <ref type="figure">Figure 4</ref>, we see that a wide range of λ ( from 0.1 to 1.0) achieves better validation accuracy than the vanilla GCN. Furthermore, alternate optimization of the FCN loss and the GCN loss achieves substantially better validation accuracy than the simultaneous optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>GCN(self-training) (c) GraphMix(GCN) (d) Class-specific Soft-Rank Figure 2: Two-dimensional representation of the hidden states of Citeseer dataset using (a) GCN, (b) GCN(self-training), (c) GraphMix(GCN), and Soft-Rank (d). GraphMix(GCN) learns better separated representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>α and γ of GraphMix(GCN) and GraphMix(GAT) , we searched over the values in the set [0.0, 0.1, 1.0, 2.0] and [0.1, 1.0, 10.0, 20.0] respectively. For GraphMix(GCN) : α = 1.0 works best across all the datasets. γ = 1.0 works best for Cora and Citeseer and γ = 10.0 works best for Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>T-SNE of hidden states for Cora (left), Pubmed (middle), and Citeseer (right). Top row is GCN baseline, bottom row is GraphMix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of node classification (% test accuracy) on the standard split of datasets. [*] means the results are taken from the corresponding papers. We conduct 100 trials and report mean and standard deviation over the trials (refer toTable 8in the Appendix for comparison with other methods on standard Train/Validation/Test split).</figDesc><table><row><cell>Algorithm</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell cols="3">81.30±0.66 70.61±0.22 79.86±0.34</cell></row><row><cell>GAT</cell><cell cols="3">82.70±0.21 70.40±0.35 79.05±0.64</cell></row><row><cell>Graph U-Net</cell><cell cols="3">81.74±0.54 67.69±1.10 77.73 ±0.98</cell></row><row><cell>BVAT*</cell><cell>83.6±0.5</cell><cell>74.0±0.6</cell><cell>79.9±0.4</cell></row><row><cell>DropEdge*</cell><cell>82.8</cell><cell>72.3</cell><cell>79.6</cell></row><row><cell>GraphSGAN*</cell><cell>83.0±1.3</cell><cell>73.1±1.8</cell><cell>-</cell></row><row><cell>GraphAT*</cell><cell>82.5</cell><cell>73.4</cell><cell>-</cell></row><row><cell>GraphVAT*</cell><cell>82.6</cell><cell>73.7</cell><cell>-</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="3">83.94±0.57 74.72±0.59 80.98±0.55</cell></row><row><cell>GraphMix (GAT)</cell><cell cols="3">83.32±0.18 73.08±0.23 81.10±0.78</cell></row><row><cell cols="4">GraphMix (Graph U-Net) 82.47±0.76 69.31±1.52 78.91±1.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of node classification (% test accuracy) using 10 random Train/Validation/Test split of datasets. We conduct 100 trials and report mean and standard deviation over the trials.</figDesc><table><row><cell>Algorithm</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell cols="3">77.84±1.45 72.56±2.46 78.74±0.99</cell></row><row><cell>GAT</cell><cell cols="3">77.74±1.86 70.41±1.81 78.48±0.96</cell></row><row><cell>Graph U-Net</cell><cell cols="3">77.59±1.60 67.55±0.69 76.79±2.45</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="3">82.07±1.17 76.45±1.57 80.72±1.08</cell></row><row><cell>GraphMix (GAT)</cell><cell cols="3">80.63±1.31 74.08±1.26 80.14±1.51</cell></row><row><cell cols="4">GraphMix (Graph-U-Net) 80.18±1.62 72.85±1.71 78.47±0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of GraphMix with other methods (% test accuracy ), for Cora-Full, Coauthor-CS, Coauthor-Physics. * refers to the results reported in<ref type="bibr" target="#b36">(Shchur et al., 2018)</ref>.</figDesc><table><row><cell>Algorithm</cell><cell>Cora-Full</cell><cell>Coauthor-CS</cell><cell>Coauthor -Physics</cell></row><row><cell>GCN*</cell><cell>62.2±0.6</cell><cell>91.1±0.5</cell><cell>92.8±1.0</cell></row><row><cell>GAT*</cell><cell>51.9±1.5</cell><cell>90.5±0.6</cell><cell>92.5±0.9</cell></row><row><cell>MoNet*</cell><cell>59.8±0.8</cell><cell>90.8±0.6</cell><cell>92.5±0.9</cell></row><row><cell>GS-Mean*</cell><cell>58.6±1.6</cell><cell>91.3±2.8</cell><cell>93.0±0.8</cell></row><row><cell>GCN</cell><cell>60.13±0.57</cell><cell>91.27±0.56</cell><cell>92.90±0.92</cell></row><row><cell>Graph-U-Net</cell><cell>59.82±0.39</cell><cell>90.89±0.43</cell><cell>92.57±0.81</cell></row><row><cell>GraphMix (GCN)</cell><cell>61.80±0.54</cell><cell>91.83±0.51</cell><cell>94.49±0.84</cell></row><row><cell cols="3">GraphMix (Graph-U-Net) 60.92 ± 0.51 91.44 ± 0.46</cell><cell>93.78 ± 0.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on Link Classification (%F1 score). * means the results are taken from the corresponding papers.</figDesc><table><row><cell>Algorithm</cell><cell>Bit OTC</cell><cell>Bit Alpha</cell></row><row><cell>DeepWalk (Perozzi et al., 2014)</cell><cell>63.20</cell><cell>62.71</cell></row><row><cell>GMNN* (Qu et al., 2019)</cell><cell>66.93</cell><cell>65.86</cell></row><row><cell>GCN</cell><cell cols="2">65.72±0.38 64.00±0.19</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="2">66.35±0.41 65.34±0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="7"># Nodes # Edges # Features # Classes # Training # Validation # Test</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell><cell>140</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>120</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>60</cell><cell>500</cell><cell>1,000</cell></row><row><cell>Bitcoin Alpha</cell><cell>3,783</cell><cell>24,186</cell><cell>3,783</cell><cell>2</cell><cell>100</cell><cell>500</cell><cell>3,221</cell></row><row><cell>Bitcoin OTC</cell><cell>5,881</cell><cell>35,592</cell><cell>5,881</cell><cell>2</cell><cell>100</cell><cell>500</cell><cell>5,947</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics for Larger datasets</figDesc><table><row><cell>Datasets</cell><cell cols="2">Classes Features Nodes</cell><cell>Edges</cell></row><row><cell>Cora-Full</cell><cell>67</cell><cell>8710 18703</cell><cell>62421</cell></row><row><cell>Coauthor-CS</cell><cell>15</cell><cell>6805 18333</cell><cell>81894</cell></row><row><cell>Coauthor-Physics</cell><cell>5</cell><cell cols="2">8415 34493 247962</cell></row><row><cell>NELL</cell><cell>210</cell><cell cols="2">5414 65755 266144</cell></row><row><cell>A.5 Ablation Study</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Since GraphMix consists of various components, some of which are common with the existing</cell></row><row><cell cols="4">literature of semi-supervised learning, we set out to study the effect of various components by</cell></row><row><cell cols="4">systematically removing or adding a component from GraphMix . We measure the effect of the</cell></row><row><cell>following:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">• Removing both the Manifold Mixup and predicted targets from the FCN training.</cell></row><row><cell cols="4">• Having only the Manifold Mixup training for FCN ( No predicted targets for FCN training)</cell></row><row><cell cols="4">• Using the predicted targets for the FCN training (No Manifold Mixup in FCN training)</cell></row><row><cell cols="4">• Using both Manifold Mixup and predicted targets for FCN training</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study results using 10 labeled samples per class (% test accuracy). We report mean and standard deviation over ten trials. See Section A.5 for the meaning of methods in leftmost column. 55±3.29 65.72±2.10 75.74±1.69With Manifold Mixup and Predicted targets 79.30±1.36 70.78±1.41 77.13±3.60</figDesc><table><row><cell>Ablation</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Without Manifold Mixup and without predicted-targets</cell><cell cols="3">68.78±3.54 61.01±1.24 72.56±1.08</cell></row><row><cell>With Predicted targets</cell><cell cols="3">69.08±5.03 62.66±1.80 73.07±0.94</cell></row><row><cell>With Manifold Mixup</cell><cell>73.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Results using less labeled samples (% test accuracy). K referes to the number of labeled samples per class. 91±3.10 55.61±5.75 64.19±3.89 66.06±3.85 75.57±1.58 GAT 68.17±5.54 73.88±4.35 55.54±1.82 61.63±0.42 64.24±4.79 73.60±1.85 82±2.73 57.6±0.64 62.24±2.90 66.61±3.69 75.96±1.70 GraphMix (Graph U-Net) 66.84±6 5.10 73.14±3.17 54.39±5.07 64.36±3.48 67.40±5.33 70.43±3.75</figDesc><table><row><cell>Algorithm</cell><cell>K = 5</cell><cell cols="2">Cora</cell><cell>K = 10</cell><cell>Citeseer K = 5 K = 10</cell><cell>Pubmed K = 5 K = 10</cell></row><row><cell cols="7">GCN 72.Graph U-Net 66.39±4.26 64.42±5.44 71.48±3.03 49.43±5.81 61.16±3.47 65.05±4.69 68.65±3.69</cell></row><row><cell>GraphMix (GCN)</cell><cell cols="2">71.99±6.46</cell><cell cols="4">79.30±1.36 58.55±2.26 70.78±1.41 67.66±3.90 77.13±3.60</cell></row><row><cell>GraphMix (GAT)</cell><cell cols="2">72.01±6.68</cell><cell cols="2">75.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">code available at https://github.com/vikasverma1077/GraphMix Preprint. Under review. arXiv:1909.11715v3 [cs.LG] 8 Oct 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We do not provide results for GAT based experiments inTable 3andTable 4because we ran out of GPU memory required to run these experiments with larger (higher number of nodes) datasets.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Accurate Target Prediction for Unlabeled data A recently proposed method for accurate target predictions for unlabeled data uses the average of predicted targets across K random augmentations of the input sample <ref type="bibr" target="#b3">(Berthelot et al., 2019)</ref>. Along these lines, in GraphMix we compute the predicted-targets as the average of predictions made by GNN on K drop-out versions of the input sample.</p><p>Many recent semi-supervised learning algorithms <ref type="bibr" target="#b25">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b30">Miyato et al., 2018;</ref><ref type="bibr" target="#b38">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b46">Verma et al., 2019b)</ref> are based on the cluster assumption <ref type="bibr">(Chapelle et al., 2010)</ref>, which posits that the class boundary should pass through the low-density regions of the marginal data distribution. One way to enforce this assumption is to explicitly minimize the entropy of the model's predictions p(y|x, θ) on unlabeled data by adding an extra loss term to the original loss term <ref type="bibr" target="#b16">(Grandvalet &amp; Bengio, 2005)</ref>. The entropy minimization can be also achieved implicitly by modifying the model's prediction on the unlabeled data such that the prediction has low entropy and using these low-entropy predictions as targets for the further training of the model. Examples include "Pseudolabels" <ref type="bibr" target="#b26">(Lee, 2013)</ref> and "Sharpening" <ref type="bibr" target="#b3">(Berthelot et al., 2019)</ref>. Pseudolabeling constructs hard (one-hot) labels for the unlabeled samples which have "high-confidence predictions". Since many of the unlabeled samples may have "low-confidence predictions", they can not be used in the Pseudolabeling technique. On the other hand, Sharpening does not require "high-confidence predictions" , and thus it can be used for all the unlabelled samples. Hence in this work, we use Sharpening for entropy minimization. The Sharpening function over the model prediction p(y|x, θ) can be formally defined as follows <ref type="bibr" target="#b3">(Berthelot et al., 2019)</ref>, where T is the temperature hyperparameter and C is the number of classes:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Connection to Co-training</head><p>The GraphMix approach can be seen as a special instance of the Co-training framework <ref type="bibr" target="#b4">(Blum &amp; Mitchell, 1998)</ref>. Co-training assumes that the description of an example can be partitioned into two distinct views and either of these views would be sufficient for learning given sufficient labeled data. In this framework, two learning algorithms are trained separately on each view and then the prediction of each learning algorithm on the unlabeled data is used to enlarge the training set of the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Farnoosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02709</idno>
		<title level="m">On Adversarial Mixup Resynthesis. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas</surname></persName>
		</author>
		<idno>1532-4435</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=1248547" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixmatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<title level="m">A Holistic Approach to Semi-Supervised Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/279943.279962</idno>
		<ptr target="http://doi.acm.org/10.1145/279943.279962" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<publisher>The MIT Press</publisher>
			<biblScope unit="page">9780262514125</biblScope>
		</imprint>
	</monogr>
	<note>1st edition, 2010. ISBN 0262514125</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks. CoRR, abs/1902.09192</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.09192" />
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3269206.3271768</idno>
		<ptr target="http://doi.acm.org/10.1145/3269206.3271768" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tat-Seng</surname></persName>
		</author>
		<idno>abs/1902.08226</idno>
		<ptr target="http://arxiv.org/abs/1902.08226" />
		<title level="m">Graph adversarial training: Dynamically regularizing based on graph structure. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/gao19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, Kamalika and Salakhutdinov, Ruslan</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Saul, L. K., Weiss, Y., and Bottou, L.</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.05163</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bklr3j0cKX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vijayaditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Edge weight prediction in weighted signed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srijan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spezzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rev2: Fraudulent user prediction in rating platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srijan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Disha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename></persName>
		</author>
		<idno>abs/1610.02242</idno>
		<ptr target="http://arxiv.org/abs/1610.02242" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<idno>1-57735-189-4</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3041838" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emanuele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<idno>abs/1611.08402</idno>
		<ptr target="http://arxiv.org/abs/1611.08402" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<title level="m">Simple Data Augmentation Method for Automatic Speech Recognition. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GMNN: Graph Markov neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, Kamalika and Salakhutdinov, Ruslan</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
		<ptr target="http://dx.doi.org/10.1109/TNN.2008.2005605" />
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation. CoRR, abs/1811.05868</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oleksandr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maximilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.05868" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lfF2NYvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Link prediction in relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ming-Fai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5486" to="5494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v9/vandermaaten08a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arantxa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/verma19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, Kamalika and Salakhutdinov, Ruslan</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<editor>Kraus, Sarit</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Montavon, Grégoire, Orr, Geneviève, and Müller, K. R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1703.02573</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chengtao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yonglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomohiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><forename type="middle">-</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/xu18c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, Jennifer and Krause, Andreas</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moustapha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
		<title level="m">mixup: Beyond empirical risk minimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganqu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhengyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
		<ptr target="http://arxiv.org/abs/1812.08434" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Comparison with State-of-the-art Methods We present the comparion of GraphMix with the recent state-of-the-art methods as well as earlier methods using the standard Train/Validation/Test split in Table 8. We additionally use self-training based baselines, where FCN, GCN, GAT and Graph-U-Net are trained with self-generated targets. These are named as FCN (self-training), GCN (self-training), GAT (self-training) and Graph-U-Net(self-training)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
	<note>respectively in Table 8. For generating the predicted-targets in above two baselines, we followed the procedure of Appendix A.1</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Comparison of GraphMix with other methods (% test accuracy ), for Cora</title>
		<imprint>
			<publisher>Citeseer and Pubmed</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Manireg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belkin</surname></persName>
		</author>
		<idno>5% 60.1% 70.7%</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Semiemb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno>0% 59.6% 71.7%</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lp (zhu</surname></persName>
		</author>
		<idno>0% 45.3% 63.0%</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deepwalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perozzi</surname></persName>
		</author>
		<idno>2% 43.2% 65.3%</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ica (lu &amp;amp; Getoor</surname></persName>
		</author>
		<idno>1% 69.1% 73.9%</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Planetoid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno>7% 64.7% 77.2%</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Chebyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Defferrard</surname></persName>
		</author>
		<idno>2% 69.8% 74.4%</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Gcn (kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>5% 70.3% 79.0%</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monti</surname></persName>
		</author>
		<idno>81.7 ± 0.5% - 78.8 ± 0.3%</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gat (veličković</surname></persName>
		</author>
		<idno>83.0 ± 0.7% 72.5 ± 0.7% 79.0 ± 0.3%</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Graphscan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<idno>7% 73.4% 80.5%</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">83</biblScope>
		</imprint>
	</monogr>
	<note>3 ±1.3 73.1±1.8 -DisenGCN</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U-Net (</forename><surname>Graph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<idno>4% 73.2% 79.6%</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">84</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bvat (deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphmix</surname></persName>
		</author>
		<idno>GCN) 83.94±0.57 74.52±0.59 80.98±0.55</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphmix</surname></persName>
		</author>
		<idno>GAT) 83.32±0.18 73.08±0.23 81.10±0.78</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">exist. For each class, we randomly sampled K ∈ {5, 10} samples for training and the same number of samples for the validation. We used all the remaining labeled samples as the test set. We repeated this process for 10 times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphmix</surname></persName>
		</author>
		<idno>82.18±0.63 69.00 ±1.32 78.76±1.09</idno>
		<imprint/>
	</monogr>
	<note>The results in Table 9 show that GraphMix achieves even better improvements when the labeled samples are fewer</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gat (veličković</surname></persName>
		</author>
		<title level="m">A.8 Implementation and Hyperparameter Details We use the standard benchmark architecture as used in GCN (Kipf &amp; Welling</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>among others. This architecture has one hidden layer and the graph convolution is applied twice : on the input layer and on the output of the hidden layer. The FCN in GraphMix shares the parameters with the GCN</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
