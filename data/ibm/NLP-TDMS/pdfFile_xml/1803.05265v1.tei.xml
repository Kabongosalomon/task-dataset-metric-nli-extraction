<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation-Sensitive Regression for Oriented Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
							<email>zzhu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
							<email>shibaoguang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<email>guisong.xia@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation-Sensitive Regression for Oriented Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotationsensitive features. The proposed method named Rotationsensitive Regression Detector (RRD) achieves state-of-theart performance on three oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reading text in the wild is an active research field in computer vision, driven by many real-world applications such as license plate recognition <ref type="bibr" target="#b31">[33]</ref>, guide board recognition <ref type="bibr" target="#b35">[37]</ref>, and photo OCR <ref type="bibr" target="#b1">[3]</ref>. A scene text reading system generally begins with localizing text regions on which the recognition is then performed. Consequently, one of the main bottleneck of such a system lies in the quality of text detection.</p><p>Despite the great success of recent general object detection algorithms <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35]</ref>, scene text detection remains challenging mainly due to arbitrary orientations, * Corresponding author. small sizes, and significantly varied aspect ratios of text in natural images. In fact, general object detection methods usually focus on detecting objects in terms of horizontal bounding boxes, which are accurate enough for most objects such as person, vehicles, etc. Yet, horizontal bounding box is not appropriate for representing long and thin objects in arbitrary orientations (see <ref type="figure" target="#fig_0">Fig. 1a</ref>). Text in natural images is a typical example of multi-oriented long and thin object, which is better covered by oriented bounding boxes. Directly applying general object detection methods to scene text detection would generally lead to poor performance. Some recent scene text detectors <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b27">29]</ref> successfully adopt general object detection methods to scene text detection with some dedicated designs, yielding a great improvement in scene text detection. Generally, recent object detection consists of predicting object category (i.e., classification) and regressing bounding box for accurate localization. Both tasks rely on shared features which are rotation-invariant attributing to the use of pooling layers in general convolutional neural network (CNN) architecture <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b16">18]</ref>. This pipeline is also adopted in recent scene text detectors inspired by general object detection methods. Although it is well known that rotation-invariant features can boost the performance of classification, the rotation invariance is not beneficial for regressing arbitrary oriented bounding boxes. This conflicting issue between classification and regression may not be very important on oriented objects with limited aspect ratio. However, unlike Latin text, there is not a "blank" between neighbor words in non-Latin text such as Chinese, Arabic, Japanese, etc, which possess long text lines frequently and are often detected at the line level instead of word spotting. In this sense, detecting long oriented text lines is obviously a non-trivial task that satisfies with the practical requirements of a more general text reading system for multi-lingual text. Thus, for scene text, especially non-Latin text lines which are usually long and thin, and of arbitrary orientations, using rotationinvariant features would hinder the regression of such oriented bounding boxes. Another important issue for detecting arbitrary oriented long text having extreme aspect ratios is the requirement of more flexible receptive field.</p><p>To alleviate the above-mentioned issues of arbitrary oriented scene text detection, we propose to separate the regression task from the classification task. More specifically, as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed method named Rotationsensitive Regression Detector (RRD) performs classification with rotation-invariant features, and oriented bounding box regression with rotation-sensitive features. For that, we adopt oriented response convolution <ref type="bibr" target="#b50">[52]</ref> instead of normal convolution in the network architecture. The rotationinvariant features for classification are then obtained by an oriented response pooling layer <ref type="bibr" target="#b50">[52]</ref>. To the best of our knowledge, this is the first time to apply oriented response network to object detection task. In addition, we also propose an inception block of three-scale convolutional kernels to give flexible receptive field better covering long text. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the regression feature map ( <ref type="figure" target="#fig_0">Fig. 1d</ref>) of RRD contains richer and more precise orientation information and its classification feature map ( <ref type="figure" target="#fig_0">Fig. 1e</ref>) is more intensive, compared with the conventional shared feature map ( <ref type="figure" target="#fig_0">Fig. 1b)</ref> for both classification and regression. The final results ( <ref type="figure" target="#fig_0">Fig. 1c</ref> and <ref type="figure" target="#fig_0">Fig. 1f</ref>) also demonstrate the quality of the feature maps.</p><p>The main contributions of this paper are three folds: 1) We propose a novel idea of using rotation-sensitive features for regressing oriented bounding boxes while using rotation-invariant features for classification. This separation gives rise to a more accurate regression in detecting arbitrary oriented long and thin objects; 2) A general framework for arbitrary oriented object (e.g., scene text) detection is proposed. It can be easily embedded into any existing detection architectures, improving the performance without obvious loss of speed. 3) The proposed RRD is also an effective and efficient oriented scene text detector, with the generality of detecting both Latin and non-Latin text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection</head><p>Recent object detectors <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b25">27]</ref> leverage the powerful learning ability of CNN to detect objects, and achieve impressive performances. Generally, most CNN-based object detectors share a common pipeline which consists of object classification predicting the object category and bounding box regression for accurate localization. Both classification and regression rely on shared translation and rotation-invariant features attributing to pooling layers involved in classical CNN architecture. Although these invariances are beneficial and important for classification task, accurate regression requires translation and rotation-sensitive features. In this sense, classification and regression are somewhat incompatible to each other about the demands of translation invariance <ref type="bibr" target="#b4">[6]</ref> and rotation invariance. R-FCN <ref type="bibr" target="#b4">[6]</ref> introduces sensitive ROI-pooling to alleviate the translation invariance problem. In <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b25">27]</ref>, the authors propose to fuse the high-resolution and lowresolution feature maps to balance the conflict. To the best of our knowledge, the rotation invariance incompatibility has never been explicitly considered for object detection. This is particularly important for regressing oriented bounding boxes, which are more appropriate for detecting arbitrary oriented long and thin objects. This paper focuses on a typical example of such object detection, i.e., scene text detection, by explicitly introducing rotation-sensitive features to the CNN pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scene Text Detection</head><p>Different from scene text proposal methods <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b0">2]</ref> which mainly concern the recall, scene text detection methods output much less output bounding boxes, considering the tradeoff between recall and precision. Recently, numerous inspiring ideas and promising methods <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b40">42]</ref> have been proposed. A great improvement has been achieved compared to traditional methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b14">16]</ref>. Based on the representation of detection output, scene text detectors could be roughly divided into two categories: 1) Horizontal text detectors <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b24">26]</ref> which detect words or text lines in terms of horizontal bounding boxes in the same way as general object detectors. This implies that horizontal scene text detection may benefit from the developments of general object detection such as <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b26">28]</ref>. The works in <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref> are such examples; 2) Multi-oriented text detectors <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b44">46]</ref> which focus on detecting text of arbitrary orientations. The detection out- The outputs of rotation-sensitive backbone are rotation-sensitive feature maps, followed by two branches: one for regression and another for classification based on oriented response pooling. Note that the inception block is optional. puts are usually represented by either oriented rectangles or more generally quadrilaterals enclosing arbitrary oriented words or text lines. Compared to horizontal bounding box representation, additional variables such as the angle or vertex coordinates are required for representing multi-oriented text bounding boxes.</p><p>All the modern scene text detectors inspired by recent CNN-based general object detectors use shared features for both classification and regression. Compared to these modern multi-oriented scene text detectors, this paper proposes to explicitly use rotation-sensitive CNN features for oriented bounding box regression while adopting rotationinvariant features for classification. This results in a more accurate oriented bounding box regression for arbitrary oriented text, especially for the long words or text lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Rotation-sensitive CNN Features</head><p>Rotation-invariant features are important for a robust classification. Modern CNN architectures usually involve pooling layers achieving rotation invariance to a certain extent. Some recent works <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b50">52]</ref> focus on enhancing the rotation invariance of the CNN features to further improve the classification performance. For example, ORN <ref type="bibr" target="#b50">[52]</ref> proposes to actively rotate during convolution, producing rotation-sensitive feature maps. This is followed by an oriented pooling operation, giving rise to enhanced rotation invariance, and thus resulting in better classification performance. The proposed RRD is inspired by ORN <ref type="bibr" target="#b50">[52]</ref>. The enhanced rotation-invariant features are used for text presence prediction. We propose to adopt rotation-sensitive features to regress oriented bounding boxes, yielding accurate detection of arbitrary oriented long objects (e.g., scene text). To the best of our knowledge, this is the first time that explicit rotation-sensitive features are used for arbitrary oriented object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rotation-Sensitive Regression Detector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>RRD is an end-to-end trainable, fully convolutional neural network whose architecture is inspired by SSD <ref type="bibr" target="#b26">[28]</ref>. Its architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, which uses VGG16 <ref type="bibr" target="#b38">[40]</ref> as its backbone network, with extra layers added in the same manner as SSD. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, six layers of the backbone network are taken for dense prediction.</p><p>The dense prediction is similar to that of SSD <ref type="bibr" target="#b26">[28]</ref>. For every default box <ref type="bibr" target="#b26">[28]</ref>, RRD classifies its label (text or nontext) and regresses relative offsets. After that, RRD applies the offsets to the default boxes classified as positive, producing a number of quadrilaterals, each with a score. The quadrilaterals are filtered by non-maximum suppression, which outputs final detections.</p><p>The key novelty of our method is the dense prediction part, where RRD extracts two types of feature maps of different characteristics for classification and regression respectively. The feature map for classification is insensitive to text orientation, while the feature map for regression is sensitive. As mentioned before, these characteristics well fit the nature of the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rotation-Sensitive Regression</head><p>Text coordinates are sensitive to text orientation. Therefore, the regression of coordinate offsets should be performed on rotation-sensitive features. Oriented response convolution encodes the rotation information by actively rotating its convolutional filters, producing rotation-sensitive features for regression. Different from standard CNN features, RRD extracts rotation-sensitive features with active rotating filters (ARF) <ref type="bibr" target="#b50">[52]</ref>. An ARF convolves a feature map with a canonical filter and its rotated clones. The filters are rotated following the method in <ref type="bibr" target="#b50">[52]</ref>. Denote the canonical filter of ARF as F 0 ∈ k×k×N , where k is the kernel size, N is the number of rotations. ARF makes N − 1 clones of the canonical filter by rotating it to different angles, respectively F j , j = 1 : N . Let M i (j) and M o (j) denote the input feature map and the output feature map of j-th orientation respectively. ARF convolves a feature map by computing:</p><formula xml:id="formula_0">M o (j) = N −1 n=0 F j (n) * M i (n), j = 0, ..., N − 1,<label>(1)</label></formula><p>where F j (n) indicates the n-th orientation channel of F j . After convolution, it produces a response map of N channels, each corresponding to the response of the canonical filter or its rotated clone. N is set to 8 in practice. ARF produces extra channels to incorporate richer rotation information. With the help of ARF, ORN produces feature maps with orientation channels, capturing rotationsensitive features and improving its generality for rotated samples which has never seen before. Besides, since the parameters between the N filters are shared, learning ARF requires much less training examples.</p><p>In addition, in order to make the receptive field suitable for long text lines, we adopt inception blocks for both branches. The inception block concatenates the output feature maps produced by three filters of different sizes. The filter sizes are respectively m × m, m × n and n × m, where m is set to 3 and n is set to <ref type="bibr" target="#b7">(9,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b3">5)</ref> in the first, the second, and the last stages respectively. Inception blocks result in receptive fields of different aspect ratios. They are particularly helpful for detecting long text. Therefore, they are used in line-based text detection, but discarded in wordbased text detection, where long text is rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rotation-Invariant Classification</head><p>In contrast to regression, the classification of text presence should be rotation-invariant, i.e., text regions of arbitrary orientations should be classified as positive. Therefore, a rotation-invariant feature map should be extracted for this task.</p><p>ORN achieves rotation invariance by pooling responses of all N response maps. As shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>, the rotationsensitive feature maps are pooled along their depth axis. Assuming that M or is a rotation-sensitive input feature map of N orientation channels, the rotation-invariant feature map M pooling is an element-wise max of M or with the index of orientations, which can be calculated as follows:</p><formula xml:id="formula_1">M pooling = N −1 max k=0 M or (k),<label>(2)</label></formula><p>Since the pooling operation is orderless and applied to all N response maps, the resulting feature map is locally invariant to object rotation. Therefore, we use this feature map for classification. Besides, the setting of inception block is the same as the regression branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Default Boxes and Prediction</head><p>The default boxes are horizontal rectangles with different sizes and aspect ratios. Let B 0 = (x 0 , y 0 , w 0 , h 0 ) denote a horizontal default box, which can also be represented by its four vertexes</p><formula xml:id="formula_2">Q 0 = (v 0 1 , v 0 2 , v 0 3 , v 0 4 ), where v 0 i = (x 0 i , y 0 i ), i ∈ {1, 2, 3, 4}</formula><p>. The regression branch predicts offsets from a default box to a quadrilateral. A quadrilateral is described as Q =</p><formula xml:id="formula_3">(v 1 , v 2 , v 3 , v 4 ), where v i = (x i , y i ), i ∈ {1, 2, 3, 4}</formula><p>are four vertexes of a quadrilateral. For each default box, the prediction layer outputs the classification scores and offsets (∆x 1 , ∆y 1 , ∆x 2 , ∆y 2 , ∆x 3 , ∆y 3 , ∆x 4 , c) between the default box Q 0 and the bounding box result Q. The final output quadrilateral is encoded with the corresponding default box:</p><formula xml:id="formula_4">x i = x 0 i + w 0 ∆x i , i = 1, 2, 3, 4, y i = y 0 i + h 0 ∆y i , i = 1, 2, 3, 4,<label>(3)</label></formula><p>where w 0 and h 0 denote the width and height of the default box respectively. In addition, a non-maximum suppression with quadrilaterals is applied in the prediction period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>Ground Truth. The ground truth of an oriented text region can be described as a quadrilateral</p><formula xml:id="formula_5">G q = (v 1 , v 2 , v 3 , v 4 ), where v i = (x i , y i ), i ∈ {1, 2, 3, 4}</formula><p>are the vertices of the quadrilateral. We argue that careful selection of the first point in quadrilateral is helpful for regression. Thus, we follow a scheme in <ref type="bibr" target="#b23">[25]</ref>, which determine the first point based on the distances from its corresponding maximum horizontal bounding box.</p><p>Loss Function. In the training phase, default boxes are matched to the ground-truth boxes according to box overlap following the match scheme in <ref type="bibr" target="#b26">[28]</ref>. For efficiency, the minimum horizontal rectangle enclosing the quadrilateral is used in the matching period. We adopt a similar loss function to the one used in <ref type="bibr" target="#b26">[28]</ref>. More specifically, let x be the match indication matrix. For the i-th default box and the j-th ground truth, x ij = 1 means a match following the box overlap between them, otherwise x ij = 0. Let c be the confidence, l be the predicted location, and g be the ground-truth location. The loss function is defined as:</p><p>L(x, c, l, g) = 1 N (L cls (x, c) + αL reg (x, l, g)), <ref type="bibr" target="#b2">(4)</ref> where N indicates the number of default boxes that match ground-truth boxes, and α is set to 0.2 for quick convergence. We adopt a smooth L1 loss <ref type="bibr" target="#b6">[8]</ref> for L reg and a 2-class softmax loss for L cls . We follow the same online hard negative mining strategy as <ref type="bibr" target="#b26">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct extensive experiments to verify the effectiveness of RRD in multiple aspects. Altogether, seven datasets are used in the experiments. We first evaluate RRD on RCTW-17 <ref type="bibr" target="#b37">[39]</ref> and MSRA-TD500 <ref type="bibr" target="#b46">[48]</ref> to show its effectiveness at detecting long and oriented text. Both datasets contain many instances of such, and they are annotated in text lines. To further assess the importance of using rotation-sensitive features for regressing long and oriented boxes, we construct a subset of RCTW-17 by picking text instances with extreme aspect ratios. COCO-Text dataset <ref type="bibr" target="#b43">[45]</ref> is evaluated to prove the accurate of regression, which provides a evaluation protocol where the IOU threshold is set to 0.75. Then we test RRD on ICDAR 2015 incidental text dataset <ref type="bibr" target="#b18">[20]</ref>, which contains oriented English words. The ICDAR 2013 focused text dataset <ref type="bibr" target="#b19">[21]</ref> is also evaluated, showing its good performance on horizontal text detection. In the end, in order to show the generality of RRC, we evaluate RRD on HRSC2016 <ref type="bibr" target="#b29">[31]</ref>, a dataset of high resolution ship collection.</p><p>Reading Chinese Text in the Wild (RCTW-17) contains 12,000 images taken from streets, screen shots and indoor scenes etc. The sizes of images range from small to extremely large; Since Chinese words are not separated by blank spaces, long text lines are common.</p><p>RCTW-Long is a sub-dataset drawn from RCTW-17 featuring long text. The training set of RCTW-Long consists of 1323 images picked from the original training set, and the test set 537 from the original test set. Specifically, a bounding box is defined as a long box if its aspect ratio is greater than t or less than 1/t, otherwise a short box. t is set to 5 for the training set and 7 for the test set. We only select the images with more long boxes than short boxes for constructing this dataset.</p><p>MSRA-TD500 contains 500 natural images taken by pocket cameras from indoor and outdoor scenes. The dataset is divided into 300 training images and 200 test images. This dataset contains both English and Chinese text. Compared to RCTW-17, this dataset contains less text instances per image, but larger variance in text orientations.</p><p>ICDAR 2015 Incidental Text (IC15) comes from the Challenge 4 of ICDAR 2015 Robust Reading Competition. Images of this dataset were captured by Google Glasses in streets, shopping malls, etc., in an incidental manner. Consequently, many images of this dataset are of low resolution, and text is in various orientations. IC15 contains 1,000 training images and 500 test images. Annotations are provided in terms of word bounding boxes.</p><p>COCO-Text is a large dataset which contains 63686 images, where 43,686 of the images is used for training, 10,000 for validation, and 10,000 for testing. It is one of the challenges of ICDAR 2017 Robust Reading Competition.</p><p>ICDAR 2013 Focused Text (IC13) is composed of 229 training images and 233 testing images. This dataset contains only horizontal and focused text. Images in IC13 are in high resolutions.</p><p>High Resolution Ship Collection 2016 (HRSC2016) contains 1061 images divided into 436, 181, 444 images for training, validation, test set, respectively. The images are from two scenarios including ships on sea and ships close inshore derived from Google Earth. Ships are abundantly labeled with rotated polygons along with some extra information such as ship types and ship head locations etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>RRD is optimized by the ADAM <ref type="bibr" target="#b20">[22]</ref> algorithms on all datasets. For all scene text datasets, RRD is pre-trained on SynthText <ref type="bibr" target="#b9">[11]</ref> for 30k iterations and fine-tuned on real data. In the first 5-10k iterations, images are resized to 384 × 384 after random cropping. The learning rate is fixed to 10 −4 . Another 5-10k iterations is followed, where images are resized to 768 × 768 and the learning rate decayed to 10 −5 . The aspect ratios of default boxes are set to 1, 2, 3, 5, 1/2, 1/3, 1/5 for word-based dataset (IC15 and IC13), and 1, 2, 3, 5, 7, 9, 15, 1/2, 1/3, 1/5, 1/7, 1/9, 1/15 for text line-based dataset (RCTW-17, RCTW-Long, and MSRA-TD500).</p><p>For HRSC2016 experiments, we scale input images to 384 × 384 due to lots of thin and long ship instances. We train the model for around 15k iterations on three NVIDIA TITAN Xp GPUs from scratch at the learning rate of 10 −4 with the batch size set to 32. Then, we continue training for about 1k iterations with learning rate decayed to 10 −5 while keeping other settings unchanged. The default boxes settings remain the same as the line-based scene text datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We apply several variants of RRD to verify the effectiveness of rotation-sensitive regression. The tested variants of RRD are summarized as follows: Baseline: architecture without inception block, using shared conventional feature maps for both regression and classification; Baseline+inc: baseline architecture using inception blocks; Baseline+inc+rs: architecture with inception block, using rotation-sensitive features for both regression and classification; Baseline+inc+rs+rotInvar: the proposed RRD. Note that for word-based datasets, inception block is not applied and we also name it RRD.  Oriented long text needs more accurate bounding boxes. Given the examples in <ref type="figure">Fig. 5</ref> , the short text can be easily covered by a bounding box even though its orientation is not  accurate, while the long text is much more sensitive to the orientation. Thus, we conduct ablation study on RCTW-Long dataset, a typical dataset which mainly consists of long and oriented text, to verify the superiority of rotationsensitive regression. The quantitative comparison of several variants of RRD described above is shown in Tab. 1. Following standard evaluation protocol on RCTW dataset, we traverse the threshold of detection scores from 0.1 to 1 with a step of 0.02 to get the best F-measure for all models. The detailed comparison is given in the following.</p><p>Inception block. Compared with the baseline model, the inception block achieves an improvement of about 6 percents. This implies that the inception block can effectively alleviate the limited receptive field problem for detecting long and thin text.</p><p>Rotation-sensitive regression and classification. We also test the architecture using rotation-sensitive features for both regression and classification. Such model "Base-line+inc+rs" further improves the "Baseline+inc" architecture by about 6 percents. Therefore, it is evident that the rotation-sensitive features are useful for long, thin and oriented text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation-sensitive regression and Rotation-invariant</head><p>classification. The last model in Tab. 1 is the proposed RRD which uses rotation-sensitive and rotation-invariant features for regression and classification respectively. It alleviates the dilemma between regression and classification, outperforming all other models in Tab 1 by a large margin. Some qualitative comparisons between the proposed RRD and its variant "Baseline+inc" are illustrated in <ref type="figure">Fig 4.</ref> As shown, the proposed RRD yields more convincing classification scores and more accurate bounding boxes. Specifically, the bounding boxes having scores lower than 0.5 are discarded in practice. Consequently, some text bounding boxes generated by "Baseline+inc" are discarded. Whereas, all text bounding boxes given by RRD are reserved. This qualitative comparison also shows the effectiveness of RRD thanks to the specialized features for regression and classification, alleviating the incompatibility of the two tasks.   <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Scene Text Benchmarks</head><p>MSRA-TD500. MSRA-TD500 is also a line-based dataset which contains Chinese and English. Thus, the inception block is applied for this dataset. As shown in Tab. 2, RRD outperforms the state-of-the-art methods in terms of precision, recall, and F-measure. To further verify the effectiveness of rotation-sensitive regression, we also evaluate the "Baseline+inc" architecture, which degrades the performance of RRD by 5 percents in terms of F-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-Text.</head><p>The experiments on COCO-Text Challenge prove the accurate regression of RRD. As shown in Tab. 3, COCO-Text Challenge provides a more strict evaluation protocol in which the IOU threshold is set to 0.75, where RRD has a huge superiority because the detection results of RRD is much more accurate, benefiting from the rotationsensitive regression. Specifically, RRD achieves comparable results with the previous state-of-the-art method when the IOU threshold is set to 0.5 while outperforms all the previous methods by at least 6.1 percents when the IOU threshold is set to 0.75.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Recall Precision F-measure FCRNall+filts <ref type="bibr" target="#b9">[11]</ref> 0.76 0.92 0.83 TextBoxes <ref type="bibr" target="#b24">[26]</ref> 0.74 0.88 0.81 TextBoxes+MS <ref type="bibr" target="#b24">[26]</ref> 0.83 0.89 0.86 Seglink <ref type="bibr" target="#b36">[38]</ref> 0.83 0.88 0.85 Tian et al. <ref type="bibr" target="#b42">[44]</ref> 0.83 0.93 0.88 Tang et al. <ref type="bibr" target="#b39">[41]</ref> 0.87 0.92 0.90 He et al. <ref type="bibr" target="#b10">[12]</ref> 0.86 0.89 0.88 He et al. <ref type="bibr" target="#b11">[13]</ref> 0.81 0.92 0.86 WordSup+MS <ref type="bibr" target="#b12">[14]</ref> 0   <ref type="bibr" target="#b24">[26]</ref>, RRD also performs better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on the Ship Collection Benchmark</head><p>There are three level tasks in HRSC2016 dataset. Level 1 is to detect ship from backgrounds. Level 2 is to further give ship categories (war craft, aircraft carrier etc.) and Level 3 steps forward to get ship types (car carrier, submarine etc.). Basically, Level 1 task is enough to show our methods' superiority on multi-directional objects detection, so we compare our models with other carefully designed models on Level 1 task.</p><p>Some visualization results are displayed in <ref type="figure" target="#fig_4">Fig. 6</ref>. As we can see, RRD accurately detects ships with arbitrary orientations. Quantitative results are shown in Tab. 5. Instead of fine-tuning the model based on a pre-trained model, RRD is trained from scratch. Even so, RRD easily surpasses others with a promotion around 8.6 points in mAP reaching peak at 84.3 <ref type="bibr" target="#b28">[30]</ref>. Thus, it is obvious that RRD is not only suitable for scene text detection, but also skilled in other oriented object detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>We observe that RRD fails to detect certain types of text. As shown in <ref type="figure" target="#fig_5">Fig. 7a</ref>, RRD fails to detect a whole bounding box for a text line with large character spacing. Another failure case is shown in <ref type="figure" target="#fig_5">Fig. 7b</ref>, where RRD incorrectly detects two vertical text lines as multiple horizontal ones.</p><p>We believe that detecting such text is beyond the current capability of RRD, also many other state-of-the-art methods. Higher-level semantic understanding and spatial analysis may be required to address this issue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed RRD, a novel text detector that perform classification and regression using rotation-insensitive and sensitive features respectively. This strategy is conceptually simple, yet its effectiveness and generality is well demonstrated on multiple datasets and tasks. RRD improves the dense prediction of text presence and offsets, which can be found in many modern text detectors. Potentially, these detectors will also benefit from the strategy we have adopted in RRD. In the future, we are interested in stronger rotation-sensitive features and rotation-invariant features to further improve oriented object detection. Also, since the principle of RRD goes beyond text detection and ship detection, we are interested in further exploiting its potentials in detecting other oriented objects such as those frequently appeared in aerial images <ref type="bibr" target="#b45">[47]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of feature maps and results of baseline and RRD. Red numbers are the classification scores. (b): the shared feature map for both regression and classification; (c): the result of shared feature; (d) and (e): the regression feature map and classification feature map of RRD; (f): the result of RRD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of RRD. (a) The rotation-sensitive backbone follows the main architecture of SSD while changing its convolution into oriented response convolution. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Some results of RRD on RCTW-17 (first row) and IC15 (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : 22 Figure 5 :</head><label>4225</label><figDesc>Some results on images in RCTW-Long dataset. Red boxes: ground truths; Yellow boxes: detections. The corresponding angle is the same while the IOU diverges hugely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of some results on HRSC2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of limited occasions. Green bounding boxes: outputs of RRD; Red dashed bounding boxes: missing ground truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results of several variants of RRD on RCTW-Long dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>RCTW-17. RCTW-17 is a large line-based dataset which mainly consists of Chinese text. Thus, the inception block is important. The quantitative results using the official evaluation scheme is depicted in Tab. 2. RRD outperforms the</figDesc><table><row><cell></cell><cell cols="2">RCTW-17</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ICDAR2015</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Recall</cell><cell cols="2">Precision F-measure</cell><cell>FPS</cell><cell>Methods</cell><cell>Recall</cell><cell cols="2">Precision F-measure</cell><cell>FPS</cell></row><row><cell>Official baseline [39]</cell><cell>0.404</cell><cell>0.76</cell><cell>0.528</cell><cell>8.9</cell><cell>Zhang et al. [50]</cell><cell>0.43</cell><cell>0.71</cell><cell>0.54</cell><cell>-</cell></row><row><cell>EAST-ResNet*</cell><cell>0.478</cell><cell>0.597</cell><cell>0.531</cell><cell>7.4</cell><cell>Tian et al. [44]</cell><cell>0.52</cell><cell>0.74</cell><cell>0.61</cell><cell>7.1</cell></row><row><cell>Baseline+inc</cell><cell>0.459</cell><cell>0.659</cell><cell>0.541</cell><cell>10.6</cell><cell>Shi et al. [38]</cell><cell>0.768</cell><cell>0.731</cell><cell>0.750</cell><cell>8.9</cell></row><row><cell>RRD</cell><cell>0.453</cell><cell>0.724</cell><cell>0.557</cell><cell>10</cell><cell>Liu et al. [29]</cell><cell>0.682</cell><cell>0.732</cell><cell>0.706</cell><cell>-</cell></row><row><cell>Baseline+inc+MS</cell><cell>0.595</cell><cell>0.744</cell><cell>0.661</cell><cell>-</cell><cell>Zhou et al. [51]</cell><cell>0.735</cell><cell>0.836</cell><cell>0.782</cell><cell>13.2</cell></row><row><cell>RRD+MS</cell><cell>0.591</cell><cell>0.775</cell><cell>0.670</cell><cell>-</cell><cell>He et al. [12]</cell><cell>0.73</cell><cell>0.80</cell><cell>0.77</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MSRA-TD500</cell><cell></cell><cell></cell><cell>Hu et al. [14]</cell><cell>0.77</cell><cell>0.793</cell><cell>0.782</cell><cell>-</cell></row><row><cell>Methods</cell><cell>Recall</cell><cell cols="2">Precision F-measure</cell><cell>FPS</cell><cell>Baseline</cell><cell>0.762</cell><cell>0.871</cell><cell>0.813</cell><cell>8.5</cell></row><row><cell>Zhang et al. [50]</cell><cell>0.67</cell><cell>0.83</cell><cell>0.74</cell><cell>0.48</cell><cell>RRD</cell><cell>0.79</cell><cell>0.856</cell><cell>0.822</cell><cell>6.5</cell></row><row><cell>He et al. [13]</cell><cell>0.7</cell><cell>0.77</cell><cell>0.74</cell><cell>1.1</cell><cell>Zhou et al. MS [51]</cell><cell>0.783</cell><cell>0.833</cell><cell>0.807</cell><cell>-</cell></row><row><cell>Shi et al. [38]</cell><cell>0.7</cell><cell>0.86</cell><cell>0.77</cell><cell>8.9</cell><cell>Hu et al. MS [14]</cell><cell>0.77</cell><cell>0.793</cell><cell>0.782</cell><cell>-</cell></row><row><cell>Zhou et al. [51]</cell><cell>0.67</cell><cell>0.87</cell><cell>0.76</cell><cell>13.2</cell><cell>He et al. MS [13]</cell><cell>0.80</cell><cell>0.82</cell><cell>0.81</cell><cell>-</cell></row><row><cell>Baseline+inc</cell><cell>0.69</cell><cell>0.79</cell><cell>0.74</cell><cell>10.6</cell><cell>Baseline+MS</cell><cell>0.785</cell><cell>0.878</cell><cell>0.828</cell><cell>-</cell></row><row><cell>RRD</cell><cell>0.73</cell><cell>0.87</cell><cell>0.79</cell><cell>10</cell><cell>RRD+MS</cell><cell>0.8</cell><cell>0.88</cell><cell>0.838</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Text detection results on multi-oriented scene text benchmarks: MSRA-TD500, RCTW-17, and IC15.</figDesc><table><row><cell>official baseline based on SegLink [38] by about 3.3 per-</cell></row><row><cell>cents in terms of F-measure. Even though EAST-ResNet 1</cell></row><row><cell>uses a stronger backbone (ResNet), RRD still performs bet-</cell></row><row><cell>ter, achieving 2.6 percents F-measure improvement. More-</cell></row><row><cell>over, using multi-scale inputs including 384 × 384, 384 ×</cell></row><row><cell>768, 768 × 384, 768 × 768, 1024 × 1024, 1536 × 1536,</cell></row><row><cell>"RRD+MS" further achieves 11.3 percents improvement</cell></row><row><cell>than RRD using single-scale input. Some qualitative results</cell></row><row><cell>of RRD on this dataset are given in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on COCO-Text Challenge. For the comparison of RRD with the state-of-theart results on IC15 dataset, we use the scale of 1024 × 1024 for single scale testing in RRD. The multi-scale testing includes the scales of 384 × 384, 768 × 768, 1024 × 1024, and 1536 × 1536. The comparison with the state-of-theart results on IC15 dataset is given in Tab. 2. Even though IC15 is a word-based dataset whose text bounding boxes are not extremely long, RRD still achieves about 1 percent performance gain than the baseline with both single scale and multi-scale settings. Furthermore, RRD outperforms the state-of-the-art results by 4 percents with single scale setting and 2.8 percents in the case of multi-scale setting.</figDesc><table /><note>1 https://github.com/argman/EAST IC15.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on IC13. MS stands for multiscale testing.IC13.Although the proposed RRD is specifically designed for multi-oriented text detection, we also evaluate RRD on IC13 dataset consisting of horizontal text. The experimental results are depicted in Tab. 4, showing that the proposed scheme has no performance loss in horizontal text detection. RRD achieves comparable performance with the state-of-the-art results on IC13. Moreover, compared with the most related work</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on HSRC2016. SRBBS(Ship Rotated Bounding Boxes Space) means labeling of ships is rotated polygon; RBB(Rotated Bounding Boxes) extends Fast-RCNN to a method capable of regressing rotated bounding boxes; RRoI(Rotated Region of Interest) pooling layer is implemented based on original RoI pooling, which is specially designed for rotated bounding boxes. For detailed description, refer to<ref type="bibr" target="#b28">[30]</ref> </figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast: Facilitated and accurate scene text proposals through fcn guided pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bazazian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep textspotter: An end-to-end trainable scene text localization and recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rotationinvariant convolutional neural networks for galaxy morphology prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<idno>abs/1503.07077</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2012" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Textproposals: A textspecific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. IC-DAR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1801.02765</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rotated region based CNN for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPRAM</title>
		<meeting>ICPRAM</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An automated vehicle license plate recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="56" to="61" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing text-based traffic guide panels with cascaded localization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV workshop</title>
		<meeting>ECCV workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICDAR2017 competition on reading chinese text in the wild (RCTW-17</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene text detection and segmentation based on cascaded convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1509" to="1520" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wetext: Scene text detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>abs/1601.07140</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal post-processing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
