<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LeViT: a Vision Transformer in ConvNet&apos;s Clothing for Faster Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
						</author>
						<title level="a" type="main">LeViT: a Vision Transformer in ConvNet&apos;s Clothing for Faster Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers.</p><p>As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https: //github.com/facebookresearch/LeViT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer neural networks were initially introduced for Natural Language Processing applications <ref type="bibr" target="#b0">[1]</ref>. They now dominate in most applications of this field. They manipulate variable-size sequences of token embeddings that are fed to a residual architecture. The model comprises two sorts for residual blocks: Multi-Layer Perceptron (MLP) and an original type of layer: the self-attention, which allows all pairs of tokens in the input to be combined via a bilinear function. This is in contrast to 1D convolutional approaches that are limited to a fixed-size neighborhood. Recently, the vision transformer (ViT) architecture <ref type="bibr" target="#b1">[2]</ref> obtained state-of-the-art results for image classification in the speed-accuracy tradeoff with pre-training on large scale dataset. The Data-efficient Image Transformer <ref type="bibr" target="#b2">[3]</ref> obtains competitive performance when training the ViT models only on ImageNet <ref type="bibr" target="#b3">[4]</ref>. It also introduces smaller models adapted for high-throughput inference.</p><p>In this paper, we explore the design space to offer even better trade-offs than ViT/DeiT models in the regime of small and medium-sized architectures. We are especially interested in optimizing the performance-accuracy tradeoff, such as the throughput (images/second) performance depicted in <ref type="figure" target="#fig_0">Figure 1</ref> for Imagenet-1k-val <ref type="bibr" target="#b4">[5]</ref>. While many works <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref> aim at reducing the memory footprint of classifiers and feature extractors, inference speed is equally important, with high throughput corresponding to better energy efficiency. In this work, our goal is to develop a Vision Transformer-based family of models with better inference speed on both highlyparallel architectures like GPU, regular Intel CPUs, and ARM hardware commonly found in mobile devices. Our solution re-introduces convolutional components in place of transformer components that learn convolutional-like features. In particular, we replace the uniform structure of a Transformer by a pyramid with pooling, similar to the LeNet <ref type="bibr" target="#b11">[11]</ref> architecture. Hence we call it LeViT.</p><p>There are compelling reasons why transformers are faster than convolutional architectures for a given computational complexity. Most hardware accelerators (GPUs, TPUs) are optimized to perform large matrix multiplications. In transformers, attention and MLP blocks rely mainly on these operations. Convolutions, in contrast, require complex data access patterns, so their operation is often IO-bound. These considerations are important for our exploration of the speed/accuracy tradeoff.</p><p>The contributions of this paper are techniques that allow ViT models to be shrunk down, both in terms of the width and spatial resolution:</p><p>• A multi-stage transformer architecture using attention as a downsampling mechanism; • A computationally efficient patch descriptor that shrinks the number of features in the first layers; • A learnt, per-head translation-invariant attention bias that replaces ViT's positional embeddding; • A redesigned Attention-MLP block that improves the network capacity for a given compute time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The convolutional networks descended from LeNet <ref type="bibr" target="#b11">[11]</ref> have evolved substantially over time <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. The most recent families of architectures focus on finding a good trade-off between efficiency and performance <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>. For instance, the EfficientNet <ref type="bibr" target="#b17">[17]</ref> family was discovered by carefully designing individual components followed by hyper-parameters search under a FLOPs constraint.</p><p>Transformers. The transformer architecture was first introduced by Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> for machine translation. Transformer encoders primarily rely on the selfattention operation in conjunction with feed-forward layers, providing a strong and explicit method for learning long range dependencies. Transformers have been subsequently adopted for NLP tasks providing state-of-the-art performance on various benchmarks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. There have been many attempts at adapting the transformer architecture to images <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, first by applying them on pixels. Due to the quadratic computational complexity and number of parameters involved by attention mechanisms, most authors <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref> initially considered images of small sizes like in CIFAR or Imagenet64 <ref type="bibr" target="#b25">[25]</ref>. Mixed text and image embeddings already use transformers with detection bounding boxes as input <ref type="bibr" target="#b26">[26]</ref>, i.e. the bulk of the image processing is done in the convolutional domain.</p><p>The vision transformer (ViT) <ref type="bibr" target="#b1">[2]</ref>. Interestingly, this transformer architecture is very close to the initial NLP version, devoid of explicit convolutions (just fixed-size image patch linearized into a vector), yet it competes with the state of the art for image classification. ViT achieves strong performance when pre-trained on a large labelled dataset such as the JFT300M (non-public, although training on Imagenet-21k also produces competitive results). The need for this pre-training, in addition to strong data augmentation, can be attributed to the fact that transformers have less built-in structure than convolutions, in particular they do not have an inductive bias to focus on nearby image elements. The authors hypothesized that a large and varied dataset is needed to regularize the training.</p><p>In DeiT <ref type="bibr" target="#b2">[3]</ref>, the need for the large pre-training dataset is replaced with a student-teacher setup and stronger data augmentation and regularization, such as stochastic depth <ref type="bibr" target="#b27">[27]</ref> or repeated augmentation <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29]</ref>. The teacher is a convolutional neural network that "helps" its student network to acquire an inductive bias for convolutions. The vision transformer has been thereafter successfully adopted by a wider range of computer vision tasks including object detection <ref type="bibr" target="#b30">[30]</ref>, semantic segmentation <ref type="bibr" target="#b31">[31]</ref> and image retrieval <ref type="bibr" target="#b32">[32]</ref>.</p><p>Positional encoding. Transformers take a set as input, and hence are invariant to the order of the input. However, in language as well as in images, the inputs come from a structure where the order is important. The origi-nal Transformer <ref type="bibr" target="#b0">[1]</ref> incorporates absolute non-parametric positional encoding with the input. Other works has replaced them with parametric encoding <ref type="bibr" target="#b33">[33]</ref> or adopt Fourier-based kernelized versions <ref type="bibr" target="#b22">[22]</ref>. Absolute position encoding enforce a fixed size for the set of inputs, but some works use relative position encoding <ref type="bibr" target="#b34">[34]</ref> that encode the relative position between tokens. In our work, we replace these explicit positional encoding by positional biases that implicitly encode the spatial information.</p><p>Attention along other mechanisms. Several works have included attention mechanisms in neural network architectures designed for vision <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref>. The mechanism is used channel-wise to capture cross-feature information that complements convolutional layers <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>, select paths in different branch of a network <ref type="bibr" target="#b42">[42]</ref>, or combine both <ref type="bibr" target="#b43">[43]</ref>. For instance, the squeeze-andexcite network of Hu et al. <ref type="bibr" target="#b44">[44]</ref> has an attention-like module to model the channel-wise relationships between the features of a layer. Li et al. <ref type="bibr" target="#b37">[37]</ref> use the attention mechanism between branches of the network to adapt the receptive field of neurons.</p><p>Recently, the emergence of transformers led to hybrid architectures that benefit from other modules. Bello <ref type="bibr" target="#b45">[45]</ref> proposes an approximated content attention with a positional attention component. Child et al. <ref type="bibr" target="#b23">[23]</ref> observe that many early layers in the network learn locally connected patterns, which resemble convolutions. This suggests that hybrid architectures inspired both by transformers and convnets are a compelling design choice. A few recent works explore this avenue for different tasks <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. In image classification, a recent work that comes out in parallel with ours is the Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b48">[48]</ref>, whose design is heavily inspired by ResNet. It is principally intended to address object and instance segmentation tasks.</p><p>Also concurrently with our work, Yuan et al. <ref type="bibr" target="#b49">[49]</ref> propose the Tokens-to-Tokens ViT (T2T-ViT) model. Similar to PVT, its design relies on re-tokenization of the output after each layer by aggregating the neighboring tokens such number of tokens are progressively reduced. Additionally, Yuan et al. <ref type="bibr" target="#b49">[49]</ref> investigate the integration of architecture design choices from CNNs <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51]</ref> that can improve the performance and efficiency of vision transformers. As we will see, these recent methods are not as much focused as our work on the trade-off between accu- racy and inference time. They are not competitive with respect to that compromise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>In this section we discuss the seemingly convolutional behavior of the transformer patch projection layer. We then carry out "grafting experiments" of a transformer (DeiT-S) on a standard convolutional architecture (ResNet-50). The conclusions drawn by this analysis will motivate our subsequent design choices in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutions in the ViT architecture</head><p>ViT's patch extractor is a 16x16 convolution with stride 16. Moreover, the output of the patch extractor is multiplied by learnt weights to form the first self-attention layer's q, k and v embeddings, so we may consider these to also be convolutional functions of the input. This is also the case for variants like DeiT <ref type="bibr" target="#b2">[3]</ref> and PVT <ref type="bibr" target="#b48">[48]</ref>. In <ref type="figure" target="#fig_1">Figure 2</ref> we visualize the first layer of DeiT's attention weights, broken down by attention head. This is a more direct representation than the principal components depicted by Dosovitskiy et al. <ref type="bibr" target="#b1">[2]</ref>. One can observe the typical patterns inherent to convolutional architectures: attention heads specialize in specific patterns (low-frequency colors / high frequency graylelvels), and the patterns are  similar to Gabor filters. In convolutions where the convolutional masks overlap significantly, the spatial smoothness of the masks comes from the overlap: nearby pixel receive approximately the same gradient. For ViT convolutions there is no overlap. The smoothness mask is likely caused by the data augmentation: when an image is presented twice, slightly translated, the same gradient goes through each filter, so it learns this spatial smoothness.</p><p>Therefore, in spite of the absence of "inductive bias" in transformer architectures, the training does produce filters that are similar to traditional convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary experiment: grafting</head><p>The authors of the ViT image classifier <ref type="bibr" target="#b1">[2]</ref> experimented with stacking the transformer layers above a traditional ResNet-50. In that case, the ResNet acts as a feature extractor for the transformer layers and the gradients can be propagated back through the two networks. However, in their experiments, the number of transformer layers was fixed (e.g. 12 layers for ViT-Base).</p><p>In this subsection, we investigate the potential of mixing transformers with convolutional network under a similar computational budget: We explore trade-offs obtained when varying the number of convolutional stages and transformer layers. Our objective is to evaluate variations of convolutional and transformer hybrids while controlling for the runtime.</p><p>Grafting. The grafting combines a ResNet-50 and a DeiT-Small. The two networks have similar runtimes.</p><p>We crop the upper stages of the ResNet-50 and likewise reduce the number of DeiT layers (while keeping the same number of transformer and MLP blocks). Since a cropped ResNet produces larger activation maps than the 14 × 14 activations consumed by DeiT, we introduce a pooling layer between them. In preliminary experiments we found average pooling to perform best. The positional embedding and classification token are introduced at the interface between the convolutional and transformer layer stack. For the ResNet-50 stages, we use ReLU activation units <ref type="bibr" target="#b52">[52]</ref> and batch normalization <ref type="bibr" target="#b53">[53]</ref>.</p><p>Results. <ref type="table">Table 1</ref> summarizes the results. The grafted architecture produces better results than both DeiT and ResNet-50 alone. The smallest number of parameters and best accuracy are with two stages of ResNet-50, because this excludes the convnet's large third stage. Note that in this experiment, the training process is similar to DeiT: 300 epochs, we measure the top-1 validation accuracy on ImageNet, and the speed as the number of images that one GPU can process per second. <ref type="figure" target="#fig_3">Figure 3</ref> is that the convergence of grafted models during training seems to be similar to a convnet during the early epochs and then switch to a convergence rate similar to DeiT-S. A hypothesis is that the convolutional layers have the ability to learn representations of the low-level information in the earlier layers more efficiently due to their strong inductive biases, noticeably their translation invariance. They rely rapidly on meaningful patch embeddings, which can explain the faster convergence during the first epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One interesting observation that we show</head><p>Discussion. It appears that in a runtime controlled regime it is beneficial to insert convolutional stages below a transformer. Most of the processing is still done in the transformer stack for the most accurate variants of the grafted architecture. Thus, the priority in the next sections will be to reduce the computational cost of the transformers. For this, instead of just grafting, the transformer architecture needs to be merged more closely with the convolutional stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section we describe the design process of the LeViT architecture and what tradeoffs were taken. The architecture is summarized in <ref type="figure" target="#fig_5">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design principles of LeViT</head><p>LeViT builds upon the ViT <ref type="bibr" target="#b1">[2]</ref> architecture and DeiT <ref type="bibr" target="#b2">[3]</ref> training method. We incorporate components that were proven useful for convolutional architectures. The first step is to get a compatible representation. Discounting the role of the classification embedding, ViT is a stack of layers that processes activation maps. Indeed, the intermediate "token" embeddings can be seen as the traditional C × H × W activation maps in FCN architectures (BCHW format). Therefore, operations that apply to activation maps (pooling, convolutions) can be applied to the intermediate representation of DeiT.</p><p>In this work we optimize the architecture for compute, not necessarily to minimize the number of parameters. One of the design decisions that makes the ResNet <ref type="bibr" target="#b14">[14]</ref> family more efficient than the VGG network <ref type="bibr" target="#b13">[13]</ref> is to apply strong resolution reductions with a relatively small computation budget in its first two stages. By the time the activation map reaches the big third stage of ResNet, its resolution has already shrunk enough that the convolutions are applied to small activation maps, which reduces the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LeViT components</head><p>Patch embedding. The preliminary analysis in Section 3 showed that the accuracy can be improved when a small convnet is applied on input to the transformer stack.  No classification token. To use the BCHW tensor format, we remove the classification token. Similar to convolutional networks, we replace it by average pooling on the last activation map, which produces an embedding used in the classifier. For distillation during training, we train separate heads for the classification and distillation tasks. At test time, we average the output from the two heads. In practice, LeViT can be implemented using either BN C or BCHW tensor format, whichever is more efficient.</p><p>Normalization layers and activations. The FC layers in the ViT architecture are equivalent to 1 × 1 convolutions. The ViT uses layer normalization before each attention and MLP unit. For LeViT, each convolution is followed by a batch normalization. Following <ref type="bibr" target="#b54">[54]</ref>, each batch normalization weight parameter that joins up with a residual connection is initialized to zero. The batch normalization can be merged with the preceding convolution for inference, which is a runtime advantage over layer normalization (for example, on EfficientNet B0, this fusion speeds up inference on GPU by a factor 2). Whereas DeiT uses the GELU function, all of LeViT's non-linear activations are Hardswish <ref type="bibr" target="#b19">[19]</ref>.</p><p>Multi-resolution pyramid. Convolutional architectures are built as pyramids, where the resolution of the activation maps decreases as their number of channels increases during processing. In Section 3 we used the ResNet-50 stages to pre-process the transformer stack.</p><p>LeViT integrates the ResNet stages within the transformer architecture. Inside the stages, the architecture is similar to a visual transformer: a residual structure with alternated MLP and activation blocks. In the following we review the modifications of the attention blocks ( <ref type="figure">Figure 5</ref>) compared to the classical setup <ref type="bibr" target="#b0">[1]</ref>.</p><p>Downsampling. Between the LeViT stages, a shrinking attention block reduces the size of the activation map: a subsampling is applied before the Q transformation, which then propagates to the output of the soft activation. This maps an input tensor of size (C, H, W ) to an output tensor of size (C , H/2, W/2) with C &gt; C. Due to the change in scale, this attention block is used without a residual connection. To prevent loss of information, we take the number of attention heads to be C/D.</p><p>Attention bias instead of a positional embedding. The positional embedding in transformer architectures is a location-dependent trainable parameter vector that is added to the token embeddings prior to inputting them to the transformer blocks. If it was not there, the transformer output would be independent to permutations of the input tokens. Ablations of the positional embedding result in a sharp drop of the classification accuracy <ref type="bibr" target="#b55">[55]</ref>.</p><p>However positional embeddings are included only on input to the sequence of attention blocks. Therefore, since the positional encoding is important for higher layers as well, it is likely that it remains in the intermediate representations and needlessly uses representation capacity.</p><p>Therefore, our goal is to provide positional information within each attention block, and to explicitly inject relative position information in the attention mechanism: we simply add an attention bias to the attention maps. The scalar attention value between two pixels (x, y) ∈ The first term is the classical attention. The second is the translation-invariant attention bias. Each head has H ×W parameters corresponding to different pixel offsets. Symmetrizing the differences x − x and y − y encourages the model to train with flip invariance.</p><p>Smaller keys. The bias term reduces the pressure on the keys to encode location information, so we reduce the size of the keys matrices relative to the V matrix. If the keys have size D ∈ {16, 32}, V will have 2D channels. Restricting the size of the keys reduces the time needed to calculate the key product QK . For downsampling layers, where there is no residual connection, we set the dimension of V to 4D to prevent loss of information.  <ref type="figure">Figure 5</ref>: The LeViT attention blocks, using similar notations to <ref type="bibr" target="#b39">[39]</ref>. Left: regular version, Right: with 1/2 reduction of the activation map. The input activation map is of size C × H × W . N is the number of heads, the multiplication operations are performed independently per head.</p><p>Attention activation. We apply a Hardswish activation to the product A h V before the regular linear projection is used to combine the output of the different heads. This is akin to a ResNet bottleneck residual block, in the sense that V is the output of a 1 × 1 convolution, A h V corresponds to a spatial convolution, and the projection is another 1 × 1 convolution.</p><p>Reducing the MLP blocks. The MLP residual block in ViT is a linear layer that increases the embedding dimension by a factor 4, applies a non-linearity and reduces it back with another non-linearity to the original embedding's dimension. For vision architectures, the MLP is usually more expensive in terms of runtime and parameters than the attention block. For LeViT, the "MLP" is a 1 × 1 convolution, followed by the usual batch normalization. To reduce the computational cost of that phase, we reduce the expansion factor of the convolution from 4 to 2. One design objective is that attention and MLP blocks consume approximately the same number of FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The LeViT family of models</head><p>The LeViT models can spawn a range of speed-accuracy tradeoffs by varying the size of the computation stages. We identify them by the number of channels input to the first transformer, e.g. LeViT-256 has 256 channels on input of the transformer stage. <ref type="table">Table 2</ref> shows how the stages are designed for the models that we evaluate in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental context</head><p>Datasets and evaluation. We model our experiments on the DeiT work, that is closest to our approach. It builds upon PyTorch <ref type="bibr" target="#b56">[56]</ref> and the Timm library <ref type="bibr" target="#b57">[57]</ref>. We train on the ImageNet-2012 dataset and evaluate on its validation set. We do not explore using more training data in this work.</p><p>Resource consumption. The generally accepted measure for inference speed is in units of multiply-add operations (aka FLOPs) because floating-point matrix multiplications and convolutions can be expressed as those. However, some operations, most notably non-linear activations, do not perform multiply-add operations. They are generally ignored in the FLOP counts (or counted as a single FLOP) because it is assumed that their cost is negligible w.r.t. the cost of higher-order matrix multiplications and convolutions. However, for a small number of channels, the runtime of complicated activations like GELU is comparable to that of convolutions. Moreover, operations with the same number of FLOPs can be more or less efficient depending on the hardware and API used.</p><p>Therefore, we additionally report raw timings on reference hardware, like recent papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b58">58]</ref>. The efficiency of transformers relies almost exclusively on matrix multiplications with a large reduction dimension.</p><p>Hardware. In this work, we run all experiments in Py-Torch, thus we are dependent on the available optimizations in that API. In an attempt to obtain more objective timings, we time the inference on three different hardware platforms, each corresponding to one use case:</p><p>• One 16GB NVIDIA Volta GPU (peak performance is 12 TFLOP/s). This is a typical training accelerator. • An Intel Xeon 6138 CPU at 2.0GHz. This is a typical server in a datacenter, that performs feature extraction on streams of incoming images. PyTorch is well optimized for this configuration, using MKL and AVX2 instructions (16 vector registers of 256 bits each). • An ARM Graviton2 CPU (Amazon C6g instance). It is a good model for the type of processors that mobile phones and other edge devices are running. The Gravi-ton2 has 32 cores supporting the NEON vector instruction set with 32 128-bit vector registers (NEON).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>LeViT-128S LeViT-128  LeViT-256 LeViT-384 (D = 16, p = 0) (D = 16, p = 0) (D = 32, p = 0) (D = 32, p = 0) (D = 32, p = 0.1) <ref type="table">Table 2</ref>: LeViT models. Each stage consists of a number of pairs of Attention and MLP blocks. N : number of heads, C: number of channels, D: output dimension of the Q and K operators. Separating the stages are shrinking attention blocks whose values of C, C are taken from the rows above and below respectively. Drop path with probability p is applied to each residual connection. The value of N in the stride-2 blocks is C/D to make up for the lack of a residual connection. Each attention block is followed by an MLP with expansion factor two.</p><formula xml:id="formula_0">Stage 1: 14 × 14 2× C=128 N =4 4× C=128 N =4 4× C=192 N =3 4× C=256 N =4 4× C=384 N =6 Subsample N =8 N =8 N =6 N =8 N =12 Stage 2: 7 × 7 3× C=256 N =6 4× C=256 N =8 4× C=288 N =5 4× C=384 N =6 4× C=512 N =9 Subsample N =16 N =16 N =9 N =12 N =18 Stage 3: 4 × 4 4× C=384 N =8 4× C=384 N =12 4× C=384 N =6 4× C=512 N =8 4× C=768 N =12</formula><p>On the GPU we run timings on large image batches because that corresponds to typical use cases; following DeiT we use the maximum power-of-two batchsize that fits in memory. On the CPU platforms, we measure inference time in a single thread, simulating a setting where several threads process separate streams of input images.</p><p>It is difficult to dissociate the impact of the hardware and software, so we experiment with several ways to optimize the network with standard PyTorch tools (the justin-time compiler, different optimization profiles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training LeViT</head><p>We use 32 GPUs that perform the 1000 training epochs in 3 to 5 days. This is more than the usual schedule for convolutional networks, but visual transformers require a long training, for example training DeiT for 1000 epochs improves by another 2 points of top-1 precision over 300 epochs. To regularize the training, we use distillation driven training, similar to DeiT. This means that LeViT is trained with two classification heads with a cross entropy loss. The first head receives supervision from the groundtruth classes, the second one from a RegNetY-16GF <ref type="bibr" target="#b18">[18]</ref> model trained on ImageNet. In fact, the LeViT training time is dominated by the teacher's inference time. <ref type="table" target="#tab_2">Table 3</ref> shows the speed-precision tradeoffs that we obtain with LeViT, and a few salient numbers are plotted in <ref type="figure" target="#fig_0">Figure 1</ref>. We compare these with two competitive architectures from the state of the art: EfficientNet <ref type="bibr" target="#b17">[17]</ref> as a strong convolutional baseline, and likewise DeiT <ref type="bibr" target="#b2">[3]</ref> a strong transformer-only architecture. Both baselines are trained under to maximize their accuracy. For example, we compare with DeiT trained during 1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Speed-accuracy tradeoffs</head><p>In the range of operating points we consider, the LeViT architecture largely outperforms both the transformer and convolutional variants. LeViT-384 is on-par with DeiT-Small in accuracy but uses half the number of FLOPs. The gap widens for faster operating points: LeViT-128S is on-par with DeiT-Tiny and uses 4× fewer FLOPs.</p><p>The runtime measurements follow closely these trends. For example      <ref type="table" target="#tab_3">Table 4</ref> reports results with other transformer based architectures for comparison with LeViT <ref type="table" target="#tab_2">(Table 3</ref>). Since our approach specializes in the high-throughput regime, we do not include very large and slow models <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b62">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the state of the art</head><p>We compare in the FLOPs-accuracy tradeoff, since the other works are very recent and do not necessarily provide reference models on which we can time the inference. All Token-to-token ViT <ref type="bibr" target="#b49">[49]</ref> variants take around 5× more FLOPs than LeViT-384 and more parameters for comparable accuracies than LeViT. Bottleneck transformers <ref type="bibr" target="#b46">[46]</ref> and "Visual Transformers" <ref type="bibr" target="#b47">[47]</ref> (not to be confused with ViT) are both generic architectures that can also be used for detection and object segmentation. Both are about 5× slower than LeViT-192 at a comparable accuracy. The same holds for the pyramid vision transformer <ref type="bibr" target="#b48">[48]</ref> (not reported in the table) but its design objectives are different. The advantage of LeViT compared to these architectures is that it benefited from the DeiT-like distillation, which makes it much more accurate when training on Im-ageNet alone. Two architecture that comes close to LeViT  are the pooling-based vision transformer (PiT) <ref type="bibr" target="#b59">[59]</ref> and CvT <ref type="bibr" target="#b60">[60]</ref>, ViT variants with a pyramid structure. PiT, the most promising one, incorporates many of the optimization ingredients for DeiT but is still 1.2× to 2.4× slower than LeViT.</p><p>Alternaltive evaluations. In <ref type="table" target="#tab_2">Table 3</ref> we evaluate LeViT on alternative test sets, Imagenet Real <ref type="bibr" target="#b63">[63]</ref> and Imagenet V2 matched frequency <ref type="bibr" target="#b64">[64]</ref>. The two datasets use the same set of classes and training set as ImageNet.</p><p>Imagenet-Real has re-assessed labels with potentially several classes per image. Imagenet-V2 (in our case match frequency) employs a different test set. It is interesting to measure the performance on both to verify that hyperparameters adjustments have not led to overfitting to the validation set of ImageNet. Thus, we measure the classification performance on the alternative test sets for models that have equivalent accuracies on ImageNet validation. LeViT-256 and EfficientNet B3: the LeViT variant achieves the same score on -Real, but is slightly worse (-0.6) on -V2. LeViT-384 and DeiT-Small: LeViT is slightly worse on -Real (-0.2) and -V2 (-0.4). Although in these evaluations LeViT is relatively slightly less accurate, the speed-accuracy trade-offs still hold, compared to EfficientNet and DeiT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablations</head><p>To evaluate what contributes to the performance of LeViT, we experiment with the default setting and replace one parameter at a time. We train the LeViT-128S model, and a number of variants, to evaluate the design changes relative to ViT/DeiT. The experiments are run with only 100 training epochs to magnify the differences and reduce training time. The conclusions remain for larger models and longer training schedules. We replace one component at a time, when the network needs to be reworked, we make sure the FLOP count remains roughly the same (see Appendix A.2 for details). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we report more details and results. Appendix A details the timings of constituent block and provides more details about our ablation. We provide visualizations of the attention bias in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed analysis A.1 Block timings</head><p>In this section we compare the differences in design between DeiT and LeViT blocks from the perspective of a detailed runtime analysis. We measure the runtime of their constituent parts side-by-side in the supplementary <ref type="table" target="#tab_7">Table 6</ref>. For DeiT-Tiny, we replace the GELU activation with Hardswish, as otherwise it dominates the runtime.</p><p>For DeiT, we consider a block from DeiT-tiny. For LeViT, we consider a block from the first stage of LeViT-256. Both operate at resolution 14×14 and have comparable run times, although LeViT is 33% wider (C = 256 vs C = 192). Note that stage 1 is the most expensive part of LeViT-256. In stages 2 and 3, the cost is lower due to the reduction in resolution (see <ref type="figure" target="#fig_5">Figure 4</ref> of the main paper).</p><p>LeViT spends less time calculating the attention QK T , but more time on the subsequent matrix product AV . Despite having the larger block width C, LeViT spends less time on the MLP component as the expansion factor is halved from four to two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More details on our ablation</head><p>Here we give additional details of the ablation experiments in Section 5.6 and <ref type="table" target="#tab_3">Table 4</ref> of the main paper.</p><p>A1 -without pyramid shape. We test the effect of the LeViT pyramid structure, we replace the three stages with a single stage of depth 11 at resolution 14 × 14. To preserve the FLOP count, we take D = 19, N = 3 and C = 2N D = 114.</p><p>A6 -without wider blocks. Compared to DeiT, LeViT blocks are relatively wide given the number of FLOPs, with smaller keys and MLP expansion factors. To test this change we modify LeViT-128S to have more traditional blocks while preserving the number of FLOPs. We therefore take Q, K, V to all have dimension D = 30, and C = N D = 120, 180, 240 for the three stages. As in DeiT, the MLP expansion ratio is 4. In the subsampling layers we use N = 4C/D = 16, 24, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualizations: attention bias</head><p>The attention bias maps from Eqn. 1 in the main paper are just two-dimensional maps. Therefore we can vizualize them, see <ref type="figure" target="#fig_8">Figure 6</ref>. They can be read as the amount of attention between two pixels that are at a certain relative position. The lowest values of the bias are low enough <ref type="bibr">(-20)</ref> to suppress the attention between the two pixels, since they are input to a softmax. We can observe that some heads are quite uniform, while other heads specialize in nearby pixels (e.g. most heads of the shrinking attention). Some are clearly directional, e.g. heads 1 and 4 of Stage 2/block 1 handle the pixels adjacent vertically and horizontally (respectively). Head 1 of stage 2, block 4 has a specific period-2 pattern that may be due to the fact that its output is fed to a subsampling filter in the next shrinking attention block. <ref type="bibr">Stage</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Speed-accuracy operating points for convolutional and visual transformers. Left plots: on 1 CPU core, Right: on 1 GPU. LeViT is a stack of transformer blocks, with pooling steps to reduce the resolution of the activation maps as in classical convolutional architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Patch-based convolutional masks in the pretrained DeiT-base model<ref type="bibr" target="#b2">[3]</ref>. The figure shows 12 of the 64 filters per head. Note that the K and Q filters are very similar, this is because the weights are entangled in the W Q W K multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Models with convolutional layers show a faster convergence in the early stages compared to their DeiT counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Block diagram of the LeViT-256 architecture. The two bars on the right indicate the relative resource consumption of each layer, measured in FLOPs, and the number of parameters.In LeViT we chose to apply 4 layers of 3 × 3 convolutions (stride 2) to the input to perform the resolution reduction. The number of channels goes C = 3, 32, 64, 128, 256. This reduces the activation map input to the lower layers of the transformer without losing salient information. The patch extractor for LeViT-256 transforms the image shape<ref type="bibr" target="#b2">(3,</ref> 224, 224)  into (256,<ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b14">14)</ref> with 184 MFLOPs. For comparison, the first 10 layers of a ResNet-18 perform the same dimensionality reduction with 1042 MFLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[H] × [W ] and (x , y ) ∈ [H] × [W ] for one head h ∈ [N ] is calculated as A h (x,y),(x ,y ) = Q (x,y),: • K (x ,y ),: + B h |x−x |,|y−y | . (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the attention bias for several blocks of a trained LeViT-256 model. The center for which the attention is computed is the upper left pixel of the map (with a square). Higher bias values are in yellow, lower values in dark blue (values range from -20 to 7). 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and LeViT-256 have about the same accuracies as EfficientNet B2 and B3 but are 5× and 7× faster on CPU, respectively. On the ARM platform,</figDesc><table><row><cell></cell><cell cols="2"># params FLOPs</cell><cell></cell><cell cols="2">inference speed</cell><cell></cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">top-1 GPU Intel ARM -Real -V2.</cell></row><row><cell>Architecture</cell><cell>(M)</cell><cell>(M)</cell><cell>%</cell><cell cols="3">im/s im/s im/s</cell><cell>%</cell><cell>%</cell></row><row><cell>LeViT-128S (ours)</cell><cell>7.8</cell><cell>305</cell><cell cols="4">76.6 12880 131.1 39.1</cell><cell cols="2">83.1 64.3</cell></row><row><cell>EfficientNet B0</cell><cell>5.3</cell><cell>390</cell><cell cols="3">77.1 4754 30.1</cell><cell>3.5</cell><cell cols="2">83.5 64.3</cell></row><row><cell>LeViT-128 (ours)</cell><cell>9.2</cell><cell>406</cell><cell cols="4">78.6 9266 94.0 30.8</cell><cell cols="2">84.7 66.6</cell></row><row><cell>LeViT-192 (ours)</cell><cell>10.9</cell><cell>658</cell><cell cols="4">80.0 8601 65.0 24.2</cell><cell cols="2">85.7 68.0</cell></row><row><cell>EfficientNet B1</cell><cell>7.8</cell><cell>700</cell><cell cols="3">79.1 2882 20.0</cell><cell>2.3</cell><cell cols="2">84.9 66.9</cell></row><row><cell>EfficientNet B2</cell><cell>9.2</cell><cell>1000</cell><cell cols="3">80.1 2149 13.1</cell><cell>1.3</cell><cell cols="2">85.9 68.8</cell></row><row><cell>LeViT-256 (ours)</cell><cell>18.9</cell><cell>1120</cell><cell cols="4">81.6 6582 42.5 16.4</cell><cell cols="2">86.8 70.0</cell></row><row><cell>DeiT-Tiny</cell><cell>5.9</cell><cell>1220</cell><cell cols="4">76.6 3973 39.1 16.8</cell><cell cols="2">83.9 65.4</cell></row><row><cell>EfficientNet B3</cell><cell>12</cell><cell>1800</cell><cell cols="2">81.6 1272</cell><cell>5.9</cell><cell>0.8</cell><cell cols="2">86.8 70.6</cell></row><row><cell>LeViT-384 (ours)</cell><cell>39.1</cell><cell>2353</cell><cell cols="3">82.6 4165 23.1</cell><cell>9.4</cell><cell cols="2">87.6 71.3</cell></row><row><cell>EfficientNet B4</cell><cell>19</cell><cell>4200</cell><cell>82.9</cell><cell>606</cell><cell>2.5</cell><cell>0.5</cell><cell cols="2">88.0 72.3</cell></row><row><cell>DeiT-Small</cell><cell>22.5</cell><cell>4522</cell><cell cols="3">82.6 1931 13.7</cell><cell>7.6</cell><cell cols="2">87.8 71.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Architecture</cell><cell>#params</cell><cell cols="2">FLOPs INET top-1</cell></row><row><cell>T2T-ViTt-14 [49]</cell><cell>21.5M</cell><cell>5200M</cell><cell>80.7</cell></row><row><cell>T2T-ViTt-19</cell><cell>39.0M</cell><cell>8400M</cell><cell>81.4</cell></row><row><cell>T2T-ViTt-24</cell><cell>64.1M</cell><cell>13200M</cell><cell>82.2</cell></row><row><cell>BoT-S1-50 [46]</cell><cell>20.8M</cell><cell>4270M</cell><cell>79.1</cell></row><row><cell>VT-R34 [47]</cell><cell>19.2M</cell><cell>3236M</cell><cell>79.9</cell></row><row><cell>VT-R50</cell><cell>21.4M</cell><cell>3412M</cell><cell>80.6</cell></row><row><cell>VT-R101</cell><cell>41.5M</cell><cell>7129M</cell><cell>82.3</cell></row><row><cell>PiT-Ti [59]</cell><cell>4.9M</cell><cell>700M</cell><cell>74.6</cell></row><row><cell>PiT-XS</cell><cell>10.6M</cell><cell>1400M</cell><cell>79.1</cell></row><row><cell>PiT-S</cell><cell>23.5M</cell><cell>2900M</cell><cell>81.9</cell></row><row><cell>CvT-13-NAS [60]</cell><cell>18M</cell><cell>4100M</cell><cell>82.2</cell></row></table><note>Characteristics of LeViT w.r.t. two strong families of competitors: DeiT [3] and EfficientNet [17]. The top-1 numbers are accuracies on ImageNet or ImageNet-Real and ImageNet-V2 (two last columns). The others are images per second on the different platforms. LeViT models optimize the trade-off between efficiency and accuracy (and not #params). The rows are sorted by FLOP counts.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the recent state of the art in the high-throughput regime. All inference are performed on images of size 224×224, and training is done on Ima-geNet only.</figDesc><table><row><cell>the float32 operations are not as well optimized compared</cell></row><row><cell>to Intel. However, the speed-accuracy trade-off remains</cell></row><row><cell>in LeViT's favor.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>#id↓ Ablation of LeViT-128S #params FLOPs INET top-1</figDesc><table><row><cell>Base model</cell><cell>7.4M</cell><cell>305M</cell><cell>71.9</cell></row><row><cell>A1 -without pyramid shape</cell><cell>1.2M</cell><cell>308M</cell><cell>56.5</cell></row><row><cell>A2 -without PatchConv</cell><cell>7.4M</cell><cell>275M</cell><cell>65.3</cell></row><row><cell>A3 -without BatchNorm</cell><cell>7.4M</cell><cell>305M</cell><cell>66.6</cell></row><row><cell>A4 -without distillation</cell><cell>7.4M</cell><cell>305M</cell><cell>69.7</cell></row><row><cell>A5 -without attention bias</cell><cell>7.4M</cell><cell>305M</cell><cell>70.4</cell></row><row><cell>A6 -without wider blocks</cell><cell>6.2M</cell><cell>312M</cell><cell>70.9</cell></row><row><cell>A7 -without attention activ.</cell><cell>7.4M</cell><cell>305M</cell><cell>71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Ablation of various components w.r.t. the base-</cell></row><row><cell>line LeViT-128S. Each row is the baseline minus some</cell></row><row><cell>LeViT component (1st column: experiment id). The train-</cell></row><row><cell>ing is run for 100 epochs only.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>This has little effect on the number of parameters, but the number of flops is 10% less. The , and has a strong negative impact on the accuracy. This can be explained because in a low-capacity regime, the convolutions are an effective way to compress the 3 · 16 2 = 768 dimensional patch input. A3-In without BatchNorm, we replaces BatchNorm with preactivated LayerNorm, as used in the ViT/DeiT architecture. This slows down the model slightly, as batch statistics need to be calculated at test time. Removing the BatchNorm also removes the zero-initialization of the residual connections, which disrupts training. A4-Removing the use of hard distillation from a RegNetY-16GF teacher model reduces performance, as seen with DeiT. A5-The without attention bias ablation replaces the attention bias component with a classical positional embedding added on input to the transformer stack (like DeiT). Allowing each attention head to learn a separate bias seems to be useful. A6-We use DeiT style blocks, i.e. Q,K and V all have dimension D = C/N , and the MLP blocks have expansion factor 4.A7-LeViT has an extra Hardswish non-linearity added to the attention, in addition to the softmax non-linearity. Removing it, the without attention activation ablation de-grades performance, suggesting that extra non-linearity is helpful for learning classification class boundaries.</figDesc><table><row><cell>shows that all changes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Timings for the components of the LeViT architecture on an Intel Xeon E5-2698 CPU core with batch size 1.</figDesc><table><row><cell>Model</cell><cell>DeiT-tiny</cell><cell>LeViT-256</cell></row><row><cell></cell><cell>C = 192</cell><cell>C = 256</cell></row><row><cell>Dimensions</cell><cell>N = 3</cell><cell>N = 4</cell></row><row><cell></cell><cell>D = 64</cell><cell>D = 32</cell></row><row><cell>Component</cell><cell>Runtime (µs)</cell><cell>Runtime (µs)</cell></row><row><cell>LayerNorm</cell><cell>49</cell><cell>n/a</cell></row><row><cell>Keys Q, K</cell><cell>299</cell><cell>275</cell></row><row><cell>Values V</cell><cell>172</cell><cell>275</cell></row><row><cell>Product QK T</cell><cell>228</cell><cell>159</cell></row><row><cell>Product Attention AV</cell><cell>161</cell><cell>206</cell></row><row><cell>Attention projection</cell><cell>175</cell><cell>310</cell></row><row><cell>MLP</cell><cell>1390</cell><cell>1140</cell></row><row><cell>Total</cell><cell>2474</cell><cell>2365</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>1, attention block 1 (first one)</figDesc><table><row><cell>head 0</cell><cell></cell><cell>head 1</cell><cell></cell><cell>head 2</cell><cell></cell><cell>head 3</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Stage 1, attention block 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head 0</cell><cell></cell><cell>head 1</cell><cell></cell><cell>head 2</cell><cell></cell><cell>head 3</cell><cell></cell></row><row><cell></cell><cell cols="5">Shrinking attention between stages 1 and 2</cell><cell></cell><cell></cell></row><row><cell>head 0</cell><cell>head 1</cell><cell>head 2</cell><cell>head 3</cell><cell>head 4</cell><cell>head 5</cell><cell>head 6</cell><cell>head 7</cell></row><row><cell></cell><cell></cell><cell cols="3">Stage 2, attention block 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head 0</cell><cell>head 1</cell><cell cols="2">head 2</cell><cell>head 3</cell><cell>head 4</cell><cell cols="2">head 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Stage 2, attention block 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head 0</cell><cell>head 1</cell><cell cols="2">head 2</cell><cell>head 3</cell><cell>head 4</cell><cell cols="2">head 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Stage 3, attention block 4 (last one)</cell><cell></cell><cell></cell></row><row><cell>head 0</cell><cell>head 1</cell><cell>head 2</cell><cell>head 3</cell><cell>head 4</cell><cell>head 5</cell><cell>head 6</cell><cell>head 7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sean Ma, Zhiheng</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00363</idno>
		<title level="m">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Haq: Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08886</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">And the bit goes down: Revisiting the quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05686</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Iasonas Kokkinos, and Matthijs Douze</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05644</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Joon-Young Lee, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks,&quot; Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Squeezeand-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Lambdanetworks: Modeling longrange interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602,2021.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Global selfattention networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hui</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03019</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>2021. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
