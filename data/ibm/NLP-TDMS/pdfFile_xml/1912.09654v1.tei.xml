<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
							<email>linzhao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Science and Technology on Multispectral Information Processing School of Artifical Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
							<email>wenbingtao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Science and Technology on Multispectral Information Processing School of Artifical Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel joint instance and semantic segmentation approach, which is called JSNet, in order to address the instance and semantic segmentation of 3D point clouds simultaneously. Firstly, we build an effective backbone network to extract robust features from the raw point clouds. Secondly, to obtain more discriminative features, a point cloud feature fusion module is proposed to fuse the different layer features of the backbone network. Furthermore, a joint instance semantic segmentation module is developed to transform semantic features into instance embedding space, and then the transformed features are further fused with instance features to facilitate instance segmentation. Meanwhile, this module also aggregates instance features into semantic feature space to promote semantic segmentation. Finally, the instance predictions are generated by applying a simple mean-shift clustering on instance embeddings. As a result, we evaluate the proposed JSNet on a large-scale 3D indoor point cloud dataset S3DIS and a part dataset ShapeNet, and compare it with existing approaches. Experimental results demonstrate our approach outperforms the state-of-theart method in 3D instance segmentation with a significant improvement in 3D semantic prediction and our method is also beneficial for part segmentation. The source code for this work is available at https://github.com/dlinzhao/JSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic segmentation is the task which is used to segment all informative regions in a scene and classify each region into a specific class. Instance segmentation is different from semantic segmentation for that different objects of the same class will have different labels. Both the two tasks have a wide applications in real-world scenarios, e.g., autonomous driving and mobile-based navigation. In 2D images, those two tasks have achieved remarkable results . However, the studies of 3D semantic and instance segmentation are still facing huge challenges, e.g., large-scale with noisy data processing, high computation as well as memory consumption. Literature research shows that 3D scene data have different representations, e.g., volumetric grids <ref type="bibr">(Wu et al. 2015;</ref><ref type="bibr" target="#b11">Thanh Nguyen et al. 2016;</ref><ref type="bibr" target="#b8">Maturana and Scherer 2015)</ref> and 3D point clouds <ref type="bibr" target="#b10">(Qi et al. 2017b;</ref><ref type="bibr" target="#b11">Wang et al. 2018b;</ref><ref type="bibr" target="#b13">Yu et al. 2018)</ref>. Compared with other representations, point cloud is a more compact and intuitive representation of 3D scene data. Recently, more efficient and powerful deep learning network architectures <ref type="bibr" target="#b10">(Qi et al. 2017b;</ref><ref type="bibr" target="#b12">Wu, Qi, and Fuxin 2019;</ref><ref type="bibr" target="#b7">Li, Chen, and Hee Lee 2018)</ref> have been proposed to directly process point clouds and shown promising results in point cloud classification and part segmentation. Those approaches are often used as feature extraction network in other tasks, e.g., instance segmentation and semantic segmentation.</p><p>In previous works, instance segmentation and semantic segmentation have often been processed respectively or instance segmentation is treated as a post-processing task of semantic segmentation <ref type="bibr" target="#b11">(Wang et al. 2018a;</ref>). However, those two problems are related because points of different categories belong to different instances and points of the same instance belong to the same class. Recently,  handles the two problems with multi-task pointwise network and multi-value Conditional Random Field (CRF). However, the CRF is an individual part behind the Convolutional Neural Network (CNN), it is difficult to explore the performance of their combination. Moreover, this method does not investigate whether semantic segmentation and instance segmentation can facili-tate each other. At the same time, <ref type="bibr">ASIS (Wang et al. 2019b</ref>) is proposed to address the two tasks simultaneously, which adapts the semantic feature to instance feature space by a fully connected layer and aggregates instance feature to semantic feature space by K Nearest Neighbor (kNN). However, the performance of this method is limited because it is difficult to choose the right K value and distance metric for kNN. Besides, it has expensive computation and memory consumption because it will generate a high order sparse tensor during training process.</p><p>In this work, we introduce a joint instance semantic segmentation neural network of 3D point clouds called JSNet to address the two fundamental problems: semantic segmentation and instance segmentation. The proposed network JS-Net includes four parts: a shared feature encoder, two parallel branch decoders, a feature fusion module for each decoder, a joint segmentation module. The feature encoder and decoders are built based on PointNet++ <ref type="bibr" target="#b10">(Qi et al. 2017b)</ref> and PointConv <ref type="bibr" target="#b12">(Wu, Qi, and Fuxin 2019)</ref> to learn more effective high-level semantic features. To obtain more discriminative features, we propose a point cloud feature fusion module to fuse the high-level and low-level information to refine the output features. In order to make the two tasks promote each other, a novel joint instance and semantic segmentation module is proposed to handle instance and semantic segmentation simultaneously. Specifically,this module transforms semantic features into instance embedding space by a 1D convolution and then the transformed features are further fused with instance features to facilitate instance segmentation. Meanwhile, this module also aggregates instance features into semantic feature space by implicit learning to promote semantic segmentation. Thus, our approach can be used to learn instance-aware semantic fusion features and semantic-aware instance embedding features, which can make the predictions of those points more accurate.</p><p>To summarize, the main contributions of our work are as follows:</p><p>• We design a more efficient Point Cloud Feature Fusion (PCFF) module to generate more discriminative features and improve the accuracy of point predictions.</p><p>• We propose a novel Joint Instance and Semantic Segmentation (JISS) module to make instance segmentation and semantic segmentation mutual promote. This module further improve the accuracy with acceptable GPU memory consumption during training process.</p><p>• We achieve impressive results on the S3DIS dataset (Armeni et al. 2016) along with a significant improvement on the 3D instance segmentation task. Additionally, our experiments on the ShapeNet dataset <ref type="bibr" target="#b13">(Yi et al. 2016)</ref> indicate that JSNet also can achieve satisfactory performance for part segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we briefly review some point cloud feature extraction works, and some existing approaches for semantic and instance segmentation in 3D scene. Especially, we concentrate on deep neural network-based methods applied to 3D point clouds because of their proven robustness and efficiency in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning for 3D Point Clouds</head><p>Although deep learning has been successfully used for 2D images, there are still many challenges in the feature learning capabilities of 3D point clouds with irregular data structures. Recently, PointNet <ref type="bibr" target="#b9">(Qi et al. 2017a</ref>  <ref type="bibr" target="#b12">(Wu, Qi, and Fuxin 2019)</ref> proposes an inverse density scale to re-weight the continuous function learned by MLP and compensate the non-uniform sampling, while it also needs high GPU memory during training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic&amp;Instance Segmentation on Point Clouds</head><p>For semantic segmentation, methods <ref type="bibr" target="#b14">(Zhao et al. 2017;</ref>) based on full convolutional networks <ref type="bibr" target="#b8">(Long, Shelhamer, and Darrell 2015)</ref> have achieved tremendous progress in 2D domain. As for 3D semantic segmentation, 3D-FCNN introduced by (Huang and You 2016) predicts a coarse voxel label with a 3D fully convolutional neural network. SEGCloud (Tchapmi et al. 2017) extends 3D-FCNN with trilinear interpolation and fully connected conditional random fields. RSNet <ref type="bibr" target="#b4">(Huang, Wang, and Neumann 2018)</ref>  However, most of the previous works tackle the two tasks separately. Very recently, ) proposes a multi-task pointwise network (MT-PNet) for predicting the semantic categories and instance embedding vectors and then uses a multi-value conditional random field (MV-CRF) as a post-processing. However, the CRF is an individual part behind the CNN, and it is difficult to explore the performance of their combination. Moreover, this method does not investigate whether semantic segmentation and instance segmentation can promote each other. Therefore, the performance improvement is not obvious. Meanwhile, ASIS <ref type="bibr" target="#b11">(Wang et al. 2019b</ref>) is proposed to segment instances and semantics for 3D point clouds at once, which uses PointNet or PointNet++ as backbone network and then concatenates a proposed module ASIS. The ASIS adapts the semantic features to instance feature space by a fully connected layer and aggregates instance features to semantic feature space by kNN. While the approach (Wang et al. 2019b) has difficult to choose the right K value and distance metric for kNN, and it also has high memory cost because it will produce a high order sparse matrix at training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>In this section, firstly, we describe the whole network architecture of our proposed JSNet for instance and semantic segmentation of 3D point clouds. Then, we elaborate on the two main components of our proposed network, including the Point Cloud Feature Fusion (PCFF) module and the Joint Instance and Semantic Segmentation (JISS) module, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>The whole network illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>(a) composes with four main components including a shared encoder, two parallel decoders, a point cloud feature fusion module for each decoder, a joint segmentation module as the last part. For the two parallel branches, one aims to extract semantic feature for each point, while the other one is for instance segmentation task. Specifically for the feature encoder and two decoders, we can directly use PointNet++ or PointConv as our backbone network by duplicating a decoder because the two decoders have the same structure. However, as is mentioned above, as for instance or semantic segmentation, the Point-Net++ may lose detailed information because of max pooling operation and the PointConv has expensive GPU memory consumption during training process. In this work, we combine the PointNet++ and PointConv to build a more effective backbone network with acceptable memory cost. The encoder of the backbone is built by concatenating a set abstraction module of PointNet++ and three feature encoding layers of PointConv. Similarly, the decoders are composed with three depthwise feature decoding layers of PointConv followed by a feature propagation module of PointNet++.</p><p>For the whole pipeline, our network takes a point cloud of size N a as input, then encodes it into a N e × 512 shaped matrix by the shared feature encoder. Next, the output of feature encoder is input into the two parallel decoders and processed by their following components separately. The semantic branch decodes the shared features and fuses the features of different layers into a semantic feature matrix F SS shaped with N a × 128. Similarly, the instance branch outputs an instance feature matrix F IS after the PCFF module. Finally, both the semantic features and the instance features are fetched and processed by the JISS module, and then output two feature matrices. One of the matrices P SSI shaped with N a × C which is used to predict the semantic categories, where C is the number of semantic categories. The other one E ISS shaped with N a × K is an instance feature matrix and it is used to predict the instance labels for each point, where K is the dimension of the embedding vector. In the embedding space, the embeddings represent the instance relationship of points: the points belonging to the same instance object are close, and the points of the different instances are kept away from each other.</p><p>At training time, the loss function L of our network consists of semantic segmentation loss L sem and instance embedding loss L ins :</p><formula xml:id="formula_0">L = L sem + L ins ,<label>(1)</label></formula><p>where L sem is defined with the classical cross entropy loss. As for the instance embedding loss, we utilize a discriminative function to express the embedding loss L ins inspired by the work in (De Brabandere, Neven, and Van Gool 2017). Specifically, the instance embedding loss function is formulated as follows:</p><formula xml:id="formula_1">L ins = L pull + L push ,<label>(2)</label></formula><p>where L pull pulls embeddings close to the mean embedding of instance, while the L push makes the mean embedding of different instances seperated from each other. Given the number of instances M , the number of elements N m in the m-th instance, the embedding e n of point, and the mean of embeddings µ m in the m-th instance. Each term is rewritten as follows:</p><formula xml:id="formula_2">L pull = 1 M M m=1 1 N m Nm n=1 [ µ m − e n 1 − δ v ] 2 + ,<label>(3)</label></formula><formula xml:id="formula_3">L push = 1 M (M − 1) M i=1 M j=1 i =j 2δ d − µ i − µ j 1 2 + ,<label>(4)</label></formula><p>where [x] + = max (0, x); · 1 is L 1 distance; δ v and δ d are margins for L pull and L push respectively.</p><p>At testing time, the final instance labels are generated by using a simple mean-shift clustering <ref type="bibr" target="#b1">(Comaniciu and Meer 2002)</ref> on the embeddings and the final semantic categories are obtained by using a argmax operation. C : Concatenates tensors along one dimension.</p><p>: Element-wise addition.</p><p>X : Element-wise product.</p><p>F : 1D Convolution with non-linear operation.</p><p>R : Mean of elements across dimension of a tensor.</p><p>S : Element-wise sigmoid.</p><p>T : Constructs a tensor by tiling a given tensor.</p><p>: PointNet feature propagation module.</p><p>: PointNet set abstraction module.</p><p>: PointConv feature encoding layer.</p><p>: PointConv depthwise feature decoding layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud Feature Fusion Module</head><p>In the segmentation and detection tasks for 2D image, only the feature of last layer is used for prediction in previous works, while different layer features are fused in subsequent approaches <ref type="bibr" target="#b7">(Lin et al. 2017;</ref> because the high level layer has richer semantic information while the low level has much more detailed information.</p><p>Those works indicate that the fused features are beneficial for better prediction. Based on observation above, we propose a Point Cloud Feature Fusion (PCFF) module for semantic and instance segmentation in point clouds. <ref type="figure" target="#fig_3">Figure 2(b)</ref> presents the details of the structure. Considering the precision, computation and GPU memory consumption, we only fuse the last three layers of the decoder. We use F a , F b and F c to represent those feature matrices of the decoder with shape N a × 128, N b × 128 and N c × 256 respectively. Firstly, we concatenate F a and F b upsampling with interpolation from F b . Then the former output is added to F c (upsampling from F c ) elementwise and a convolution is applied to the previous result. Following <ref type="bibr" target="#b10">(Qi et al. 2017b</ref>), the interpolation is achieved by using an inverse square distance weighted average based on three nearest neighbors. Finally, the PCFF generates a fused feature matrix with shaped N a × 128. This module can refine the output features from the decoder with acceptable computation and memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Instance and Semantic Segmentation</head><p>In fact, both the semantic segmentation and the instance segmentation map the initial point cloud features to different new high-level feature spaces separately. In the semantic feature space, points of the same semantic category are clustered together, while the different classes are separated. In the instance feature space, points of the same instance object are closely assembled, while points of different instances are separated. It indicates that we could extract semantic awareness information from the semantic feature space to integrate the information into the instance features and generates semantic-aware instance embedding features, and vice versa.</p><p>Based on this observation, we propose a Joint Instance Semantic Segmentation (JISS) module to obtain semantic labels and segment instance objects simultaneously, as is illustrated in <ref type="figure" target="#fig_3">Figure 2(c)</ref>. The JISS module transforms se-mantic features into instance embedding space and then the transformed features are further fused with instance features to facilitate instance segmentation. Meanwhile, this module also aggregates instance features into semantic feature space to promote semantic segmentation. Specifically, the semantic feature matrix F SS is transformed into instance feature space as F SST by a 1D convolution (Conv1D), and the F SST is added to instance feature matrix F IS element-wise as F ISS . Then, we model the spatial correlation of point features to enhance important features by concatenating the feature F IS and F ISS into a F ISSC , and then the F ISSC is applied a mean of elements across dimension (Mean) and an element-wise sigmoid (Sigmoid) to generate a weight matrix F ISR . Finally, the feature matrix F ISSC multiply the F ISR to generate the feature matrix F ISSR followed by two 1D convolution to produce the instance embedding feature E ISS shaped with N a × K. The process can be formulated as follows:</p><formula xml:id="formula_4">F ISSC = Concat (F IS , F IS + Conv1D (F SS )) , (5) F ISSR = F ISSC · Sigmoid(M ean(F ISSC )), (6) E ISS = Conv1D (Conv1D (F ISSR )) ,<label>(7)</label></formula><p>where instance embedding feature matrix E ISS is used to generate final instance labels by using mean-shift clustering.</p><p>For the semantic segmentation branch, given the instance embeddings F ISSR , this module integrates the F ISSR into semantic feature space as F ISST with a 1D convolution followed by a mean of elements across dimension and a tiling operation. Next, other operations are similar to the instance branch except the last layer which outputs an instance-aware semantic feature matrix P SSI shaped with N a × C. We also formulate this procedure as follows:</p><formula xml:id="formula_5">F ISST = T ile (M ean (Conv1D (F ISSR ))) ,<label>(8)</label></formula><formula xml:id="formula_6">F SSI = Concat(F SS , F SS + F ISST ), (9) F SSIR = F SSI · Sigmoid(M ean(F SSI )),<label>(10)</label></formula><formula xml:id="formula_7">P SSI = Conv1D (Conv1D (F SSIR )) ,<label>(11)</label></formula><p>where F SSI is a instance-fused feature matrix and the F SSIR is a feature fusion matrix for semantic segmentation. The final instance-aware semantic features are fed into the last classifier to predict the categories for each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Evaluation Metrics</head><p>We evaluate our approach on the following two public datasets: Stanford Large-Scale 3D Indoor Spaces (S3DIS) <ref type="bibr">ShapeNet (Yi et al. 2016</ref>). The S3DIS is an indoor 3D point cloud dataset that contains six areas of three different buildings and have 272 rooms and involve 13 categories in total. For a principled evaluation, we follow the same k-fold cross validation as in <ref type="bibr" target="#b9">(Qi et al. 2017a)</ref>, and we also present the results of the 5-th fold (Area 5) following <ref type="bibr" target="#b11">(Tchapmi et al. 2017</ref>) because Area 5 is not in the same building as other areas and there exist some differences between the objects in Area 5 and other areas. Moreover, we also evaluate our algorithm on ShapeNet dataset. This dataset contains 16881 CAD models from 16 categories annotated with 50 types of parts and the models in each category are labeled with two to five parts. We follow the official split of 795 scenes as training set, 654 scenes as testing set. The instance annotations generated following <ref type="bibr" target="#b11">(Wang et al. 2018a</ref>) are regarded as instance ground truth labels.</p><p>For semantic segmentation evaluation, overall accuracy (oAcc), mean accuracy (mAcc) and mean IoU (mIoU) are calculated across over all the categories. For instance segmentation, we evaluate our method including mean precision (mPrec), mean recall (mRec) with IoU threshold 0.5 and (weighted) coverage (Cov, WCov) <ref type="bibr" target="#b11">(Ren and Zemel 2017;</ref><ref type="bibr" target="#b11">Wang et al. 2019b</ref>). The Cov scores measure the instance-wise IoU for each prediction matched with ground truth instance averaged over the scene. And then the Cov is further weighted with the size of ground-truth instances to obtain WCov. Given the predicted regions P and ground truth regions G, the Cov and WCov are formulated as:</p><formula xml:id="formula_8">Cov (G, P) = |G| m=1 1 |G| max n IoU r G m , r P n ,<label>(12)</label></formula><formula xml:id="formula_9">W Cov (G, P) = |G| m=1 w m max n IoU r G m , r P n ,<label>(13)</label></formula><formula xml:id="formula_10">w m = r G m k r G k ,<label>(14)</label></formula><p>where r G m is the number of points in ground truth region m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the large scale dataset S3DIS, each point in our model is represented by a 9-dim vector (XYZ, RGB and normalized location as to the room). Following experimental settings in PointNet <ref type="bibr" target="#b9">(Qi et al. 2017a</ref>), we split the rooms into overlapped blocks of area 1m × 1m, and each block contains 4096 points. During training process, we configure the network with δ v = 0.5, δ d = 1.5 and K = 5, where K is the dimension of the embedding. We train the network for 100 epochs with batch size 24 on a single NVIDIA GTX1080Ti. We use Adam optimizer to optimize the network with momentum set to 0.9, base learning rate set to 0.001, and decay by 0.5 every 12.5k iterations. At test time, We use mean-shift clustering with bandwidth 0.6 to generate instance objects and merge instances of different blocks by using BlockMerging algorithm <ref type="bibr" target="#b11">(Wang et al. 2018a</ref>). For ShapeNet dataset, each model is sampled into a point cloud with 2048 points represented by a 6-dim vector (XYZ and normal) as in <ref type="bibr" target="#b10">(Qi et al. 2017b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Segmentation on the S3DIS dataset</head><p>As is depicted in <ref type="table" target="#tab_2">Table 1</ref>, we present the performance of our approach in instance segmentation task on S3DIS dataset. In this task, we evaluate and compare our method with exist state-of-the-art methods including SGPN (Wang et al.   al. 2019). We can see our network outperforms the other methods on S3DIS. Among them, ASIS is the most similar approach to our method. Compared with ASIS, our network JSNet achieves significant improvements on the four evaluation metrics. Especially on Area 5 of S3DIS, the improvements are more significant for each metric: 4.1 mCov, 3.7 mWCov, 4.5 mRec and 6.8 mPrec. Compared with the latest method 3D-BoNet on six fold experiments, our approach is also slightly better. Qualitative results are presented in <ref type="figure">Figure</ref> 3. <ref type="table" target="#tab_3">Table 2</ref> presents the quantitative results of our architecture in semantic segmentation task on S3DIS dataset. As is seen from <ref type="table" target="#tab_3">Table 2</ref>, our approach outperforms the baseline Point-Net <ref type="bibr" target="#b9">(Qi et al. 2017a</ref>) by 11.4 mAcc, 8.4 oAcc and 12.8 mIoU in over all accuracy on six fold cross validation experiments. For the generalizability evaluation on Area 5 of S3DIS, the performance is improved with 9.3 mAcc, 4.2 oAcc and 11.1 mIoU respectively. In addition, we also compare our method with other state-of-the-art methods on 6 fold or 5-th fold of S3DIS. Our model is slightly better than SEGCloud </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation on the S3DIS dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShapeNet Results</head><p>Besides evaluation on the large scale indoor real scene benchmark S3DIS, we also conduct experiments on Real Scene ASIS JSNet (Ours) Ground Truth <ref type="figure">Figure 4</ref>: Comparison results of ASIS and our method in semantic segmentation task on S3DIS. Different numbers indicate that the segmentation results of our method are better than the ASIS in nearby area.</p><p>ShapeNet dataset. Following <ref type="bibr" target="#b11">(Wang et al. 2018a)</ref>, the instance annotations are generated as ground truth to train our network. Since these annotations are fake ground truth labels, we only present the qualitative results of part instance segmentation, as is illustrated in <ref type="figure" target="#fig_7">Figure 5</ref>. The results of semantic segmentation are reported in <ref type="table" target="#tab_4">Table 3</ref>. We use Point-Net++ <ref type="bibr" target="#b10">(Qi et al. 2017b</ref>) as our baseline, and JSNet outperforms the baseline by 0.9-point mIoU. Compared with ASIS (Wang et al. 2019b), our approach also achieve an improvement of 0.8 mIoU. These results show that our approach is also favorable for the part segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To better validate the effectiveness of each component in our network, we conduct 7 groups of ablation experiments on Area 5 of S3DIS dataset. In addition, we also conduct addi-  (1) Base Network. The base network includes a shared encoder and two parallel decoders. The encoder is built by stacking four set abstraction modules of PointNet++ <ref type="bibr" target="#b10">(Qi et al. 2017b)</ref>, and the decoders are built by stacking four feature propagation modules of PointNet++.</p><p>(2) Backbone Network. The encoder of backbone is built by concatenating a set abstraction module of PointNet++ and three feature encoding layers of PointConv <ref type="bibr" target="#b12">(Wu, Qi, and Fuxin 2019)</ref>. Similarly, the decoders are composed with three depthwise feature decoding layers of PointConv following a feature propagation module of PointNet++.</p><p>(3)-(6) Single Module Evaluation. We remove other components from the full framework (7) and only retain a module for the ablation experiments respectively. (8)-(10) Different strategies. we train the full model with early stopping or random sample.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we present the ablation experimental results of different components in the full framework. Compared with the base network, the backbone network indeed benefits from a more efficient real convolution with density weighted. Compared with (2) and (3), the experimental results shows that fusing the feature of different layers could improve the segmentation precision because of the richer features after fusing. As for the only instance fusion semantic segmentation and only semantic awareness instance seg- mentation, the results indicate that better instance predictions could assign more reliable category labels to semantic branch, which can improve the performance of semantic segmentation. Similarly, the semantic awareness could enhance the instance predictions. In sixth ablation experiment, we combine instance fusion with semantic awareness, and the performance improvement is larger than only using one of them. <ref type="table" target="#tab_5">Table 4</ref> also depicts the ablation experiments results of the full framework trained with different schemes including fix sample, fix sample with early stopping, random sample and random sample with early stopping. Compared with <ref type="formula" target="#formula_4">(7)</ref> and <ref type="formula" target="#formula_5">(8)</ref>, our model has a slight overfitting and we alleviate this phenomenon by training the network with early stopping. Compared the fix sample (7) with random sample (9), the results indicates that we train the full framework with the strategies random sample, which makes our model avoid overfitting and has stronger generalization ability. In addition, we also use the early stopping strategies to train the network with random sample. our approach achieves 62.9 mPrec and 55.0 mIoU for instance segmentation and semantic segmentation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose JSNet, which is a novel end-to-end approach based on deep learning framework for 3D instance segmentation and semantic segmentation on point clouds. The framework consists of a shared feature encoder, two parallel feature decoders followed by a point cloud feature fusion (PCFF) module respectively and a joint instance semantic segmentation (JISS) module. On the one hand, the feature encoder, the feature decoders and the PCFF module can learn more effective and more discriminative features. On the other hand, the JISS module make the instance and semantic segmentation take advantage of each other. Finally, our approach achieves a significant improvement in both instance and semantic segmentation tasks on S3DIS dataset. In the future, spatial geometric topology of point clouds can be added into our framework for better segmentation results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our network JSNet takes raw point clouds as inputs and gets outputs instance and semantic segmentation results for each point. JISS stands for Joint Instance and Semantic Segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>models local dependencies for point clouds with a slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. 3P-RNN (Ye et al. 2018) models the inherent contextual features for semantic segmentation by using a pointwise pyramid pooling module and explores long-range spatial dependencies with two-direction hierarchical RNNs. Recently, GAC (Wang et al. 2019a), a graph attention convolution, is proposed to capture the structured feature of point clouds with dynamical kernels to adapt the structure of an object. However, there are few previous works which focus on semantics segmentation by using the advantages of instance embedding. For instance segmentation, approaches (Li et al. 2019; Huang et al. ) based on Mask R-CNN (He et al. 2017) dominate it on 2D images. However, there are few studies for 3D instance segmentation. SGPN (Wang et al. 2018a) generates instance proposals from learning a similarity matrix of the point features with a double-hinge loss. GSPN (Yi et al. 2019) generates proposals by reconstructing shapes and outputs the final segmentation results based on PointNet++. 3D-BoNet (Yang et al. 2019) directly regresses 3D bounding boxes and predicts point-level masks for all instances simultaneously. Similarly, there are few works which segment instances using the advantages of semantic fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the Joint Instance Semantic Segmentation Neural Network of 3D Point Cloud (JSNet). (a) Illustration of the network architecture. (b) Components of the Point Cloud Feature Fusion (PCFF) module. (c) Components of the Joint Instance and Semantic Segmentation (JISS) module. Different colored blocks represent different modules in (a), while those blocks represent different features in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2018a), MT-PNet (Pham et al. 2019b), MV-CRF (Pham et al. 2019b), ASIS (Wang et al. 2019b), 3D-BoNet (Yang et</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(Tchapmi et al. 2017), RSNet (Huang, Wang, and Neumann 2018), 3P-RNN (Ye et al. 2018), MT-PNet (Pham et al. 2019b), MV-CRF (Pham et al. 2019b), and ASIS (Wang et al. 2019b). Qualitative results are presented in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Comparison results of ASIS and our method in instance segmentation task on S3DIS. Different colors represent different instances. Different numbers indicate that the segmentation results of our method are better than the ASIS in nearby area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of JSNet on ShapeNet dataset. (a) Semantic prediction results of JSNet. (b) Ground truth of semantic segmentation. (c) Instance prediction results of JSNet. (d) Generated instance annotation for instance segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Instance segmentation results on S3DIS dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">mCov mWCov mRec mPrec</cell></row><row><cell></cell><cell>SGPN</cell><cell>32.7</cell><cell>35.5</cell><cell>28.7</cell><cell>36.0</cell></row><row><cell>5-th fold</cell><cell>ASIS</cell><cell>44.6</cell><cell>47.8</cell><cell>42.4</cell><cell>55.3</cell></row><row><cell></cell><cell>JSNet (Ours)</cell><cell>48.7</cell><cell>51.5</cell><cell>46.9</cell><cell>62.1</cell></row><row><cell></cell><cell>SGPN</cell><cell>37.9</cell><cell>40.8</cell><cell>31.2</cell><cell>38.2</cell></row><row><cell></cell><cell>MT-PNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.9</cell></row><row><cell>6 fold</cell><cell>MV-CRF ASIS</cell><cell>-51.2</cell><cell>-55.1</cell><cell>-47.5</cell><cell>36.3 63.6</cell></row><row><cell></cell><cell>3D-BoNet</cell><cell>-</cell><cell>-</cell><cell>47.6</cell><cell>65.6</cell></row><row><cell></cell><cell>JSNet (Ours)</cell><cell>54.1</cell><cell>58.0</cell><cell>53.9</cell><cell>66.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation results on S3DIS dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">mAcc oAcc mIoU</cell></row><row><cell></cell><cell>PointNet</cell><cell>52.1</cell><cell>83.5</cell><cell>43.4</cell></row><row><cell></cell><cell>SEGCloud</cell><cell>57.4</cell><cell>-</cell><cell>48.9</cell></row><row><cell>5-th fold</cell><cell>RSNet 3P-RNN</cell><cell>59.4 71.3</cell><cell>-85.7</cell><cell>51.9 53.4</cell></row><row><cell></cell><cell>ASIS</cell><cell>60.9</cell><cell>86.9</cell><cell>53.4</cell></row><row><cell></cell><cell>JSNet (Ours)</cell><cell>61.4</cell><cell>87.7</cell><cell>54.5</cell></row><row><cell></cell><cell>PointNet</cell><cell>60.3</cell><cell>80.3</cell><cell>48.9</cell></row><row><cell></cell><cell>3P-RNN</cell><cell>73.6</cell><cell>86.9</cell><cell>56.3</cell></row><row><cell>6 fold</cell><cell>MT-PNet MV-CRF</cell><cell>--</cell><cell>86.7 87.4</cell><cell>--</cell></row><row><cell></cell><cell>ASIS</cell><cell>70.1</cell><cell>86.2</cell><cell>59.3</cell></row><row><cell></cell><cell>JSNet (Ours)</cell><cell>71.7</cell><cell>88.7</cell><cell>61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation results on ShapeNet dataset. tional experiments to validate the effects of different training strategies on the same dataset. For all ablation experiments, if there are no extra notes, we use the same configuration in the subsection Implementation Details.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>PointNet</cell><cell>83.7</cell></row><row><cell>PointNet++</cell><cell>84.9</cell></row><row><cell>ASIS</cell><cell>85.0</cell></row><row><cell cols="2">JSNet (Ours) 85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments results on Area 5 of the S3DIS. The short names for components and strategies are defined as: BN−Base network, BBN−Backbone network, IF−Instance fusion branch of JISS module, SF−Semantic fusion branch of JISS module, ES−Early stopping, RS−Random sample.</figDesc><table><row><cell>Group (1) (2) (3) (4) (5) (6) (7) (8) (9) (10)</cell><cell>Component BN BBN PCFF IF SF ES RS mPrec mIoU Strategy Metric √ 52.3 52.7 √ 55.9 53.0 √ √ 58.6 54.5 √ √ 56.9 53.5 √ √ 57.2 53.5 √ √ √ 58.6 54.3 √ √ √ √ 57.6 54.3 √ √ √ √ √ 58.7 54.4 √ √ √ √ √ 62.1 54.5 √ √ √ √ √ √ 62.9 55.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China under Grants 61772213 and 91748204.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Armeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van ; De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">235</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Papers</note>
	<note>ICCV. Point cloud labeling using 3d convolutional neural network. In ICPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask Scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-guided unified network for panoptic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for realtime object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelhamer</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jsis3d: Joint semanticinstance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<editor>WACV. [Pham et al. 2019b] Pham, Q.-H.</editor>
		<editor>Nguyen, T.</editor>
		<editor>Hua, B.-S.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tchapmi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
	</analytic>
	<monogr>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<editor>CVPR. [Wang et al. 2018b</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<idno type="arXiv">arXiv:1906.01140</idno>
	</analytic>
	<monogr>
		<title level="m">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">210</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
