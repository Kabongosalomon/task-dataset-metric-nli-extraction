<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Compose Dynamic Tree Structures for Visual Contexts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
							<email>kaihua001@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Compose Dynamic Tree Structures for Visual Contexts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&amp;A. Our visual context tree model, dubbed VCTREE, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., "clothes" and "pants" are usually co-occur and belong to "person"; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTREE, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q&amp;A, show that VCTREE outperforms state-of-the-art results while discovering interpretable visual context structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Objects are not alone. They are placed in the visual context: a coherent object configuration attributed to the fact that they co-vary with each other. Extensive studies in cognitive science show that our brains inherently exploit visual contexts to understand cluttered visual scenes comprehensively <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">34]</ref>. For example, even the girl's leg and the horse are not fully observed in <ref type="figure">Figure 1</ref>, we can still infer "girl riding horse". Inspired by this, modeling visual con- <ref type="bibr">Figure 1</ref>. Illustrations of different object-level visual context structures: chains <ref type="bibr" target="#b53">[51]</ref>, fully-connected graphs <ref type="bibr" target="#b48">[46]</ref>, and dynamic tree structures constructed by the proposed VCTREE. For the purpose of efficient context encoding by using TreeLSTM <ref type="bibr" target="#b43">[41]</ref>, we transform the multi-branch trees (left) to the equivalent left-child rightsibling binary trees <ref type="bibr" target="#b12">[13]</ref>, where the left branches (red) indicate the hierarchical relations and right branches (blue) indicate the parallel relations. The key advantages of VCTREE over chains and graphs are hierarchical, dynamic, and efficient. texts is also indispensable in many modern computer vision systems. For example, state-of-the-art CNN architectures capture the context by convolutions of various receptive fields and encode it into multi-scale feature map pyramid <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b56">54]</ref>. Such pixel-level visual context (or local context <ref type="bibr" target="#b14">[15]</ref>) arguably plays one of the key roles in closing the performance gap of the "mid-level" vision between humans and machines, such as R-CNN based object detection <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">37]</ref>, instance segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">35]</ref>, and FCN based semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">50]</ref>.</p><p>Modeling visual contexts explicitly on the object-level has also been shown effective in "high-level" vision tasks such as image captioning <ref type="bibr" target="#b50">[48]</ref> and visual Q&amp;A <ref type="bibr" target="#b45">[43]</ref>. In fact, the visual context serves as a powerful inductive bias that connects objects in a particular layout for high-level reasoning <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b50">48]</ref>. For example, the spatial layout of "person" on "horse" is useful for determining the relationship "ride", which is in turn informative to localize the "person" if we want to answer "who is riding on the horse?". However, those works assume that the context is a scene graph, whose detection per se is a high-level task and not yet reliable. Without high-quality scene graphs, we have to use a prior layout structure. As shown in <ref type="figure">Figure 1</ref>, two popular structures are chains <ref type="bibr" target="#b53">[51]</ref> and fully-connected graphs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b51">49]</ref>, where the context is encoded by sequential models such as bidirectional LSTM <ref type="bibr" target="#b17">[18]</ref> for chains and CRF-RNN <ref type="bibr" target="#b57">[55]</ref> for graphs.</p><p>However, these two prior structures are sub-optimal. First, chains are oversimplified and may only capture simple spatial information or co-occurrence bias; though fullyconnected graphs are complete, they lack the discrimination between hierarchical relations, e.g., "helmet affiliated to head", and parallel relations, e.g., "girl on horse"; in addition, dense connections could also lead to message passing saturation in the subsequent context encoding <ref type="bibr" target="#b48">[46]</ref>. Second, visual contexts are inherently content-/task-driven, e.g., the object layouts should vary from content to content, question to question. Therefore, fixed chains and graphs are incompatible with the dynamic nature of visual contexts <ref type="bibr" target="#b46">[44]</ref>.</p><p>In this paper, we propose a model dubbed VCTREE, pioneering to compose dynamic tree structures for encoding object-level visual context for high-level visual reasoning tasks, such as scene graph generation (SGG) and visual Q&amp;A (VQA). Given a set of object proposals in an image (e.g., obtained from Faster-RCNN <ref type="bibr" target="#b39">[37]</ref>), we maintain a trainable task-specific score matrix of the objects, where each entry indicates the contextual validity of the pairwise objects. Then, a maximum spanning tree can be trimmed from the score matrix, e.g., the multi-branch trees shown in <ref type="figure">Figure 1</ref>. This dynamic structure represents a "hard" hierarchical layout bias of what objects should gain more contextual information from others, e.g., objects on the person's head are most informative given the question "what on the little girl's head?"; while the whole person's body is more important given the question "Is the girl sitting on the horse correctly?". To avoid the saturation issue caused by the densely connected arbitrary number of children, we further morph the multi-branch trees to the equivalent left-child right-sibling binary trees <ref type="bibr" target="#b12">[13]</ref>, where the left branches (red) indicate the hierarchical relations and right branches (blue) indicate the parallel relations, then use TreeLSTM <ref type="bibr" target="#b43">[41]</ref> to encode the context.</p><p>As the above VCTREE construction is in a discrete and non-differentiable nature, we develop a hybrid learning strategy using REINFORCE <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b47">45]</ref> for tree structure exploration and supervised learning for context encoding and its subsequent tasks. In particular, the evaluation result (Recall for SGG and Accuracy for VQA) from supervised task can be exploited as a critic function that guide the "action" of tree construction. We evaluate VCTREE on two benchmarks: Visual Genome <ref type="bibr" target="#b23">[23]</ref> for SGG and VQA2.0 <ref type="bibr" target="#b15">[16]</ref> for VQA. For SGG, we achieve a new stateof-the-art on all three standard tasks, i.e., Scene Graph Generation, Scene Graph Classification, and Predicate Classification; for VQA, we achieve competitive results on single model performances. In particular, VCTREE helps highlevel vision models fight against the dataset bias. For example, we achieve 4.1% absolute gain in proposed Mean Recall@100 metric of Predicate Classification than MO-TIFS <ref type="bibr" target="#b53">[51]</ref>, and observe higher improvement in VQA2.0 balanced pair subset <ref type="bibr" target="#b44">[42]</ref> than normal validation set. Qualitative results also show that VCTREE composes interpretable structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Context Structures. Despite the consensus on the value of visual contexts, existing context models are diversified into a variety of implicit or explicit approaches. Implicit models directly encode surrounding pixels into multiscale feature maps, e.g., dilated convolution <ref type="bibr" target="#b52">[50]</ref> presents a efficient way to increase receptive field, applicable in various dense prediction tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>; feature pyramid structure <ref type="bibr" target="#b26">[26]</ref> combines low-resolution contextual features with high-resolution detailed features, facilitating object detection with rich semantics. Explicit models incorporate contextual cues through object connections. However, such methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b53">51]</ref> group objects into fixed layouts, i.e., chains or graphs. Learning to Compose Structures. Learning to compose structures is becoming popular in NLP for sentence representation, e.g., Cho et al. <ref type="bibr" target="#b9">[10]</ref> applied a gated recursive convolutional neural network (grConv) to control the bottom-up feature flow for a dynamic structure; Choi et al. <ref type="bibr" target="#b10">[11]</ref> combines TreeLSTM with Gumbel-Softmax, allowing task-specific tree structures automatically learned from plain text. Yet, only few works compose visual structures for images. Conventional approaches construct a statistical dependency graph/tree for the entire dataset based on object categories <ref type="bibr" target="#b11">[12]</ref> or exemplars <ref type="bibr" target="#b31">[30]</ref>. Those statistical methods cannot put per-image objects in a context as a whole to reason over content-/task-specific fashion. Socher et al. <ref type="bibr" target="#b42">[40]</ref> constructed a bottom-up tree structure to parse images; however, their tree structure learning is supervised while ours is reinforced, which does not require tree ground-truth.  <ref type="figure">Figure 2</ref>. The framework of the proposed VCTREE model. We extract visual features from proposals and construct a dynamic VCTREE using the learnable score matrix. The tree structure is used to encode the object-level visual context, which will be decoded for each specific end-task. Parameters in stages (c)&amp;(d) are trained by supervised learning, while those in stage (b) are using REINFORCE with a self-critic baseline.</p><p>Visual Reasoning Tasks. Scene Graph Generation (SGG) task is derived from Visual Relationship Detection (VRD).</p><p>Early work on VRD <ref type="bibr" target="#b30">[29]</ref> treats objects as isolated individuals, while SGG considers each image as a whole. Along with the widely used message passing mechanism <ref type="bibr" target="#b48">[46]</ref>, a variety of context models <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b49">47]</ref> have been exploited in SGG to fine-tune local predictions through rich global contexts, making it the best competition field for different contextual models. Visual Question Answering (VQA) as a high-level task bridges the gap between computer vision and natural language processing. State-of-theart VQA models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">42]</ref> rely on bag-of-object visual attentions which can be considered as a trivial context structure. However, we propose to learn a tree context structure that is dynamic to visual content and questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>As illustrated in <ref type="figure">Figure 2</ref>, our VCTREE model can be summarized into the following four steps. (a) We adopt Faster-RCNN to detect object proposals <ref type="bibr" target="#b39">[37]</ref>. The visual feature of each proposal i is presented as x i , concatenating a RoIAlign feature <ref type="bibr" target="#b16">[17]</ref> v i ∈ R 2048 and spatial feature b i ∈ R 8 , where 8 elements indicate the bounding box coordinates (x 1 , y 1 , x 2 , y 2 ), center ( x1+x2 2 , y1+y2</p><p>2 ), and size (x 2 − x 1 , y 2 − y 1 ), respectively. Note that the visual feature x i is not limited to bounding box; segment feature from instance segmentations <ref type="bibr" target="#b16">[17]</ref> or panoptic segmentations <ref type="bibr" target="#b22">[22]</ref> could also be alternatives. (b) In Section 3.1, a learnable matrix will be introduced to construct VCTREE. Moreover, since the VCTREE construction is discrete in nature and the score matrix is non-differentiable from the loss of end-task, we develop a hybrid learning strategy in Section 3.5. (c) In Section 3.2, we employ Bidirectional Tree LSTM (Bi-TreeLSTM) to encode the contextual cues using the constructed VCTREE. (d) The encoded contexts will be decoded for each specific end-task detailed in Section 3.3 and Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VCTREE Construction</head><p>VCTREE construction aims to learn a score matrix S, which approximates the task-dependent validity between each object pair. Two principles guide the formulation of this matrix: 1) inherent object correlations should be maintained, e.g., "man wears helmet" in <ref type="figure">Figure 2;</ref> (2) task related object pair has higher score than irrelevant ones, e.g.,</p><p>given question "what is on the man's head?", "man-helmet" pair should be more important than "man-motorcycle" and "helmet-motorcycle" pairs. Therefore, we define each element of S as the product of the object correlation f (x i , x j ) and the pairwise task-dependency g(x i , x j , q):</p><formula xml:id="formula_0">   S ij = f (x i , x j ) · g(x i , x j , q), f (x i , x j ) = σ (MLP(x i , x j )) , g(x i , x j , q) = σ(h(x i , q)) · σ(h(x j , q)),<label>(1)</label></formula><p>where σ(·) is the sigmoid function; q is the task feature, e.g., the question feature encoded by GRU in VQA; MLP is a multi-layer perceptron; h(x i , q) is the object-task correlation in VQA, which will be introduced later in Section 3.4. In SGG, the entire g(x i , x j , q) is set to 1, as we assume that each object pair contributes equally without the question prior. We pretrain f (x i , x j ) on Visual Genome <ref type="bibr" target="#b23">[23]</ref> for a reasonable binary prior if two objects are related. Yet, such a pretrained model is not perfect due to the lack of coherent graph-level constraint or question prior, so it will be further fine-tuned in Section 3.5.</p><p>Considering S as a symmetric adjacency matrix, we T P T P <ref type="figure">Figure 3</ref>. The maximum spanning tree from S. In each step, a node in the remaining pool is connected to the current tree, if it has the highest validity score.</p><p>can obtain a maximum spanning tree using the Prim's algorithm <ref type="bibr" target="#b38">[36]</ref>, with a root (source node) i satisfying arg max i j = i S ij . In a nutshell, as illustrated in <ref type="figure">Figure 3</ref>, we construct the tree recursively by connecting the node from the pool to the tree node if it has the most validity. Note that during the tree structure exploration in Section 3.5, each of the i-th step t (i) in the above tree construction is sampled from all possible choices in a multinomial distribution with the probability p(t (i) |t <ref type="bibr" target="#b0">(1)</ref> , ..., t (i−1) , S) in proportion to the validity. The resultant tree is multi-branch and is merely a sparse graph with only one kind of connection, which is still unable to discriminate the hierarchical and parallel relations in the subsequent context encoding.</p><p>To this end, we convert the multi-branch tree into an equivalent binary tree, i.e., VCTREE by changing non-leftmost edges into right branches as in <ref type="figure">Figure 1</ref>. In this fashion, the right branches (blue) indicate parallel contexts, and left ones (red) indicate hierarchical contexts. Such a binary tree structure achieves significant improvements in our SGG and VQA experiments compared to its multi-branch alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">TreeLSTM Context Encoding</head><p>Given the above constructed VCTREE, we adopt Bi-TreeLSTM as our context encoder:</p><formula xml:id="formula_1">D = BiTreeLSTM({z i } i=1,2,...,n ),<label>(2)</label></formula><p>where z i is the input node feature, which will be specified in each task, and</p><formula xml:id="formula_2">D = [d 1 , d 2 , ..., d n ] is the encoded object- level visual context. Each d i = [ h i ; h i ]</formula><p>is the concatenated hidden states from both TreeLSTM <ref type="bibr" target="#b43">[41]</ref> directions:</p><formula xml:id="formula_3">h i = TreeLSTM(z i , h p ),<label>(3)</label></formula><formula xml:id="formula_4">h i = TreeLSTM(z i , [ h l ; h r ]),<label>(4)</label></formula><p>where and denote the top-down and bottom-up directions, respectively; we slightly abuse the subscripts p, l, r to denote the parent, left child, and right child of node i. The order of the concatenation [ h l ; h r ] in Eq. (4) indicates the explicit discrimination between the left and right branches in context encoding. We use zero vectors to pad all the missing branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Relation</head><p>Union Box RoI Feature Bounding Box Feature Relation Context</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Decoding</head><p>Relation Prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship Decoding</head><p>Object Prediction <ref type="figure">Figure 4</ref>. The overview of our SGG Model. The object context feature will be used to decode object categories, and the pairwise relationship decoding jointly fuses the relation context feature, RoIAlign feature of union box, and bounding box feature, before prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene Graph Generation Model</head><p>Now we detail the implementation of Eq. (2) and how to decode them for the SGG task as illustrated in <ref type="figure">Figure 4</ref>. Object Context Encoding. We employ BiTreeLSTM from Eq. (2) to encode object context representation into  <ref type="bibr" target="#b53">[51]</ref>, we adopt a dynamic object prediction which can be viewed as a decoding process in a top-down direction using Eq. (3), that is, the object class of a child is dependent on its parent. Specifically, we set the input z i of Eq.</p><formula xml:id="formula_5">D o = [d o 1 , d o 2 , ..., d o n ], d o i ∈ R 512 . We set inputs z i of Eq. (2) to [x i ; W 1ĉi ], i.e.,</formula><formula xml:id="formula_6">(3) to be [d o i ; W 2 c p ],</formula><p>where c p is the predicted label distribution of the i's parent, and W 2 embeds it into R 200 , then the output hidden is passed to a softmax classifier to achieve object label distribution c i .</p><p>The relationship prediction is in a pairwise fashion. First, we collect three pairwise features for each object pair: (1)</p><formula xml:id="formula_7">d ij = MLP([d r i ; d r j ]) as the context feature, (2) b ij = MLP([b i ; b j ; b i∪j ; b i∩j ])</formula><p>as the bounding box pair feature, with i ∪ j, i ∩ j being union box and intersection box, (3) v ij as the RoIAlign feature <ref type="bibr" target="#b16">[17]</ref> from the union bounding box of the object pair. All d ij , v ij , b ij are under the same dimension R 2048 . Then, we fuse them into a final pairwise feature: g ij = d ij · v ij · b ij , before feed it into the softmax predicate classifier, where · is element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visual Question Answering Model</head><p>Now we detail the implementation of Eq. (2) for VQA, and illustrate our VQA model in <ref type="figure">Figure 5</ref>.  <ref type="figure">Figure 5</ref>. The overview of our VQA framework. It contains two multimodal attention models for visual feature and context feature. Outputs from both models will be concatenated and passed to a question-guided gate before answer prediction.</p><p>Context Encoding. The context feature in VQA:</p><formula xml:id="formula_8">D q = [d q 1 , d q 2 , ..., d q n ], d q i ∈ R 1024 is directly encoded from the bounding box visual feature x i by Eq. (2).</formula><p>Multimodal Attention Feature. We adopt a popular attention model from previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">42]</ref> to calculate the multimodal joint feature m ∈ R 1024 for each question and image pair:</p><formula xml:id="formula_9">m = f d (ẑ, q),<label>(5)</label></formula><p>where q ∈ R 1024 is the question feature from a one-layer GRU encoding the sentence;ẑ = N i=1 α i z i is the attentive image feature calculated from the input feature set {z i }, α i = exp (u i )/ k exp (u k ) is the attention weight from object-task correlation u i = h(z i , q) = MLP f d (z i , q) , with the output of MLP being a scalar; f d can be any multi-modal feature fusion function, in particular, we adopt f d (x, y) = ReLU(W 3 x + W 4 y) − (W 3 x − W 4 y) 2 as in <ref type="bibr" target="#b55">[53]</ref>, with W 3 and W 4 projecting x, y into the same dimension. Therefore, we can use Eq. (5) to obtain both the multimodal visual attention feature m x by setting input z i to x i and multimodal contextual attention feature m d by setting z i to d q i . Question Guided Gate Decoding. However, the importance of m x and m d varies from question to question, e.g., "is there a dog?" only requires visual features for detection, while "is the man dressed formally?" is highly context dependent. Inspired by <ref type="bibr" target="#b41">[39]</ref>, we adopt a question guided gate to select the most related channels from [m x ; m d ]. The gate vector g ∈ R 2048 is defined as:</p><formula xml:id="formula_10">g = σ MLP([q; W 5 l q ]) ,<label>(6)</label></formula><p>where l q ∈ R 65 is a one-hot question type vector defined by prefixed words of questions, which is embedded into R 256 by matrix W 5 , and σ(·) denotes the sigmoid function. Finally, we fuse g · [m x ; m d ] as the final VQA feature and feed it into the softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Hybrid Learning</head><p>Due to the discrete nature of VCTREE construction, the score matrix S is not fully differentiable from the loss backpropagated from the end-task loss. Inspired by <ref type="bibr" target="#b18">[19]</ref>, we use a hybrid learning strategy that combines reinforcement learning, i.e., policy gradient <ref type="bibr" target="#b47">[45]</ref> for the parameters θ of S in the tree construction and supervised learning for the rest parameters. Suppose a layout l, i.e., a constructed VC-TREE, is sampled from π(l|I, q; θ), i.e., the construction procedure in Section 3.1, where I is the given image, q is the task, e.g., questions in VQA. To avoid clutter, we drop I and q. Then, we define the reinforcement learning loss L r (θ) as:</p><formula xml:id="formula_11">L r (θ) = −E l∼π(l|θ) [r(l)],<label>(7)</label></formula><p>where L r (θ) aims to minimize the negative expected reward r(l), which can be the end-task evaluation metrics such as Recall@100 for SGG and Accuracy for VQA. Then, the above gradient will be</p><formula xml:id="formula_12">∇ θ L r (θ) = −E l∼π(l|θ) [r(l)∇ θ logπ(l|θ)].</formula><p>Since it is impractical to estimate all possible layouts, we use the Monte-Carlo sampling to estimate the gradient:</p><formula xml:id="formula_13">∇ θ L r (θ) ≈ − 1 M M m=1 r(l m )∇ θ logπ(l m |θ) ,<label>(8)</label></formula><p>where we set M to 1 in our implementation.</p><p>To reduce the gradient variance, we apply a self-critic baseline <ref type="bibr" target="#b40">[38]</ref> b = r(l), wherel is the greedy constructed tree without sampling. So the original reward r(l m ) can be replaced by r(l m ) − b in Eq. <ref type="bibr" target="#b7">(8)</ref>. We observe faster convergence than using a traditional moving baseline <ref type="bibr" target="#b32">[31]</ref>.</p><p>The overall hybrid learning will be alternatively conducted between supervised learning and reinforcement learning, where we first train the supervised end-task on pretrained π(l|θ), then fix the end-task as reward function to learn our reinforcement policy network, after that, we update the supervised end-task by new π(l|θ). The latter two stages are running alternatively 2 times in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Scene Graph Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Dataset. Visual Genome (VG) <ref type="bibr" target="#b23">[23]</ref> is a popular benchmark for SGG. It contains 108,077 images with tens of thousands of unique object and predicate relation categories, yet most of categories have very limited instances. Therefore, previous works <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b54">52]</ref> proposed various VG splits that remove rare categories. We adopted the most popular one from <ref type="bibr" target="#b48">[46]</ref>, which selects top-150 object categories and top-50 predicate categories by frequency. The entire dataset is divided into the training set and test set by 70%, 30%, respectively. We further picked 5,000 images from training set as the validation set for hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Classification</head><p>Predicate Classification Model R@20 R@50 R@100 R@20 R@50 R@100 R@20 R@50 R@100 VRD <ref type="bibr" target="#b30">[29]</ref> -0.3 0. Protocols. We followed three conventional protocols to evaluate our SGG model: (1) Scene Graph Generation (SGGen): given an image, detect object bounding boxes and their categories, and predict their relationships; (2) Scene Graph Classification (SGCls): given ground-truth object bounding boxes in an image, predict the object categories and their relationships; (3) Predicate Classification (PredCls): given the object categories and their bounding boxes in the image, predict their relationships. Metrics. Since the annotation in VG is incomplete and biased, we followed the conventional Recall@K (R@K = 20,50,100) as the evaluation metrics <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b53">51]</ref>. However, it is well-known that SGG models trained on biased datasets such as VG have low performances for less frequent categories. To this end, we introduced a balanced metric called: Mean Recall (mR@K). It calculates the recall on each predicate category independently, and then averages the results. So, each category contributes equally. Such a metric reduces the influence of some common yet meaningless predicates, e.g., "on", "of", and gives equal attention to those infrequent predicates, e.g., "riding", "carrying", which are more valuable to high-level reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We adopted Faster-RCNN <ref type="bibr" target="#b39">[37]</ref> with VGG backbone to detect object bounding boxes and extract RoI features. Since the performance of SGG highly depends on the underlying detector, we used the same set of parameters as <ref type="bibr" target="#b53">[51]</ref> for fair comparison. Object correlations f (x i , x j ) in Eq. <ref type="formula" target="#formula_0">(1</ref>  will be pretrained on ground-truth bounding boxes with class-agnostic relationships (i.e., foreground/background relationships), using all possible symmetric pairs without sampling. In SGGen, top-64 object proposals were selected after non-maximal suppression (NMS) with 0.3 IoU. We set background/foreground ratio for predicate classification to 3, and capped the number of training samples at 64 (retained all foreground pairs if possible). Our model is optimized by SGD with momentum, using learning rate lr = 6 · 10 −3 and batch size b = 5 for supervised learning, and lr = 6 · 10 −4 , b = 1 for reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We investigated the influence of different structure construction policies. They are reported on the bottom half of <ref type="table" target="#tab_1">Table 1</ref>. The ablative methods are (1) Chain: sorting all the objects by j:j =i S ij , then constructing a chain, which is different from the left-to-right ordered chain in MOTIFS <ref type="bibr" target="#b53">[51]</ref>; (2) Overlap: iteratively constructing a binary tree by selecting the node with largest number of overlapped objects as parent, and dividing the rest nodes into left/right sub-trees by relatively positions of their bounding boxes; (3) Multi-Branch: the maximum spanning tree generated from score matrix S, using Child-Sum TreeL-STM <ref type="bibr" target="#b43">[41]</ref>   proposed VCTREE trained by supervised learning; (5) VC-TREE-HL: the complete version of VCTREE, trained by hybrid learning for structure exploration in Section 3.5. As we will show that Multi-Branch is significantly worse than VCTREE, so there is no need to conduct hybrid learning experiment on Multi-Branch. We observe that VCTREE performs better than other structures, and it is further improved by hybrid learning for structure exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-Arts</head><p>Comparing Methods. We compared VCTREE with stateof-the-art methods in <ref type="table" target="#tab_1">Table 1</ref>: (1) VRD <ref type="bibr" target="#b30">[29]</ref>, FREQ <ref type="bibr" target="#b53">[51]</ref> are methods without using visual contexts. (2) AssocEmbed <ref type="bibr" target="#b33">[32]</ref> assembles implicit contextual features by stacked hourglass backbone <ref type="bibr" target="#b34">[33]</ref>.</p><p>(3) IMP <ref type="bibr" target="#b48">[46]</ref>, TFR <ref type="bibr" target="#b19">[20]</ref>, MO-TIFS <ref type="bibr" target="#b53">[51]</ref>, Graph-RCNN <ref type="bibr" target="#b49">[47]</ref> are explicit context models with a variety of structures.</p><p>Quantitative Analysis. From <ref type="table" target="#tab_1">Table 1</ref>, compared with the previous state-of-the-art MOTIFS <ref type="bibr" target="#b53">[51]</ref>, the proposed VC-TREE has the best performances. Interestingly, Overlap tree and Multi-Branch tree are better than other non-tree context models. From <ref type="table">Table 2</ref>, the proposed VCTREE-HL shows larger absolute gains of PredCls under mR@100, which indicates that our model learns non-trivial visual context, i.e., not merely class distribution bias as in FREQ and partially in MOTIFS. Note that MOTIFS <ref type="bibr" target="#b53">[51]</ref> is even worse than its FREQ <ref type="bibr" target="#b53">[51]</ref> baseline under mR@100.</p><p>Qualitative Analysis.</p><p>To better understand what context is learned by VCTREE, we visualized a statistics of left-/rightbranch nodes for nodes classified as "street" in <ref type="figure" target="#fig_2">Figure 6</ref>. From the left pie, the hierarchical relations, we can see the node categories are long-tailed, i.e., top-10 categories cover the 73% of the instances; while the right pie, the parallel relations, are more uniformly distributed. This demonstrates that VCTREE captures the two types of context successfully. More qualitative examples of VCTREEs and their generated scene graph can be viewed in <ref type="figure">Figure 7</ref>. The common errors are generally synonymous labels, e.g., "jeans" vs. "pants", "man" vs. "person", and over-interpretation, e.g., the "tail" of bottom left "dog" is considered as "leg", as it appears at the place where "leg" should be.   , 1) over all 10 select 9 sets. Question-answer pairs are organized in three answer types: i.e. "Yes/No", "Number", "Other". There are also 65 question types determined by prefixed words, which we used to generate question-guided gates. We also tested our models on a balanced subset of validation set, called Balanced Pairs <ref type="bibr" target="#b44">[42]</ref>, which requires the same question on different images with two different yet perfect (with 1.0 ground-truth score) answers. Since Balanced Pairs strictly removes question-related bias, it reflects the ability of a context model to distinguish subtle differences between images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We employed a simple text preprocessing for questions and answers, which changes all characters into lower-case and removes special characters. Questions were encoded into a vocabulary of the size 13,758 without trimming. Answers used a 3,000 vocabulary selected by frequency. For fair comparison, we used the same bottom-up feature <ref type="bibr" target="#b0">[1]</ref> as previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b55">53]</ref>, which contains 10 to 100 object proposals per image extracted by Faster-RCNN <ref type="bibr" target="#b39">[37]</ref>.  <ref type="figure">Figure 7</ref>. Left: the learned tree structure and generated scene graphs in VG. Black color indicates correctly detected objects or predicates; red indicates the misclassified ones; blue indicates correct predictions that not labeled as ground-truth. Right: interpretable and dynamic trees subject to different questions in VQA2.0.</p><p>We used the same Faster-RCNN detector to pretrain the f (x i , x j ). Since candidate answers were represented by probabilities rather than one-hot vectors in VQA2.0, we allowed the cross-entropy loss calculating soft categories, i.e., probabilities of ground-truth candidate answers. We used Adam optimizer with learning rate lr = 0.0015 and batch size b = 256, lr decayed at ratio of 0.5 every 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>In addition to the 5 structure construction policies introduced in Section 4.3, we also implemented a fullyconnected graph structure using the message passing mechanism <ref type="bibr" target="#b48">[46]</ref>. From <ref type="table" target="#tab_4">Table 3</ref>, the proposed VCTREE-HL outperforms all the context models on three answer types.</p><p>We further evaluated the above context models on VQA2.0 balanced pair subset <ref type="bibr" target="#b44">[42]</ref>: the last column of Table 3, and found that the absolute gains between VCTREE-HL and other structures are even larger than those on the original validation set. Meanwhile, as reported in <ref type="bibr" target="#b44">[42]</ref>, different architectures or hyper-parameters in non-contextual VQA model normally gain less improvements on the balanced pair subset than overall validation set. Thus, it suggests that VCTREE indeed use better context structures to alleviate the question-answer bias in VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with State-of-the-Arts</head><p>Comparing Methods. <ref type="table" target="#tab_6">Table 4</ref> &amp; 5 reports the single-model performances of various state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b55">53]</ref> on both test-dev and test-standard sets. For fair comparison, the reported methods are all using the same Faster-RCNN features <ref type="bibr" target="#b0">[1]</ref> as ours. Quantitative Analysis. The proposed VCTREE-HL shows the best overall performance in both test-dev and teststandard. Note that though Count <ref type="bibr" target="#b55">[53]</ref> has close overall performance to our VCTREE, it mainly improves the "Number" task by the elaborately designed model, while the proposed VCTREE is a more general solution. Qualitative Analysis. We visualized several examples of VCTREE-HL on the validation set. They illustrate that the proposed VCTREE is able to learn dynamic structures with interpretability, e.g., in <ref type="figure">Figure 7</ref>, given the right middle image with the question "Is there any snow on the trees?", the generated VCTREE locates the "tree" then searching for the "snow", while with question "What sport is the man doing?", the "man" appears to be the root.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed a dynamic tree structure called VCTREE to capture task-specific visual contexts, which can be encoded to support two high-level vision tasks: SGG and VQA. By exploiting VCTREE, we ob-served consistent performance gains in SGG on Visual Genome and in VQA on VQA2.0, compared to models with or without visual contexts. Besides, to justify that VCTREE learns non-trivial contexts, we conducted additional experiments against the category bias in SGG and the questionanswer bias in VQA, respectively. In the future, we intend to study the potential of a dynamic forest as the underlying context structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bidirectional TreeLSTM</head><p>In this section, we will introduce the details of the bidirectional TreeLSTM applied to encode the object-level visual contexts. For the bottom-up direction, we employ Nary TreeLSTM <ref type="bibr" target="#b43">[41]</ref> for binary trees, i.e., VCTREEs and Overlap Trees, and the normalized Child-Sum <ref type="bibr" target="#b43">[41]</ref> TreeL-STM for Multi-Branch Trees. For the top-down direction, since each node only has one parent, TreeLSTM is similar to the traditional LSTM <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. N -ary TreeLSTM for Binary Trees</head><p>According to the definition of N -ary TreeLSTM <ref type="bibr" target="#b43">[41]</ref>, it can be applied to the tree structures with at most N ordered branches for each node. In our work, we adopt binary TreeLSTM as our bottom-up TreeLSTM for the proposed binary tree structures, i.e., VCTREEs and Overlap Trees. It can be formulated as follows:</p><formula xml:id="formula_14">h t = TreeLSTM z t , [ h l ; h r ] ,<label>(9)</label></formula><formula xml:id="formula_15">i t = σ W (i) z t + U (i) [ h l ; h r ] + b (i) ,<label>(10)</label></formula><formula xml:id="formula_16">f l = σ W (f ) l z t + U (f ) l [ h l ; h r ] + b (f ) l ,<label>(11)</label></formula><formula xml:id="formula_17">f r = σ W (f ) r z t + U (f ) r [ h l ; h r ] + b (f ) r ,<label>(12)</label></formula><formula xml:id="formula_18">o t = σ W (o) z t + U (o) [ h l ; h r ] + b (o) ,<label>(13)</label></formula><formula xml:id="formula_19">u t = tanh W (u) z t + U (u) [ h l ; h r ] + b (u) ,<label>(14)</label></formula><formula xml:id="formula_20">c t = i t u t + f l c l + f r c r ,<label>(15)</label></formula><formula xml:id="formula_21">h t = o t tanh( c t ),<label>(16)</label></formula><p>where z t ∈ R d is the input feature for node t; h t , h l , h r ∈ R h are the hidden states; c t , c l , c r ∈ R h are memory cells;</p><formula xml:id="formula_22">W (i) , W (f ) l , W (f ) r , W (o) , W (u) ∈ R h×d and U (i) , U (f ) l , U (f ) r , U (o) , U (u) ∈ R h×2h are learnable ma- trices; b (i) , b (f ) l , b (f ) r , b (o) , b (u) ∈ R h</formula><p>are vectors; σ denotes sigmoid function; tanh denotes tanh activation function; means element-wise product. Note that we slightly abuse the subscripts l, r of c l , c r , h l , h r to denote hidden states and memory cells from the left-child and right-child of node t. The hidden states and memory cells of the missing branches will be filled with zero vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Child-Sum TreeLSTM for Multi-Branch Trees</head><p>The Child-Sum TreeLSTM <ref type="bibr" target="#b43">[41]</ref> is able to deal with the tree structure where each node has arbitrary number of children. Therefore, we adopt it as the bottom-up TreeLSTM of the context encoder for the Multi-Branch Trees in the ablation studies. For each node t of a Multi-Branch Tree, we define C(t) as the set of its children. Compared with the original paper <ref type="bibr" target="#b43">[41]</ref>, we replace the Child-Sum with the Child-Mean in our implementation for better normalization, then it is formulated as:</p><formula xml:id="formula_23">h t = TreeLSTM z t , { h k } , k ∈ C(t),<label>(17)</label></formula><formula xml:id="formula_24">h mean = k∈C(t) h k |C(t)| ,<label>(18)</label></formula><formula xml:id="formula_25">i t = σ W (i) z t + U (i) h mean + b (i) ,<label>(19)</label></formula><formula xml:id="formula_26">f k = σ W (f ) z t + U (f ) h k + b (f ) ,<label>(20)</label></formula><formula xml:id="formula_27">o t = σ W (o) z t + U (o) h mean + b (o) ,<label>(21)</label></formula><formula xml:id="formula_28">u t = tanh W (u) z t + U (u) h mean + b (u) ,<label>(22)</label></formula><formula xml:id="formula_29">c t = i t u t + k∈C(t) f k c k |C(t)| ,<label>(23)</label></formula><formula xml:id="formula_30">h t = o t tanh( c t ),<label>(24)</label></formula><p>where h t , h k , ∈ R h are the hidden states; c t , c k ∈ R h are memory cells;</p><formula xml:id="formula_31">W (i) , W (f ) , W (o) , W (u) ∈ R h×d and U (i) , U (f ) , U (o) , U (u) ∈ R h×h are learnable matrices; b (i) , b (f ) , b (o) , b (u) ∈ R h are vectors; |C(t)|</formula><p>is the number of children for node t; h mean denotes the mean hidden state of all the children of node t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Top-Down TreeLSTM</head><p>We use the traditional LSTM <ref type="bibr" target="#b17">[18]</ref> as the top-down TreeLSTM for all the VCTREEs, Overlap Trees, and Multi-Branch Trees, because each node only has at most one parent. The only difference with the traditional LSTM is that our structures are trees rather than chains, the previous hidden state is from the parent of node t.</p><p>For the proposed VCTREE, we assigned different learnable matrices for the hidden states from the left-branch parents and right-branch parents. However, the result didn't show significant improvements in the end-tasks, so we employ traditional LSTM as our top-down LSTM for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Mean Recall for Scene Graph</head><p>We also report more detailed results of the proposed Mean Recall (mR@K) in <ref type="table">Table 6</ref>. The proposed VC-TREE-HL shows best performance among all the ablative structures. Note that MOTIFS <ref type="bibr" target="#b53">[51]</ref> has lower mR@100 than FREQ <ref type="bibr" target="#b53">[51]</ref> baseline in SGCls and PredCls, which means that MOTIFS is even worse at predicting infrequent predicate categories. However, its mR@20 and mR@50 are higher than FREQ in SGCls and PredCls, which indicates that MOTIFS better separates the foreground relationships from the background ones than FREQ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Predicate Recall Analysis</head><p>To better visualize the improvement of the proposed VC-TREE-HL on infrequent predicate categories, we rank all the predicate categories by frequency, and show the PredCls Recall@100 of MOTIFS <ref type="bibr" target="#b53">[51]</ref> and VCTREE-HL for each top-35 category independently in <ref type="figure" target="#fig_3">Figure 8</ref>. We can observe significant improvements on those less frequent but more semantically meaningful predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Scene Graph Generation</head><p>We further investigated more misclassified results of the proposed VCTREE-HL. The corresponding tree structures and the generated scene graphs are reported in <ref type="figure">Figure 9</ref>. We observed 3 types of interesting misclassifications: 1) In the image (a) of <ref type="figure">Figure 9</ref>, the proposed VCTREE-HL predicts more appropriate predicates "in front of" and "behind" than original "near". 2) In the image (b) and (d), the ground truth "man in snow" and "window near building" are improper, while our method shows more appropriate predicates. 3) In the image (c) and (d), the objects isolated from the Scene Graph (only considering R@20 predicates) are easier to be misclassified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Visual Question Answering</head><p>More constructed VCTREEs for VQA2.0 are visualized in <ref type="figure">Figure 10</ref>. The dynamic tree structures are subject to different questions, which allow the objects in an image to incorporate the different contextual cues according to each question. The proposed VCTREE also helps us understand how the model predicts the answer of the question given the image, e.g., in image (a) of <ref type="figure">Figure 10</ref>, given the question "does this dog have a collar?", we find that our model first focuses on the collar-like object rather than the dog; in image (b) of <ref type="figure">Figure 10</ref>, given the question "what sport is being played?", we find that our model focuses on the sportsman rather than playground to answer this question.  <ref type="figure">Figure 10</ref>. The dynamic and interpretable tree structures that subject to different questions, which allow the objects in an image incorporate different contextual cues according to each question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>The statistics of left-branch (hierarchical) nodes and right-branch (parallel) nodes of the "street" category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Recall@100 of MOTIFS [51] and the proposed VCTREE-HL under PredCls for each Top-35 category ranking by frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>concatenation of object visual features and embedded N-way original Faster-RCNN class probabilities, where W 1 is the embedding matrix that maps each original label distributionĉ i into R 200 . Relation Context Encoding. We apply an additional Bi-TreeLSTM using the above d o i as input z i to further encode the relation context D r = [d r 1 , d r 2 , ..., d r n ], d r i ∈ R 512 . Context Decoding. The goal of SGG is to detect objects and then predict their relationship. Similar to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>SGG performances (%) of various methods. denotes the methods using the same Faster-RCNN detector as ours. IMP is reported from the re-implemented version<ref type="bibr" target="#b53">[51]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>-</cell><cell>11.8</cell><cell>14.1</cell><cell>-</cell><cell>27.9</cell><cell>35.0</cell></row><row><cell cols="2">AsscEmbed [32]</cell><cell>6.5</cell><cell>8.1</cell><cell>8.2</cell><cell>18.2</cell><cell>21.8</cell><cell>22.6</cell><cell>47.9</cell><cell>54.1</cell><cell>55.4</cell></row><row><cell cols="2">IMP [46]</cell><cell>14.6</cell><cell>20.7</cell><cell>24.5</cell><cell>31.7</cell><cell>34.6</cell><cell>35.4</cell><cell>52.7</cell><cell>59.3</cell><cell>61.3</cell></row><row><cell>TFR [20]</cell><cell></cell><cell>3.4</cell><cell>4.8</cell><cell>6.0</cell><cell>19.6</cell><cell>24.3</cell><cell>26.6</cell><cell>40.1</cell><cell>51.9</cell><cell>58.3</cell></row><row><cell cols="2">FREQ [51]</cell><cell>20.1</cell><cell>26.2</cell><cell>30.1</cell><cell>29.3</cell><cell>32.3</cell><cell>32.9</cell><cell>53.6</cell><cell>60.6</cell><cell>62.2</cell></row><row><cell cols="2">MOTIFS [51]</cell><cell>21.4</cell><cell>27.2</cell><cell>30.3</cell><cell>32.9</cell><cell>35.8</cell><cell>36.5</cell><cell>58.5</cell><cell>65.2</cell><cell>67.1</cell></row><row><cell cols="2">Graph-RCNN [47]</cell><cell>-</cell><cell>11.4</cell><cell>13.7</cell><cell>-</cell><cell>29.6</cell><cell>31.6</cell><cell>-</cell><cell>54.2</cell><cell>59.1</cell></row><row><cell>Chain</cell><cell></cell><cell>21.2</cell><cell>27.1</cell><cell>30.3</cell><cell>33.3</cell><cell>36.1</cell><cell>36.8</cell><cell>59.4</cell><cell>66.0</cell><cell>67.7</cell></row><row><cell>Overlap</cell><cell></cell><cell>21.4</cell><cell>27.3</cell><cell>30.4</cell><cell>33.7</cell><cell>36.5</cell><cell>37.1</cell><cell>59.5</cell><cell>66.0</cell><cell>67.8</cell></row><row><cell cols="2">Multi-Branch</cell><cell>21.5</cell><cell>27.3</cell><cell>30.6</cell><cell>34.3</cell><cell>37.1</cell><cell>37.8</cell><cell>59.5</cell><cell>66.1</cell><cell>67.8</cell></row><row><cell cols="2">VCTREE-SL</cell><cell>21.7</cell><cell>27.7</cell><cell>31.1</cell><cell>35.0</cell><cell>37.9</cell><cell>38.6</cell><cell>59.8</cell><cell>66.2</cell><cell>67.9</cell></row><row><cell cols="2">VCTREE-HL</cell><cell>22.0</cell><cell>27.9</cell><cell>31.3</cell><cell>35.2</cell><cell>38.1</cell><cell>38.8</cell><cell>60.1</cell><cell>66.4</cell><cell>68.1</cell></row><row><cell></cell><cell>SGGen</cell><cell>SGCls</cell><cell cols="2">PredCls</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">mR@100 mR@100 mR@100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MOTIFS [51]</cell><cell>6.6</cell><cell>8.2</cell><cell>15.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FREQ [51]</cell><cell>7.1</cell><cell>8.5</cell><cell>16.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VCTREE-HL</cell><cell>8.0</cell><cell>10.8</cell><cell>19.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 2. Mean recall (%) of various methods across all the 50 pred-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>icate categories.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>49% Right Branch of 'Street' Node</head><label></label><figDesc></figDesc><table><row><cell cols="6">Left Branch of 'Street' Node</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell>Sidewalk</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Man</cell><cell></cell><cell>Street</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tree</cell><cell></cell><cell>Sign</cell></row><row><cell>21%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Person</cell><cell>11%</cell><cell>Car</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">30%</cell><cell>Sign</cell><cell>9%</cell><cell>Light</cell></row><row><cell>4% 3% 3% 4% 4% 5%</cell><cell>5%</cell><cell>5%</cell><cell>5%</cell><cell>5%</cell><cell>7%</cell><cell>Bus Sidewalk Street Pole Vehicle</cell><cell>5% 5% 4% 3% 3% 3% 2% 2% 2% 2%</cell><cell>Pole Tree Person Building Door</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Truck</cell><cell></cell><cell>People</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building</cell><cell></cell><cell>Man</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Others</cell><cell></cell><cell>Others</cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Accuracies (%) of various context structures on the VQA2.0 validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Single-model accuracies (%) on VQA2.0 test-dev, where</cell></row><row><cell cols="5">MUTAN and MLB are re-implemented versions from [3].</cell></row><row><cell></cell><cell cols="2">VQA2.0 test-standard</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Yes/No Number Other</cell><cell>All</cell></row><row><cell>Teney [42]</cell><cell>82.20</cell><cell>43.90</cell><cell cols="2">56.26 65.67</cell></row><row><cell>MUTAN [5]</cell><cell>83.06</cell><cell>44.28</cell><cell cols="2">56.91 66.38</cell></row><row><cell>MLB [21]</cell><cell>83.96</cell><cell>44.77</cell><cell cols="2">56.52 66.62</cell></row><row><cell>DA-NTN [3]</cell><cell>84.60</cell><cell>47.13</cell><cell cols="2">58.20 67.94</cell></row><row><cell>Count [53]</cell><cell>83.56</cell><cell>51.39</cell><cell cols="2">59.11 68.41</cell></row><row><cell>Chain</cell><cell>83.06</cell><cell>47.38</cell><cell cols="2">58.95 67.68</cell></row><row><cell>Graph</cell><cell>84.03</cell><cell>47.08</cell><cell cols="2">58.82 68.0</cell></row><row><cell>VCTREE-HL</cell><cell>84.55</cell><cell>47.36</cell><cell cols="2">59.34 68.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Single-model accuracies (%) on VQA2.0 test-standard,</cell></row><row><cell>where MUTAN and MLB are re-implemented versions from [3].</cell></row><row><cell>5. Experiments on Visual Q&amp;A</cell></row><row><cell>5.1. Settings</cell></row><row><cell>Datasets. We evaluated the proposed VQA model on</cell></row><row><cell>VQA2.0 [16]. Compared with VQA1.0 [2], VQA2.0</cell></row><row><cell>has more question-image pairs for training (443,757)</cell></row><row><cell>and validation (214,354), and all the question-answer</cell></row><row><cell>pairs are balanced by making sure the same question</cell></row><row><cell>can have different answers. In VQA2.0, the ground-</cell></row><row><cell>truth accuracy of a candidate answer is considered as</cell></row><row><cell>the average of min( #Humans votes 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>was the picture taken of the man? Q: What is the man wearing on his head? Q: Is there any snow on the trees? Q: What sport is the man doing?</head><label></label><figDesc></figDesc><table><row><cell cols="3">Man Pant(Jean) near Wearing On near Dog Plate Has Sign Head near near Of Has Of Has Of Has Of Has Of Food Table Fork Glass Horse Tail Has Of near near Ear_1 Ear_2 Head Nose Cup On On On On Near Near Under With Near Q: Where On Q: What type of food is on the plate? A: Outside GT: Forest A: No A: Hot dog Near On Of</cell><cell>Q: Does this kid look excited? A: Hat A: Snowboarding A: Yes</cell></row><row><cell>Leg Paw</cell><cell>Has Of Has Of</cell><cell>Chair</cell></row><row><cell>Leg(Tail)</cell><cell>Has Of</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In front of Near <ref type="figure">Figure 9</ref>. The learned tree structures and generated scene graphs in VG. We selectively report the predicates from R@20 and all the groundtruth predicates. Black color indicates correctly detected objects or predicates; red indicates the misclassified ones; blue indicates correct predictions that not labeled as ground-truth.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep attention neural tensor network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">617</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene perception: Detecting and judging objects undergoing relational violations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSST-8</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to compose task-specific tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tree-based context model for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>McGraw-Hill Higher Education</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tensorize, factorize and regularize: Robust visual relationship learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Jae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Factorizable net: An efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="346" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and caption regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instancelevel relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6985" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond categories: The visual memex model for reasoning about object relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shortest connection networks and some generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Prim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1389" to="1401" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Question type guided attention in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Task-dependent influences of attention on the activation of human primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Harner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miyauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Palomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mukai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="11489" to="11492" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Elgammal. Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prügel-Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Mean recall (%) of various methods across all the 50 predicate categories</title>
		<imprint/>
	</monogr>
	<note>Table 6. MOTIFS [51] and FREQ [51] are using the same Faster-RCNN detector as ours</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
