<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Explanation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
							<email>alshedivat@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Avinava Dubey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University &amp; Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University &amp; Petuum Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Explanation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CENs)a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Model interpretability is a long-standing problem in machine learning that has become quite acute with the accelerating pace of the widespread adoption of complex predictive algorithms. While high performance often supports our belief in the predictive capabilities of a system, perturbation analysis reveals that black-box models can be easily broken in an unintuitive and unexpected manner <ref type="bibr" target="#b78">(Szegedy et al., 2013;</ref>. Therefore, for a machine learning system to be used in a social context (e.g., in healthcare) it is imperative to provide sound reasoning for each prediction or decision it makes.  <ref type="figure">Figure 1</ref>: High-level functionality of CENs: The context is represented by satellite imagery and used to generate instance-specific linear models (explanations). The latter act on a set of interpretable attributes from regional survey data and produce predictions.</p><p>To design such systems, we may restrict the class of models to only human-intelligible <ref type="bibr" target="#b15">(Caruana et al., 2015)</ref>. However, such an approach is often limiting in modern practical settings. Alternatively, we may fit a complex model and explain its predictions post-hoc, e.g., by searching for linear local approximations of the decision boundary <ref type="bibr" target="#b70">(Ribeiro et al., 2016)</ref>. While such methods achieve their goal, explanations are generated a posteriori require additional computation per data instance and, most importantly, are never the basis for the predictions made in the first place, which may lead to erroneous interpretations, as we show in this paper, or even be exploited <ref type="bibr" target="#b26">(Dombrowski et al., 2019;</ref><ref type="bibr" target="#b53">Lakkaraju and Bastani, 2019)</ref>.</p><p>Explanation is a fundamental part of the human learning and decision process <ref type="bibr" target="#b60">(Lombrozo, 2006)</ref>. Inspired by this fact, we introduce contextual explanation networks (CENs)-a class of architectures that learn to predict and to explain jointly, alleviating the drawbacks of the post-hoc methods. To make a prediction, CENs operate as follows ( <ref type="figure">Figure 1</ref>). First, they process a subset of inputs and generate parameters for a simple probabilistic model (e.g., sparse linear model) which is regarded interpretable by a domain expert. Then, the generated model is applied to another subset of inputs and produces a prediction. To motivate such an architecture, we consider the following example.</p><p>A motivating illustration. One of the tasks we consider in this paper is classification of households into poor and not poor having access to satellite imagery and categorical data from surveys <ref type="bibr" target="#b40">(Jean et al., 2016)</ref>. If a human were to solve this task, to make predictions, they might assign weights to features in the categorical data and explain their predictions in terms of the most relevant variables (i.e., come up with a linear model). Moreover, depending on the type of the area (as seen from the imagery), they might select slightly different weights for different areas (e.g., when features indicative of poverty are different for urban, rural, and other types of areas).</p><p>The CEN architecture given in <ref type="figure">Figure 1</ref> imitates this process by making predictions using sparse linear models applied to interpretable categorical features. The weights of the linear models are contextual, generated by a learned encoder that maps images (the context) to the weight vectors. The learned encoder is sensitive to the infrastructure presented in the input images and generates different linear models for urban and rural areas. The generated models not only are used for prediction but also play the role of explanations and can encode arbitrary prior knowledge. CENs can represent complex model classes by using powerful encoders. At the same time, by offsetting complexity into the encoding process, we achieve simplicity of explanations and can interpret predictions in terms the variables of interest.</p><p>The proposed architecture opens a number of questions: What are the fundamental advantages and limitations of CEN? How much of the performance should be attributed to the context encoder and how much to the explanations? Are there any degenerate cases and do they happen in practice? Finally, how do CEN-generated explanations compare to alternatives, e.g., produced with LIME <ref type="bibr" target="#b70">(Ribeiro et al., 2016)</ref>? In the rest of this paper, we formalize our intuitions and answer these questions theoretically and experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>The main four contributions of this paper are as follows:</p><p>(i) We formally define CENs as a class of probabilistic models, consider special cases, and derive learning and inference algorithms for scalar and structured outputs. (ii) We design CENs in the form of new deep learning architectures trainable end-to-end for prediction and survival analysis tasks. (iii) Empirically, we demonstrate the value of learning with explanations for both prediction and model diagnostics. Moreover, we find that explanations can act as a regularizer and result in improved sample efficiency. (iv) We also show that noisy features can render post-hoc explanations inconsistent and misleading, and how CENs can help to detect and avoid such situations.</p><p>Our code is available at https://github.com/alshedivat/cen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization</head><p>The paper is organized as follows. Section 2 presents the notation and some background on post-hoc interpretability methods. In Sections 3, we introduce the general CEN framework, describe specific implementations, learning, and inference. In Section 4, we overview broadly related work. In Section 5, we discuss and analyze properties of CEN theoretically. Section 6 presents a number of case studies: experimental results for scalar prediction tasks (Section 6.1), an empirical analysis of consistency of linear explanations generated by CEN vs. alternatives (Section 6.2), and finally how CENs with structured explanations can efficiently solve survival analysis tasks (Section 6.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Contextual Explanation Networks</head><p>We consider the same problem of learning from a collection of data represented by context variables, c ∈ C, attributes, x ∈ X , and targets, y ∈ Y. We denote the corresponding random variables by capital letters, C, X, and Y, respectively. Our goal is to learn a model, P w (Y | x, c), parametrized by w that can predict y from x and c. We define contextual explanation networks as probabilistic models that assume the following form ( <ref type="figure" target="#fig_1">Figure 2</ref></p><formula xml:id="formula_0">): 2 y ∼ P (Y | x, θ) , θ ∼ P w (θ | c) , P w (Y | x, c) = P (Y | x, θ) P w (θ | c) dθ<label>(3)</label></formula><p>where P (Y | x, θ) is a predictor parametrized by θ. We call such predictors explanations, since they explicitly relate interpretable attributes, x, to the targets, y. For example, when the targets are scalar and binary, explanations may take the form of linear logistic models; when the targets are more complex, dependencies between the components of y can be represented by a graphical model, e.g., conditional random field <ref type="bibr" target="#b52">(Lafferty et al., 2001)</ref>. CENs assume that each explanation is context-specific: P w (θ | c) defines a conditional probability of an explanation θ being valid in the context c. To make a prediction, we marginalize out θ. To interpret a prediction,ŷ, for a given data instance, (x, c), we infer the posterior, P w (θ |ŷ, x, c). The main advantage of this approach is to allow modeling conditional probabilities, P w (θ | c), in a black-box fashion while keeping the class of explanations, P (Y | x, θ), simple and interpretable. For instance, when the context is given as raw text, we may choose P w (θ | c) to be represented with a recurrent neural network, while P (Y | x, θ) be in the class of linear models.</p><p>Implications of these assumptions are discussed in Section 5. Here, we continue with a discussion of a number of practical choices for P w (θ | c) and P (Y | x, θ) <ref type="table" target="#tab_0">(Table 1)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Parameter distribution, P (θ | c)</p><formula xml:id="formula_1">Deterministic δ (φ(c), θ) where φ(c) is arbitrary Constrained δ (φ(c), θ) where φ(c) := α(c) D MoE K k=1 P (k | c) δ(θ, θ k ) Explanation Predictive distribution, P (y | x, θ) Linear softmax θ x Structured ∝ exp {−E θ (x, y)} where E θ (·, ·)</formula><p>is some energy function, linear in θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Encoders</head><p>In practice, we represent P w (θ | c) with a neural network that encodes the context into the parameter space of the explanation models. There are two simple ways to construct an encoder, which we consider below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Deterministic encoding</head><formula xml:id="formula_2">Let P w (θ | c) := δ (φ w (c), θ), where δ(·, ·)</formula><p>is a delta-function and φ w (·) is the network that maps c to θ. Collapsing the conditional distribution to a delta-function makes θ depend deterministically on c and results into the following conditional likelihood:</p><formula xml:id="formula_3">P (y | x, c; w) = P (y | x, θ) δ (φ w (c), θ) dθ = P (y | x, θ = φ w (c)) (4) Modeling P w (θ | c) with a delta-function is convenient since the posterior, P w (θ | y, x, c) ∝ P (y | x, θ) δ (φ w (c), θ) also collapses to θ = φ w (c)</formula><p>, hence the inference is done via a single forward pass and the posterior can be regularized by imposing L 1 or L 2 losses on φ w (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Constrained deterministic encoding</head><p>The downside of deterministic encoding is the lack of constraints on the generated explanations. There are multiple reasons why this might be an issue: (i) when the context encoder is unrestricted, it might generate unstable, overfitted local models, (ii) when we want to reason about the patterns in the data as a whole, local explanations are not enough. To address these issues, we constrain the space of explanations by introducing a context-independent, global dictionary, D := {θ k } K k=1 , where each atom, θ k , is sparse. The encoder generates context-specific explanations using soft attention over the dictionary <ref type="table">(Figure 3)</ref>:</p><formula xml:id="formula_4">φ w,D (c) := K k=1 P w (k | c) θ k = α w (c) D, K k=1 α (k) w (c) = 1, ∀k : α (k) w (c) ≥ 0,<label>(5)</label></formula><p>where α w (c) is the attention over the dictionary produced by the encoder. Attention-based construction of explanations using a global dictionary (i) forces the encoder to produce models shared across different contexts, (ii) allows us to interpret the learned dictionary atoms as global "explanation modes." Again, since P w (θ | c) is a delta-distribution, the likelihood is the same as given in (4) and inference is conveniently done via a forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary dot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Context Encoder</p><formula xml:id="formula_5">Attention C Attributes X θ X 1 X 2 X 3 X 4 Y 1 Y 2 Y 3 Y 4</formula><p>Explanation <ref type="figure">Figure 3</ref>: An example of a CEN architecture. In this example, the context is represented by an image and transformed by a convnet encoder into an attention vector, which is used to softly select parameters for a contextual linear probabilistic model.</p><p>The two proposed context encoders represent P (θ | c) with delta-functions, which simplifies learning, inference, and interpretation of the model, and are used in our experiments. Other ways to represent P (θ | c) include: (i) using a mixture of delta-functions (which makes CEN function similar to a mixture-of-experts model and further discussed in Section 5.1), or (ii) using variational autoencoding. We leave more complex approaches to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explanations</head><p>In this paper, we consider two types of explanations: linear that can be used for regression or classification and structured that are suitable for structured prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Linear Explanations</head><p>In case of classification, CENs with linear explanations assume the following P (Y | x, θ):</p><formula xml:id="formula_6">P (Y = i | x, θ) := exp {(Wx + b) i } j∈Y exp {(Wx + b) j } ,<label>(6)</label></formula><p>where θ := (W, b) and i, j index classes in Y. If x is d-dimensional and we are given m-class classification problem, then W ∈ R m×d and b ∈ R m . The case of regression is similar.</p><p>In Section 5.4, we show that if we apply LIME to interpret CEN with linear explanations, the local linear models inferred by LIME are guaranteed to recover the original CENgenerated explanations. In other words, linear explanations generated by CEN have similar properties, e.g., local faithfulness <ref type="bibr" target="#b70">(Ribeiro et al., 2016)</ref>. However, we emphasize the key difference between LIME and CEN: the former regards explanation as a post-processing step (done after training) while the latter integrates explanation into the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Structured Explanations</head><p>While post-hoc methods, such as LIME, can easily generate local linear explanations for scalar outputs, using such methods for structured outputs is non-trivial. At the same time, CENs let us represent P (Y | x, θ) using arbitrary graphical models. To be concrete, we consider the case where the targets are binary vectors, y ∈ {0, 1} m , and explanations are represented by CRFs <ref type="bibr" target="#b52">(Lafferty et al., 2001)</ref> with linear potential functions.</p><p>The predictive distribution P (Y | x, θ) represented by a CRF takes the following form:</p><formula xml:id="formula_7">P (Y | x, θ) := 1 Z θ (x) a∈A Ψ a (y a , x a ; θ)<label>(7)</label></formula><p>where Z θ (x) is the normalizing constant and a ∈ A indexes subsets of variables in x and y that correspond to the factors:</p><formula xml:id="formula_8">Ψ a (y a , x a ; θ) := exp K k=1 θ ak f ak (x a , y a ) ,<label>(8)</label></formula><p>where {f ak (x a , y a )} K k=1 is a collection of feature vectors associated with factor Ψ a (y a , x a ; θ). For interpretability purposes, we are interested in CRFs with feature vectors that are linear or bi-linear in x and y. There is a variety of application-specific CRF models developed in the literature (e.g., see <ref type="bibr" target="#b77">Sutton et al., 2012)</ref>. While in the following section, we discuss learning and inference more generally, in Section 6.3 we develop a CEN model with linear chain CRF explanations for solving survival analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference and Learning</head><p>CENs with deterministic encoders are convenient since the posterior, P (θ | y, x, c), collapses to a point θ = φ(c). Inference in such models is done in two steps: (1) first, compute θ , then (2) using θ as parameters, compute the predictive distribution, P (y | x, θ ). To train the model, we can optimize its log likelihood on the training data. To make a prediction using a trained CEN model, we inferŷ = arg max y P (y | x, θ ). For classification (and regression) computing predictions is straightforward. Below, we show how to compute predictions for CEN with CRF-based explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Inference for CEN with Structured Explanations</head><p>Given a CRF model <ref type="formula" target="#formula_7">(7)</ref>, we can make a predictionŷ for inputs (c, x) by performing inference:</p><formula xml:id="formula_9">y(θ ) = arg max y∈Y P (y | x, θ ) = arg max y∈Y A a=1 K k=1 θ ak f ak (x a , y a )<label>(9)</label></formula><p>Depending on the structure of the CRF model (e.g., linear chain, tree-structured model, etc.), we could use different inference algorithms, such the Viterbi algorithm or variational inference, in order to solve (9) (see Ch. 4, <ref type="bibr" target="#b77">Sutton et al., 2012</ref>, for an overview and examples).</p><p>The key point here is that having P (y | x, θ ) orŷ(θ ) computable in an (approximate) functional form, lets us construct different objective functions, e.g.,</p><formula xml:id="formula_10">L({y i , x i , c i } N i=1 , w)</formula><p>, and learn parameters of the CEN model end-to-end using gradient methods, which are standard in deep learning. In Section 6.3, we construct a specific objective function for survival analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Learning via Likelihood Maximization and Posterior Regularization</head><p>In this paper, we use the negative log likelihood (NLL) objective for learning CEN models:</p><formula xml:id="formula_11">L({y i , x i , c i } N i=1 , w) := 1 N N i=1 log P (y i | x i , θ = φ w (c i ))<label>(10)</label></formula><p>L 1 , L 2 , and other types of regularization imposed on θ can be added to the objective <ref type="bibr" target="#b2">(10)</ref>. Such regularizers, as well as the dictionary constraint introduced in Section 3.1.2, can be seen as a form of posterior regularization <ref type="bibr" target="#b30">(Ganchev et al., 2010)</ref> and are important for achieving the best performance and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>Contextual explanation networks combine multiple threads of research that we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep graphical models</head><p>The idea of combining deep networks with graphical models has been explored extensively. Notable threads of recent work include: replacing task-specific feature engineering with task-agnostic general representations (or embeddings) discovered by deep networks <ref type="bibr" target="#b18">(Collobert et al., 2011;</ref><ref type="bibr" target="#b71">Rudolph et al., 2016</ref><ref type="bibr" target="#b72">Rudolph et al., , 2017</ref>, representing energy functions <ref type="bibr" target="#b11">(Belanger and McCallum, 2016)</ref> and potential functions <ref type="bibr" target="#b39">(Jaderberg et al., 2014)</ref> with neural networks, encoding learnable structure into Gaussian processes with deep and recurrent networks <ref type="bibr" target="#b85">(Wilson et al., 2016;</ref><ref type="bibr" target="#b10">Al-Shedivat et al., 2017)</ref>, or learning state-space models on top of nonlinear embeddings of the observations <ref type="bibr" target="#b31">(Gao et al., 2016;</ref><ref type="bibr" target="#b51">Krishnan et al., 2017)</ref>. The goal of this body of work is to design principled structured probabilistic models that enjoy the flexibility of deep learning. The key difference between CENs and the previous art is that the latter directly integrate neural networks into graphical models as components (embeddings, potential functions, etc.). While flexible, the resulting deep graphical models could no longer be interpreted in terms of crisp relationships between specific variables of interest. 3 CENs, on the other hand, preserve the simplicity of the explanations and shift complexity into conditioning on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context representation</head><p>Generating probabilistic models after conditioning on a context is the key aspect of our approach. Previous work on context-specific graphical models represented contexts with a discrete variable that enumerated a finite number of possible contexts <ref type="bibr">(Koller and Friedman, 2009, Ch. 5.3)</ref>. CENs, on the other hand, are designed to handle arbitrary complex context representations. Context-specific approaches are widely used in language modeling where the context is typically represented with trainable embeddings <ref type="bibr" target="#b71">(Rudolph et al., 2016)</ref>. We also note that few-shot learning explicitly considers a setup where the context is represented by a small set of labeled examples <ref type="bibr" target="#b74">(Santoro et al., 2016;</ref><ref type="bibr" target="#b32">Garnelo et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Meta-learning</head><p>The way CENs operate resembles the meta-learning setup. In meta-learning, the goal is to learn a meta-model which, given a task, can produce another model capable of solving the task <ref type="bibr" target="#b79">(Thrun and Pratt, 1998)</ref>. The representation of the task can be seen as the context while produced task-specific models are similar to CEN-generated explanations. Meta-training a deep network that generates parameters for another network has been successfully used for zero-shot <ref type="bibr" target="#b56">(Lei Ba et al., 2015;</ref><ref type="bibr" target="#b16">Changpinyo et al., 2016)</ref> and few-shot <ref type="bibr" target="#b28">(Edwards and Storkey, 2016;</ref><ref type="bibr" target="#b82">Vinyals et al., 2016)</ref> learning, cold-start recommendations <ref type="bibr" target="#b81">(Vartak et al., 2017)</ref>, and a few other scenarios <ref type="bibr" target="#b12">(Bertinetto et al., 2016;</ref><ref type="bibr" target="#b24">De Brabandere et al., 2016;</ref><ref type="bibr" target="#b34">Ha et al., 2016)</ref>, but is not suitable for interpretability purposes. In contrast, CENs generate parameters for models from a restricted class (potentially, based on domain knowledge) and use the attention mechanism <ref type="bibr" target="#b87">(Xu et al., 2015)</ref> to further improve interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model interpretability</head><p>While there are many ways to define interpretability <ref type="bibr" target="#b58">(Lipton, 2016;</ref><ref type="bibr" target="#b27">Doshi-Velez and Kim, 2017)</ref>, our discussion focuses on explanations defined as simple models that locally approximate behavior of a complex model. A few methods that allow to construct such explanations in a post-hoc manner have been proposed recently <ref type="bibr" target="#b70">(Ribeiro et al., 2016;</ref><ref type="bibr" target="#b75">Shrikumar et al., 2017;</ref><ref type="bibr" target="#b61">Lundberg and Lee, 2017)</ref>, some of which we review in the next section. In contrast, CENs learn to generate such explanations along with predictions. There are multiple other complementary approaches to interpretability ranging from a variety of visualization techniques <ref type="bibr" target="#b64">Mahendran and Vedaldi, 2015;</ref><ref type="bibr" target="#b46">Karpathy et al., 2015)</ref>, to explanations by example <ref type="bibr" target="#b14">(Caruana et al., 1999;</ref><ref type="bibr" target="#b47">Kim et al., 2014</ref><ref type="bibr" target="#b48">Kim et al., , 2016</ref><ref type="bibr" target="#b49">Koh and Liang, 2017)</ref>, to natural language rationales <ref type="bibr" target="#b55">(Lei et al., 2016)</ref>. Finally, our framework encompasses the so-called personalized or instance-specific models that learn to partition the space of inputs and fit local sub-models <ref type="bibr" target="#b83">(Wang and Saligrama, 2012)</ref>.</p><p>or defining potentials via neural networks would result in a more powerful model. However, precise relationships between the variables will be no longer directly readable from the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>In this section, we dive into the analysis of CEN as a class of probabilistic models. First, we mention special cases of CEN model class known in the literature, such as mixture-ofexperts <ref type="bibr" target="#b38">(Jacobs et al., 1991)</ref> and varying-coefficient models <ref type="bibr" target="#b36">(Hastie and Tibshirani, 1993)</ref>. Then, we discuss implications of the CEN structure, a potential failure mode of CEN with deterministic encoders and how to rectify it using conditional entropy regularization, and finally analyze relationship between CEN-generated and post-hoc explanations. Readers who are mostly interested in empirical properties and applications may skip this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Special Cases of CEN</head><p>Mixtures of Experts. So far, we have represented P w (θ | c) by a delta-function centered around the output of the encoder. It is natural to extend P w (θ | c) to a mixture of deltadistributions, in which case CENs recover the mixtures-of-experts model <ref type="bibr">(MoE, Jacobs et al., 1991)</ref>. To see this, let D be a dictionary of experts, and define P w,</p><formula xml:id="formula_12">D (θ | c) := K k=1 P w (k | c) δ(θ, θ k ).</formula><p>The log-likelihood for CEN in such case is the same as for MoE:</p><formula xml:id="formula_13">X1 X2 X3 X4 Y1 Y2 Y3 Y4 Y 1 Y 2 Y 3 Y 4 X1 X2 X3 X4 Y1 Y2 Y3 Y4 X1 X2 X3 X4 Y1 Y2 Y3 Y4 Mixture of Experts dot Attention log P w,D (y i | x i , c i ) = log P (y i |x i , θ) P w,D (θ|c i ) dθ = log K k=1 P w (k|c i ) P (y i |x i , θ k )<label>(11)</label></formula><p>As in Section 3.1.2, P w (k | C) is represented with a soft attention over the dictionary, D, which is now used to combine predictions of the experts with parameters {θ k } K k=1 instead of constructing a single context-specific explanation. Learning of MoE models is done either by optimizing the likelihood or via expectation maximization (EM). Note another difference between CEN and MoE is that the latter assumed that c ≡ x and that both P (y | x, θ) and P (θ | c) can be represented by arbitrary complex model classes, ignoring interpretability. Varying-Coefficient Models. In statistics, there is a class of (generalized) regression models, called varying-coefficient models (VCMs, <ref type="bibr" target="#b36">Hastie and Tibshirani, 1993)</ref>, in which coefficients of linear models are allowed to be smooth deterministic functions of other variables (called the "effect modifiers"). Interestingly, the motivation for VCM was to increase flexibility of linear regression. In the original work, <ref type="bibr" target="#b36">Hastie and Tibshirani (1993)</ref> focused on simple dynamic (temporal) linear models and on nonparametric estimation of the varying coefficients, where each coefficient depended on a different effect variable. CEN generalizes VCM by (i) allowing parameters, θ, to be random variables that depend on the context, c, nondeterministically, (ii) letting the "effect modifiers" to be high-dimensional context variables (not just scalars), and (iii) modeling the effects using deep neural networks. In other words, CEN alleviates the limitations of VCM by leveraging the probabilistic graphical models and deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implications of the Structure of CENs</head><p>CENs represent the predictive distribution in a compound form <ref type="bibr" target="#b57">(Lindsay, 1995)</ref>:</p><formula xml:id="formula_14">P (Y | X, C) = P (Y | X, θ) P (θ | C) dθ</formula><p>and we assume that the data is generated according to</p><formula xml:id="formula_15">Y ∼ P (Y | X, θ), θ ∼ P (θ | C).</formula><p>We would like to understand:</p><p>Can CEN represent any conditional distribution, P (Y | X, C), when the class of explanations is limited ( e.g., to linear models)? If not, what are the limitations?</p><p>Generally, CEN can be seen as a mixture of predictors. Such mixture models could be quite powerful as long as the mixing distribution, P (θ | C), is rich enough. In fact, even a finite mixture exponential family regression models can approximate any smooth d-dimensional density at a rate O(m −4/d ) in the KL-distance <ref type="bibr" target="#b41">(Jiang and Tanner, 1999)</ref>. This result suggests that representing the predictive distribution with contextual mixtures should not limit the representational power of the model. However, there are two caveats:</p><p>(i) In practice, P (θ | C) is limited, since we represent it either with a delta-function, a finite mixture, or a simple distribution parametrized by a deep network.</p><p>(ii) Classical predictive mixtures (including MoE) do not separate input features into two subsets, c and x. We do this intentionally to produce explanations in terms of specific variables of interest that could be useful for interpretability or model diagnostics down the line. However, it could be the case that x contains only some limited information about y, which could limit the predictive power of the full model.</p><p>To address point (i), we consider P (θ | c) that fully factorizes over the dimensions of θ: P (θ | c) = j P (θ j | c), and assume that explanations, P (Y | x, θ), also factorize according to some underlying graph,</p><formula xml:id="formula_16">G Y = (V Y , E Y ).</formula><p>The following proposition shows that in such case P (Y | x, c) inherits the factorization properties of the explanation class.</p><formula xml:id="formula_17">Proposition 1 Let P (θ | c) := j P (θ j | c) and let P (Y | x, θ) factorize according to some graph G Y = (V Y , E Y ). Then, P (Y | x, c) defined by CEN with P (θ | c) encoder and P (Y | x, θ) explanations also factorizes according to G.</formula><p>Proof The statement directly follows from the definition of CEN (see Appendix A.1).</p><p>Remark 2 All encoders, P (θ | c), considered in this paper, including delta functions and their mixtures, fully factorize over the dimensions of θ.</p><p>Remark 3 The proposition has no implications for the case of scalar targets, y. However, in case of structured prediction, regardless of how good the context encoder is, CEN will strictly assume the same set of independencies as given by the explanation class, P (Y | x, θ).</p><p>As indicated in point (ii), CENs assume a fixed split of the input features into context, c, and variables of interest, x, which has interesting implications. Ideally, we would like x to be a good predictor of y in any context c. For instance, following our motivation example (see <ref type="figure">Figure 1</ref>), if c distinguishes between urban and rural areas, x must encode enough information for predicting poverty within urban or rural neighborhoods. However, since the variables of interest are often manually selected (e.g., by a domain expert) and limited, we may encounter the following (not mutually exclusive) situations:</p><p>(a) c may happen to be a strong predictor of y and already contain information available in x (e.g., it is the case when x is derived from c).</p><p>(b) x may happen to be a poor predictor of y, even within the context specified by c.</p><p>In both cases, CEN may learn to ignore x, leading to essentially meaningless explanations.</p><p>In the next section, we show that, if (a) is the case, regularization can help eliminate such behavior. Additionally, if (b) is the case, i.e., x are bad features for predicting y (and for seeking explanation in terms of these features), CEN must indicate that. It turns out that the accuracy of CEN depends on the quality of x, as empirically shown in Section 6.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Conditional Entropy Regularization</head><p>CEN has a failure mode: when the context c is highly predictive of the targets y and the encoder is represented by a powerful model, CEN may learn to rely entirely on the context variables. In such case, the encoder would generate spurious explanations, one for each target class. For example, for binary targets, y ∈ {0, 1}, CEN may learn to always map c to either θ 0 or θ 1 when y is 0 or 1, respectively. In other words, θ (as a function of c) would become highly predictive of y on its own, and hence P (Y | x, θ) ≈ P (Y | θ), i.e., Y would be (approximately) conditionally independent of X given θ. This is problematic from the interpretation point of view since explanations would become spurious, i.e., no longer used to make predictions from the variables of interest.</p><p>Note that such a model would be accurate only when the generated θ is always highly predictive of Y, i.e., when the conditional entropy H(Y | θ) is low. Following this observation, we propose to regularize the model by approximately maximizing H(Y | θ). For a CEN with a deterministic encoder (Sections 3.1.1 and 3.1.2), we can compute an unbiased estimate of H(Y | θ) given a mini-batch of samples from the dataset as follows: In the given expressions, elements of B index training samples (e.g., B represents a minibatch), <ref type="formula" target="#formula_0">(13)</ref> is obtained by using the definition of CEN and marginalizing out θ, <ref type="formula">(14)</ref> is a stochastic estimate that approximates expectations using a mini-batch and samples from P (X | c i ). In practice, approximate samples x from the latter distribution can be obtained either by simply perturbing x i or first learning P (X | C) and then sampling from it. Intuitively, if the predictions are accurate while H(Y | θ) is high, we can be sure that CEN learned to generate contextual θ's that are uncorrelated with the targets but result into accurate conditional models, P (Y | x, θ).</p><formula xml:id="formula_18">H(Y | θ) = P (y, θ) log P (y | θ) dydθ (12) = E (c,x)∼P(C,X) P (y | x, φ(c)) log E x ∼P(X|c) P y | x , φ(c) dy (13) ≈ 1 |B| i∈B P (y | x i , φ(c i )) log   x ∼P(X|c i ) P y | x , φ(c i )   dy (14) C 1 C 2 C 3 C 4 X 1 X 2 (a) C 1 C 2 C 3 C 4 X 1 X 2 (b)</formula><p>An illustration on synthetic data. To illustrate the problem, we consider a toy synthetic 3D dataset with 2 classes that are not separable linearly ( <ref type="figure" target="#fig_2">Figure 4</ref>). The coordinates along the vertical axis C correspond to different contexts, and (X 1 , X 2 ) represent variables of interest. Note we can perfectly distinguish between the two classes by using only the context information. CEN with a dictionary of size 2 learns to select one of the two linear explanations for each of the contexts. When trained without regularization <ref type="figure" target="#fig_2">(Figure 4a</ref>), selected explanations are spurious hyperplanes since each of them is used for points of a single class only. Adding entropy regularization ( <ref type="figure" target="#fig_2">Figure 4b</ref>) makes CEN select hyperplanes that meaningfully distinguish between the classes within different contexts.</p><p>Quantifying contribution of the explanations. Starting from the introduction, we have argued that explanations are meaningful when they are used for prediction. In other words, we would like explanations have a non-zero contribution to the overall accuracy of the model. The following proposition quantifies the contribution of explanations to the predictive performance of entropy-regularized CEN.</p><p>Proposition 4 Let CEN with linear explanations have the expected predictive accuracy</p><formula xml:id="formula_19">E X,θ∼P(X,θ) P Ŷ = Y | X, θ ≥ 1 − ε,<label>(15)</label></formula><p>where ε ∈ (0, 1) is small. Let also the conditional entropy be H(Y | θ) ≥ δ for some δ ≥ 0. Then, the expected contribution of the explanations to the predictive performance of CEN is given by the following lower bound:</p><formula xml:id="formula_20">E X,θ∼P(X,θ) P Ŷ = Y | X, θ − P Ŷ = Y | θ ≥ δ − 1 log |Y| − ε,<label>(16)</label></formula><p>where |Y| denotes the cardinality of the target space.</p><p>Proof The statement follows from Fano's inequality. For details, see Appendix A.2.</p><p>Remark 5 The proposition states that explanations are meaningful (as contextual models) only when CEN is accurate ( i.e., the expected predictive error is less than ε) and the conditional entropy H(Y | θ) is high. High accuracy and low entropy imply spurious explanations. Low accuracy and high entropy imply that x features are not predictive of y within the class of explanations, suggesting to reconsider our modeling assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CEN-generated vs. Post-hoc Explanations</head><p>In this section, we analyze the relationship between CEN-generated and LIME-generated post-hoc explanations. Given a trained CEN, we can use LIME to approximate its decision boundary and compare the explanations produced by both methods. The question we ask:</p><p>How does the local approximation,θ, relate to the actual explanation, θ , generated and used by CEN to make a prediction in the first place?</p><p>For the case of binary 4 classification, it turns out that when the context encoder is deterministic and the space of explanations is linear, local approximations,θ, obtained by solving (1) recover the original CEN-generated explanations, θ . Formally, our result is stated in the following theorem.</p><p>Theorem 6 Let the explanations and the local approximations be in the class of linear models, P (Y = 1 | x, θ) ∝ exp x θ . Further, let the encoder be L-Lipschitz and pick a sampling distribution, π x,c , that concentrates around the point (x, c), such that</p><formula xml:id="formula_21">P πx,c ( z − z &gt; t) &lt; ε(t), where z := (x, c) and ε(t) → 0 as t → ∞.</formula><p>Then, if the loss function is defined as</p><formula xml:id="formula_22">L = 1 K K k=1 (logit {P (Y = 1 | x k , c k )} − logit {P (Y = 1 | x k , θ)}) 2 , (x k , c k ) ∼ π x,c , (17) the solution of (1) concentrates around θ as P πx,c θ − θ &gt; t ≤ δ K,L (t), δ K,L −→ t→∞ 0.</formula><p>Intuitively, by sampling from a distribution sharply concentrated around (x, c), we ensure thatθ will recover θ with high probability. A detailed proof is given in Appendix A.3. This result establishes an equivalence between the explanations generated by CEN and those produced by LIME post-hoc when approximating CEN. Note that when LIME is applied to a model other than CEN, equivalence between explanations is not guaranteed. Moreover, as we further show experimentally, certain conditions such as incomplete or noisy interpretable features may lead to LIME producing inconsistent and erroneous explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Case Studies</head><p>In this section, we move to a number of case studies where we empirically analyze properties of the proposed CEN framework on classification and survival analysis tasks. In particular, we evaluate CEN with linear explanations on a few classification tasks that involve different data modalities of the context (e.g., images or text). For survival prediction, we design CEN architectures with structured explanations, derive learning and inference algorithms, and showcase our models on problems from the healthcare domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Solving Classification using CEN with Linear Explanations</head><p>We start by examining the properties of CEN with linear explanations <ref type="table" target="#tab_0">(Table 1</ref>) on a few classification tasks. Our experiments are designed to answer the following questions:</p><p>(i) When explanation is a part of the learning and prediction process, how does that affect performance of the final predictive model quantitatively? (ii) Qualitatively, what kind of insight can we gain by inspecting explanations? (iii) Finally, we analyze consistency of linear explanations generated by CEN versus those generated using LIME (Ribeiro et al., 2016), a popular post-hoc method.</p><p>Details on our experimental setup, all hyperparameters, and training procedures are given in the tables in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Poverty Prediction</head><p>We consider the problem of poverty prediction for household clusters in Uganda from satellite imagery and survey data. Each household cluster is represented by a collection of 400 × 400 satellite images (used as the context) and 65 categorical variables from living standards measurement survey (used as the interpretable attributes). The task is binary classification of the households into being either below or above the poverty line. We follow the original study of <ref type="bibr" target="#b40">Jean et al. (2016)</ref> and use a VGG-F network (pre-trained on nightlight intensity prediction) to compute 4096-dimensional embeddings of the satellite images on top of which we build contextual models. Note that this datasets is fairly small (500 training and 142 test points), and so we keep the VGG-F part frozen to avoid overfitting.   <ref type="figure">Figure 3</ref>). Finally, we evaluate a mixture-of-experts (MoE) model of the same architecture as CEN, since it is a special case (see Section 5.1). Both CEN and MoE are trained with the dictionary constraint and L 1 regularization over the dictionary elements to encourage sparse explanations.</p><p>Performance. The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. Both in terms of accuracy and AUC, CEN models outperform both simple logistic regression and vanilla MLP. Even though the results suggest that categorical features are better predictors of poverty than VGG-F embeddings of images, note that using embeddings to contextualize linear models reduces the error. This indicates that different linear models are optimal in different contexts.</p><p>Qualitative analysis. We have discovered that, on this task, CEN encoder tends to sharply select one of the two explanations from the dictionary (denoted M1 and M2) for different household clusters in Uganda <ref type="figure" target="#fig_3">(Figure 5a</ref>). In the survey data, each household cluster is marked as either urban or rural. Conditional on a satellite image, CEN tends to pick M1 more often for urban areas and M2 for rural ( <ref type="figure" target="#fig_3">Figure 5b</ref>). Notice that different explanations weigh categorical features, such as reliability of the water source or the proportion of houses with walls made of unburnt brick, quite differently. When visualized on the map, we see that CEN selects M1 more frequently around the major city areas <ref type="figure" target="#fig_3">(Figures 5c)</ref>, which also correlates with high nightlight intensity in those areas <ref type="figure" target="#fig_3">(Figures 5d)</ref>.</p><p>The estimated approximate conditional entropy of the binary targets (poor vs. not poor) given the selected model: H(Y | θ = M1) ≈ 77% and H(Y | θ = M2) ≈ 72%. The high performance of CEN along with high conditional entropy makes us confident in the produced explanations (Section 5.3) and allows us to draw conclusions about what causes the model to classify certain households in different neighborhoods as poor in terms of interpretable categorical variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Sentiment Analysis</head><p>The next problem we consider is sentiment prediction of IMDB reviews <ref type="bibr" target="#b63">(Maas et al., 2011b)</ref>. The reviews are given in the form of English text (sequences of words) and the sentiment labels are binary (good/bad movie). This dataset has 25k labelled reviews used for training and validation, 25k labelled reviews that are held out for test, and 50k unlabelled reviews. <ref type="formula" target="#formula_6">(2016)</ref>, we use a bi-directional LSTM with maxpooling as our baseline that predicts sentiment directly from text sequences. The same architecture is used as the context encoder in CEN that produces parameters for linear explanations. The explanations are applied to either (a) a bag-of-words (BoW) features (with a vocabulary limited to 2,000 most frequent words excluding English stop-words) or (b) a 200-dimensional topic representation produced by a separately trained off-the-shelf topic model <ref type="bibr" target="#b13">(Blei et al., 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models. Following Johnson and Zhang</head><p>Performance. <ref type="table">Table 3</ref> compares CEN with other models from the literature. Not only CEN achieves the state-of-the-art accuracy on this dataset in the supervised setting, it also outperforms or comes close to many of the semi-supervised methods. This indicates that the inductive biases provided by the CEN architecture lead to a more significant performance improvement than most of the semi-supervised training methods on this dataset. We also remark that classifiers derived from large-scale language models pretrained on massive unsupervised corpora (e.g., <ref type="bibr" target="#b33">Gray et al., 2017;</ref><ref type="bibr" target="#b37">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b86">Xie et al., 2019)</ref> have become popular and now dominate the leaderboard for this task.</p><p>Qualitative analysis. After training CEN-tpc with linear explanations in terms of topics on the IMDB dataset, we generate explanations for each test example and visualize histograms of the weights assigned by the explanations to the 6 selected topics in <ref type="figure" target="#fig_5">Figure 6</ref>. The 3 topics in the top row are acting-and plot-related (and intuitively have positive, negative, or neutral connotation), while the 3 topics in the bottom are related to particular genre of the movies. Note that acting-related topics turn out to be bimodal, i.e., contributing either positively, negatively, or neutrally to the sentiment prediction in different contexts. CEN assigns a high negative weight to the topic related to "bad acting/plot" and a high positive <ref type="table">Table 3</ref>: Sentiment classification error rate on IMDB dataset. The standard error (±) is based on 5 different runs. It is interesting to note that CENs establishes a new state of the art performance on the supervised prediction task while also outperforming or coming close to many of the semisupervised methods that used additional 50k unlabeled reviews for pretraining. All current state of the art methods leverage large-scale pretraining (the bottom section of the table); these results are not directly comparable with methods trained on IMDB data only and included for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Method Error ↓ (%)</p><p>Supervised (trained on 25K labeled reviews only) weight to "great story/performance" in most of the contexts (and treats those neutrally conditional on some of the reviews). Interestingly, genre-related topics almost always have a negligible contribution to the sentiment which indicates that the learned model does not have any particular bias towards or against a given genre. Bad acting/plot: <ref type="bibr">['script', 'acting', 'bad', 'plot', 'film']</ref> −5 0 5</p><p>Great story/performance: <ref type="bibr">['great', 'story', 'film', 'brilliant']</ref> −5 0 5</p><p>A movie one has seen: <ref type="bibr">['just', 'movie', 'watched', 'good']</ref> 0.000 0.001 10 1 10 3</p><p>Bollywood movies: <ref type="bibr">['bollywood', 'indian', 'action', 'kumar']</ref> 0.000 0.001 0.002 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Image Classification</head><p>For the purpose of completeness, we also provide results on two classical image datasets: MNIST and CIFAR-10. For CEN, full images are used as the context; to imitate high-level features, we use (a) the original images cubically downscaled to 20 × 20 pixels, gray-scaled and normalized, and (b) HOG descriptors computed using 3 × 3 blocks <ref type="bibr" target="#b23">(Dalal and Triggs, 2005)</ref>. For each task, we use linear regression and vanilla convolutional networks as baselines (a small convnet for MNIST and VGG-16 for CIFAR-10). The results are reported in <ref type="table" target="#tab_3">Table 4</ref>. CENs are competitive with the baselines and do not exhibit deterioration in performance. Visualization and analysis of the learned explanations is given in Appendix B.2 and the details on the architectures, hyperparameters, and training are given in Appendix B.3  <ref type="figure">Figure 7</ref>: Analysis of the behavior of different CEN models with different dictionary sizes (varied between 1 and 512), feature types, trained on full or on a subset of the data. Shaded regions denote 95% CI based on 5 runs with different random seeds. (a) CEN is sensitive to the size of the dictionary-there is a critical size such that models with explanation dictionaries smaller than that tend to significantly underperform. (b) Sample complexity of CENs. Models are trained with early stopping based on validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Properties of Explanations</head><p>In this section, we look at the explanations from the regularization and consistency point of view. As we show next, prediction via explanation not only has a strong regularization effect, but also always produces consistent locally linear models. Additionally, we analyze the effect of entropy regularization, quantify how much CEN's performance relies on explanations, and discuss computational considerations and tradeoffs for CEN and LIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Explanations as a Regularizer</head><p>By controlling the dictionary size, we can control the expressivity of the model class specified by CEN. For example, when the dictionary size is 1, CEN becomes equivalent to a linear model. 5 For larger dictionaries, CEN becomes as flexible as a deep network <ref type="figure">(Figure 7a</ref>). Adding a small sparsity penalty to each element of the dictionary (between 10 −6 and 10 −3 , see Appendix B.3) helps to avoid overfitting for very large dictionary sizes, so that the model learns to use only a few dictionary atoms for prediction while shrinking the rest to zero. Generally, dictionary size is a hyperparameter which optimal value depends on the data and the type of the interpretable features (cf., CEN-bow and CEN-tpc on <ref type="figure">Figure 7a</ref>). If explanations can act as a proper regularizer, we must observe improved sample efficiency of the model. To verify this, we trained CEN models on subsets of the data (size varied between 1% and 30% for MNIST and 2% and 50% for IMDB) with early stopping based on the validation performance. The test error on MNIST and IMDB for different training set sizes is presented on <ref type="figure">Figure 7b</ref>. On the IMBD dataset, CEN-tpc required an 5. Note that CENs with the dictionary size of 1 is still trained using stochastic optimization method as a neural network, which tends to yield a somewhat worse performance than the vanilla logistic regression.   order of magnitude fewer samples to match the baseline's performance, indicating efficient use of explanations for prediction. Note that such drastic sample efficiency gains were observed on IMDB only for CEN-tpc (i.e., when using topics as interpretable features); gains for CEN-bow were noticeable but moderate; no sample efficiency gains were observed on MNIST for any of our CEN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Quantifying Contribution of the Explanations</head><p>Even though improved sample efficiency and regularizing effects of explanations indicate their non-trivial contribution indirectly, we wish to further quantify such contribution of explanations to the predictive performance of CEN. To do so, we run a set of experiments where we vary conditional entropy regularization coefficient and measure (a) performance of CEN on the validation set and (b) expected lower bound on the relative reduction of predictive error due to explanations, defined as</p><formula xml:id="formula_23">P Ŷ = Y | c − P Ŷ = Y | x, c P Ŷ = Y | c .</formula><p>As we have shown in Section 5.3, conditional entropy regularization encourages CEN models to learn context representations that are minimally correlated with the targets, and hence makes the model rely on the explanations rather than contextual information only. <ref type="figure" target="#fig_8">Figure 8a</ref> shows that entropy regularization generally does not affect predictive performance of a CEN model, unless the regularization coefficient becomes too large (e.g., an order of magnitude larger than the predictive cross-entropy loss). Increasing conditional entropy regularization leads to CEN models whose performance relies more on explanations <ref type="figure" target="#fig_8">(Figure 8b</ref>). However, note that even without entropy regularization, explanations have a significant relative contribution to the reduction of the predictive error of CEN, ranging between 10-20% on MNIST and 40-60% on IMDB. This indicates that, while conditional entropy regularization is beneficial, even without it CEN still learns to generate meaningful, non-spurious explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Consistency of Explanations</head><p>While regularization is a useful aspect, the main use case for explanations is model diagnostics. Linear explanations assign weights to the interpretable features, X, and thus the quality of explanations depends on the quality of the selected features. In this section, we evaluate explanations generated by CEN and LIME (a post-hoc method). In particular, we consider two cases: (a) the features are corrupted with additive noise, and (b) the selected features are incomplete. For analysis, we use MNIST and IMDB datasets. Our key question is:  The effect of noisy features. In this experiment, we inject noise 6 into the features X and ask LIME and CEN to fit explanations to the corrupted features. Note that after injecting noise, each data point has a noiseless representation C and a noisy X. LIME constructs explanations by approximating the decision boundary of the baseline model trained to predict Y from C features only. CEN is trained to construct explanations given C and then make predictions by applying explanations to X. The predictive performance of the produced explanations on noisy features is given on <ref type="figure" target="#fig_10">Figure 9a</ref>. Since baselines take only C as inputs, their performance stays the same (dashed line) Regardless of the noise level, LIME "successfully" overfits explanations-it is able to almost perfectly approximate the decision boundary of the baselines essentially using pure noise. On the other hand, performance of CEN degenerates with the increasing noise level indicating that the model fails to learn when the selected interpretable representation is of very low quality.</p><p>The effect of feature selection. Using the same setup, instead of injecting noise into X, we construct X by randomly subsampling a set of dimensions. 7 <ref type="figure" target="#fig_10">Figure 9b</ref> demonstrates that 6. We use Gaussian noise with zero mean and select variance for each signal-to-noise ratio level appropriately. 7. Subsampling dimensions from X is done to resemble human subjectivity in selecting semantically meaningful features for model interpretation.</p><p>while performance of CENs degrades proportionally to the size of X (i.e., less informative features imply worse performance for CEN), we see that, again, LIME is again able to perfectly fit explanations to the decision boundary of the original models, despite the loss of information in the interpretable features X.</p><p>These two experiments indicate a major drawback of explaining predictions post-hoc: when constructed on poor, noisy, or incomplete features, such explanations can overfit an arbitrary decision boundary of a predictor and are likely to be meaningless or misleading. For example, predictions of a perfectly valid model might end up getting absurd explanations which is unacceptable from the decision support point of view. 8 On the other hand, if we use CEN to generate explanations, high predictive performance would indicate presence of a meaningful signal the selected interpretable features and explanations.  <ref type="table" target="#tab_4">(Table 5</ref>). Note that the models we used in our experiments are tiny by the modern standards, and we expect CEN's relative compute overhead to be even smaller for modern large-scale architectures. Also note that CENs generate explanations more than three orders of magnitude faster than LIME, manly because the latter has to solve an optimization problem for each instance of interest to obtain an explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Computational Overhead and Considerations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Solving Survival Analysis using CEN with Structured Explanations</head><p>In this final case study, we design CENs with structured explanations for survival prediction.</p><p>We provide some general background on survival analysis and the structured prediction approach proposed by <ref type="bibr" target="#b89">Yu et al. (2011)</ref>, then introduce CENs with linear CRF-based explanations for survival analysis, and conclude with experimental results on two public datasets from the healthcare domain.</p><p>8. Similar behavior has been observed in recent work that studied post-hoc explanation systems in adversarial settings <ref type="bibr" target="#b26">(Dombrowski et al., 2019;</ref><ref type="bibr" target="#b53">Lakkaraju and Bastani, 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Background on Survival Analysis via Structured Prediction</head><p>In survival time prediction, our goal is to estimate the risk and occurrence time of an undesirable event in the future (e.g., death of a patient, earthquake, hard drive failure, customer turnover, etc.). A common approach is to model the survival time, T , either for a population (i.e., average survival time) or for each instance. Classical approaches, such as Aalen additive hazard <ref type="bibr" target="#b8">(Aalen, 1989)</ref> and Cox proportional hazard <ref type="bibr" target="#b20">(Cox, 1972)</ref> models, view survival analysis as continuous time prediction and hence a regression problem. Alternatively, the time can be discretized into intervals (e.g., days, weeks, etc.), and the survival time prediction can be converted into a multi-task classification problem <ref type="bibr" target="#b29">(Efron, 1988)</ref>. Taking this approach one step further, <ref type="bibr" target="#b89">Yu et al. (2011)</ref> noticed that the output space of such a multitask classifier is structured in a particular way, and proposed a model called sequence of dependent regressors. The model is essentially a CRF with a particular structure of the pairwise potentials between the labels. We introduce the setup in our notation below.</p><p>Let the data instances be represented by tuples (c, x, y), where targets are now sequences of m binary variables, y := (y 1 , . . . , y m ), that indicate occurrence of an event at the corresponding time intervals. 9 If the event occurred at time t ∈ [t i , t i+1 ), then y j = 0, ∀j ≤ i and y k = 1, ∀k &gt; i. If the event was censored (i.e., we lack information for times after t), we represent targets (y i+1 , . . . , y m ) with latent variables. Importantly, only m + 1 sequences are valid under these conditions, i.e., assigned non-zero probability by the model. This suggests a linear CRF model defined as follows:</p><formula xml:id="formula_24">P Y = (y 1 , y 2 , . . . , y m ) | x, θ 1:m ∝ exp m t=1 y i (x θ t ) + ω(y t , y t+1 )<label>(18)</label></formula><p>The potentials between x and y 1:m are linear functions parameterized by θ 1:m . The pairwise potentials between targets, ω(y i , y i+1 ), ensure that non-permissible configurations where (y i = 1, y i+1 = 0) for some i ∈ {0, . . . , m − 1} are improbable (i.e., ω(1, 0) = −∞ and ω(0, 0) = ω 00 , ω(0, 1) = ω 01 , ω(1, 1) = ω 10 are learnable parameters). To train the model, <ref type="bibr" target="#b89">Yu et al. (2011)</ref> optimize the following objective:</p><formula xml:id="formula_25">min Θ C 1 m t=1 θ t 2 + C 2 m−1 t=1 θ t+1 − θ t 2 − log L(Y, X; θ 1:m )<label>(19)</label></formula><p>where the first two terms are regularization and the last term is the log of the likelihood:</p><formula xml:id="formula_26">L(Y, X; Θ) = i∈NC P (T = t i | x i , Θ) + j∈C P (T &gt; t j | x j , Θ)<label>(20)</label></formula><p>where NC denotes the set of non-censored instances (for which we know the outcome times, t i ) and C is the set of censored inputs (for which we only know the censorship times, t j ).</p><formula xml:id="formula_27">c h 1 h 2 h 3 x 1 x 2 x 3 y 1 y 2 y 3 θ 1 θ 2 θ 3 t ∈ [t2, t3) (a) Architecture used for SUPPORT2. c1 c2 c3 h1 h2 h3 h 1 h 2 h 3 x 1 x 2 x 3 y 1 y 2 y 3 θ 1 θ 2 θ 3 t ∈ [t2, t3) (b)</formula><p>Architecture used for PhysioNet. The likelihood of an uncensored and a censored event at time t ∈ [t j , t j+1 ) are as follows:</p><formula xml:id="formula_28">P T = t | x, θ 1:m = exp    m i=j x θ i    m k=0 exp m i=k+1 x θ i P T ≥ t | x, θ 1:m = m k=j+1 exp m i=k+1 x θ i m k=0 exp m i=k+1</formula><p>x θ i (21)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">CEN with Structured Explanations for Survival Analysis</head><p>To construct CEN for survival analysis, we follow the structured survival prediction setup described in the previous section. We define CEN with linear CRF explanations as follows:</p><formula xml:id="formula_29">θ t ∼ P w θ t | c , y ∼ P Y | x, θ 1:m , P Y = (y 1 , y 2 , . . . , y m ) | x, θ 1:m ∝ exp m t=1 y i (x θ t ) + ω(y t , y t+1 ) , P w θ t | c := δ(θ t , φ t w,D (c)), φ t w,D (c) := α(h t ) D, h t := RNN(h t−1 , c)<label>(22)</label></formula><p>Note that an RNN-based context encoder generates different explanations for each time point, θ t <ref type="figure" target="#fig_11">(Figure 10</ref>). All θ t are generated using context-and time-specific attention α(h t ) over the dictionary D. We adopt the training objective from <ref type="formula" target="#formula_9">(19)</ref> with the same likelihood (20). The model is a special case of CENs with structured explanations (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Survival Analysis of Patients in Intense Care Units</head><p>We evaluate the proposed model against baselines on two survival prediction tasks. Datasets. We use two publicly available datasets for survival analysis of of the intense care unit (ICU) patients: (a) SUPPORT2, 10 and (b) data from the PhysioNet 2012 challenge. 11</p><p>The data was preprocessed and used as follows. SUPPORT2: The data had 9105 patient records (7105 training, 1000 validation, 1000 test) and 73 variables. We selected 50 variables for both C and X features (i.e., the context and the variables of interest were identical). Categorical features (such as race or sex) were one-hot encoded. The values of all features were non-negative, and we filled the missing values with -1 to preserve the information about missingness. For CRF-based predictors, we capped the survival timeline at 3 years and converted it into 156 discrete 7-day intervals.</p><p>PhysioNet: The data had 4000 patient records, each represented by a 48-hour irregularly sampled 37-dimensional time-series of different measurements taken during the patient's stay at the ICU. We resampled and mean-aggregated the time-series at 30 min frequency. This resulted in a large number of missing values that we filled with 0. The resampled time-series were used as the context, C. For the attributes, X, we took the values of the last available measurement for each variable in the series. For CRF-based predictors, we capped the survival timeline at 60 days and converted into 60 discrete intervals.</p><p>Models. For baselines, we use the classical Aalen and Cox models 12 and the CRF from <ref type="bibr" target="#b89">(Yu et al., 2011)</ref>. All the baselines used X as their inputs. Next, we combine CRFs with neural encoders in two ways:</p><p>(i) We apply CRFs to the outputs from the neural encoders (the models denoted MLP-CRF and LSTM-CRF). <ref type="bibr" target="#b0">13</ref> Note that parameters of such CRF layer assign weights to the latent features and are not interpretable in terms of the attributes of interest. (ii) We use CENs with CRF-based explanations, that process the context variables, C, using the same neural networks as in (i) and output the sequence of parameters θ 1:m for CRFs, while the latter act on the attributes, X, to make structured predictions.  <ref type="figure">Figure 11</ref>: Weights of the CEN-generated CRF explanations for two patients from SUPPORT2 dataset for a set of the most influential features: dementia (comorbidity), avtisst (avg. TISS, days 3-25), slos (days from study entry to discharge), hday (day in hospital at study admit), ca_yes (the patient had cancer), sfdm2_Coma or Intub (intubated or in coma at month 2), sfdm2_SIP (sickness impact profile score at month 2). Higher weight values correspond to higher contributions to the risk of death after a given time.</p><p>More details on the architectures and training are given in Appendix B.3.</p><p>Metrics. Following <ref type="bibr" target="#b89">Yu et al. (2011)</ref>, we use two metrics specific to survival analysis: (a) Accuracy of correctly predicting survival of a patient at times that correspond to 25%, 50%, and 75% population-level temporal quantiles (i.e., the time points such that the corresponding % of the population in the data were discharged from the study due to censorship or death). Performance. The results for all models are given in <ref type="table" target="#tab_5">Table 6</ref>. Our implementation of the CRF baseline slightly improves upon the performance reported by <ref type="bibr" target="#b89">Yu et al. (2011)</ref>. MLP-CRF and LSTM-CRF improve upon plain CRFs but, as we noted, can no longer be interpreted in terms of the original variables. CENs outperform or closely match neural CRF models on all metrics while providing interpretable explanations for the predicted risk for each patient at each point in time.</p><p>Qualitative analysis. To inspect predictions of CENs qualitatively, for any given patient, we can visualize the weights assigned by the corresponding explanation to the respective attributes. <ref type="figure">Figure 11</ref> shows weights of the explanations for a subset of the most influential features for two patients from SUPPORT2 dataset who were predicted as survivor/non-survivor. These temporal charts help us (a) to better understand which features the model selects as the most influential at each point in time, and (b) to identify potential inconsistencies in the model or the data-for example, using a chart as in <ref type="figure">Figure 11</ref> we identified and excluded a feature (hospdead) from SUPPORT2 data, which initially was included but leaked information about the outcome as it directly indicated in-hospital death. Finally, explanations also allow us to better understand patient-specific temporal dynamics of the contributing factors to the survival rates predicted by the model <ref type="figure" target="#fig_1">(Figure 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have introduced contextual explanation networks (CENs)-a class of models that learn to predict by generating and leveraging intermediate context-specific explanations. We have formally defined CENs as a class of probabilistic models, considered a number of special cases (e.g., the mixture-of-experts model), and derived learning and inference algorithms within the encoder-decoder framework for simple and sequentially-structured outputs. We have shown that there are certain conditions when post-hoc explanations are erroneous and misleading. Such cases are hard to detect unless explanation is a part of the prediction process itself, as in CEN. Finally, learning to predict and to explain jointly turned out to have a number of benefits, including strong regularization, consistency, and ability to generate explanations with no computational overhead, as shown in our case studies.</p><p>We would like to point out a few limitations of our approach and potential ways of addressing those in the future work. Firstly, while each prediction made by CEN comes with an explanation, the process of conditioning on the context is still uninterpretable. Ideas similar to context selection <ref type="bibr" target="#b59">(Liu et al., 2017)</ref> or rationale generation <ref type="bibr" target="#b55">(Lei et al., 2016)</ref> may help improve interpretability of the conditioning. Secondly, the space of explanations considered in this work assumes the same graphical structure and parameterization for all explanations and uses a simple sparse dictionary constraint. This might be limiting, and one could imagine using a more hierarchically structured space of explanations instead, bringing to bear amortized inference techniques <ref type="bibr" target="#b72">(Rudolph et al., 2017)</ref>. Nonetheless, we believe that the proposed class of models is useful not only for improving prediction capabilities, but also for model diagnostics, pattern discovery, and general data analysis, especially when machine learning is used for decision support in high-stakes applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>We thank Willie Neiswanger and Mrinmaya Sachan for many useful comments on an early draft of the paper, and Ahmed Hefny, Shashank J. Reddy, Bryon Aragam, and Ruslan Salakhutdinov for helpful discussions. This work was supported by NIH R01GM114311. M.A. was supported in part by the CMLH Fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Proposition 1</head><p>Assume that P (Y | X, θ) factorizes as a∈V Y P Y a | Y MB(a) , X, θ a , where a denotes subsets of the Y variables and MB(a) stands for the corresponding Markov blankets. Using the definition of CEN given in (3), we have:</p><formula xml:id="formula_30">P (Y | X, C) = P (Y | X, θ) P (θ | C) dθ = a∈V Y P Y a | Y MB(a) , X, θ a j P (θ j | C) dθ = a∈V Y   P Y a | Y MB(a) , X, θ a j∈a P (θ j | C) dθ a   = a∈V Y P Y a | Y MB(a) , X, C (A.1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 4</head><p>To derive the lower bound on the contribution of explanations in terms of expected accuracy, we first need to bound the probability of the error when only θ are used for prediction:</p><formula xml:id="formula_31">P e := P Ŷ (θ) = Y = E θ∼P(θ) P Ŷ = Y | θ ,</formula><p>which we bound using the Fano's inequality <ref type="bibr">(Ch. 2.11, Cover and Thomas, 2012)</ref>:</p><formula xml:id="formula_32">H (P e ) + P e log (|Y| − 1) ≥ H (Y | θ) (A.2)</formula><p>Since the error (Ŷ(θ) = Y) is a binary random variable, then H (P e ) ≤ 1. After weakening the inequality and using H (Y | θ) ≥ δ from the proposition statement, we get:</p><formula xml:id="formula_33">E θ∼P(θ) P Ŷ = Y | θ ≥ H (Y | θ) − 1 log |Y| ≥ δ − 1 log |Y| (A.3)</formula><p>The claimed lower bound (16) follows after we combine (A.3) and the assumed bound on the accuracy of the model in terms of ε given in (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 6</head><p>To prove the theorem, consider the case when f is defined by a CEN, instead of x we have (c, x), and the class of approximations, G, coincides with the class of explanations, and hence can be represented by θ. In this setting, we can pose the same problem as:</p><formula xml:id="formula_34">θ = arg min θ L(f, θ, π c,x ) + Ω(θ) (A.4)</formula><p>Suppose that CEN produces θ explanation for the context c using a deterministic encoder, φ. The question is whether and under which conditionsθ can recover θ . Theorem 6 answers the question in affirmative and provides a concentration result for the case when hypotheses are linear. Here, we prove Theorem 6 for a little more general class of log-linear explanations:</p><formula xml:id="formula_35">logit {P (Y = 1 | x, θ)} = a(x) θ,</formula><p>where a is a C-Lipschitz vector-valued function whose values have a zero-mean distribution when (x, c) are sampled from π x,c 14 . For simplicity of the analysis, we consider binary classification and omit the regularization term, Ω(g). We define the loss function, L(f, θ, π x,c ), as:</p><formula xml:id="formula_36">L = 1 K K k=1 (logit {P (Y = 1 | x k − x, c k )} − logit {P (Y = 1 | x k − x, θ)}) 2 (A.5)</formula><p>where (x k , c k ) ∼ π x,c and π x,c := π x π c is a distribution concentrated around (x, c). Without loss of generality, we also drop the bias terms in the linear models and assume that a(x k − x) are centered.</p><p>Proof The optimization problem (A.4) reduces to the least squares linear regression:</p><formula xml:id="formula_37">θ = arg min θ 1 K K k=1 logit {P (Y = 1 | x k − x, c k )} − a(x k − x) θ 2 (A.6)</formula><p>We consider deterministic encoding, P (θ | c) := δ(θ, φ(c)), and hence we have:</p><formula xml:id="formula_38">logit {P (Y = 1 | x k − x, c k )} = logit {P (Y = 1 | x k − x, θ = φ(c k ))} = a(x k − x) φ(c k ) (A.7)</formula><p>To simplify the notation, we denote a k := a(x k − x), φ k := φ(c k ), and φ := φ(c). The solution of (A.6) now can be written in a closed form:</p><formula xml:id="formula_39">θ = 1 K K k=1 a k a k + 1 K K k=1 a k a k φ k (A.8)</formula><p>Note thatθ is a random variable since (x k , c k ) are randomly generated from π x,c . To further simplify the notation, denote M := 1 K K k=1 a k a k . To get a concentration bound on θ − θ , we will use the continuity of φ(·) and a(·), concentration properties of π x,c around (x, c), and some elementary results from random matrix theory. To be more concrete, since we assumed that π x,c factorizes, we further let π x and π c concentrate such that P πx ( x − x &gt; t) &lt; ε x (t) and P πc ( c − c &gt; t) &lt; ε c (t), respectively, where ε x (t) and ε c (t) both go to 0 as t → ∞, potentially at different rates. First, we have the following bound from the convexity of the norm:</p><formula xml:id="formula_40">P θ − θ &gt; t = P 1 K K k=1 M + a k a k (φ k − φ) &gt; t (A.9) ≤ P 1 K K k=1 M + a k a k (φ k − φ) &gt; t (A.10)</formula><p>By making use of the inequality Ax ≤ A x , where A denotes the spectral norm of the matrix A, the L-Lipschitz property of φ(c), the C-Lipschitz property of a(x), and the concentration of x k around x, we have</p><formula xml:id="formula_41">P θ − θ &gt; t ≤ P L 1 K K k=1 M + a k a k c k − c &gt; t (A.11) ≤ P CL M + 1 K K k=1 a k a k c k − c &gt; t (A.12) ≤ P CL λ min (M ) 1 K K k=1 x k − x c k − c &gt; t (A.13) ≤ P CLτ 2 λ min (M ) &gt; t + P x k − x c k − c &gt; τ 2 (A.14) ≤ P λ min M/(Cτ ) 2 &lt; L C 2 t + ε x (τ ) + ε c (τ ) (A.15)</formula><p>Note that we used the fact that the spectral norm of a rank-1 matrix, a(x k )a(x k ) , is simply the norm of a(x k ), and the spectral norm of the pseudo-inverse of a matrix is equal to the inverse of the least non-zero singular value of the original matrix:</p><formula xml:id="formula_42">M + ≤ λ max (M + ) = λ −1 min (M ).</formula><p>Finally, we need a concentration bound on λ min M/(Cτ ) 2 to complete the proof.</p><formula xml:id="formula_43">Note that M C 2 τ 2 = 1 K K k=1 a k Cτ a k Cτ</formula><p>, where the norm of a k Cτ is bounded by 1. If we denote µ min (Cτ ) the minimal eigenvalue of Cov a k Cτ , we can write the matrix Chernoff inequality (Tropp, 2012) as follows:</p><formula xml:id="formula_44">P λ min M/(Cτ ) 2 &lt; α ≤ d exp {−KD(α µ min (Cτ ))} , α ∈ [0, µ min (Cτ )]</formula><p>where d is the dimension of a k , α := L C 2 t , and D(a b) denotes the binary information divergence:</p><formula xml:id="formula_45">D(a b) = a log a b + (1 − a) log 1 − a 1 − b .</formula><p>The final concentration bound has the following form:</p><formula xml:id="formula_46">P θ − θ &gt; t ≤ d exp −KD L C 2 t µ min (Cτ ) + ε x (τ ) + ε c (τ ) (A.16)</formula><p>We see that as τ → ∞ and t → ∞ all terms on the right hand side vanish, and henceθ concentrates around θ . Note that as long as µ min (Cτ ) is far from 0, the first term can be made negligibly small by sampling more points around (x, c). Finally, we set τ ≡ t and denote the right hand side by δ K,L,C (t) that goes to 0 as t → ∞ to recover the statement of the original theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 7</head><p>We have shown thatθ concentrates around θ under mild conditions. With more assumptions on the sampling distribution, π x,c , (e.g., sub-gaussian) one could derive precise convergence rates. Note that we are in total control of any assumptions we put on π x,c since precisely that distribution is used for sampling. This is a major difference between the local approximation setup here and the setup of linear regression with random design; in the latter case, we have no control over the distribution of the design matrix, and any assumptions we make could potentially be unrealistic.</p><p>Remark 8 Note that concentration analysis of a more general case when the loss L is a general convex function and Ω(g) is a decomposable regularizer could be done by using results from the M-estimation theory <ref type="bibr" target="#b66">(Negahban et al., 2009</ref>), but would be much more involved and unnecessary for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Experimental Details</head><p>This section provides details on the experimental setups including architectures, training procedures, etc. Additionally, we provide and discuss qualitative results for CENs on the MNIST and IMDB datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Additional Details on the Datasets and Experiment Setups</head><p>MNIST. We used the classical split of the dataset into 50k training, 10k validation, and 10k testing points. All models were trained for 100 epochs using the AMSGrad optimizer <ref type="bibr" target="#b69">(Reddi et al., 2019)</ref> with the learning rate of 10 −3 . No data augmentation was used in any of our experiments. HOG representations were computed using 3 × 3 blocks.</p><p>CIFAR10. For this set of experiments, we followed the setup given by <ref type="bibr" target="#b90">Zagoruyko (2015)</ref>, reimplemented in Keras <ref type="bibr" target="#b17">(Chollet et al., 2015)</ref> with TensorFlow <ref type="bibr" target="#b9">(Abadi et al., 2016)</ref> backend. The input images were global contrast normalized (a.k.a. GCN whitened) while the rescaled image representations were simply standardized. Again, HOG representations were computed using 3 × 3 blocks. No data augmentation was used in our experiments.</p><p>IMDB. We considered the labeled part of the data only (50,000 reviews total). The data were split into 20,000 train, 5,000 validation, and 25,000 test points. The vocabulary was limited to 20,000 most frequent words (and 5,000 most frequent words when constructing BoW representations). All models were trained with the AMSGrad optimizer () with 10 −2 learning rate. The models were initialized randomly; no pre-training or any other unsupervised/semi-supervised technique was used.</p><p>Satellite. As described in the main text, we used a pre-trained VGG-16 network 15 to extract features from the satellite imagery. Further, we added one fully connected layer network with 128 hidden units used as the context encoder. For the VCEN model, we used dictionary-based encoding with Dirichlet prior and logistic normal distribution as the output of the inference network. For the decoder, we used an MLP of the same architecture as the encoder network. All models were trained with Adam optimizer with 0.05 learning rate. The results were obtained by 5-fold cross-validation.</p><p>Medical data. We have used minimal pre-processing of both SUPPORT2 and PhysioNet datasets limited to standardization and missing-value filling. We found that denoting missing values with negative entries (−1) often led a slightly improved performance compared to any other NA-filling techniques. PhysioNet time series data was irregularly sampled across the time, so we had to resample temporal sequences at regular intervals of 30 minutes (consequently, this has created quite a few missing values for some of the measurements). All models were trained using Adam optimizer with 10 −2 learning rate. <ref type="figure" target="#fig_15">Figures 13a, 13b</ref>, and 13c visualize explanations for predictions made by CEN-pxl on MNIST. The figures correspond to 3 cases where CEN (a) made a correct prediction, (b) made a mistake, and (c) was applied to an adversarial example (and made a mistake). Each chart consists of the following columns: true labels, input images, explanations for the top 3 classes (as given by the activation of the final softmax layer), and attention vectors used to select explanations from the global dictionary. A small subset of explanations from the dictionary is visualized in <ref type="figure" target="#fig_15">Figure 13d</ref> (the full dictionary is given in <ref type="figure" target="#fig_2">Figure 14)</ref>, where each image is a weight vector used to construct the pre-activation for a particular class. Note that different elements of the dictionary capture different patterns in the data (in <ref type="figure" target="#fig_15">Figure 13d</ref>, different styles of writing the 0 digit) which CEN actually uses for prediction. Also note that confident correct predictions <ref type="figure" target="#fig_15">(Figures 13a)</ref> are made by selecting a single explanation from the dictionary using a sharp attention vector. However, when the model makes a mistake, its attention is often dispersed <ref type="figure" target="#fig_15">(Figures 13b and 13c)</ref>, i.e., there is uncertainty in which pattern it tries to use for prediction. <ref type="figure" target="#fig_15">Figure 13e</ref> further quantifies this  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 More on Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 IMDB</head><p>Similar to MNIST, we train CEN-tpc with linear explanations in terms of topics on the IMDB dataset. Then, we generate explanations for each test example and visualize histograms of the weights assigned by the explanations to 6 selected topics in <ref type="figure" target="#fig_5">Figure 6</ref>. The 3 topics in the top row are acting-and plot-related (and intuitively have positive, negative, or neutral connotation), while the 3 topics in the bottom are related to particular genre of the movies. Note that acting-related topics turn out to be bi-modal, i.e., contributing either positively, negatively, or neutrally to the sentiment prediction in different contexts. As expected intuitively, CEN assigns highly negative weight to the topic related to "bad acting/plot" and highly positive weight to "great story/performance" in most of the contexts (and treats those neutrally conditional on some of the reviews). Interestingly, genre-related topics almost always have a negligible contribution to the sentiment (i.e., get almost 0 weights assigned by explanations) which indicates that the learned model does not have any particular bias towards or against a given genre. Importantly, inspecting summary statistics of the explanations generated by CEN allows us to explore the biases that the model picks up from the data and actively uses for prediction 16 . <ref type="figure" target="#fig_3">Figure 15</ref> visualizes the full dictionary of size 16 learned by CEN-tpc. Each column corresponds to a dictionary atom that represents a typical explanation pattern that CEN attends to before making a prediction. By inspecting the dictionary, we can find interesting patterns. For instance, atoms 5 and 11 assign inverse weights to topics [kid, child, disney, family] and [sexual, violence, nudity, sex]. Depending on the context of the review, CEN may use one of these patterns to predict the sentiment. Note that these two topics are negatively correlated across all dictionary elements, which again is quite intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Satellite</head><p>We visualize the two explanations, M1 and M2, learned by CEN-att on the Satellite dataset in full in Figures 16a and provide additional correlation plots between the selected explanation and values of each survey variable in <ref type="figure" target="#fig_5">Figure 16b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Model Architectures</head><p>Architectures of the model used in our experiments are summarized in <ref type="bibr">Tables 7,</ref><ref type="bibr">8,</ref><ref type="bibr">9.</ref> 16. If we wish to enforce or eliminate certain patterns from explanations (e.g., to ensure fairness), we may impose additional constraints on the dictionary. However, this is beyond the scope of this work.  <ref type="figure" target="#fig_3">Figure 15</ref>: The full dictionary learned by CEN-tpc model: rows correspond to topics and columns correspond to dictionary atoms. Very small values were thresholded for visualization clarity. Different atoms capture different prediction patterns; for example, atom 5 assigns a highly positive weight to the [kid, child, disney, family] topic and down-weighs [sexual, violence, nudity, sex], while atom 11 acts in an opposite manner. Given the context of the review, CEN combines just a few atoms to make a prediction.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) A graphical model for CEN with a context encoder parameterized by w and linear explanations. (b) A graphical model for CEN with context encoder and CRF-based explanations. The model is parameterized by w. (c) A graphical model for CEN with context autoencoding via the inference, q, and generator, p, networks and CRF-based explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>A toy synthetic dataset and two linear explanations (green and orange) produced by a CEN model trained (a) with no regularization or (b) with conditional entropy regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results for the Satellite dataset: (a) Weights given to a subset of features by the two models (M1 and M2) discovered by CEN. (b) How frequently M1 and M2 are selected for areas marked rural or urban (top) and the average proportion of Tenement-type households in an urban/rural area for which M1 or M2 was selected. (c) M1 and M2 models selected for different areas on the Uganda map. M1 tends to be selected for more urbanized areas while M2 is selected for the rest. (d) Nightlight intensity of different areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>', 'soap', 'russian', 'opera']    −0.0005 0.0000 0.0005Art/nature movies:['art', 'earth', 'nature', 'jungle']    Histograms of test weights assigned by CEN to 6 topics: acting-and plot-related topics (upper charts), genre topics (bottom charts).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Validation error vs. entropy regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Expected contribution of the explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The effects of entropy regularization on (a) the predictive performance of a CEN model and (b) the lower bound on the contribution of the explanations to the relative predictive error reduction. Shaded regions are 95% CI based on 5 runs with different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Can we trust the explanations built on noisy or incomplete features? Explanation test error vs. feature size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>The effect of feature quality on explanations. (a) Explanation test error vs. the level of the noise added to the interpretable features. (b) Explanation test error vs. the total number of interpretable features. Error bars indicate 95% CI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>CEN architectures used in our survival analysis experiments. Context encoders were (a) single hidden layer MLP and (b) LSTM. Encoders produced inputs for another LSTM over the output time intervals (denoted with h 1 , h 2 , h 3 hidden states respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>(b) The relative absolute error (RAE) between the predicted and actual time of death for non-censored patients. CEN-predicted survival curves for 100 random test patients from SUPPORT2. Color indicates death within 1 year after leaving the hospital. Shaded regions are 99% CI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>14. In case of logistic regression, a(x) = [1, x1, . . . , x d ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Explanations generated by CEN for the 3 top classes and the corresponding attention vectors for (a) correctly classified, (b) misclassified, and (c) adversarially constructed images. Adversarial examples were generated using the fast gradient sign method (FGSM) (Papernot et al., 2016). (d) Elements from the learned 32-element dictionary that correspond to different writing styles of 0 digits. (e) Histogram of the attention entropy for correctly and incorrectly classified test instances for CEN-pxl on MNIST and CEN-tpc on IMDB. phenomenon by plotting histogram of the attention entropy for all test examples which were correctly and incorrectly classified. While CENs are certainly not adversarial-proof, high entropy of the attention vectors is indicative of ambiguous or out-of-distribution examples which is helpful for model diagnostics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Visualization of the model dictionary learned by CEN on MNIST. Each row corresponds to a dictionary element, and each column corresponds to the weights of the model voting for each class of digits. Images visualize the weights of the models. Red corresponds to high positive values, dark gray to high negative values, and white to values that are close to 0. 0 -0.1 0.2 0.2 0.4 -0.1 0.0 0.3 -0.2 0.3 -0.1 -0.2 0.1 0.0 0.2 0.0 0.0 -0.1 0.2 0.0 0.1 0.0 -0.2 0.2 0.1 0.0 -0.1 0.2 0.0 -0.1 0.0 0.0 0.2 0.2 -0.2 -0.1 0.0 0.3 0.3 0.3 0.0 0.2 0.2 0.0 0.0 0.0 0.0 0.1 0.0 0.2 0.2 0.0 0.1 0.0 0.0 -0.2 0.0 0.0 0.2 0.0 0.0 -0.2 0.0 -0.2 0.0 0.0 0.3 0.0 0.0 0.2 0.0 -0.1 0.0 -0.2 0.0 0.3 -0.1 -0.2 0.2 0.0 0.0 -0.2 0.0 0.3 0.0 0.0 0.3 0.0 0.2 0.0 -0.2 -0.2 0.0 -0.2 0.0 0.1 0.0 0.0 -0.2 0.2 -0.3 0.0 0.0 -0.2 0.3 0.0 0.2 -0.1 0.1 0.0 0.0 -0.2 0.0 0.0 0.2 -0.2 0.0 0.0 -0.2 -0.2 -0.3 -0.2 0.0 0.1 0.0 -0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Full visualization of models M1 and M2 learned by CEN on Satellite data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Correlation between the selected explanation and the value of a particular survey variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Additional visualizations for CENs trained on the Satellite data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different types of encoders and explanations used in CEN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of the models on the poverty prediction task. For baselines, we use logistic regression (LR) and multi-layer perceptrons (MLP) with 1 hidden layer. The LR uses either VGG-F embeddings (LR emb ) or the categorical attributes (LR att ) as inputs. The input of the MLP is concatenated VGG-F embeddings and categorical attributes. Context encoder of the CEN model uses VGG-F to process images, followed by an attention layer over a dictionary of 16 trainable linear explanations defined over the categorical features (</figDesc><table><row><cell></cell><cell cols="2">Acc ↑ AUC ↑</cell></row><row><cell>LR emb</cell><cell>62.5%</cell><cell>68.1%</cell></row><row><cell>LR att</cell><cell>75.7%</cell><cell>82.2%</cell></row><row><cell>MLP</cell><cell>77.4%</cell><cell>78.7%</cell></row><row><cell>MoE att</cell><cell>77.9%</cell><cell>85.4%</cell></row><row><cell cols="2">CEN att 81.5%</cell><cell>84.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Prediction error of the models on image classification tasks (averaged over 5 runs; the std. are on the order of the least significant digit). The subscripts denote the features on which the linear models are built: pixels (pxl), HOG (hog). Validation error vs. dictionary size.</figDesc><table><row><cell cols="3">MNIST (Error ↓, %)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR10 (Error ↓, %)</cell><cell></cell><cell></cell></row><row><cell cols="5">LRpxl LRhog CNN MoEpxl MoEhog CENpxl CENhog</cell><cell cols="6">LRpxl LRhog VGG MoEpxl MoEhog CENpxl CENhog</cell></row><row><cell>8.00 2.98 0.75</cell><cell>1.23</cell><cell>1.10</cell><cell>0.76</cell><cell>0.73</cell><cell>60.1 48.6</cell><cell>9.4</cell><cell>13.0</cell><cell>11.7</cell><cell>9.6</cell><cell>9.2</cell></row></table><note>(b) Test error vs. training data size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Compute time overhead.</figDesc><table><row><cell>Dataset</cell><cell>CEN</cell><cell>LIME</cell></row><row><cell cols="3">Training time overhead</cell></row><row><cell>MNIST</cell><cell>18.6 ± 1.7%</cell><cell>-</cell></row><row><cell>IMDB</cell><cell>1.8 ± 0.5%</cell><cell>-</cell></row><row><cell>Satellite</cell><cell>0.4 ± 0.1%</cell><cell>-</cell></row><row><cell cols="3">Explanation time per data point</cell></row><row><cell>MNIST</cell><cell>0.05 ± 0.03 ms</cell><cell>77 ± 9 ms</cell></row><row><cell>IMDB</cell><cell>0.07 ± 0.03 ms</cell><cell>38 ± 5 ms</cell></row><row><cell>Satellite</cell><cell>0.01 ± 0.01 ms</cell><cell>22 ± 6 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance of the baselines and CENs with structured explanations. The numbers are averages from 5-fold cross-validation; the std. are on the order of the least significant digit. "Acc@K" denotes accuracy at the K-th temporal quantile (see main text for explanation).</figDesc><table><row><cell></cell><cell cols="2">SUPPORT2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PhysioNet Challenge 2012</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Acc@25 Acc@50 Acc@75 RAE</cell><cell>Model</cell><cell cols="4">Acc@25 Acc@50 Acc@75 RAE</cell></row><row><cell>Cox</cell><cell>84.1</cell><cell>73.7</cell><cell>47.6</cell><cell>0.90</cell><cell>Cox</cell><cell>93.0</cell><cell>69.6</cell><cell>49.1</cell><cell>0.24</cell></row><row><cell>Aalen</cell><cell>87.1</cell><cell>66.2</cell><cell>45.8</cell><cell>0.98</cell><cell>Aalen</cell><cell>93.3</cell><cell>78.7</cell><cell>57.1</cell><cell>0.31</cell></row><row><cell>CRF</cell><cell>84.4</cell><cell>89.3</cell><cell>79.2</cell><cell>0.59</cell><cell>CRF</cell><cell>93.2</cell><cell>85.1</cell><cell>65.6</cell><cell>0.14</cell></row><row><cell>MLP-CRF</cell><cell>87.7</cell><cell>89.6</cell><cell>80.1</cell><cell>0.62</cell><cell>LSTM-CRF</cell><cell>93.9</cell><cell>86.3</cell><cell>68.1</cell><cell>0.11</cell></row><row><cell>MLP-CEN</cell><cell>84.4</cell><cell>96.2</cell><cell>83.3</cell><cell>0.52</cell><cell>LSTM-CEN</cell><cell>94.8</cell><cell>87.5</cell><cell>70.1</cell><cell>0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>10. http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets. 11. https://physionet.org/challenge/2012/. 12. Implementation based on https://github.com/CamDavidsonPilon/lifelines. 13. Similar models have been very successful in the natural language applications<ref type="bibr" target="#b18">(Collobert et al., 2011)</ref>.</figDesc><table><row><cell>sfdm2_SIP&gt;=30 sfdm2_Coma or Intub ca_yes hday slos avtisst dementia</cell><cell>0</cell><cell>10 Time after leaving hospital (weeks) 20 30 40 Patient ID: 3520 (Died)</cell><cell>50</cell><cell>0</cell><cell>10 Time after leaving hospital (weeks) 20 30 40 Patient ID: 1100 (Survived)</cell><cell>50</cell><cell>0 2 4</cell><cell>4 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Top-performing architectures used in our experiments on MNIST and IMDB datasets.(a) MNIST</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) IMDB</cell><cell></cell></row><row><cell cols="3">Convolutional Encoder</cell><cell cols="2">Contextual Explanations</cell><cell cols="2">Squential Encoder</cell><cell cols="2">Contextual Explanations</cell></row><row><cell></cell><cell>layer</cell><cell>Conv2D</cell><cell>model</cell><cell>Logistic regr.</cell><cell>layer</cell><cell>Embedding</cell><cell>model</cell><cell>Logistic reg.</cell></row><row><cell></cell><cell># filters</cell><cell>32</cell><cell>features</cell><cell>HOG (3, 3)</cell><cell>vocabulary</cell><cell>20k</cell><cell>features</cell><cell>BoW</cell></row><row><cell></cell><cell>kernel size</cell><cell>3 × 3</cell><cell># features</cell><cell>729</cell><cell>dimension</cell><cell>1024</cell><cell># features</cell><cell>20k</cell></row><row><cell>Convolutional Block</cell><cell>strides padding activation layer # filters kernel size strides padding</cell><cell>1 × 1 valid ReLU Conv2D 32 3 × 3 1 × 1 valid</cell><cell>standardized dictionary l 1 penalty l 2 penalty model features # features standardized</cell><cell>Yes 256 5 · 10 −5 1 · 10 −6 Logistic reg. Pixels (20, 20) 400 Yes</cell><cell>layer bidirectional units max length dropout rec. dropout layer</cell><cell>LSTM Yes 256 200 0.25 0.25 MaxPool1D</cell><cell>Dictionary l 1 penalty l 2 penalty model features # features Dictionary l 1 penalty</cell><cell>32 5 · 10 −5 1 · 10 −6 Logistic reg. Topics 50 16 1 · 10 −6</cell></row><row><cell></cell><cell>activation</cell><cell>ReLU</cell><cell>dictionary</cell><cell>64</cell><cell># params</cell><cell>23.1M</cell><cell>l 2 penalty</cell><cell>1 · 10 −8</cell></row><row><cell></cell><cell>layer pooling size dropout</cell><cell>MaxPoo2D 2 × 2 0.25</cell><cell cols="2">l 1 penalty l 2 penalty Contextual VAE 5 · 10 −5 1 · 10 −6</cell><cell></cell><cell></cell><cell cols="2">Contextual VAE Dir(0.1) Sampler Prior LogisticNormal</cell></row><row><cell></cell><cell>layer</cell><cell>Dense</cell><cell>prior</cell><cell>Dir(0.2)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>units</cell><cell>128</cell><cell>sampler</cell><cell>LogisticNormal</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>dropout</cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># blocks</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># params</cell><cell>1.2M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Top-performing architectures used in our experiments on CIFAR10 and Satellite datasets. VGG-16 architecture for CIFAR10 was taken from https://github.com/szagoruyko/cifar.torch but implemented in Keras with TensorFlow backend. Weights of the pre-trained VGG-F model for the Satellite experiments were taken from https://github.com/nealjean/predicting-poverty.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) CIFAR10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Satellite</cell></row><row><cell cols="3">Convolutional Encoder</cell><cell cols="2">Contextual Explanations</cell><cell cols="3">Convolutional Encoder</cell><cell cols="3">Contextual Explanations</cell></row><row><cell>VGG-16</cell><cell>model pretrained fixed weights</cell><cell>VGG-16 No No</cell><cell>model features # features</cell><cell>Logistic reg. HOG (3, 3) 1024</cell><cell>VGG-F</cell><cell>model pretrained fixed weights</cell><cell>VGG-F Yes Yes</cell><cell cols="2">model features # features</cell><cell>Logistic reg. Survey 64</cell></row><row><cell>MLP</cell><cell>layer pretrained fixed weights units</cell><cell>Dense No No 16</cell><cell cols="2">dictionary l 1 penalty l 2 penalty Contextual VAE 1 · 10 −5 16 1 · 10 −6</cell><cell>MLP</cell><cell>layer pretrained fixed weights units</cell><cell>Dense No No 128</cell><cell cols="2">dictionary l 1 penalty l 2 penalty # params</cell><cell>16 1 · 10 −3 1 · 10 −4</cell></row><row><cell></cell><cell>dropout activation</cell><cell>0.25 ReLU</cell><cell>prior sampler</cell><cell>Dir(0.2) LogisticNormal</cell><cell></cell><cell>dropout activation</cell><cell>0.25 ReLU</cell><cell>prior</cell><cell cols="2">Contextual VAE Dir(0.2)</cell></row><row><cell cols="2"># params</cell><cell>20.0M</cell><cell></cell><cell></cell><cell cols="3"># trainable params 0.5M</cell><cell cols="2">sampler</cell><cell>LogisticNormal</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Top-performing architectures used in our experiments on SUPPORT2 and PhysioNet.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) SUPPORT2</cell><cell></cell><cell></cell><cell cols="4">(b) PhysioNet Challenge 2012</cell></row><row><cell></cell><cell cols="2">MLP Encoder</cell><cell cols="2">Contextual Explanations</cell><cell cols="3">Sequential Encoder</cell><cell cols="2">Contextual Explanations</cell></row><row><cell></cell><cell>layer</cell><cell>Dense</cell><cell>model</cell><cell>Linear CRF</cell><cell></cell><cell>layer</cell><cell>LSTM</cell><cell>model</cell><cell>Linear CRF</cell></row><row><cell>MLP</cell><cell>pretrained fixed weights units</cell><cell>No No 64</cell><cell>features # features dictionary</cell><cell>Measurements 50 16</cell><cell>LSTM</cell><cell>bidirectional units max length</cell><cell>No 32 150</cell><cell>features # features dictionary</cell><cell>Statistics 111 16</cell></row><row><cell></cell><cell>dropout</cell><cell>0.50</cell><cell>l 1 penalty</cell><cell>1 · 10 −3</cell><cell></cell><cell>dropout</cell><cell>0.25</cell><cell>l 1 penalty</cell><cell>1 · 10 −3</cell></row><row><cell></cell><cell>activation</cell><cell>ReLU</cell><cell>l 2 penalty</cell><cell>1 · 10 −4</cell><cell></cell><cell>rec. dropout</cell><cell>0.25</cell><cell>l 2 penalty</cell><cell>1 · 10 −4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. To see why this is the case, consider graphical models given inFigure 2which relate input, X, and target, Y, variables using linear pairwise potential functions. Linearity allows to directly interpret parameters of the model as associations between the variables. Substituting inputs, X, with deep representations</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Analysis of the multi-class case can be reduced to the binary in the one-vs-all fashion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">. We assume that the occurrence time is lower bounded by t0 = 0, upper bounded by some tm = T , and discretized into intervals [ti, ti+1), where i ∈ {0, . . . , m − 1}.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">. The model was taken form https://github.com/nealjean/predicting-poverty.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Dist. to water src</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Num. of rooms 11 Avg. dist. to road</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Avg. dist. to market</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avg</surname></persName>
		</author>
		<imprint>
			<date>vegetation dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Avg. vegetation inc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Straw 28 Roof: Other 27 Roof: Mud 26 Roof: Iron sheets 25 Roof: Concrete 24 Roof: Asbestos 23 HH type: Uniport 22 HH type: Tenement 21 HH type</title>
		<idno>Planks 31 Roof: Tin 30 Roof: Tiles 29 Roof: Thatch</idno>
		<imprint/>
	</monogr>
	<note>Shared house 20 HH type: Other 19 HH type: Private house 18 HH type: Private apt 17 HH type: Hut</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno>M1 M2 48 Floor: Stone 47 Floor: Other 46</idno>
		<title level="m">Mosaic/tiles 45 Floor: Cow dung 44 Floor: Earth 43 Floor: Cement 42 Floor: Bricks 41 Walls: Stone 40 Walls: Unburnt bricks 39 Walls: Timber 38 Walls: Thatch, Straw 37 Walls: Other 36 Walls: Mud, poles 35 Walls: Cement blocks 34 Walls: Brick w/ mud 33 Walls: Brick w/ cement</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Water: Long queues 59 Water: Far away 58 Water src: Vendor truck 57 Water src: Unprotected well 56 Water src: River/lake/pond 55 Water src: Rain water 54 Water src: Public tap 53 Water src: Protected well 52 Water src: Private tap 51 Water src: Other 50 Water src</title>
		<idno>Water: Unreliable 63 Water: Contribution 62 Water: Bad taste 61 Water: Unprotect. OK 60</idno>
	</analytic>
	<monogr>
		<title level="m">Gravity flow 49 Water src</title>
		<meeting><address><addrLine>Bore-hole</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A linear regression model for the analysis of life time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Aalen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="907" to="925" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning scalable deep kernels with recurrent structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Casebased explanation of non-case-based learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hooshang</forename><surname>Kangarloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usha</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00550</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regression Models and Life-Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dr Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="187" to="220" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines on word observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno>978-1-4503-1285-1</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3042573.3042723" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML&apos;12</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning, ICML&apos;12<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1163" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Topicrnn: A recurrent neural network with long-range semantic dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explanations can be manipulated and geometry is to blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann-Kathrin</forename><surname>Dombrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Kessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13567" to="13578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Logistic regression, survival analysis, and the kaplan-meier curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">402</biblScope>
			<biblScope unit="page" from="414" to="425" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linear dynamical neural population models through nonlinear embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">W</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Gpu kernels for block-sparse weights</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph star net for generalized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Haonan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiuyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12330</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Varying-coefficient models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="757" to="796" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5903</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining satellite imagery and machine learning to predict poverty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6301</biblScope>
			<biblScope unit="page" from="790" to="794" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures-of-experts for exponential family regression models: approximation and maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="987" to="1011" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="919" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Supervised and semi-supervised text categorization using lstm for region embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="526" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The bayesian case model: A generative approach for case-based reasoning and prototype classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1952" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oluwasanmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structured inference networks for nonlinear state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2101" to="2109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning, ICML</title>
		<meeting>the eighteenth international conference on machine learning, ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">how do i fool you</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06473</idno>
	</analytic>
	<monogr>
		<title level="m">Manipulating user trust via misleading black box explanations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Rationalizing neural predictions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mixture models: theory, geometry and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSF-CBMS regional conference series in probability and statistics</title>
		<imprint>
			<publisher>JSTOR</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page">163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Context selection for embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4817" to="4826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The structure and function of explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Lombrozo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="464" to="470" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07874</idno>
		<title level="m">A unified approach to interpreting model predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<idno>978-1-932432-87-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2002472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sahand Negahban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep K</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1348" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Why Should I Trust You?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Exponential family embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Structured embedding models for grouped data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="250" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Revisiting lstm networks for semi-supervised text classification via mixed objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6940" to="6948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="267" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">User-friendly tail bounds for sums of random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of computational mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="434" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A meta-learning perspective on cold-start recommendations for items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manasi</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Thiagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6888" to="6898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Local supervised learning through space partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2390665.2390688" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>ACL &apos;12</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Unsupervised data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning patientspecific cancer survival distributions as a sequence of dependent regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Chin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Baracos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1845" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2015/07/30/cifar.html" />
		<title level="m">92.45% on CIFAR-10 in Torch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
