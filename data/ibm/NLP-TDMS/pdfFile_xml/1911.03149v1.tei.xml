<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quality Aware Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parimala</forename><surname>Kancharla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumohana</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
							<email>sumohana@iith.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quality Aware Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have become a very popular tool for implicitly learning high-dimensional probability distributions. Several improvements have been made to the original GAN formulation to address some of its shortcomings like mode collapse, convergence issues, entanglement, poor visual quality etc. While a significant effort has been directed towards improving the visual quality of images generated by GANs, it is rather surprising that objective image quality metrics have neither been employed as cost functions nor as regularizers in GAN objective functions. In this work, we show how a distance metric that is a variant of the Structural SIMilarity (SSIM) index (a popular full-reference image quality assessment algorithm), and a novel quality aware discriminator gradient penalty function that is inspired by the Natural Image Quality Evaluator (NIQE, a popular no-reference image quality assessment algorithm) can each be used as excellent regularizers for GAN objective functions. Specifically, we demonstrate state-ofthe-art performance using the Wasserstein GAN gradient penalty (WGAN-GP) framework over CIFAR-10, STL10 and CelebA datasets.</p><p>• We make explicit use of objective image quality assessment (IQA) metrics and their variants for regularizing WGAN with gradient penalty (WGAN-GP), and propose Quality Aware GANs (QAGANs).</p><p>• We propose a novel quality aware discriminator gradient penalty function based on the local statistical signature of natural images as in NIQE [5].</p><p>• We demonstrate state-of-the-art performance on CIFAR-10, STL 10 and CelebA datasets for non-progressive GANs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> have become a very popular tool for implicitly learning high-dimensional probability distributions. A large number of very interesting and useful applications have emerged due to the ability of GANs to learn complex real-world distributions. Some of these include image translation <ref type="bibr" target="#b1">[2]</ref>, image super-resolution <ref type="bibr" target="#b2">[3]</ref>, image saliency detection <ref type="bibr" target="#b3">[4]</ref> etc. While GANs have indeed become very popular, they suffer from drawbacks such as mode collapse, convergence issues, entanglement, poor visual quality etc. A significant amount of research effort has focused on addressing these shortcomings in the original GAN formulation. While the literature does consider the quality of the generated images as a performance metric, it is rather surprising that the use of objective image quality metrics in the GAN cost function has been very limited. We address this lacuna with our contributions as summarized below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We review relevant works on GANs and IQA algorithms to setup the background necessary to present our proposed quality aware GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Networks (GANs)</head><p>Given the explosive growth in the literature on GANs, we only review representative works in the following. GANs <ref type="bibr" target="#b0">[1]</ref> pose the the problem of learning a high-dimensional distribution from data samples in a game-theoretic framework. A typical GAN architecture consists of a generator modelled by a neural network and denoted by G and parameterized by θ g , and a discriminator also modelled by a neural network denoted by D and parameterized by θ d . The goal of the generator is to generate samples denoted G(z) (where z is a noise random variable with prior P z ) that "mimic" true data samples x drawn from a distribution P r . The discriminator's goal is to maximize its ability to tell G(z) apart from x. The generator and discriminator engage in an adversarial combat or game to train each other (simultaneously). This game is formulated as</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼Pr [log(D(x))] + E z∼Pz [log(1 − D(G(z)))]<label>(1)</label></formula><p>The value function V (D, G) is defined so that the discriminator tries to maximize the probability of assigning the correct label to G(z) and x via the term E x∼Pr [log(D(x))] while the generator simultaneously tries to minimize the term E z∼Pz [log(1 − D(G(z)))]. The model parameters θ g , θ d are learnt by solving this optimization in an iterative fashion. This formulation suffered from the drawbacks mentioned earlier -mode collapse, convergence issues while training, poor visual quality etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Wasserstein GAN with Gradient Penalty (WGAN-GP)</head><p>Wasserstein GAN (WGAN) <ref type="bibr" target="#b5">[6]</ref> was one of the first works to address training issues in the original GAN formulation. It introduced the Wasserstein distance to compare distributions and showed that it possesses a number of useful convergence and continuity properties. It also showed that 1-Lipschitz functions can be used in practise to find the Wasserstein distance between distributions (at the discriminator). These 1-Lipschitz functions are realized using a neural network and the Lipschitz condition is enforced by clipping the network weights. Despite these improvements, Gulrajani et al. <ref type="bibr" target="#b6">[7]</ref> showed that weight clipping in WGAN led to undesirable behaviour such as poor samples or failing to converge. They propose a stabler solution called WGAN with Gradient Penalty (WGAN-GP) where they penalize the norm of the discriminator's gradient with respect to its input. They showed that the norm of the discriminator gradient is in fact 1 almost everywhere for 1-Lipschitz functions. We will make use of the WGAN-GP in our work given its stability in training GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Banach Wasserstein GAN (BWGAN)</head><p>Adler and Lunz <ref type="bibr" target="#b7">[8]</ref> introduced Banach WGANs (BWGANs) that provide a framework for using arbitrary norms to train Wasserstein GANs (WGANs). They note that WGANs and their variants only consider l 2 as the underlying metric. Specifically, they generalize the WGAN-GP framework to Banach spaces and show how WGANs can be trained with arbitrary norms. The motivation for BWGANs is to allow the generator to emphasize desired image features such as edges as demonstrated using Sobolev and L p norms. The motivation for our work comes from BWGANs and is similar in spirit. They also suggest that WGAN training can be extended to a general metric space (with metric d(X, Y )) by using a penalty term of the form</p><formula xml:id="formula_1">E X∼Pr,Y ∼P G |D(X) − D(Y )| d(X, Y ) − 1 2 .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Zero-Centered Gradient Penalty Approaches</head><p>Other recent approaches to improving the stability of GAN training revisit the original formulation in <ref type="bibr" target="#b0">[1]</ref>. Roth et al. <ref type="bibr" target="#b8">[9]</ref> present a regularization approach where the inputs to the discriminator are smoothed by convolving them with noise (realized by adding noise to the samples during training). This is shown to result in a zero-centered gradient penalty regularizer. Mescheder et al. <ref type="bibr" target="#b9">[10]</ref> prove that zero-centered gradient penalties make training more stable. Thanh-Tung et al. <ref type="bibr" target="#b10">[11]</ref> propose another variant of the zero-centered gradient penalty for improving the convergence and generalizing capability of GANs. These works are significant in that they provide a clear theoretical explanation to the issues in training GANs and how they can be overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Quality Assessment</head><p>Objective image quality assessment (IQA) metrics can be classified into three classes depending on their use of the reference (or undistorted) image for quality assessment. Full-reference (FR) IQA metrics make use of the complete reference image while reduced reference (RR) IQA metrics make use of partial reference image information for quality prediction. No-reference (NR) IQA metrics on the other hand predict the quality of an image in a reference-free or stand-alone fashion. It should be noted that the IQA metrics assume that the images in question are of natural (photographic) scenes.</p><p>In this work, we show how an FR and an NR IQA algorithm can each be used for the quality aware design of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">The Structural SIMilarity (SSIM) index</head><p>It would not be an exaggeration to claim that the invention of the SSIM index <ref type="bibr" target="#b11">[12]</ref> heralded a revolution in the design of objective quality assessment algorithms. The SSIM index is an FR IQA metric that is based on the premise that distortions lead to change in local image structure, and that the human visual system is sensitive to these structural changes. The SSIM index quantifies the change in structural information in the test image relative to the reference and computes a quality score. Specifically, the SSIM index computes changes to local mean, local variance and local structure (or correlation) and pools them to find the local quality score. These local scores are then averaged across the image to find the image quality score. This is summarized as follows.</p><formula xml:id="formula_2">SSIM(P (i,j) , T (i,j) ) = L(P (i,j) , T (i,j) ).C(P (i,j) , T (i,j) ).S(P (i,j) , T (i,j) ),<label>(3)</label></formula><p>where P, T refer to the pristine and test image respectively, the subscript (i, j) is the pixel index, L(P (i,j) , T (i,j) ), C(P (i,j) , T (i,j) ), S(P (i,j) , T (i,j) ) are the local luminance, contrast and structure scores at pixel (i, j) respectively. Further, <ref type="bibr" target="#b1">2</ref> are the local mean and variance of the pristine image patch of size (2K + 1)</p><formula xml:id="formula_3">L(P (i,j) , T (i,j) ) = 2µ P (i, j)µ T (i, j) + C 1 µ 2 P (i, j) + µ 2 T (, j) + C 1 , C(P (i,j) , T (i,j) ) = 2σ P (i, j)σ T (i, j) + C 2 σ P 2 (i, j) + σ T 2 (i, j) + C 2 , S(P (i,j) , T (i,j) ) = σ P T (i, j) + C 3 σ P (i, j)σ T (i, j) + C 3 ,<label>(4)</label></formula><formula xml:id="formula_4">where µ P (i, j) = 1 (2K+1) 2 K m=−K K n=−K P (i − m, j − n), σ 2 P (i, j) = 1 (2K+1) 2 K m=−K K n=−K (P (i − m, j − n) − µ P (i, j))</formula><formula xml:id="formula_5">× (2K + 1) centered at (i, j). µ T (i, j), σ 2 P (i, j) are defined similarly for the test image T . The cross covariance is defined as σ P T (i, j) = 1 (2K+1) 2 K m=−K K n=−K (P (i − m, j − n) − µ P (i, j)) × (T (i − m, j − n) − µ T (i, j))</formula><p>. The constants C 1 , C 2 , C 3 are used to avoid division-by-zero issues. For simplicity, C 3 = C 2 /2 in the standard implementation which leads to</p><formula xml:id="formula_6">SSIM(P (i,j) , T (i,j) ) = L(P (i,j) , T (i,j) ).CS(P (i,j) , T (i,j) ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">CS(P (i,j) , T (i,j) ) = 2σ P T (i, j) + C 2 σ P 2 (i, j) + σ T 2 (i, j) + C 2 .<label>(6)</label></formula><p>The image level SSIM index is given by:</p><formula xml:id="formula_8">SSIM(P, T ) = 1 M × N M i=1 N j=1 SSIM(P (i,j) , T (i,j) ),<label>(7)</label></formula><p>where the images are of size M × N (assuming appropriate boundary handling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Natural Image Quality Estimator (NIQE)</head><p>NIQE <ref type="bibr" target="#b4">[5]</ref> is a popular NR IQA metric that is based on the statistics of mean subtracted and contrast normalized (MSCN) natural scenes. An MSCN imageÎ is generated from an input image I according to:Î</p><formula xml:id="formula_9">(i, j) = I(i, j) − µ(i, j) σ(i, j) + 1 ,<label>(8)</label></formula><p>where (i, j) is the pixel index and µ(i, j) and σ(i, j) are the local mean and standard deviation computed as in the case of SSIM index (see Section 2.2.1). The constant 1 in the denominator is to prevent division-by-zero issues. NIQE relies on the following observations about the statistics of MSCN naturals scenes: a) the statistics of MSCN natural images reliably follow a Gaussian distribution <ref type="bibr" target="#b12">[13]</ref>, b) the statistics of MSCN pristine and distorted images can be modeled well using a generalized Gaussian distribution (GGD) <ref type="bibr" target="#b13">[14]</ref>. NIQE, as the name suggests, quantifies the naturalness of an image. To do so, it proposes a statistical model for the class of pristine natural scenes and uses the model's parameters for quality estimation. Specifically, it models MSCN pristine image coefficients using a GGD, and models the products of neighbouring MSCN coefficients using an asymmetric GGD (AGGD). The parameters of these GGD and AGGD models are in turn modeled using a Multivariate Gaussian (MVG) distribution whose parameters µ P , Σ P are then used as representatives of the entire class of pristine natural images. The quality of a test image is measured in terms of the "distance" of its MVG parameters µ T , Σ T from the pristine MVG parameters. This is quantified as:</p><formula xml:id="formula_10">D(µ P , µ T , Σ P , Σ T ) = (µ P − µ T ) T Σ P + Σ T 2 −1 (µ P − µ T ),<label>(9)</label></formula><p>assuming that the sum of the matrices is invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Quality Aware GANs (QAGANs)</head><p>While significant progress has been made in the design of objective IQA metrics, their usage as cost functions has been limited by their unwieldy mathematical formulation. Non-convexity, difficulty in gradient computation, not satisfying the properties of distances and norms are some of the impediments to their usage in formulating optimization problems. In the following, we identify variants of the SSIM index and NIQE that are mathematically amenable and lend themselves to being used as regularizers in the WGAN-GP optimization framework. Importantly, we empirically demonstrate the ability of our approach in augmenting the capability of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quality Aware BWGAN Regularization</head><p>From the definitions in <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_8">(7)</ref>, the SSIM index is bounded in the interval [-1, 1] that immediately renders it an invalid distance metric (since it can take negative values). This makes the direct application of the SSIM index in the flexible BWGAN framework infeasible. Brunet et al. <ref type="bibr" target="#b14">[15]</ref> have analyzed the mathematical properties of SSIM index and show that valid distance metrics can be derived from the components of the SSIM index defined in <ref type="bibr" target="#b3">(4)</ref>. For e.g.,</p><formula xml:id="formula_11">d 1 (P (i,j) , T (i,j) )) := 1 − L(P (i,j) , T (i,j) ), d 2 (P (i,j) , T (i,j) )) := 1 − CS(P (i,j) , T (i,j)</formula><p>) are shown to be valid normalized distance metrics. Importantly, they show that</p><formula xml:id="formula_12">d Q (P (i,j) , T (i,j) ) = 2 − L(P (i,j) , T (i,j) ) − CS(P (i,j) , T (i,j) )<label>(10)</label></formula><p>is a valid distance metric that also preserves the quality discerning properties of the SSIM index. Further, as in the SSIM index, the image level distance metric for an M × N image is defined as</p><formula xml:id="formula_13">d Q (P, T ) = 1 M × N M i=1 N j=1 d Q (P (i,j) , T (i,j) ).<label>(11)</label></formula><p>We refer the reader to <ref type="bibr" target="#b14">[15]</ref> for a detailed exposition of the properties of this distance metric. We call this a quality aware distance metric that serves as a good candidate for regularizing GANs. We hypothesize that by making the discriminator Lipschitz with respect to d Q (X, Y ) in the image space, the gradients computed from such a regularized discriminator emphasize the structural information in the generated images. Further, since d Q (X, Y ) is bounded below and we are operating in the general metric space of images, we are in a position to impose the Lipschitz constraint directly. In order to do so, we follow the approach suggested in BWGAN <ref type="bibr" target="#b7">[8]</ref> and introduce a gradient penalty regularization term of the form</p><formula xml:id="formula_14">SSIM GP = E X∼Pr,Y ∼P G |D(X) − D(Y )| d Q (X, Y ) − 1 2 .<label>(12)</label></formula><p>In addition to the SSIM GP regularizer, we also employ a 1-GP regularizer (i.e., WGAN-GP) to ensure stable training. The overall discriminator loss function is</p><formula xml:id="formula_15">L d = min D D E z∼Pz D(G(z)) − E X∼Pr D(X) + λ 1 Ex ∼Px (||∇xD(x)|| 2 − 1) 2 + λ 2 E X∼Pr,G(z)∼P G |D(X) − D(G(z))| d Q (X, G(z)) − 1 2 ,<label>(13)</label></formula><p>where λ 1 and λ 2 are empirically chosen. Further, as in the WGAN-GP setting,x is sampled from a line joining the real and fake image distributions. Our coupled gradient penalty with respect to d Q (X, Y ) imposes a strong Lipschitz constraint. We believe that this quality aware discriminator penalty results in a quality aware GAN formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quality Aware Gradient Penalty</head><p>As discussed in Section 2.2.2, the MSCN coefficient distribution of pristine and distorted images of natural scenes have a unique statistical signature. We claim that if the MSCN coefficient statistics possess a unique and reliably consistent signature, then so must the MSCN coefficient statistics of the spatial gradients and discriminator gradients of natural images. We empirically show that this is indeed the case, and that a NIQE-like formulation works well in quantifying the naturalness of discriminator gradients as well. <ref type="figure" target="#fig_0">Fig. 1b</ref> shows the empirical histograms of the MSCN spatial gradient image of a representative natural scene and its distorted versions. <ref type="figure" target="#fig_0">Fig. 1c</ref> shows the empirical histograms of the MSCN discriminator gradient image of a representative natural scene and its distorted versions. We have chosen a minimally trained (∼ 100 iterations) deep neural network for finding the discriminator gradients. We make the following observations about the histograms: a) unimodal, b) Gaussian-like, c) distortions affect statistics. All these observations are identical to the MSCN coefficient statistics of natural images shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. We believe that the discriminator gradients show this statistical behavior since discriminators are smooth functions and natural images possess a unique local statistical signature. Another visual example that clearly illustrates our observations and motivates our work is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Based on these observations, we propose a NIQE-like score for the naturalness of the discriminator gradients of natural scenes. As in NIQE, a GGD is used to model the MSCN coefficients of pristine image discriminator gradients, and an AGGD is used to model the product of these coefficients. Finally, an MVG is used to model the parameters of the GGD and AGGD of MSCN of discriminator gradients from pristine images. The pristine discriminator gradient MVG model is characterized by its parameters µ P , Σ P . These parameters represent the class of all discriminator gradients computed with respect to pristine natural images.</p><p>The naturalness of a test discriminator gradient image T is computed to be its "distance" from the pristine image gradient class and is given by</p><formula xml:id="formula_16">||(T |µ P , Σ P )|| NIQE := (µ P − µ T ) T Σ P + Σ T 2 −1 (µ P − µ T ),<label>(14)</label></formula><p>where µ T , Σ T are the model parameters of the test image's MVG model. This function serves as a quality aware gradient penalty that we use to regularize a GAN as discussed next. As an aside, <ref type="figure" target="#fig_0">Fig.  1b</ref> shows that our hypothesis would work even if a spatial gradient regularizer is used. As discussed in Section 2, several regularization approaches have been proposed in the literature to improve the image quality, stability, generalization ability of GANs. These include regularizing with non-zero mean and zero mean gradient penalty, Sobolev norm penalty etc. While these approaches consider the norm of the discriminator gradient, they do not make use of the local correlation present in the  gradient. Based on our hypothesis on the statistics of the discriminator gradient values of natural scenes presented in the previous section, we propose a novel regularization term that helps impose these statistical constraints on the generated images. We have shown through <ref type="figure" target="#fig_0">Fig. 1c</ref> that ∇ x D(x) contains useful local spatial statistics information. Our regularizer is designed to force the local statistics of the discriminator gradient ofx to be as close to those of real images as possible. Our claim is that such a regularization strategy results in improving visual quality of the generated images. The NIQE "distance" function in <ref type="bibr" target="#b13">(14)</ref> serves as the statistics preserving regularizer. As mentioned earlier, we work in the WGAN-GP framework to demonstrate our method. The overall discriminator cost function includes the 1-GP regularizer and the NIQE function regularizer as defined in</p><formula xml:id="formula_17">L d = min D D E z∼Pz D(G(z)) − E X∼Pr D(X) + λ 1 Ex ∼Px (||∇xD(x)|| 2 − 1) 2 + λ 2 Ex ∼Px (||(∇xD(x)|µ P , Σ P )|| NIQE ),<label>(15)</label></formula><p>where λ 1 and λ 2 are hyper parameters chosen empirically. As before,x is sampled from a line joining the real and fake image distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We have evaluated the efficacy of proposed regularizers on three datasets: 1) CIFAR-10 [35] (60K images of 32 × 32 resolution), 2) CelebA <ref type="bibr" target="#b15">[16]</ref>(202.6K face images cropped and resized to resolution 64 × 64. 3) STL-10 <ref type="bibr" target="#b16">[17]</ref> (100K images of resolution 96 × 96 and 48 × 48). Network Details: All our experiments are done using a residual architecture for discriminator and generator used in WGAN-GP <ref type="bibr" target="#b6">[7]</ref>. Batch normalization is applied to each resnet layer in the generator. We have used Adam as the optimizer with the standard momentum parameters β 1 = 0. and β 2 = 0.9. The initial learning rate was set to 0.0002 for CIFAR-10 and STL-10 datasets and 0.0001 for the CelebA dataset. The learning rate is decreased adaptively. We have empirically chosen the hyper parameters λ 1 and λ 2 to be 1 and 0.1 respectively. All our models are trained for 100K iterations with a batch size of 64. The discriminator is updated five times for every update of the generator. Evaluation: We evaluated our method using two quantitative measures: 1) Inception Score (IS) <ref type="bibr" target="#b17">[18]</ref> and 2) Frechet Inception Distance (FID) <ref type="bibr" target="#b18">[19]</ref>. IS measures the sample quality and diversity by finding the entropy of the predicted labels. Higher IS indicates a better model. FID score measures the similarity between real and fake samples by fitting a multi variate Gaussian (MVG) model to the intermediate representation for the real and fake samples respectively. In case of FID, lower scores indicate a better model. We have used 50K randomly generated samples for computing the inception score and FID score. For comparison with previous models, we have computed the FID scores for CIFAR-10 and CelebA datasets using the official Tensorflow implementation, and for computing the FID scores of STL-10 dataset, we have used the Chainer implementation used by SNGAN <ref type="bibr" target="#b19">[20]</ref>. IS and FID are computed five times for the best model and the mean and variance are reported.</p><p>Results: Our primary motivation in this work is to formulate quality aware loss functions for GANs primarily in the non-progressive setting. We present representative samples from the SSIM based QAGAN in <ref type="figure" target="#fig_3">Fig. 3</ref> and the NIQE based QAGAN in <ref type="figure" target="#fig_2">Fig. 4</ref>. IS and FID are reported in <ref type="table" target="#tab_0">Tables 1, 2</ref>    <ref type="bibr" target="#b20">[21]</ref> 6.16 ± 0.07 -WGAN-GP <ref type="bibr" target="#b6">[7]</ref> 7.86 ± 0.10 40.2 ± 0.0 CTGAN <ref type="bibr" target="#b21">[22]</ref> 8.12 ± 0.12 -SNGAN <ref type="bibr" target="#b19">[20]</ref> 8.12 ± 0.12 21.5 ± 0.21 W − 3 2 ,2 -Banach WGAN <ref type="bibr" target="#b7">[8]</ref> 8.26 ± 0.07 -L 10 -Banach WGAN <ref type="bibr" target="#b7">[8]</ref> 8.31 ± 0.07 -MMD GAN-rep-b <ref type="bibr" target="#b22">[23]</ref> 8   From the figures and tables, we see that QAGANs are very competitive with the state-of-the-art methods on all three datasets. Importantly and interestingly, QAGANs deliver consistently good performance with respect to both IS and FID, while other methods do well mostly with respect to IS. This provides clear quantitative evidence of the improved quality of images generated by QAGANs. Also, it underscores our claim that explicitly using objective IQA metrics in GAN cost functions is not only a promising way forward but also long overdue.</p><p>We see that the NIQE-based regularizer shows inconsistent performance only with respect to IS on the CIFAR-10 dataset. We attribute this inconsistency to the small image size (32 × 32) of this dataset. This would lead to poorer estimates of the model parameters (compared to other resolutions) which in turn reduces performances.</p><p>Further, we believe that the proposed quality aware regularizers can be applied to progressive architectures as well. We demonstrate this by applying the proposed regularizers to the PGGAN architecture <ref type="bibr" target="#b23">[24]</ref> (both original and growing) at resolutions of 128 × 128 and 256 × 256 on the CelebA dataset. The qualitative results at an image resolution of 256 × 256 are shown in <ref type="figure" target="#fig_5">Fig. 5</ref> and the quantitative results are given in <ref type="table" target="#tab_5">Table 5</ref>. These results are shown after 6K iterations. Interestingly, we observed that the proposed regularizers resulted in faster convergence and improved visual quality of the generated images. The quantitative improvement in performance is clear from the FID values in <ref type="table" target="#tab_5">Table 5</ref>. 26.08 ± 0.26 7.9 WGAN-GP <ref type="bibr" target="#b6">[7]</ref> 9.05± 0.12 55.1 ± 0.0 SNGAN <ref type="bibr" target="#b19">[20]</ref> 9.10 ± 0.04 40.10 ± 0.50 MMD GAN-rep <ref type="bibr" target="#b22">[23]</ref> 9.36 ± 0.0 36.67 ± 0.0 QAGAN (SSIM) 9.29 ± 0.05 19.77 ± 0.0091 QAGAN (NIQE) 9.1720 ± 0.08 19.45 ± 0.0013  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Based on insights from both FR and NR IQA metrics, we have proposed two novel regularization approaches for the WGAN-GP framework. The key takeaway from our work is that the unique local structural and statistical signature of pristine natural images must be preserved in the generated images. We demonstrated how the SSIM and NIQE based regularizers guide the generator towards the class of pristine natural images and thereby ensure its unique local structural and statistical signature. The performance of QAGANs was shown to be very competitive with the state-of-the-art methods over three popular datasets. We believe that this work opens up new and exciting directions in image and video generative modeling, given the plethora of excellent QA metrics. The challenge however lies in translating the QA metrics into a form that fits the GAN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>We would like to thank Dr. J. Balasubramaniam of the Math department at IIT Hyderabad for his valuable suggestions and insights during the course of this work. We would also like to thank NVIDIA for the GPU donation. SSC would like to acknowledge the sound track of Kavaludaari for inspiration while writing. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The empirical histograms of MSCN coefficients. (1a) Pristine natural scenes and their distorted versions. (1b) Spatial gradient of pristine natural scenes and their distorted versions. (1c) Discriminator gradient of pristine natural scenes and their distorted versions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top row (L-R) shows a real image, its corresponding spatial gradient and discriminator gradient maps. Middle row (L-R) shows their corresponding mean subtracted contrast normalized (MSCN) coefficients. Bottom row (L-R) shows the normalized histograms of the respective MSCN coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>, 3 and 4 .</head><label>4</label><figDesc>(a) CIFAR-10 dataset (32 × 32).(b) STL-10 dataset (48 × 48).(c) CelebA dataset (64 × 64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Randomly sampled images generated using QAGANs with quality aware distance metric regularizer (SSIM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Randomly sampled images generated using QAGANs with quality aware gradient penalty regularizer (NIQE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Randomly sampled images generated using QAGANs for CelebA dataset with a progressively growing architecture (256 × 256) Top row: PGGAN [24]. Middle row: PGGAN with SSIM. Bottom row: PGGAN with NIQE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inception Score (IS) and Fréchet Inception Distance (FID) computed from 50,000 samples of the CIFAR-10 dataset (32 × 32). Scores that are unavailable are marked with a '-'.</figDesc><table><row><cell>Model</cell><cell>IS</cell><cell>FID</cell></row><row><cell>Real data</cell><cell>11.24 ± 0.12</cell><cell>7.80</cell></row><row><cell>DCGAN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID on the CelebA dataset (64 × 64).</figDesc><table><row><cell>Model</cell><cell>FID</cell></row><row><cell>Real Faces (CelebA)</cell><cell>1.09</cell></row><row><cell>WGAN-GP [7]</cell><cell>12.89</cell></row><row><cell>Banach WGAN [8]</cell><cell>10.5</cell></row><row><cell cols="2">MMD GAN-rep-b [23] 6.79</cell></row><row><cell>QAGAN (SSIM)</cell><cell>6.421</cell></row><row><cell>QAGAN (NIQE)</cell><cell>6.504</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>IS and FID on the STL-10 dataset (48 × 48).</figDesc><table><row><cell>Model</cell><cell>IS</cell><cell>FID</cell></row><row><cell>Real Data (48 × 48)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>IS and FID on the STL-10 dataset (96 × 96). Model IS FID QAGAN (SSIM) 9.66 ± 0.18 3.7155 ± 0.004 QAGAN (NIQE) 8.948 ± 0.01 3.1951 ± 0.013</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>FID on the CelebA dataset for PGGAN.</figDesc><table><row><cell>Model</cell><cell>FID</cell></row><row><cell>Resolution (128 × 128)</cell><cell></cell></row><row><cell>PGGAN [24]</cell><cell>64.50</cell></row><row><cell cols="2">PGGAN + QAGAN (SSIM) 47.46</cell></row><row><cell cols="2">PGGAN + QAGAN (NIQE) 49.80</cell></row><row><cell>Resolution (256 × 256)</cell><cell></cell></row><row><cell>PGGAN [24]</cell><cell>62.86</cell></row><row><cell cols="2">PGGAN + QAGAN (SSIM) 40.83</cell></row><row><cell cols="2">PGGAN + QAGAN (NIQE) 47.27</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salgan: Visual saliency prediction with adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>Oconnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Scene Understanding Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Banach wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6754" to="6763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving generalization and stability of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Hoang Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03984</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel L Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: computation in neural systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistics of natural image distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="962" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the mathematical properties of the structural similarity index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Vrscay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1488" to="1499" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ct-gan: Conditional transformation generative adversarial network for image attribute modification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Winovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04812</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
