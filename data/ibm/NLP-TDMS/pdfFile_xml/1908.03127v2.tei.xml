<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Andraghetti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univrses AB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panteleimon</forename><surname>Myriokefalitakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univrses AB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pier</forename><forename type="middle">Luigi</forename><surname>Dovesi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univrses AB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Luque</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univrses AB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Pieropan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univrses AB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating depth from a single image represents an attractive alternative to more traditional approaches leveraging multiple cameras. In this field, deep learning yielded outstanding results at the cost of needing large amounts of data labeled with precise depth measurements for training. An issue softened by self-supervised approaches leveraging monocular sequences or stereo pairs in place of expensive ground truth depth annotations. This paper enables to further improve monocular depth estimation by integrating into existing self-supervised networks a geometrical prior. Specifically, we propose a sparsity-invariant autoencoder able to process the output of conventional visual odometry algorithms working in synergy with depth-from-mono networks. Experimental results on the KITTI dataset show that by exploiting the geometrical prior, our proposal: i) outperforms existing approaches in the literature and ii) couples well with both compact and complex depth-from-mono architectures, allowing for its deployment on high-end GPUs as well as on embedded devices (e.g., NVIDIA Jetson TX2).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Researchers have tackled the problem of estimating depth from images for decades. Understanding depth allows to interpret the environment in three dimensions and ultimately enabling the construction of 3D maps particularly useful for autonomous navigation of mobile robotics platforms or augmented and virtual reality applications.</p><p>Most of the traditional approaches to estimate depth rely on the assumption of having multiple observations of the scene, either in time (e.g structure from motion) or in space (e.g. stereo or multi-view setup), and exploit hand-crafted features to find correspondences between images to estimate sparse depth measurements of the observed scene <ref type="bibr" target="#b17">[18]</ref>. More recently, machine learning approaches have shown remarkable advances in the field <ref type="bibr" target="#b12">[13]</ref>, enabling the dense estimation of depth from a single image, given that a large amount of labelled data is available at training time. Self-supervised paradigms relax this constraint <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51]</ref>, replacing the need for ground truth depth annotations, usually obtained by means of active sensors <ref type="bibr" target="#b14">[15]</ref>, with additional images acquired with stereo cameras <ref type="bibr" target="#b15">[16]</ref> or a single moving camera <ref type="bibr" target="#b50">[51]</ref>. The former strategy is usually more effective, being both the relative pose between the two cameras and the scale factor known.</p><p>Despite the promising results achieved by depth-frommono frameworks, they often fail in presence of ambiguous environments or elements rarely observed during the training procedure. This is caused by the absence of geometrical cues during the depth predictions, which is mostly learned upon context and semantic content of the observed scene. Inspired by the ability of humans in inferring depth from a single eye by leveraging prior knowledge (e.g. the size of known objects) <ref type="bibr" target="#b20">[21]</ref>, we propose to improve the estimation of depth by introducing a geometrical prior at inference time. Such prior comes in the form of sparse 3D measurements obtained by a traditional visual odometry (VO) algorithm that estimates the structure from consecutive images, most likely scenario occurring in autonomous navigation.</p><p>Specifically, we propose a framework that combines a sparsity-invariant <ref type="bibr" target="#b40">[41]</ref> autoencoder, which enriches our geometrical prior produced by a traditional VO algorithm, with stereo self-supervised models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> to predict depthfrom-mono avoiding the need of ground truth data which is hard to obtain. Experimental results on the KITTI dataset <ref type="bibr" target="#b14">[15]</ref> support the two main contributions of our work:</p><p>• our framework outperforms self-supervised approaches in literature.</p><p>• our strategy couples well with both complex <ref type="bibr" target="#b15">[16]</ref> and compact <ref type="bibr" target="#b34">[35]</ref> models, making it suited for deployment on high-end GPUs, as well as on embedded devices.</p><p>We point out that, conversely to traditional depth completion task <ref type="bibr" target="#b40">[41]</ref> whose aim is to densify an accurate, but sparse set of depth measurements usually sourced by an active sensor such as LiDAR <ref type="bibr" target="#b40">[41]</ref>, our approach keeps the depth estimation task in the image domain and does not rely on data from any other external source. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the proposed approach: given a single image (a) and a set of 3D points obtained through VO (b), these latter are processed by the autoencoder (c) and exploited to support final depth map estimation (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review the literature concerning VO, moving then to the advances in monocular depth estimation.</p><p>Visual odometry algorithms. Large progress has been achieved in the development of VO and SLAM methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Although a stereo setup <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref> avoids scale ambiguity, recent trend aims at recovering the scale of monocular VO exploiting geometry <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12]</ref> or deep learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Conversely to approaches leveraging depth to improve monocular VO and SLAM <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>, in this work we aim at boosting depth-from-mono accuracy exploiting VO. Supervised depth-from-mono. The first approaches were supervised and they needed indeed ground truth data to enforce the network to infer depth. Among seminal works, Saxena et al. <ref type="bibr" target="#b36">[37]</ref> proposed a method to estimate the absolute scales of different image patches and inferred the depth image using a Markov Random Field model, Ladick et al. <ref type="bibr" target="#b24">[25]</ref> incorporated semantic segmentation into their model to improve results. With the increasing availability of ground truth depth data, supervised approaches based on CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> appeared and rapidly outperformed <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref> previous techniques. State-of-the-art in this field is DORN <ref type="bibr" target="#b12">[13]</ref> trained with ordinal regression loss.</p><p>Self-supervised depth-from-mono An attractive trend concerns the possibility of learning depth-from-mono by replacing depth labels with multiple views of the sensed scene and leveraging on image synthesis to obtain supervision signals by having a loss on the reconstructed image. In general, acquiring images from a stereo camera enables a more effective training than using a single, moving camera, since the pose between frames is known. Concerning stereo supervision, Garg et al. <ref type="bibr" target="#b13">[14]</ref> first followed this approach, while Godard et al. <ref type="bibr" target="#b15">[16]</ref> introduced a leftright consistency loss. Other methods improved efficiency <ref type="bibr" target="#b34">[35]</ref>, deploying a pyramidal architecture, and accuracy by simulating a trinocular setup <ref type="bibr" target="#b35">[36]</ref>, including joint semantic segmentation <ref type="bibr" target="#b48">[49]</ref> or adding adversarial term <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. In <ref type="bibr" target="#b32">[33]</ref>, a strategy was proposed to reduce further the energy efficiency of <ref type="bibr" target="#b34">[35]</ref> leveraging fixed-point quantization. In <ref type="bibr" target="#b33">[34]</ref> knowledge distillation and cycle consistency proved to improve results, while <ref type="bibr" target="#b39">[40]</ref> introduces a stacked architecture, namely MonoResMatch, embodying virtual view synthesis and disparity computation and additional proxysupervision self-sourced by running a traditional stereo algorithm <ref type="bibr" target="#b19">[20]</ref>. Concerning supervision from single camera sequences, Zhou et al. <ref type="bibr" target="#b50">[51]</ref> were the first to follow this direction. Their approach was improved including additional cues such as point-cloud alignment <ref type="bibr" target="#b29">[30]</ref>, differentiable Direct Visual Odometry (DVO) <ref type="bibr" target="#b41">[42]</ref> and optical flow <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b47">48]</ref>. As for stereo supervision, traditional structure-from-motion algorithms (SFM) have been used to provide additional supervision <ref type="bibr" target="#b22">[23]</ref>. More recently, Casser et al. <ref type="bibr" target="#b2">[3]</ref> introduced moving object segmentation and online refinement. Finally, few works combined the best of the two worlds, as in <ref type="bibr" target="#b49">[50]</ref>. In particular, Yang et al. <ref type="bibr" target="#b45">[46]</ref> proposed Deep Virtual Stereo Odometry (DVSO), a framework for monocular depth and ego-motion estimation trained on proxylabels obtained from a stereo odometry algorithm. Finally, some approaches combine self-supervision with groundtruth labels from either LiDAR <ref type="bibr" target="#b23">[24]</ref> or synthetic datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>As proven in prior works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref>, we believe that traditional knowledge can provide additional cues to learningbased frameworks for monocular depth estimation. On the other hand, while existing works leverage such knowledge at training time only, we deploy a monocular VO algorithm to obtain geometrical priors to feed our network with. Being such priors sourced by a monocular setup, they are available at inference time in contrast to others available from stereo images <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b45">46]</ref> and thus available at training time only.</p><p>Depth completion. This category covers a collection of methods with a variety of different input modalities (e.g., relatively dense depth input <ref type="bibr" target="#b37">[38]</ref> vs sparse depth measurements <ref type="bibr" target="#b28">[29]</ref>; with color images for guidance <ref type="bibr" target="#b28">[29]</ref> vs. without <ref type="bibr" target="#b40">[41]</ref>). The completion problem becomes particularly challenging when the input depth image has very low density, because the inverse problem is ill-posed. One of the most popular scenario concerns with the use of 3D LiDARs, pro- <ref type="figure">Figure 2</ref>. Overview of our framework. Sparse depths (SD) provided by a VO algorithm are fed to a sparse auto-encoder producing more dense depths (DD), forwarded then to the main network together with color image. Self-supervision is obtained by means of stereo image reprojection plus consistency (green and lightblue) and the sparse points themselves (orange and violet). At deployment, monocular cues only are required (red path) viding roughly 5% pixels when reprojected on images <ref type="bibr" target="#b40">[41]</ref>. Specifically, Ma and Karaman <ref type="bibr" target="#b28">[29]</ref> proposed an end-to-end deep regression model for depth completion. Uhrig et al. <ref type="bibr" target="#b40">[41]</ref> proposed sparse convolution, a variant of regular convolution operations capable of dealing with data sparsity in neural networks. Eldesokey et al. <ref type="bibr" target="#b7">[8]</ref> improved the normalized convolution for confidence propagation. Chodosh et al. <ref type="bibr" target="#b3">[4]</ref> incorporated the traditional dictionary learning with deep learning into a single framework for depth completion. All learning based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> are trained in a supervised manner deploying depth labels.</p><p>In contrast we leverage depth estimates obtained by means of a VO algorithm, distinguishing our approach from reviewed depth completion models usually exploiting Li-DAR points that are i) sourced from very accurate, active sensors and ii) have an average density of 5% with respect to the entire image, while the VO pipeline used in our experiments only provides about 0.06% (i.e. 1 every 1600+ pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the rationale behind the proposed method and the modules deployed in our framework. We argue that it is unlikely that the entire life-cycle of an application is constrained, on most cases, to a single image acquisition at a single time frame. A popular example is represented by the autonomous driving scenario, where continuous image acquisition by means of a single camera is necessary. We aim at improving monocular depth estimation by leveraging this assumption in order to recover the geometry that is missing from a single image acquisition. For this purpose, we choose traditional VO algorithms to obtain a set of 3D points used as additional input to our framework to guide it towards more accurate estimations. In particular, sparse 3D points are mapped to image pixels and converted to an equivalent representation with respect to the one of the final depth output. For instance, in case of stereo self-supervision <ref type="bibr" target="#b15">[16]</ref> 3D points' depth is back-triangulated to disparity according to the specific setup (i.e., baseline and focal length) deployed for training. <ref type="figure">Figure 2</ref> shows our pipeline, made of two main modules: a sparsity-invariant autoencoder, processing the aforementioned 3D points to obtain more dense priors, and a depth estimator that outputs the final depth map when fed with the reference image and densified priors. While stereo images are required at training time, only the monocular input is processed at deployment (connected by the red path in the figure). In order to provide meaningful information to the network, the input cues are scale-aware. This can be easily obtained at training time by running a stereo VO algorithm <ref type="bibr" target="#b30">[31]</ref>, while at test time a monocular VO framework with any scale recovery, as for instance <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12]</ref>, is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparsity-invariant autoencoder</head><p>The first step in our pipeline consists in processing the 3D points retrieved by means of VO. Because of their sparse nature, we design a sparsity-invariant autoencoder, since traditional convolutions results in poor performance when dealing with such kind of data, as proven in <ref type="bibr" target="#b40">[41]</ref>. As shown in <ref type="figure">Figure 2</ref>, our autoencoder obtains a more dense depth map by means of five sparsity-invariant convolutional layers. The output of this module, namely DD in the figure, is supervised by the inner loss shown in the figure and better described in the remainder. <ref type="figure" target="#fig_1">Figure 3</ref> shows how the autoencoder is composed: 4 sparse-convolution layers with decreasing kernel size (9, 5, 3, 3), each one with 16 filters and stride fixed to 1 in order to keep the same resolution of the input. One final sparse-convolution pixel-wise filter is added in order to get an image that represents a denser disparity map which is used for the inner loss. Then, it is concatenated to the input image and forwarded both to the main depth estimator and to a skip residual module that will be further discussed. Since the output of the VO system is a set of sparse 3D points, it is possible to reproject them onto both left and right camera planes, generating sparse disparity maps SD L and SD R . During training we employ two autoencoders with shared weights to generate both DD L and DD R from left and right sparse ones. Therefore, we enforce consistency in the losses keeping the whole system symmetric. The rationale behind this choice will be discussed shortly, while ablation experiments will highlight the contribution introduced by such strategy. This symmetry is employed during training only, while at test time a single autoencoder processes left sparse disparity map SD L to generate DD L which is given to the depth estimator after a concatenation with RGB color image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth estimator</head><p>The recent literature provides multiple architectures for self-supervised monocular depth estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>. To prove that our proposal is compatible with both complex and compact networks, making it suited for a wide range of applications on both high-end GPUs as well as on lowpower devices, we choose two main models for our experiments: monodepth by Godard et al. <ref type="bibr" target="#b15">[16]</ref> and PyD-Net by Poggi et al. <ref type="bibr" target="#b34">[35]</ref>. The main difference between the two consists into the backbone used for features extraction.</p><p>The former represents the first model proposed for selfsupervised monocular depth estimation from stereo images. In its more accurate version, it consists of a ResNet50 <ref type="bibr" target="#b18">[19]</ref> encoder and about 58 million parameters. The latter deploys a compact, pyramidal features extractor, counting fewer than 2 million parameters and dramatically reducing both memory and runtime requirements <ref type="bibr" target="#b34">[35]</ref>. Experimental results will show how the proposed pipeline is compatible with different architectures designed to maximize either accuracy <ref type="bibr" target="#b15">[16]</ref> or efficiency <ref type="bibr" target="#b34">[35]</ref>.</p><p>In <ref type="figure">Figure 2</ref>, the aforementioned architecture (yellow block) is fed with an RGB image and densified depth cues and outputs two inverse depth maps D LR and D RL , i.e. disparity maps, aligned respectively with the input image (left frame of a training stereo pair) and the additional one used for supervision (right frame).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Skip module</head><p>In order to lighten the estimation task of the depth estimator, we add a residual skip module, further processing DD. This module is made of a single ResNet block <ref type="bibr" target="#b18">[19]</ref>, built by three layers respectively with kernels 1 × 1, 3 × 3, 1 × 1 and extracting 16, 16 and 64 features. In parallel, a skip connection made by a single 1 × 1 layer produces 64 features summed to those extracted by the latter of the previous three layers. A final 1 × 1 layer produces a residual correction DD . For symmetry, both DD L , DD R are processed by a shared skip module to obtain DD L , DD R .</p><p>Finally, we obtain two maps d L and d R as last output, respectively summing D LR to DD L and D RL to DD R . An outer loss is computed between these final outputs and SD making the depth estimator, in other words, focusing on the remaining portions of the image for which no prior depth is available. Since both d L and d R are optimized, the symmetry kept by the autoencoder avoids unbalancing between losses computed on the two, as explained in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>Following successful attempts in literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>, we deploy a multi-component loss function defined as</p><formula xml:id="formula_0">L = α st 4 s=1 L s st + α in L in + α out L out<label>(1)</label></formula><p>where L st , L in and L out are respectively stereo selfsupervision, inner and outer losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Stereo self-supervision</head><p>We train our network using stereo self-supervision <ref type="bibr" target="#b13">[14]</ref>. At training time, each sample consists of a stereo pair made of L and R, respectively, the image input to the model and the one used for image reprojection and the subsequent loss computation. According to Equation 1, at each scale we compute L st as</p><formula xml:id="formula_1">L st =β ap (L L ap + L R ap ) + β ds (L L ds + L R ds ) +β lr (L L lr + L R lr ) + β o (L L occ + L R occ )<label>(2)</label></formula><p>respectively made of appearance matching, disparity smoothness, left-right consistency and occlusion terms. Appearance Matching Loss. enforces the reconstructed image to appear similar to the corresponding training input, combination of L1 and single scale Structured Similarity Index Measure (SSIM) <ref type="bibr" target="#b43">[44]</ref> which compares, for each pixel of coordinates (i, j), the input image I L and its reprojected I L obtained by means of bilinear warping according to disparity estimations.</p><formula xml:id="formula_2">L L ap = 1 N ij γ 1 − SSIM (I L ij , I L ij ) 2 + (1 − γ)||I L ij − I L ij || (3)</formula><p>where N is the number of pixels and γ = 0.85.</p><p>Disparity Smoothness Loss enforces smooth disparities exploiting an L1 penalty on the disparity gradients ∂d, weighted by an edge aware term from the image.</p><formula xml:id="formula_3">L L ds = 1 N ij |∂ x d L ij |e −||∂xI L ij || + |∂ y d L ij |e −||∂yI L ij ||<label>(4)</label></formula><p>Left-Right Consistency Loss enforces the left and the right disparities to be consistent by using an L1 penalty between the left-to-right disparity map and the reconstructed one which comes from sampling the right-to-left in a similar manners as for the left and right images:</p><formula xml:id="formula_4">L L lr = 1 N ij |d L ij − d R ij+d L ij |<label>(5)</label></formula><p>Occlusion Loss discourages artifacts near occlusions <ref type="bibr" target="#b45">[46]</ref> by minimizing the sum of all disparities</p><formula xml:id="formula_5">L L occ = 1 N ij d L ij<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Inner loss</head><p>The purpose of sparsity-invariant autoencoder is to provide the depth estimator with more dense depth priors. To this aim, we enforce the output map DD to be consistent with the input cues SD where these are defined</p><formula xml:id="formula_6">L in = 1 N ij |DD ij − SD Lij |<label>(7)</label></formula><p>For symmetry, this is carried out on both DD L and DD R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Outer loss</head><p>The final prediction d L by our network is a sum of DD L produced by the skip module and D LR by the depth estimator. Again, since we want to preserve the information sourced by VO, we apply a second, outer loss to enforce consistency between SD and the final output</p><formula xml:id="formula_7">L out = 1 N ij |d ij − SD ij |<label>(8)</label></formula><p>As for the inner loss, this is carried out on both d L and d R as well for symmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we describe the dataset and the implementation details, and report results concerning our framework in various training/testing configurations, showing that our approach is consistently beneficial to traditional self-supervised approaches. To conform to the literature, we assess the performance of monocular depth estimation techniques following the protocol by Eigen et al. <ref type="bibr" target="#b6">[7]</ref>, extracting data from the KITTI <ref type="bibr" target="#b14">[15]</ref> dataset and using sparse LiDAR measurements as ground truth for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For all our experiments we compute standard metrics <ref type="bibr" target="#b6">[7]</ref> in the field of monocular depth estimation. Abs rel, Sq rel, RMSE and RMSE log represent error measures, while δ &lt; ε represents the percentage of predictions whose maximum between ratio and inverse ratio with respect to the ground truth is lower than ε, traditionally set to 1.25, 1.25 2 and 1.25 <ref type="bibr" target="#b2">3</ref> .</p><p>For our experiments we use the KITTI dataset <ref type="bibr" target="#b14">[15]</ref>. It consists of about 42382 rectified stereo pairs grouped into 61 sequences, with an image resolution of 1242×375. During acquisition, a LiDAR sensor gathered sparse depth measurements. For our experiments, we divide the entire dataset into a training and a testing set, following the traditional split by Eigen et al. <ref type="bibr" target="#b6">[7]</ref>. In particular, since our method is coupled with VO algorithms, we define different subdivisions, still compliant with the Eigen split:</p><p>• for training, we adopt the K r , K o sets introduced by Yang et al. <ref type="bibr" target="#b45">[46]</ref>, being the latter part of sequences 01, 02, 06, 08, 09 and 10 from KITTI odometry dataset.</p><p>• for testing, we adopt sequences 00, 04, 05 and 07 from KITTI odometry, that partially overlaps with the Eigen testing set.</p><p>This split allows for full deployment of VO algorithms both at training time, described in detail in the remainder, as well as for evaluation. Focusing on the testing split, the one we introduce counts 8691 frames, in contrast with the original one by Eigen et al. <ref type="bibr" target="#b6">[7]</ref> counting only 697 images, yet being fully consistent with it (i.e., there is no overlap between the 8691 frames with any of the frames from the Eigen original training set). This allows for a fair comparison with any method proposed in literature, if trained on the Eigen split and whose weights are provided by the authors, without the need for retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Our framework is implemented using the Tensorflow library. We designed two variants, namely VOMonodepth and VOPyD-Net, respectively built around the depth estimators proposed by <ref type="bibr">Godard</ref>   <ref type="bibr" target="#b34">[35]</ref>. In both cases, at training time we fed the network with batches of 8 images, using Adam Optimizer <ref type="bibr" target="#b21">[22]</ref> with β 1 = 0.9, β 2 = 0.999 and = 10 −8 and a learning rate of λ = 10 −4 , halved twice after 3 5 and 4 5 of the total epochs. The weights in our loss function have been tuned respectively to α st = 1, α in = 5, α out = 2, β app = 1, α lr = 1, α occ = 0.01 and β ds = 0.1/r, being r the downsampling factor of each scale. According to the chosen depth estimator, we run different training schedules: for VOPyD-Net, we run 200 epochs halving the learning rate at 120 and 160, while for VOMonodepth we run 50 epochs and halve at 30 and 40, following in both cases the guidelines suggested by the authors of PyD-Net and Monodepth respectively. We perform established data augmentation procedures <ref type="bibr" target="#b15">[16]</ref>, consisting of horizontal flip of the input and color augmentation with with a 50% chance, random gamma, brightness and color shifts. At inference time, the same postprocessing from <ref type="bibr" target="#b15">[16]</ref> is applied by VOMonodepth. Images are resized to 256 × 512 and VO points are reprojected accordingly, except for VOPyD-Net where SD maps are provided at half the resolution for the sake of speed, then estimated DD are upsampled to the original resolution.</p><p>In our experiments, we deploy two VO odometry algorithms for training. The two respectively exploit stereo and monocular sequences. While the former provides the correct scale, the second requires a scale recovery mechanisms as for instance in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12]</ref>. In order to ease the learning process, we perform a first round of training on K r + K o feeding the network with the 3D points from stereo VO. Then, we run a further round on K o switching to the monocular VO used at testing time. We will show in the experiments how this strategy is beneficial to our framework.</p><p>For stereo VO we use ORB-SLAM2 <ref type="bibr" target="#b30">[31]</ref>. As monocular VO algorithm with scale recovery, we use the pipeline developed by Zenuity 1 by their kind concession, the same is deployed for inference in our evaluation keeping our method fully monocular at deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>We first run a set of experiments to study the impact of each design choice introduced in our framework. Pur-1 https://www.zenuity.com/ posely, we train VOMonodepth in five configurations obtained by: i) directly feeding the depth estimator with VO input (baseline) ii) introducing the sparse-autoencoder iii) adding the skip module iv) performing symmetric training on the sparse points v) fine-tuning on K o and monocular VO. <ref type="table" target="#tab_0">Table 1</ref> collects the outcome of this evaluation, carried out on the testing split mentioned above and made of 8691 frames. For comparison, we report the results achieved by Monodepth-ResNet <ref type="bibr" target="#b15">[16]</ref>; for all models, we perform the post-processing step introduced in <ref type="bibr" target="#b15">[16]</ref>. We point out that, while the baseline barely outperforms <ref type="bibr" target="#b15">[16]</ref> on some metrics, the introduction of the sparse-autoencoder is crucial to boost the effectiveness of our approach. Adding the skip module (+skip) to our architecture enables for a slight improvement on Sq Rel, RMSE and δ scores. A major contribution is given by adding the symmetric training (+sym.), optimizing on VO points aligned both on the left and right images. It is worth noting that the aforementioned three configurations have been trained to leverage VO input from a stereo algorithm, while at deployment such cues comes from a monocular VO algorithm. Although the nature of input VO differs between training and testing, our technique is effective indeed at improving monocular depth estimation. Finally, by running a fine-tuning (+ft) switching from stereo to monocular VO input allows to a further, major boost on all metrics, leading to the best configuration. <ref type="table" target="#tab_1">Table 2</ref> reports results on the same testing split defined before. We point out once more that, since compliant with the Eigen split <ref type="bibr" target="#b6">[7]</ref>, we can compare our proposal to most existing methods. Specifically, we report in the table competitors for whose the source code or trained models are available, self-supervised either using monocular or stereo images. Unfortunately, code and models are not available for <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b33">34]</ref>, and hence, we are not able to compare with them. Methods marked with * have been pre-trained on CityScapes dataset <ref type="bibr" target="#b4">[5]</ref>, for whose the authors do not provide weights trained on KITTI only. The table is divided into three portions, from top to bottom: i) monocular supervised, ii) lightweight stereo-supervised and iii) complex stereo-supervised models. Our variants, VOPyD-Net and VOMonodepth, belong respectively to categories ii) and iii). Starting from compact models, we evaluate PyD-Net <ref type="bibr" target="#b34">[35]</ref> and its VO variant with and without applying the postprocessing (PP) step introduced in <ref type="bibr" target="#b15">[16]</ref>. However, it is worth observing that since PP requires to forward the input image twice, it adds a non-negligible overhead that is undesirable in case of deployment on embedded systems or when targeting the maximum efficiency <ref type="bibr" target="#b34">[35]</ref>. VOPyD-Net outperforms PyD-Net by a notable margin on most metrics, except Sq Rel and RMSE. In particular, δ &lt; 1.25 receives the highest improvements, i.e. 87.5% pixels are below the threshold versus the 82.5 of PyD-Net. By running the postprocessing, VOPyD-Net consistently outperforms it on all metrics by a significant margin. Moreover, this model also outperforms Monodepth-ResNet on all metrics and 3Net-ResNet on all except δ &lt; 1.25 2 and δ &lt; 1.25 3 , despite the much lower complexity of the network and the lower runtime, as discussed further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art</head><p>By coupling our strategy with a more complex architecture, as in the case of VOMonodepth-ResNet, we can outperform even MonoResMatch <ref type="bibr" target="#b39">[40]</ref> which deploys a more accurate architecture and leverages additional supervision from SGM algorithm <ref type="bibr" target="#b19">[20]</ref> at training time. This experiment further proves the effectiveness of our strategy, outperforming state-of-the-art methods for self-supervised monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Runtime analysis</head><p>We benchmark the performance of our VO variants and traditional models on two very different hardware platforms: an NVIDIA 2080 Ti GPU, having 250W of power consumption, and a Jetson TX2 module with a maximum average consumption of about 15W. The latter device represents an appealing platform for a wide range of applications. In all experiments, the TX2 board was configured for maximum performance. <ref type="table">Table 3</ref>  ysis, comparing Monodepth and PyD-Net with their VO counterparts. We report Fps both enabling and disabling post-processing. Focusing on the TX2 platform, we can notice how the difference between PyD-Net and VOPyD-Net is about 30%, running respectively at 24 and 18 Fps, if post-processing is disabled, about 7× and 5× faster than <ref type="bibr" target="#b15">[16]</ref> (3.41). A similar overhead, about 20%, is introduced comparing VOMonodepth-ResNet to Monodepth-ResNet. Enabling PP, VOPyD-Net still runs at more than 8 Fps, but it produces better results on each metrics (see <ref type="table" target="#tab_1">Table 2</ref>) compared to Monodepth-ResNet, despite running more than 3× faster. It also outperforms on most metrics 3Net-ResNet, running almost 4× faster. VOMonodepth achieves much higher accuracy at the cost of a further drop in speed (below 2 Fps), but still 40% faster than MonoResMatch.</p><p>Switching to NVIDIA 2080Ti GPU, a similar overhead between each model and its VO variant can be noticed. Even enabling post-processing, VOPyD-Net still runs at about 100 Fps versus the 62 and 49 by Monodepth-ResNet and 3Net-ResNet, while VOMonodepth-ResNet reaches  about 40 Fps versus the 29 of MonoResMatch, making the former the best choice when high-end GPUs are available for deployment thanks to its superior accuracy. Finally, <ref type="figure" target="#fig_2">Figure 4</ref> shows some qualitative examples of depth map inferred by both PyD-Net and Monodepth as well as their VO counterparts. We can notice how sparse inputs improve, for instance, estimates on thin structures (left column). <ref type="figure" target="#fig_3">Figure 5</ref> shows a comparison between Mon-odepth <ref type="bibr" target="#b15">[16]</ref> and VOMonodepth, reporting error maps (d), (f) to highlight how the former fails at estimating the depth for trees on the left, whereas our method is successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel framework that takes into account prior knowledge to improve monocular depth estimation. We have introduced a geometrical prior obtained by estimating the movement of the camera, as it commonly happens in an autonomous navigation scenario. Our network is able to leverage on the sparse 3D measurements of a VO algorithm to improve depth accuracy. Extensive experimental results on the KITTI dataset prove that our framework: i) outperforms existing models for selfsupervised depth estimation and ii) it is practical and couples with complex and compact models, allowing for accurate, real-time monocular depth estimation on high-end GPUs as well as on embedded systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Monocular depth estimation enhanced by visual odometry (VO). (a) Reference image, (b) sparse 3D points by a monocular VO pipeline, (c) initial densification, (d) final depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Structure of the sparsity-invariant autoencoder. Four convolutional layers extract 16 features each, respectively with 9 × 9, 5 × 5, 3 × 3 and 3 × 3 kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on KITTI dataset. On each column, from top to bottom: reference image, sparse VO points, depth map outputs from PyD-Net<ref type="bibr" target="#b34">[35]</ref>, VOPyD-Net, Monodepth<ref type="bibr" target="#b15">[16]</ref>, VOMonodepth and LiDAR points used for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison on KITTI dataset. (a) input image, (b) LiDAR points, depth and error maps respectively by Monodepth (c,d) and VOMonodepth (e,f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>et al. [16] and Poggi et al. Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Ablation experiments for VOMonodepth on KITTI [15] odometry sequences from the Eigen split [7] (8691 frames).</figDesc><table><row><cell>Lower is better</cell><cell>Higher is better</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Lower is better Higher is better Method Supervision PP VO Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Evaluation on KITTI [15] odometry sequences from the Eigen split [7] (8691 frames).</figDesc><table><row><cell>SfmLearner [51]</cell><cell>Mono  *</cell><cell>0.175</cell><cell>1.309</cell><cell>5.515</cell><cell>0.247</cell><cell>0.740</cell><cell>0.916</cell><cell>0.971</cell></row><row><cell>Vid2depth [30]</cell><cell>Mono  *</cell><cell>0.143</cell><cell>0.827</cell><cell>4.702</cell><cell>0.213</cell><cell>0.812</cell><cell>0.943</cell><cell>0.979</cell></row><row><cell>GeoNet -ResNet [48]</cell><cell>Mono  *</cell><cell>0.141</cell><cell>0.842</cell><cell>4.688</cell><cell>0.209</cell><cell>0.812</cell><cell>0.945</cell><cell>0.981</cell></row><row><cell>LKVO [42]</cell><cell>Mono  *</cell><cell>0.135</cell><cell>0.812</cell><cell>4.246</cell><cell>0.200</cell><cell>0.836</cell><cell>0.952</cell><cell>0.982</cell></row><row><cell>DF-Net [52]</cell><cell>Mono  *</cell><cell>0.131</cell><cell>0.706</cell><cell>4.365</cell><cell>0.196</cell><cell>0.831</cell><cell>0.952</cell><cell>0.984</cell></row><row><cell>Struct2depth (M) [3]</cell><cell>Mono  *</cell><cell>0.135</cell><cell>0.792</cell><cell>4.356</cell><cell>0.197</cell><cell>0.836</cell><cell>0.955</cell><cell>0.984</cell></row><row><cell>PyD-Net [35]</cell><cell>Stereo</cell><cell>0.130</cell><cell>0.833</cell><cell>4.569</cell><cell>0.219</cell><cell>0.825</cell><cell>0.938</cell><cell>0.974</cell></row><row><cell>VOPyD-Net</cell><cell>Stereo</cell><cell>0.105</cell><cell>0.916</cell><cell>4.916</cell><cell>0.203</cell><cell>0.874</cell><cell>0.946</cell><cell>0.974</cell></row><row><cell>PyD-Net [35]</cell><cell>Stereo</cell><cell>0.123</cell><cell>0.733</cell><cell>4.333</cell><cell>0.210</cell><cell>0.834</cell><cell>0.943</cell><cell>0.976</cell></row><row><cell>VOPyD-Net</cell><cell>Stereo</cell><cell>0.102</cell><cell>0.611</cell><cell>3.810</cell><cell>0.188</cell><cell>0.876</cell><cell>0.952</cell><cell>0.979</cell></row><row><cell>Monodepth -ResNet [16]</cell><cell>Stereo</cell><cell>0.108</cell><cell>0.679</cell><cell>4.123</cell><cell>0.194</cell><cell>0.868</cell><cell>0.952</cell><cell>0.978</cell></row><row><cell>3Net -ResNet [36]</cell><cell>Stereo</cell><cell>0.106</cell><cell>0.627</cell><cell>3.982</cell><cell>0.192</cell><cell>0.869</cell><cell>0.953</cell><cell>0.979</cell></row><row><cell>MonoResMatch [40]</cell><cell>Stereo+SGM</cell><cell>0.102</cell><cell>0.563</cell><cell>3.725</cell><cell>0.183</cell><cell>0.885</cell><cell>0.964</cell><cell>0.986</cell></row><row><cell>VOMonodepth -ResNet</cell><cell>Stereo</cell><cell>0.091</cell><cell>0.548</cell><cell>3.690</cell><cell>0.181</cell><cell>0.892</cell><cell>0.956</cell><cell>0.979</cell></row></table><note>* means pre-training on CityScapes [5].</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Zenuity for providing their VO pipeline for our experiments. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion: A structured approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018 -14th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="499" to="513" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1604.01685</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Deep Learning for Visual SLAM, (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network. CoRR, abs/1406.2283</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">14</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LSD-SLAM: Largescale direct monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale direct slam with stereo cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1935" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal scale estimation for monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sturck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<meeting><address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno>abs/1603.04992</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno>abs/1609.03677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>intelligence</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Perceiving in depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
		<idno>mechanisms. 2012. 1</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning sfm from sfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1702.02706</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018-05-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ORB-SLAM2: an opensource SLAM system for monocular, stereo and RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
		<idno>abs/1610.06475</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enabling energy-efficient unsupervised monocular depth estimation on armv7-based platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peluso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cipolletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calimera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation and Test in Europe (DATE)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/JRS Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Layer depth denoising and completion for structured-light rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cnn-slam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6243" to="6252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1708.06500</idno>
	</analytic>
	<monogr>
		<title level="j">Sparsity invariant cnns. CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monocular visual odometry scale recovery using geometrical constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scale recovery for monocular visual odometry using depth estimated with deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5870" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Geometry meets semantic for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
