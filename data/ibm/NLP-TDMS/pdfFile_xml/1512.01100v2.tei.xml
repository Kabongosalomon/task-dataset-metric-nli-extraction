<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective LSTMs for Target-Dependent Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<email>dytang@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
							<email>xcfeng@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective LSTMs for Target-Dependent Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis, also known as opinion mining <ref type="bibr" target="#b17">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b14">Liu, 2012)</ref>, is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification <ref type="bibr" target="#b7">(Jiang et al., 2011;</ref><ref type="bibr" target="#b4">Dong et al., 2014;</ref><ref type="bibr" target="#b26">Vo and Zhang, 2015)</ref>, which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: "I bought a new camera. The picture quality is amazing but the battery life is too short". If the target string is picture quality, the expected sentiment polarity is "positive" as the sentence expresses a positive opinion towards picture quality. If we consider the target as battery life, the correct sentiment polarity should be "negative".</p><p>Target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. Majority of existing studies build sentiment classifiers with supervised machine learning approach, such as feature based Supported Vector Machine <ref type="bibr" target="#b7">(Jiang et al., 2011)</ref> or neural network approaches <ref type="bibr" target="#b4">(Dong et al., 2014;</ref><ref type="bibr" target="#b26">Vo and Zhang, 2015)</ref>. Despite the effectiveness of these approaches, we argue that target-dependent sentiment classification remains a challenge: how to effectively model the semantic relatedness of a target word with its context words in a sentence. One straight forward way to address this problem is to manually design a set of target-dependent features, and integrate them into existing feature-based SVM. However, feature engineering is labor intensive and the "sparse" and "discrete" features are clumsy in encoding side information like target-context relatedness. In addition, a person asked to do this task will naturally "look at" parts of relevant context words which are helpful to determine the sentiment polarity of a sentence towards the target. These motivate us to develop a powerful neural network approach, which is capable of learning continuous features (representations) without feature engineering and meanwhile capturing the intricate relatedness between target and context words. </p><formula xml:id="formula_0">LSTM L 1 LSTM L …… ℎ 1 ℎ LSTM L +1 LSTM L −1 …… ℎ −1 ℎ +1 LSTM R +1 LSTM R −1 …… ℎ LSTM R LSTM R …… ℎ target words target words ℎ −1 ℎ +1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head><p>Target-Dependent Long Short-Term Memory <ref type="figure">Figure 1</ref>: The basic long short-term memory (LSTM) approach and its target-dependent extension TD-LSTM for target-dependent sentiment classification. w stands for word in a sentence whose length is n, {w l+1 , w l+2 , ..., w r−1 } are target words, {w 1 , w 2 , ..., w l } are preceding context words, {w r , ..., w n−1 , w n } are following context words.</p><p>In this paper, we present neural network models to deal with target-dependent sentiment classification. The approach is an extension on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) by incorporating target information. Such target-dependent LSTM approach models the relatedness of a target word with its context words, and selects the relevant parts of contexts to infer the sentiment polarity towards the target. The model could be trained in an end-to-end way with standard backpropagation, where the loss function is cross-entropy error of supervised sentiment classification.</p><p>We apply the neural model to target-dependent sentiment classification on a benchmark dataset <ref type="bibr" target="#b4">(Dong et al., 2014)</ref>. We compare with feature-based SVM <ref type="bibr" target="#b7">(Jiang et al., 2011)</ref>, adaptive recursive neural network <ref type="bibr" target="#b4">(Dong et al., 2014)</ref> and lexicon-enhanced neural network <ref type="bibr" target="#b26">(Vo and Zhang, 2015)</ref>. Empirical results show that the proposed approach without using syntactic parser or external sentiment lexicon obtains state-ofthe-art classification accuracy. In addition, we find that modeling sentence with standard LSTM does not perform well on this target-dependent task. Integrating target information into LSTM could significantly improve the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Approach</head><p>We describe the proposed approach for target-dependent sentiment classification in this section. We first present a basic long short-term memory (LSTM) approach, which models the semantic representation of a sentence without considering the target word being evaluated. Afterwards, we extend LSTM by considering the target word, obtaining the Target-Dependent Long Short-Term Memory (TD-LSTM) model. Finally, we extend TD-LSTM with target connection, where the semantic relatedness of target with its context words are incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long Short-Term Memory (LSTM)</head><p>In this part, we describe a long short-term memory (LSTM) model for target-dependent sentiment classification. It is a basic version of our approach. In this setting, the target to be evaluated is ignored so that the task is considered in a target independent way.</p><p>We use LSTM as it is a state-of-the-art performer for semantic composition in the area of sentiment</p><formula xml:id="formula_1">LSTM L 1 LSTM L …… ℎ 1 ℎ LSTM L +1 LSTM L −1 …… ℎ −1 ℎ +1 LSTM R +1 LSTM R −1 …… ℎ LSTM R LSTM R …… ℎ ℎ −1 ℎ +1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head><p>Target-Connection Long Short-Term Memory <ref type="figure">Figure 2</ref>: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {w l+1 , w l+2 , ..., w r−1 } are target words, v target is target representation, {w 1 , w 2 , ..., w l } are preceding context words, {w r , ..., w n−1 , w n } are following context words.</p><p>analysis <ref type="bibr" target="#b12">(Li et al., 2015a;</ref>. It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence.</p><p>Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b19">Pennington et al., 2014;</ref>. All the word vectors are stacked in a word embedding matrix L w ∈ R d×|V | , where d is the dimension of word vector and |V | is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms <ref type="bibr" target="#b19">(Pennington et al., 2014;</ref> to make better use of semantic and grammatical associations of words.</p><p>We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in <ref type="figure">Figure 1</ref>. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping vectors of words with variable length to a fixed-length vector by recursively transforming current word vector w t with the output vector of the previous step h t−1 . The transition function of standard RNN is a linear layer followed by a pointwise non-linear layer such as hyperbolic tangent function (tanh).</p><formula xml:id="formula_2">h t = tanh(W · [h t−1 ; w t ] + b) (1) where W ∈ R d×2d , b ∈ R d , d</formula><p>is dimension of word vector. However, standard RNN suffers the problem of gradient vanishing or exploding <ref type="bibr" target="#b1">(Bengio et al., 1994;</ref><ref type="bibr">Hochreiter and Schmidhuber, 1997)</ref>, where gradients may grow or decay exponentially over long sequences. Many researchers use a more sophisticated and powerful LSTM cell as the transition function, so that long-distance semantic correlations in a sequence could be better modeled. Compared with standard RNN, LSTM cell contains three additional neural gates: an input gate, a forget gate and an output gate. These gates adaptively remember input vector, forget previous history and generate output vector (Hochreiter and Schmidhuber, 1997). LSTM cell is calculated as follows.</p><formula xml:id="formula_3">i t = σ(W i · [h t−1 ; w t ] + b i ) (2) f t = σ(W f · [h t−1 ; w t ] + b f ) (3) o t = σ(W o · [h t−1 ; w t ] + b o ) (4) g t = tanh(W r · [h t−1 ; w t ] + b r ) (5) c t = i t g t + f t c t−1 (6) h t = o t tanh(c t )<label>(7)</label></formula><p>where stands for element-wise multiplication, σ is sigmoid function,</p><formula xml:id="formula_4">W i , b i , W f , b f , W o , b</formula><p>o are the parameters of input, forget and output gates.</p><p>After calculating the hidden vector of each position, we regard the last hidden vector as the sentence representation <ref type="bibr" target="#b12">(Li et al., 2015a;</ref>. We feed it to a linear layer whose output length is class number, and add a sof tmax layer to output the probability of classifying the sentence as positive, negative or neutral. Softmax function is calculated as follows, where C is the number of sentiment categories.</p><formula xml:id="formula_5">sof tmax i = exp(x i ) C i =1 exp(x i ) (8) 2.2 Target-Dependent LSTM (TD-LSTM)</formula><p>The aforementioned LSTM model solves target-dependent sentiment classification in a targetindependent way. That is to say, the feature representation used for sentiment classification remains the same without considering the target words. Let us again take "I bought a new camera. The picture quality is amazing but the battery life is too short" as an example. The representations of this sentence with regard to picture quality and battery life are identical. This is evidently problematic as the sentiment polarity labels towards these two targets are different.</p><p>To take into account of the target information, we make a slight modification on the aforementioned LSTM model and introduce a target-dependent LSTM (TD-LSTM) in this subsection. The basic idea is to model the preceding and following contexts surrounding the target string, so that contexts in both directions could be used as feature representations for sentiment classification. We believe that capturing such target-dependent context information could improve the accuracy of target-dependent sentiment classification.</p><p>Specifically, we use two LSTM neural networks, a left one LSTM L and a right one LSTM R , to model the preceding and following contexts respectively. An illustration of the model is shown in <ref type="figure">Figure 1</ref>. The input of LSTM L is the preceding contexts plus target string, and the input of LSTM R is the following contexts plus target string. We run LSTM L from left to right, and run LSTM R from right to left. We favor this strategy as we believe that regarding target string as the last unit could better utilize the semantics of target string when using the composed representation for sentiment classification. Afterwards, we concatenate the last hidden vectors of LSTM L and LSTM R , and feed them to a sof tmax layer to classify the sentiment polarity label. One could also try averaging or summing the last hidden vectors of LSTM L and LSTM R as alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Target-Connection LSTM (TC-LSTM)</head><p>Compared with LSTM model, target-dependent LSTM (TD-LSTM) could make better use of the target information. However, we think TD-LSTM is still not good enough because it does not capture the interactions between target word and its contexts. Furthermore, a person asked to do target-dependent sentiment classification will select the relevant context words which are helpful to determine the sentiment polarity of a sentence towards the target.</p><p>Based on the consideration mentioned above, we go one step further and develop a target-connection long short-term memory (TC-LSTM). This model extends TD-LSTM by incorporating an target connection component, which explicitly utilizes the connections between target word and each context word when composing the representation of a sentence.</p><p>An overview of TC-LSTM is illustrated in <ref type="figure">Figure 2</ref>. The input of TC-LSTM is a sentence consisting of n words {w 1 , w 2 , ...w n } and a target string t occurs in the sentence. We represent target t as {w l+1 , w l+2 ...w r−1 } because a target could be a word sequence of variable length, such as "google" or "harry potter". When processing a sentence, we split it into three components: target words, preceding context words and following context words. We obtain target vector v target by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities <ref type="bibr" target="#b20">(Socher et al., 2013a;</ref><ref type="bibr" target="#b22">Sun et al., 2015)</ref>. When compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in TD-LSTM. The difference is that in TC-LSTM the input at each position is the concatenation of word embedding and target vector v target , while in TD-LSTM the input at each position only includes the embedding of current word. We believe that TC-LSTM could make better use of the connection between target and each context word when building the representation of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Training</head><p>We train LSTM, TD-LSTM and TC-LSTM in an end-to-end way in a supervised learning framework. The loss function is the cross-entropy error of sentiment classification.</p><formula xml:id="formula_6">loss = − s∈S C c=1 P g c (s) · log(P c (s))<label>(9)</label></formula><p>where S is the training data, C is the number of sentiment categories, s means a sentence, P c (s) is the probability of predicting s as class c given by the sof tmax layer, P g c (s) indicates whether class c is the correct sentiment category, whose value is 1 or 0. We take the derivative of loss function through back-propagation with respect to all parameters, and update parameters with stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We apply the proposed method to target-dependent sentiment classification to evaluate its effectiveness. We describe experimental setting and empirical results in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We conduct experiment in a supervised setting on a benchmark dataset <ref type="bibr" target="#b4">(Dong et al., 2014)</ref>. Each instance in the training/test set has a manually labeled sentiment polarity. Training set contains 6,248 sentences and test set has 692 sentences. The percentages of positive, negative and neutral in training and test sets are both 25%, 25%, 50%. We train the model on training set, and evaluate the performance on test set. Evaluation metrics are accuracy and macro-F1 score over positive, negative and neutral categories <ref type="bibr" target="#b15">(Manning and Schütze, 1999;</ref><ref type="bibr" target="#b8">Jurafsky and Martin, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to Other Methods</head><p>We compare with several baseline methods, including:</p><p>In SVM-indep, SVM classifier is built with target-independent features, such as unigram, bigram, punctuations, emoticons, hashtags, the numbers of positive or negative words in General Inquirer sentiment lexicon. In SVM-dep, target-dependent features <ref type="bibr" target="#b7">(Jiang et al., 2011)</ref> are also concatenated as the feature representation.</p><p>In Recursive NN, standard Recursive neural network is used for feature learning over a transfered target-dependent dependency tree <ref type="bibr" target="#b4">(Dong et al., 2014)</ref>. AdaRNN-w/oE, AdaRNN-w/E and AdaRNNcomb are different variations of adaptive recursive neural network <ref type="bibr" target="#b4">(Dong et al., 2014)</ref>, whose composition functions are adaptively selected according to the inputs.</p><p>In Target-dep, SVM classifier is built based on rich target-independent and target-dependent features <ref type="bibr" target="#b26">(Vo and Zhang, 2015)</ref>. In Target-dep + , sentiment lexicon features are further incorporated.</p><p>The neural models developed in this paper are abbreviated as LSTM, TD-LSTM and TC-LSTM, which are described in the previous section. We use 100-dimensional Glove vectors learned from Twitter, randomize the parameters with uniform distribution U (−0.003, 0.003), set the clipping threshold of softmax layer as 200 and set learning rate as 0.01.</p><p>Experimental results of baseline models and our methods are given in <ref type="table">Table 1</ref>. Comparing between SVM-indep and SVM-dep, we can find that incorporating target information can improve the classification accuracy of a basic SVM classifier. AdaRNN performs better than feature based SVM by making use of dependency parsing information and tree-structured semantic composition. We can find that targetdep is a strong performer even without using lexicon features. It benefits from rich automatic features generated from word embeddings.</p><p>Among LSTM based models described in this paper, the basic LSTM approach performs worst. This is not surprising because this task requires understanding target-dependent text semantics, while the basic LSTM model does not capture any target information so that it predicts the same result for different targets in a sentence. TD-LSTM obtains a big improvement over LSTM when target signals are taken into consideration. This result demonstrates the importance of target information for target-dependent sentiment classification. By incorporating target-connection mechanism, TC-LSTM obtains the best performances and outperforms all baseline methods in term of classification accuracy.</p><p>Comparing between Target-dep + and Target-dep, we find that sentiment lexicon feature could further improve the classification accuracy. Our final model TC-LSTM without using sentiment lexicon information performs comparably with Target-dep + . We believe that incorporation lexicon information in TC-LSTM could get further improvement. We leave this as a potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effects of Word Embeddings</head><p>It is well accepted that a good word embedding is crucial to composing a powerful text representation at higher level. We therefore study the effects of different word embeddings on LSTM, TD-LSTM and TC-LSTM in this part. Since the benchmark dataset from <ref type="bibr" target="#b4">(Dong et al., 2014)</ref> comes from Twitter, we compare between sentiment-specific word embedding (SSWE) 2  and Glove vectors 3 <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref>. All these word vectors are 50-dimensional and learned from Twitter. SSWE h , SSWE r and SSWE u are different embedding learning algorithms introduced in . SSWE h and SSWE r learn word embeddings by only using sentiment of sentences. SSWE u takes into account of sentiment of sentences and contexts of words simultaneously.  We compare between SSWE h , SSWE r , SSWE u and Glove vectors.</p><p>From <ref type="figure" target="#fig_1">Figure 3</ref>, we can find that SSWE h and SSWE r perform worse than SSWE u , which is consistent with the results reported on target-independent sentiment classification of tweets . This shows the importance of context information for word embedding learning as both SSWE h and SSWE r do not encode any word contexts. Glove and SSWE u perform comparably, which indicates the importance of global context for estimating a good word representation. In addition, the target connection model TC-LSTM performs best when considering a specific word embedding. <ref type="table" target="#tab_2">LSTM  27  95  329  LSTM-TD  20  93  274  LSTM-TC  65  280</ref> 1,165 We compare between Glove vectors with different dimensions (50/100/200). Classification accuracy and time cost are given in <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="table" target="#tab_2">Table 2</ref>, respectively. We can find that 100-dimensional word vectors perform better than 50-dimensional word vectors, while 200-dimensional word vectors do not show significant improvements. Furthermore, TD-LSTM and LSTM have similar time cost, while TD-LSTM gets higher classification accuracy as target information is incorporated. TC-LSTM performs slightly better than TD-LSTM while at the cost of longer training time because the parameter number of TC-LSTM is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50dms 100dms 200dms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study</head><p>In this section, we explore to what extent the target-dependent LSTM models including TD-LSTM and TC-LSTM improve the performance of a basic LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>gold LSTM i hate my ipod look at my last tweet before the argh one that 's for you -1 0 okay soooo ... ummmmm .... what is going on with lindsay lohan' s face? boring day at the office = perez and tomorrow overload. not good 0 -1 i heard ShannonBrown did his thing in the lakers game!! got ta love him 0 1 Hey google, thanks for all these great Labs features on Chromium, but how about " Create Application Shortcut"?! 1 0 <ref type="table">Table 3</ref>: Examples drawn from the test set whose polarity labels are incorrectly inferred by LSTM but correctly predicted by both TD-LSTM and TC-LSTM. For each example, target words are in bold, "gold" is the ground truth and "LSTM" means the predicted sentiment label from LSTM model.</p><p>In <ref type="table">Table 3</ref>, we list some examples whose polarity labels are incorrectly inferred by LSTM but correctly predicted by both TD-LSTM and TC-LSTM. We observe that LSTM model prefers to assigning the polarity of the entire sentence while ignoring the target to be evaluated. TD-LSTM and TC-LSTM could take into account of target information to some extend. For example, in the 2nd example the opinion holder expresses a negative opinion about his work, but holds a neutral sentiment towards the target "lindsay lohan". In the last example, the whole sentence expresses a neutral sentiment while it holds a positive opinion towards "google".</p><p>We analyse the error cases that both TD-LSTM and TC-LSTM cannot well handle, and find that 85.4% of the misclassified examples relate to neutral category. The positive instances are rarely misclassified as negative, and vice versa. A example of errors is: "freaky friday on television reminding me to think wtf happened to lindsay lohan, she was such a terrific actress , + my huge crush on haley hudson.", which is incorrectly predicted as positive towards target "indsay lohan" in both TD-LSTM and TC-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>In order to capture the semantic relatedness between target and context words, we extend TD-LSTM by adding a target connection component. One could also try other extensions to capture the connection between target and context words. For example, we also tried an attention-based LSTM model, which is inspired by the recent success of attention-based neural network in machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> and document encoding <ref type="bibr" target="#b13">(Li et al., 2015b)</ref>. We implement the soft-attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> to enhance TD-LSTM. We incorporate two attention layers for preceding LSTM and following LSTM, respectively. The output vector for each attention layer is the weighted average among hidden vectors of LSTM, where the weight of each hidden vector is calculated with a feedforward neural network. The outputs of preceding and following attention models are concatenated and fed to sof tmax for sentiment classification. However, we cannot obtain better result with such an attention model. The accuracy of this attention model is slightly lower than the standard LSTM model (around 65%), which means that the attention component has a negative impact on the model. A potential reason might be that the attention based LSTM has larger number of parameters, which cannot be easily optimized with the small number of corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>We briefly review existing studies on target-dependent sentiment classification and neural network approaches for sentiment classification in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Target-Dependent Sentiment Classification</head><p>Target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. Therefore, standard text classification approach such as feature-based Supported Vector Machine <ref type="bibr">(Pang et al., 2002;</ref><ref type="bibr" target="#b7">Jiang et al., 2011)</ref> can be naturally employed to build a sentiment classifier. Despite the effectiveness of feature engineering, it is labor intensive and unable to discover the discriminative or explanatory factors of data. To handle this problem, some recent studies <ref type="bibr" target="#b4">(Dong et al., 2014;</ref><ref type="bibr" target="#b26">Vo and Zhang, 2015)</ref> use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. <ref type="bibr" target="#b4">Dong et al. (2014)</ref> transfer a dependency tree of a sentence into a target-specific recursive structure, and get higher level representation based on that structure. <ref type="bibr" target="#b26">Vo and Zhang (2015)</ref> use rich features including sentiment-specific word embedding and sentiment lexicons. Different from previous studies, the LSTM models developed in this work are purely data-driven, and do not rely on dependency parsing results or external sentiment lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Network for Sentiment Classification</head><p>Neural network approaches have shown promising results on many sentence/document-level sentiment classification <ref type="bibr" target="#b21">(Socher et al., 2013b;</ref>. The power of neural model lies in its ability in learning continuous text representation from data without any feature engineering. For sentence/document level sentiment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref>. Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality <ref type="bibr">(Frege, 1892)</ref>. Representative compositional approaches to learn sentence representation include recursive neural networks <ref type="bibr" target="#b21">(Socher et al., 2013b;</ref><ref type="bibr" target="#b6">Irsoy and Cardie, 2014)</ref>, convolutional neural network <ref type="bibr" target="#b9">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b10">Kim, 2014)</ref>, long short-term memory <ref type="bibr" target="#b12">(Li et al., 2015a)</ref> and tree-structured LSTM <ref type="bibr" target="#b23">(Tai et al., 2015;</ref><ref type="bibr" target="#b28">Zhu et al., 2015)</ref>. There also exists some studies focusing on learning continuous representation of documents <ref type="bibr" target="#b11">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b3">Bhatia et al., 2015;</ref><ref type="bibr" target="#b27">Yang et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracy of LSTM, TD-LSTM and TC-LSTM with different word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Time cost of each model with 50dms, 100dms and 200dms Glove vectors. Each value means how many seconds cost in each training iteration.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">SSWE vectors are publicly available at http://ir.hit.edu.cn/˜dytang 3 Glove vectors are publicly available at http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We greatly thank Yaming Sun for tremendously helpful discussions. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61632011 and No.61273321). According to the meaning given to this role by Harbin Institute of Technology, the contact author of this paper is Bing Qin.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<idno>Frege1892] Gottlob Frege. 1892</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Ludlow</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="563" to="584" />
		</imprint>
	</monogr>
	<note>On sense and reference</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
	<note>Irsoy and Cardie2014</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Target-dependent twitter sentiment classification. ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
	<note>Jurafsky and Martin2000</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mikolov2014] Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<title level="m">When are tree structures necessary for deep learning of representations? EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><surname>Schütze1999</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
	<note>Pang and Lee2008</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
	<note>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<title level="m">Reasoning with neural tensor networks for knowledge base completion. NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Sun et al.2015</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Duyu Tang, Bing Qin, and Ting Liu</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
