<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REAL-TIME MULTIPLE PEOPLE TRACKING WITH DEEPLY LEARNED CANDIDATE SELECTION AND PERSON RE-IDENTIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
							<email>l-chen16@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Lab for Info. Sci. &amp; Tech. (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Lab for Info. Sci. &amp; Tech. (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Lab for Info. Sci. &amp; Tech. (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Lab for Info. Sci. &amp; Tech. (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REAL-TIME MULTIPLE PEOPLE TRACKING WITH DEEPLY LEARNED CANDIDATE SELECTION AND PERSON RE-IDENTIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-object tracking</term>
					<term>convolutional neu- ral network</term>
					<term>person re-identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on largescale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Tracking multiple objects in a complex scene is a challenging problem in many video analysis and multimedia applications, such as visual surveillance, sport analysis, and autonomous driving. The objective of multi-object tracking is to estimate trajectories of objects in a specific category. Here we tackle the problem of people tracking by taking advantage of person re-identification.</p><p>Multi-object tracking benefits a lot from advances in object detection in the past decade. The popular tracking-bydetection methods apply the detector on each frame, and associate detection across frames to generate object trajectories. Both intra-category occlusion and unreliable detection are tremendous challenges in such a tracking framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Intra-category occlusion and similar appearances of objects can result in ambiguities in data association. Multiple cues, including motion, shape and object appearances, are fused to mitigate this problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. On the other hand, detection results are not always reliable. Pose variation and occlusion in crowded scenes often cause detection failures such as false positives, missing detection, and non-accurate bounding. Some studies proposed to handle unreliable detection in a batch mode <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. These methods address detection noises by introducing information from future frames. Detection results in whole video frames or a temporal window are employed and linked to trajectories by solving a global optimization problem. Tracking in a batch mode is non-causal and not suitable for time-critical applications. In contrast to these works, we focus on the online multiple people tracking problem, using only the current and past frames. In order to handle unreliable detection in an online mode, our tracking framework optimally selects candidates from outputs of both detection and tracks in each frame (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>). In most of the existing tracking-by-detection methods, when talking about data association, candidates to be associated with existing tracks are only made up of detection results. Yan et al. <ref type="bibr" target="#b3">[4]</ref> proposed to treat the tracker and object detector as two independent identities, and keep results of them as candidates. They selected candidates based on hand-crafted features, e.g., color histogram, optical flow, and motion features. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. On the one hand, reliable predictions from the tracker can be used for short-term association in case of missing detection or non-accurate bounding. On the other hand, confident detection results are essential to prevent tracks drifting to backgrounds in the long term. How to score outputs of both detection and tracks in an unified way is still an open question.</p><p>Recently, deep neural networks, especially convolutional neural networks (CNN), have made great progress in the field of computer vision and multimedia. In this paper, we take full advantage of deep neural networks to tackle unreliable detection and intra-category occlusion. Our contribution is three fold. First, we handle unreliable detection in online tracking by combining both detection and tracking results as candidates, and selecting optimal candidates based on deep neural networks. Second, we present a hierarchical data association strategy, which utilizes spatial information and deeply learned person re-identification (ReID) features. Third, we demonstrate real-time and state-of-the-art performance of our tracker on a widely used people tracking benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Tracking-by-detection is becoming the most popular strategy for multi-object tracking. Bae et al. <ref type="bibr" target="#b0">[1]</ref> associated tracklets with detection in different ways according to their confidence values. Sanchez-Matilla et al. <ref type="bibr" target="#b6">[7]</ref> exploited multiple detectors to improve tracking performance. They collected outputs from multiple detectors, during the so called overdetection process. Combining results from multiple detectors can improve the tracking performance but is not efficient for real-time applications. In contrast, our tracking framework needs only one detector and generates candidates from existing tracks. Chu et al. <ref type="bibr" target="#b7">[8]</ref> used a binary classifier and single object tracker for online multi-object tracking. They shared the feature maps for classification but still had a high computation complicity.</p><p>Batch methods formulate tracking as a global optimization problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>. These methods utilized information from future frames to handle noisy detection and reduce ambiguities in data association. Liu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a rewind to track strategy to generate backward tracklets involving future information, to obtain a more stable similarity measurement for association. Person re-identification was also explored in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> for the global optimization. Our framework leverages deeply learned ReID features in an online mode, to improve the identification ability when coping with the problem of intra-category occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>In this work, we extend traditional tracking-by-detection by collecting candidates from outputs of both detection and tracks. Our framework consists of two sequential tasks, that is, candidate selection and data association.</p><p>We first measure all the candidates using an unified scoring function. A discriminatively trained object classifier and a well-designed tracklet confidence are fused to formulate the scoring function, as described in Section 3.2 and Section 3.3. Non-maximal suppression (NMS) is subsequently performed with the estimated scores. After obtaining candidates without redundancy, we use both appearance representations and spatial information to hierarchically associate existing tracks with the selected candidates. Our appearance representations are deeply learned from the person re-identification as described in Section 3.4. Hierarchical data association is detailed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Real-Time Object Classification</head><p>Combining outputs of both detection and tracks will result in an excessive amount of candidates. Our classifier shares most computations on the entire image by using a region-based fully convolutional neural network (R-FCN) <ref type="bibr" target="#b11">[12]</ref>. Thus, it is much more efficient comparing to classification on image patches, which are cropped from heavily overlapped candidate regions. The comparison of time consumption of these two methods can be found in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Our efficient classifier is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Given an image frame, score maps of the entire image are predicted using a fully convolutional neural network with an encoderdecoder architecture. The encoder part is a light-weight convolutional backbone for real-time performance, and we introduce the decoder part with up-sampling to increase the spatial resolution of output score maps for later classification. Each candidate to be classified is defined as a region of interest (RoI) by x = (x 0 , y 0 , w, h), where (x 0 , y 0 ) denotes the topleft point and w, h represent width and height of the region. For computational efficiency, we expect that the classification probability of each RoI is directly voted by the shared score maps. A straightforward approach for voting is to construct foreground probabilities for all points on the image, and then calculate the average probability of points inside the RoI. However, this simple strategy loses the spatial information of objects. For instance, even if the RoI only covers a part of the object, a high confidence score still can be obtained.</p><p>In order to explicitly encode spatial information into the score maps, we employ the position-sensitive RoI pooling layer <ref type="bibr" target="#b11">[12]</ref> and estimate the classification probability from k 2 position-sensitive score maps z. In particular, we split a RoI into k × k bins by a regular grid. Each of the bins has the same size w k × h k , and represents a specific spatial location of the object. We extract responses of k×k bins from k 2 score maps. Each score map only corresponds to one bin. The final classification probability of a RoI x is formulated as:</p><formula xml:id="formula_0">p(y|z, x) = σ( 1 wh k 2 i=1 (x,y)∈bini z i (x, y)),<label>(1)</label></formula><p>where σ(x) = 1 1+e −x is the sigmoid function, and z i denotes the i-th score map.</p><p>During the training procedure, we randomly sample RoIs around the ground truth bounding boxes as positive examples, and take the same number of RoIs from backgrounds as negative examples. By training the network end-to-end, the output on the top of the decoder part, that is, the k 2 score maps, learns to response to specific spatial locations of the object. For example, if k = 3, we have 9 score maps response to top-left, top-center, top-right, ..., bottom-right of the object, respectively. In this way, the RoI pooling layer is sensitive to spatial positions and has a strong discriminative ability for object classification without using learnable parameters. Please note that the proposed neural network is trained only for candidate classification, not for bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tracklet Confidence and Scoring Function</head><p>Given a new frame, we estimate the new location of each existing track using the Kalman filter. These predictions are adopted to handle detection failures caused by varying visual properties of objects and occlusion in crowded scenes. But they are not suitable for long-term tracking. The accuracy of the Kalman filter could decrease if it is not updated by detection over a long time. Tracklet confidence is designed to measure the accuracy of the filter using temporal information.</p><p>A tracklet is generated through temporal association of candidates from consecutive frames. We can split a track into a set of tracklets since a track can be interrupted and retrieved for times during its lifetime. Every time a track is retrieved from lost state, the Kalman filter will be reinitialized. Therefore, only the information of the last tracklet is utilized to formulate the confidence of the track. Here we define L det as the number of detection results associated to the tracklet, and L trk as the number of track predictions after the last detection is associated. The tracklet confidence is defined as:</p><formula xml:id="formula_1">s trk = max(1 − log(1 + α · L trk ), 0) · 1(L det ≥ 2),<label>(2)</label></formula><p>where 1(·) is the indicator function that equals 1 if the input is true, otherwise equals 0. We require L det ≥ 2 to construct a reasonable motion model using observed detection before the track is used as a candidate.</p><p>The unified scoring function for a candidate x is formated by fusing classification probability and tracklet confidence:</p><formula xml:id="formula_2">s = p(y|z, x) · (1(x ∈ C det )) + s trk 1(x ∈ C trk )). (3)</formula><p>Here we use C det to denote the candidates from detection, and C trk for candidates from tracks, and s trk ∈ [0, 1] to punish candidates from uncertain tracks. Candidates for data association are finally selected based on the unified scores using non-maximal suppression. We define the maximum intersection over union (IoU) by a threshold τ nms , also there is a threshold τ s for the minimum score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Appearance Representation with ReID Features</head><p>The similarity function between candidates is the key component of data association. We argue that the object appearance, which are deeply learned by a data driven approach, outperforms traditional hand-crafted features on the task of similarity estimation. For the purpose of learning the object appearance and similarity function, we employ a deep neural network to extract feature vectors from RGB images, and formulate the similarity using the distance between the obtained features.</p><p>We utilize the network architecture proposed in <ref type="bibr" target="#b12">[13]</ref> and train the network on a combination of several large scale person re-identification datasets. The network H reid consists of the convolutional backbone from GoogLeNet <ref type="bibr" target="#b13">[14]</ref> followed by K branches of part-aligned fully connected (FC) layers. We refer to <ref type="bibr" target="#b12">[13]</ref> for more details on the network architecture. Given an RGB image I of a person, the appearance representation is formulated as f = H reid (I). We directly use Euclidean distance between the feature vectors to measure the distance d ij of two images I i and I j . During the training procedure, images of identities in training datasets are formed as a set of triplets T = { I i , I j , I k }, where I i , I j is a positive pair from the same person, and I i , I k is the negative pair from two different people. Given N triplets, the loss function going to be minimized is formulated as:</p><formula xml:id="formula_3">l triplet = 1 N Ii,Ij ,I k ∈T max(d ij − d ik + m, 0),<label>(4)</label></formula><p>where m &gt; 0 is a predefined margin. We ignore triplets that are easy to handle, i.e. d ik − d ij &gt; m, to enhance the discriminative ability of learned feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Hierarchical Data Association</head><p>Predictions of tracks are utilized to handle missing detection occurred in crowded scenes. Influenced by intra-category occlusion, these predictions may be involved with other objects.</p><p>To avoid taking other unwanted objects and backgrounds into appearance representations, we hierarchically associate tracks with different candidates using different features.</p><p>In particular, we first apply data association on candidates from detection, using appearance representations with a threshold τ d for the maximum distance. Then, we associate the remaining candidates with unassociated tracks based on IoU between candidates and tracks, with a threshold τ iou . We only update appearance representations of tracks when they are associated to detection. The updating is conducted by saving ReID features from the associated detection. Finally, new tracks are initialized based on the remaining detection results. The detail of the proposed online tracking algorithm is illustrated in Algorithm 1. With the hierarchical data association, we only need to extract ReID features for candidates from detection once per frame. Combining this with the previous efficient scoring function and tracklet confidences, our framework can run at real-time speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>To evaluate the performance of the proposed online tracking method, we conduct extensive experiments on the MOT16 dataset <ref type="bibr" target="#b14">[15]</ref>, which is a widely used benchmark for multiple people tracking. This dataset contains a training set and a test set, each with 7 challenging video sequences filmed in unconstrained environments. We form a validation set with 5 video sequences from the training set to analyze the contribution of each component in our framework. Afterwards, we submit the tracking result on the test set to the benchmark, and compare it with state-of-the-art methods on the benchmark. Implementation details. We employ SqueezeNet <ref type="bibr" target="#b15">[16]</ref>, as the backbone of R-FCN for real-time performance. Our fully convolutional network, consisting of SqueezeNet and the decoder part, costs only 8ms to estimate score maps for an input image with the size of 1152x640 on a GTX1080Ti GPU. We set k = 7 for position-sensitive score maps, and train the network using RMSprop optimizer with the learning rate of 1e-4 and the batch size of 32 for 20k iterations. The training data for person classification is collected from MS COCO <ref type="bibr" target="#b16">[17]</ref> and the remaining two training video sequences. We set τ nms = 0.3 and τ s = 0.4 for candidate selection. When coping with the ReID network, we train it on a combination of three large scale person re-identification datasets, i.e. Market1501 <ref type="bibr" target="#b17">[18]</ref>, CUHK01 and CUHK03 <ref type="bibr" target="#b18">[19]</ref>, to enhance the generation ability for tracking. We set τ d = 0.4 and τ iou = 0.3 for hierarchical data association. The following experiments are based on the same hyper-parameters.</p><p>Evaluation metrics. In order to measure accuracies of bounding boxes and identities at the same time, we adopt multiple metrics used in the benchmark to evaluate the proposed method, including multiple object tracking accuracy (MOTA) <ref type="bibr" target="#b19">[20]</ref>, false alarm per frame (FAF), the number of mostly tracked targets (MT, &gt; 80% recovered), the number of mostly lost targets (ML, &lt; 20% recovered) <ref type="bibr" target="#b20">[21]</ref>, false positives (FP), false negatives (FN), identity switches (IDS), identification recall (IDR), identification F1 score (IDF1) <ref type="bibr" target="#b21">[22]</ref>, and processing speed (frames per second, FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on Validation Set</head><p>Contribution of each component. In order to demonstrate the effectiveness of the proposed method, we investigate contribution of different components in our framework in <ref type="table" target="#tab_0">Table  1</ref>. The baseline method predicts new location of each track using the Kalman filter, and then associates tracks with detection based on the IoU. Using the classification probability to select candidates from both detection and tracks, in which case, improves the MOTA by 4.6%, comparing to the baseline  method. By punishing candidates from uncertain tracks, the combination of tracklet confidence with classification probability further improves the MOTA and reduces false positives, as we expected in Section 3.3. On the other hand, by introducing appearance representations based on ReID features, we can obtain a significant improvement on the performance of identification (evaluated by IDF1 and IDS). Our proposed method that combining the unified scoring function and ReID features has the best results for all metrics. Comparison with different appearance features. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we compare representations learned by a data driven approach detailed in Section 3.4 with two typical hand-crafted features, i.e. color histogram, histogram of oriented gradient (HOG). Following the fixed part model, which is widely used for appearance descriptors <ref type="bibr" target="#b22">[23]</ref>, we divide each image of a person into six horizontal stripes with an equal size for the color histogram. The color histogram of each stripe is built from the HSV color space with 125 bins. We normalize both the color histogram and HOG features by L 2 norm, and formulate the similarity using the cosine similarity function. As shown in the table, our appearance representation outperforms traditional hand-crafted features by a large margin in terms of IDF1 and IDS, in spite of the shorter feature vector comparing to other methods. The evaluation result on the validation set verifies the effectiveness of our data driven approach for multiple people tracking. The proposed tracking framework can be easily transfered for other categories, by learning the appearance representation from corresponding datasets, such as vehicle re-identification <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Test Set</head><p>We first analyze the time consumption of the proposed tracking framework on MOT16-03 sequence. As shown in <ref type="figure" target="#fig_3">Figure  3</ref>, the proposed method is much more time efficient by sharing computations on the entire image.</p><p>We report evaluation results on the test set of MOT16, and compare our tracker with other offline and online trackers in <ref type="table" target="#tab_2">Table 3</ref>. Note that the tracking performance depends heavily on the quality of detection. For the fair comparison, all the trackers in the table use the same detection provided by the benchmark. As shown in the table, Our tracker runs at real-time speed, and outperforms existing online trackers on most of the metrics, especially for IDF1, IDR, MT, and ML. The identification ability is enhanced by the deeply learned appearance representation. The improvement on MT and ML demonstrates the advantage of our unified scoring function for candidate selection. Selecting candidates from detection and tracks indeed reduces tracking failures caused by missing detection. Moreover, our online tracker has much lower computational complexity and is about 5~20 times faster than most of the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose an online multiple people tracking framework, which takes full advantage of recent deep neural networks. We tackle unreliable detection by selecting candidates from outputs of both detection and tracks. The scoring function for candidate selection is formulated by an efficient R-FCN, which shares computations on the entire image. Moreover, we improve the identification ability when coping with intra-category occlusion by introducing ReID features for data association. ReID features trained by a data driven approach outperforms traditional hand-crafted features by a large margin. The proposed tracker achieves real-time and state-of-the-art performance on the MOT16 benchmark. A future study is planed to further improve efficiency by sharing convolutional layers with both classification and appearance extraction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Candidate selection based on unified scores. Candidates from detection and tracks are visualized as blue solid rectangles and red dotted rectangles, respectively. Detection and tracks can complement each other for data association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>R-FCN architecture for efficient classification. Features from the encoder network are concatenated with upsampled features in the decoder part, to capture both the semantic and low-level information. Each color in the last block represents a specific score map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 4 C 5 foreach t in T do 6 7 C 9 C 10 S 13 F 16 F 20 F 21 C 22 F 23 T</head><label>14567910131620212223</label><figDesc>The proposed tracking algorithm.Input: A video sequence v with Nv frames and object detection{D k } Nv k=1Output: Tracks T of the video 1 Initialization: T ← ∅; appearance of tracks F trk ← ∅ 2 foreach frame f k in v do<ref type="bibr" target="#b2">3</ref> Estimate score maps z from f using R-FCN/ * collect candidates * / det ← D k ; C trk ← ∅ Predict new location x * of t using Kalman filter trk ← C trk ∪ {x * } 8 end / * select candidates * / ← C det ∪ C trk ← unifiedscores computed from Equation 3 11 C, S ← NMS(C, S, τnms) 12 C, S ← Filter(C, S, τs) // filter out if s &lt; τs / * extract appearance features * / det ← ∅ 14 foreach x in C det do 15 Ix ← Crop(f k , x) det ← F det ∪ H reid (Ix) 17 end / * hierarchical data association * / 18 Associate T and C det using distances of F trk and F det 19Associate remaining tracks and candidates using IoU trk ← F trk ∪ F det / * initialize new tracks * / remain ← remaining candidates from C det remain ← features of C remain , F trk ← T ∪ C remain , F trk ∪ F remain 24 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Average time consumption of one frame on MOT16-03 sequence, which contains over 50 people per frame. Patch: classification based on image patches using Squeezenet and two FC layers; R-FCN: classification using the same CNN backbone and a position-sensitive RoI pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on the validation set in terms of different components used. C: classification probability, T: tracklet confidence, A: appearance representations with ReID feature. The arrow after each metric indicates that the higher (↑) or lower (↓) value is better.</figDesc><table><row><cell>Method</cell><cell cols="4">C T A MOTA↑ IDF1↑ IDS↓ FAF↓</cell></row><row><cell>Baseline</cell><cell>28.4</cell><cell>32.8</cell><cell>628</cell><cell>0.85</cell></row><row><cell></cell><cell>33.0</cell><cell>37.6</cell><cell>445</cell><cell>0.77</cell></row><row><cell></cell><cell>33.7</cell><cell>37.3</cell><cell>475</cell><cell>0.63</cell></row><row><cell></cell><cell>30.6</cell><cell>42.4</cell><cell>234</cell><cell>1.01</cell></row><row><cell>Proposed</cell><cell>35.7</cell><cell>45.3</cell><cell>184</cell><cell>0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the validation set in terms of different appearance representations.</figDesc><table><row><cell>Method</cell><cell cols="5">Length MOTA↑ IDF1↑ IDS↓ FAF↓</cell></row><row><cell>None</cell><cell>-</cell><cell>33.7</cell><cell>37.3</cell><cell>475</cell><cell>0.63</cell></row><row><cell>Color histogram</cell><cell>750</cell><cell>34.9</cell><cell>38.6</cell><cell>250</cell><cell>0.73</cell></row><row><cell>HOG</cell><cell>1152</cell><cell>34.6</cell><cell>38.5</cell><cell>317</cell><cell>0.70</cell></row><row><cell>Color + HOG</cell><cell>1902</cell><cell>34.7</cell><cell>39.3</cell><cell>307</cell><cell>0.68</cell></row><row><cell>ReID feature</cell><cell>512</cell><cell>35.7</cell><cell>45.3</cell><cell>184</cell><cell>0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on the MOT16 test set.</figDesc><table><row><cell>Tracker</cell><cell cols="6">Method MOTA(%)↑ IDF1(%)↑ IDR(%)↑ MT(%)↑ ML(%)↓</cell><cell>FP↓</cell><cell>FN↓</cell><cell cols="2">IDS↓ FPS↑</cell></row><row><cell>LINF1 [2]</cell><cell>batch</cell><cell>41.0</cell><cell>45.7</cell><cell>34.2</cell><cell>11.6</cell><cell>51.3</cell><cell>7,896</cell><cell>99,224</cell><cell>430</cell><cell>4.2</cell></row><row><cell>MHT DAM [5]</cell><cell>batch</cell><cell>45.8</cell><cell>46.1</cell><cell>35.3</cell><cell>16.2</cell><cell>43.2</cell><cell>6,412</cell><cell>91,758</cell><cell>590</cell><cell>0.8</cell></row><row><cell>JMC [11]</cell><cell>batch</cell><cell>46.3</cell><cell>46.3</cell><cell>35.6</cell><cell>15.5</cell><cell>39.7</cell><cell>6,373</cell><cell>90,914</cell><cell>657</cell><cell>0.8</cell></row><row><cell>LMP [6]</cell><cell>batch</cell><cell>48.8</cell><cell>51.3</cell><cell>40.1</cell><cell>18.2</cell><cell>40.1</cell><cell>6,654</cell><cell>86,245</cell><cell>481</cell><cell>0.5</cell></row><row><cell>EAMTT [7]</cell><cell>online</cell><cell>38.8</cell><cell>42.4</cell><cell>31.5</cell><cell>7.9</cell><cell>49.1</cell><cell cols="2">8,114 102,452</cell><cell>965</cell><cell>11.8</cell></row><row><cell>CDA DDAL [1]</cell><cell>online</cell><cell>43.9</cell><cell>45.1</cell><cell>34.1</cell><cell>10.7</cell><cell>44.4</cell><cell>6,450</cell><cell>95,175</cell><cell>676</cell><cell>0.5</cell></row><row><cell>STAM [8]</cell><cell>online</cell><cell>46.0</cell><cell>50.0</cell><cell>38.5</cell><cell>14.6</cell><cell>43.6</cell><cell>6,895</cell><cell>91,117</cell><cell>473</cell><cell>0.2</cell></row><row><cell>AMIR [3]</cell><cell>online</cell><cell>47.2</cell><cell>46.3</cell><cell>34.8</cell><cell>14.0</cell><cell>41.6</cell><cell>2,681</cell><cell>92,856</cell><cell>774</cell><cell>1.0</cell></row><row><cell>MOTDT (Ours)</cell><cell>online</cell><cell>47.6</cell><cell>50.9</cell><cell>40.3</cell><cell>15.2</cell><cell>38.3</cell><cell>9,253</cell><cell>85,431</cell><cell>792</cell><cell>20.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Seung-Hwan Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving multi-frame data association with sparse representations for robust near-online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Fagot-Bouquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">To track or to detect? an ensemble framework for optimal selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuqing</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with strong and weak detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Sanchez-Matilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-person tracking-by-detection with local particle filtering and global occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowen</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rewind to track: Parallelized apprenticeship learning with backward tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-person tracking by multicut and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt; 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to associate: Hybridboosted multi-target tracker for crowded scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Appearance descriptors for person reidentification: a comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Satta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.5748</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Largescale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
