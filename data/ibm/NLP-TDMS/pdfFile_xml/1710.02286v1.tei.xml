<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional Neural Networks as Generic Feature Extractors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hertel</surname></persName>
							<email>hertel@isip.uni-luebeck.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Neuro-and Bioinformatics</orgName>
								<orgName type="institution">University of Luebeck</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
							<email>barth@inb.uni-luebeck.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Neuro-and Bioinformatics</orgName>
								<orgName type="institution">University of Luebeck</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Käster</surname></persName>
							<email>kaester@inb.uni-luebeck.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Neuro-and Bioinformatics</orgName>
								<orgName type="institution">University of Luebeck</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Martinetz</surname></persName>
							<email>martinetz@inb.uni-luebeck.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Neuro-and Bioinformatics</orgName>
								<orgName type="institution">University of Luebeck</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Signal Processing</orgName>
								<orgName type="institution">University of Luebeck</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">‡ Pattern Recognition Company GmbH</orgName>
								<address>
									<settlement>Luebeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional Neural Networks as Generic Feature Extractors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-theart approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC-12 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100, compared to the previous state-of-the-art result of 65.43 %. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recognizing objects in natural images is an intricate task for a machine, involving multiple conflicting objectives. The effortlessness of the human brain deceives the complex underlying process. Inspired by the mammalian visual system, convolutional neural networks were proposed <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. They are the state-of-the-art approach for various pattern recognition tasks. Unlike many other learning algorithms, convolutional networks combine both feature extraction and classification. The advantage of this approach was impressively demonstrated by LeCun et al. <ref type="bibr" target="#b3">[4]</ref> on <ref type="bibr">MNIST</ref> and Krizhevsky et al. <ref type="bibr" target="#b4">[5]</ref> on <ref type="bibr"></ref> achieving better results than previous learning methods.</p><p>A schematic representation of a convolutional network is shown in <ref type="figure">Figure 1</ref>. The given network comprises five different layers, i.e. input, convolution, pooling, fully-connected and output layer. The input layer specifies a fixed size for the input images, i.e. images may have to be resized accordingly. The image is then convolved with multiple learned kernels using shared weights. Next, the pooling layer reduces the size of the image while trying to maintain the contained information. These two layers compose the feature extraction part. Afterwards, the extracted features are weighted and combined in the fully-connected layer. This represents the classification part of the convolutional network. Finally, there exists one output neuron for each object category in the output layer.</p><p>Recent results indicate that very deep networks achieve even better results on various benchmarks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Moreover, an ensemble of multiple networks and additional training data are often used to further increase the performance <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Thus, the general formula for a convincing performance are seemingly multiple deep convolutional networks with many layers and a huge amount of training data.</p><p>One drawback of this trend, however, is the long time needed to train such deep networks. To tackle this problem, we reused a previously trained network. For this purpose, we first trained a convolutional network on a large dataset, maintained the learned feature extraction part, and only retrained the classification part on multiple different datasets. We then compared the results to a full training, i.e. both feature extraction and classification, of the same network on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASETS</head><p>In this work, we used four different datasets, namely ILSVRC-12, MNIST, CIFAR-10 and CIFAR-100. We used ILSVRC-12 to pretrain our network and MNIST, CIFAR-10 and CIFAR-100 to retrain our network afterwards. In the following section, we will briefly introduce each dataset. An overview of some statistics of all four datasets is given in <ref type="table" target="#tab_1">Table I</ref>.</p><p>The IMAGENET <ref type="bibr" target="#b9">[10]</ref> dataset contains more than 14 million labeled high-resolution color images of natural objects and scenes belonging to over 21,000 different categories. The images were collected from the web and labeled by humans using Amazon Mechanical Turk 1 . A subset of these images is taken as an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). We used the dataset from the competition in 2012 (ILSVRC-12). It consists of nearly 1.5 million color images, which are varying in size, and 1,000 different categories.</p><p>The MNIST <ref type="bibr" target="#b10">[11]</ref> dataset contains grayscale images of handwritten digits. 100 randomly selected images from MNIST are shown in <ref type="figure">Figure 2</ref>. It possesses ten different categories, namely one for each digit from zero to nine. Each grayscale image has a fixed size of 28 × 28 pixels. The digits are centered inside the image and normalized in size. In total, MNIST contains 70,000 images, split into 60,000 training and 10,000 test images.</p><p>The CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b11">[12]</ref> datasets contain small color images of natural objects. An excerpt of 96 randomly chosen images is shown in <ref type="figure">Figure 3</ref>. They are labeled subsets of the 80 Million Tiny Images 2 database. CIFAR-10 possesses ten and CIFAR-100 possesses 100 different categories, respectively. Each color image has a fixed size of 32×32 pixels. In total, they both consist of 60,000 images, split into 50,000 training and 10,000 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>The architecture of our trained convolutional network is shown in <ref type="table" target="#tab_1">Table II</ref>. It is based on the architecture proposed by Krizhevsky et al. <ref type="bibr" target="#b4">[5]</ref>. The network comprises 24 layers. In particular, five convolution and three maximum pooling layers with different square kernel sizes and kernel strides. Moreover, we added zero padding in some cases to obtain convenient sizes of the feature maps. As a nonlinear activation function, we settled for the rectified linear unit <ref type="bibr" target="#b12">[13]</ref>. Moreover, layers of dropout <ref type="bibr" target="#b13">[14]</ref> were applied after each fully-connected layer. The probability to randomly drop a unit in the network is 50 %. Finally, to obtain a probability distribution, we employed a softmax layer.</p><p>Just like Krizhevsky et al. <ref type="bibr" target="#b4">[5]</ref>, we trained the network on ILSVRC-12 and reached comparable results, i.e. a top-1 accuracy of 59.23 % and a top-5 accuracy of 83.58 %. For this purpose, we resized all images of the dataset to a fixed 2 http://groups.csail.mit.edu/vision/tinyimages/ <ref type="figure">Fig. 3</ref>.</p><p>96 randomly selected images from CIFAR-100. The dataset represents 100 different natural objects. It contains 60.000 color images with dimensions of 32 × 32. CIFAR-10 possesses the same statistics, except that it has ten classes with 6,000 images each. size of 256 × 256 pixels and randomly cropped a subimage of 227 × 227 pixels. This increased the number of training images by a factor of 900. We used the deep learning framework Caffe <ref type="bibr" target="#b14">[15]</ref> to train our convolutional networks. It allowed us to employ the GPU 3 of our computer for faster training. This resulted in a speedup of approximately ten, compared to training with the CPU 4 .</p><p>96 learned kernels of the first convolution layer are shown in <ref type="figure">Figure 4</ref>. Each kernel is a color image of 11 × 11 pixels. Though the colors are difficult to interpret, Gaborlike filters of different frequencies and orientations can be recognized. They are used to extract edges of the input image. Further learned kernels of deeper convolution layers with a size of 5 × 5 and 3 × 3 are too small to provide any noticeable information and are therefore not shown.</p><p>We then maintained the feature extraction part -i.e. the learned convolution kernels -of the previously trained network and only retrained the classification part -i.e. the fully-connected layers -on different datasets, namely MNIST, CIFAR-10 and CIFAR-100. To adjust the three datasets to our network architecture, we had to alter two things. First, to resize the images to 256×256 pixels. Note that we had to convert the grayscale images of MNIST to color images by simply copying the grayscale channel three times. Secondly, to change the number of output neurons to the number of different object categories, depending on the dataset.</p><p>To evaluate the performance of the trained network on an independent test set, we averaged ten different predictions of a single image as proposed by Krizhevsky et al. <ref type="bibr" target="#b4">[5]</ref>. For this purpose, we averaged the output of the four corner crops and the center crop of the input image and -except for MNIST -additionally mirrored each image along the vertical axis. We then trained the same network a second time for each dataset. This time, however, we trained the full network, i.e. both feature extraction and classification, and compared both obtained results.</p><p>The networks were trained for 30 epochs. An epoch means a complete training cycle over all images of the training set. We started with a fixed base learning rate η = 0.001 and decreased it by a factor of ten to η = 0.0001 after 20 epochs. Furthermore, we selected a momentum value of µ = 0.9, a weight decay value of λ = 0.0005, and a batch size of β = 80. These parameters were determined based on a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>Our results are shown in <ref type="figure" target="#fig_0">Figure 5</ref>. The horizontal axis represents the number of epochs. The vertical axis shows the accuracy of the independent test set. Note that, for simplicity, the results in <ref type="figure" target="#fig_0">Figure 5</ref> were computed only for the center crop of the input images. The solid lines represent full training, i.e. the learning of both feature extraction and classification. The dashed lines represent the retraining of the classification part only. The red lines correspond to MNIST, the blue lines correspond to CIFAR-10 and the green lines correspond to CIFAR-100, respectively.</p><p>The results show that, regarding full training and retraining, comparable accuracy rates are achieved after 30 epochs for all three datasets. More precisely, while both rates for MNIST are nearly identical, the rate for retraining is slightly worse in case of CIFAR-10 and slightly better in case of CIFAR-100 compared to full training.</p><p>Note that training the full network is considerably slower than retraining: it takes about ten epochs to achieve the accuracy that the retrained network has after a single epoch in case of CIFAR-100 and about eight epochs in case of CIFAR-10, respectively. Further note the slight increase in accuracy for CIFAR-10 and CIFAR-100 after 20 epochs due to the decrease of the learning rate.</p><p>Further results are given in <ref type="table" target="#tab_1">Table III</ref>. It shows the accuracy of the independent test set after 30 epochs both for full training and retraining. Moreover, the state-of-the-art results from the literature are given as reference values. Note that this time the accuracy rates were calculated by averaging the predictions of multiple crops of the input image. For CIFAR-10 and CIFAR-100 we averaged the outputs of ten crops. Since we did not mirror the images for MNIST, for obvious reasons, we only averaged the outputs of five crops.</p><p>The results additionally underline the comparable accuracy rates between full training and retraining for the respective dataset. As for the CIFAR-100 dataset, retraining the network achieves an accuracy rate of 67.68 %, which is 2.25 % better than the state-of-the-art accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Deep convolutional neural networks are the state-of-theart approach for object recognition. One drawback, however, is the long time needed to train such deep networks, especially on large datasets. We tackled this problem by reusing the feature extraction part of a previously trained network and only retrained the classification part on multiple different datasets.</p><p>As expected, our findings show that this approach considerably reduces the necessary amount of training time. Our results indicate a speedup by a factor of up to ten, depending on the dataset. Besides the three presented datasets, we applied the proposed method to a self-made dataset with nearly 500, 000 images from Flickr belonging to 110 different object categories and obtained very good results in a short time. We did not present the results, however, since the accuracy rates of the fully trained network are not available for comparison. More interestingly, both fully trained and retrained networks achieve comparable accuracy rates on all three datasets. The learned feature extractor from our pretrained network is therefore applicable to multiple situations. Even though the feature extractor was trained on ILSVRC-12, containing natural images and scenes, it still achieves excellent results even on digits from the MNIST dataset. This finding indicates that further datasets can be classified with the same feature extractor.</p><p>Our experiments confirm and extend the results reported by Razavian et al. <ref type="bibr" target="#b15">[16]</ref>, Donahue et al. <ref type="bibr" target="#b16">[17]</ref> and Girshick et al. <ref type="bibr" target="#b17">[18]</ref>, who have also trained linear and nonlinear classifiers on features obtained from deep learning with convolutional networks.</p><p>Note that we trained all networks -including the one from pretraining -in a supervised manner using backpropagation <ref type="bibr" target="#b18">[19]</ref>. The obtained feature extractor was therefore trained for a specific purpose. Its generic characteristics are somewhat surprising. Previously, mostly unsupervised algorithms, like sparse coding and related representation learning algorithms, have been used for pretraining. This approach achieves state-of-the-art results on the STL-10 <ref type="bibr" target="#b19">[20]</ref> dataset, as shown by Miclut et al. <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, our results show that it is also possible to perform supervised pretraining and obtain excellent results. This approach even improved the accuracy on CIFAR-100, compared to a fully trained deep convolutional network. This suggests that more appropriate kernels were learned from ILSVRC-12 than from CIFAR-100 itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Reusing a previously trained convolutional network not only vastly reduces the necessary time for training, but also achieves comparable results regarding the full training of the network. For particular datasets, the accuracy is even increased. This finding is especially relevant for practical applications, e.g. when only limited computing power or time is available. Our results indicate the existence of a generic feature extractor concerning the three used datasets. To either support or reject this hypothesis, further research for multiple datasets in different situations should be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison between full training and retraining on three different datasets. We trained the network for 30 epochs. The solid lines represent full training, i.e. both feature extraction and classification. The dashed lines represent retraining, i.e. classification only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>STATISTICS OF THE USED DATASETS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dimension</cell><cell></cell><cell></cell><cell>No. Images</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">Classes Width Height Depth</cell><cell cols="2">Training Validation</cell><cell>Test</cell></row><row><cell>MNIST</cell><cell>10</cell><cell>28</cell><cell>28</cell><cell>1</cell><cell>60,000</cell><cell>-</cell><cell>10,000</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>32</cell><cell>32</cell><cell>3</cell><cell>50,000</cell><cell>-</cell><cell>10,000</cell></row><row><cell>CIFAR-100</cell><cell>100</cell><cell>32</cell><cell>32</cell><cell>3</cell><cell>50,000</cell><cell>-</cell><cell>10,000</cell></row><row><cell>ILSVRC-12</cell><cell>1,000</cell><cell>-</cell><cell>-</cell><cell cols="2">3 1,281,167</cell><cell cols="2">50,000 150,000</cell></row><row><cell cols="8">Fig. 2. 100 randomly selected images from MNIST. The dataset represents handwritten digits. It has ten different classes, one for each digit from zero to nine. The digits are centered and normalized in size. MNIST comprises 70,000 grayscale images with dimensions of 28 × 28.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">ARCHITECTURE OF OUR IMPLEMENTED</cell><cell></cell></row><row><cell cols="5">CONVOLUTIONAL NETWORK.</cell><cell></cell><cell></cell></row><row><cell>No. Layer</cell><cell></cell><cell>Dimension</cell><cell></cell><cell cols="3">Kernel Stride Padding</cell></row><row><cell></cell><cell cols="3">Width Height Depth</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 Input</cell><cell>227</cell><cell>227</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>1 Convolution</cell><cell>55</cell><cell>55</cell><cell>96</cell><cell>11</cell><cell>4</cell><cell>-</cell></row><row><cell>2 Relu</cell><cell>55</cell><cell>55</cell><cell>96</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3 Pooling</cell><cell>27</cell><cell>27</cell><cell>96</cell><cell>3</cell><cell>2</cell><cell>-</cell></row><row><cell>4 Normalization</cell><cell>27</cell><cell>27</cell><cell>96</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>5 Convolution</cell><cell>27</cell><cell>27</cell><cell>256</cell><cell>5</cell><cell>1</cell><cell>2</cell></row><row><cell>6 Relu</cell><cell>27</cell><cell>27</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>7 Pooling</cell><cell>13</cell><cell>13</cell><cell>256</cell><cell>3</cell><cell>2</cell><cell>-</cell></row><row><cell>8 Normalization</cell><cell>13</cell><cell>13</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>9 Convolution</cell><cell>13</cell><cell>13</cell><cell>384</cell><cell>3</cell><cell>1</cell><cell>1</cell></row><row><cell>10 Relu</cell><cell>13</cell><cell>13</cell><cell>384</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>11 Convolution</cell><cell>13</cell><cell>13</cell><cell>384</cell><cell>3</cell><cell>1</cell><cell>1</cell></row><row><cell>12 Relu</cell><cell>13</cell><cell>13</cell><cell>384</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>13 Convolution</cell><cell>13</cell><cell>13</cell><cell>256</cell><cell>3</cell><cell>1</cell><cell>1</cell></row><row><cell>14 Relu</cell><cell>13</cell><cell>13</cell><cell>256</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>15 Pooling</cell><cell>6</cell><cell>6</cell><cell>256</cell><cell>3</cell><cell>2</cell><cell>-</cell></row><row><cell>16 Fully Connected</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>17 Relu</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>18 Dropout</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>19 Fully Connected</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>20 Relu</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>21 Dropout</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>22 Fully Connected</cell><cell>1</cell><cell>1</cell><cell>1000</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>23 Softmax</cell><cell>1</cell><cell>1</cell><cell>1000</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>Fig. 4. 96 learned kernels of the first convolution layer. Each kernel has dimensions of 11 × 11 × 3. They were maintained when retraining a network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III .</head><label>III</label><figDesc>RESULTS OF THE TEST SET FOR FULL TRAINING ANDRETRAINING AFTER 30 EPOCHS.</figDesc><table><row><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell></cell></row><row><cell>Dataset</cell><cell>Full Training</cell><cell>Retraining</cell><cell>State of the Art</cell></row><row><cell>MNIST</cell><cell>99.68</cell><cell>99.54</cell><cell>99.79 [22]</cell></row><row><cell>CIFAR-10</cell><cell>89.99</cell><cell>89.14</cell><cell>91.78 [23]</cell></row><row><cell>CIFAR-100</cell><cell>63.65</cell><cell>67.68</cell><cell>65.43 [23]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.mturk.com arXiv:1710.02286v1 [cs.CV] 6 Oct 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">NVIDIA GeForce GTX 770 with 2 GB of memory<ref type="bibr" target="#b3">4</ref> Intel Core i7-4770</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="591" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 25</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">presented at the Workshop ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artif. Intell. (IJCAI)</title>
		<meeting>Int. Joint Conf. Artif. Intell. (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Toronto, Canada, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Artif. Intell. and Stat. (AISTATS)</title>
		<meeting>14th Int. Conf. Artif. Intell. and Stat. (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving neural networks with dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Univ. Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Mach. Learning (ICML)</title>
		<meeting>31st Int. Conf. Mach. Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1988-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>14th Int. Conf. Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Committees of deep feedforward networks trained with few data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miclut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Käster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5947</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learning (ICML)</title>
		<meeting>30th Int. Conf. Mach. Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Artif. Intell. and Stat. (AISTATS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
