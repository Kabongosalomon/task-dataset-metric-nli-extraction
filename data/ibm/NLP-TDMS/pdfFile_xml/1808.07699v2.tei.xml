<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Neural Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
							<email>nikos_kolitsas@hotmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
							<email>octavian.ganea@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<email>thomas.hofmann@inf.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<title level="a" type="main">End-to-End Neural Entity Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention -entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>Towards the goal of automatic text understanding, machine learning models are expected to accurately extract potentially ambiguous mentions of entities from a textual document and link them to a knowledge base (KB), e.g. Wikipedia or Freebase. Known as entity linking, this problem is an essential building block for various Natural Language Processing tasks, e.g. automatic KB construction, question-answering, text summarization, or relation extraction.</p><p>An EL system typically performs two tasks: i) Mention Detection (MD) or Named Entity Recog- * Equal contribution. 1) MD may split a larger span into two mentions of less informative entities: B. Obama's wife gave a speech <ref type="bibr">[...]</ref> Federer's coach <ref type="bibr">[...]</ref> 2) MD may split a larger span into two mentions of incorrect entities: Obama Castle was built in 1601 in Japan. The Kennel Club is UK's official kennel club. A bird dog is a type of gun dog or hunting dog. Romeo and Juliet by Shakespeare <ref type="bibr">[...]</ref> Natural killer cells are a type of lymphocyte Mary and Max, the 2009 movie <ref type="bibr">[...]</ref> 3) MD may choose a shorter span, referring to an incorrect entity: The Apple is played again in cinemas. The New York Times is a popular newspaper. 4) MD may choose a longer span, referring to an incorrect entity: Babies Romeo and Juliet were born hours apart.  <ref type="bibr">(underlined)</ref> can be avoided by proper context understanding. The correct spans are shown in blue. nition (NER) when restricted to named entitiesextracts entity references in a raw textual input, and ii) Entity Disambiguation (ED) -links these spans to their corresponding entities in a KB. Until recently, the common approach of popular systems <ref type="bibr">(Ceccarelli et al., 2013;</ref><ref type="bibr">van Erp et al., 2013;</ref><ref type="bibr">Piccinno and Ferragina, 2014;</ref><ref type="bibr">Daiber et al., 2013;</ref><ref type="bibr">Hoffart et al., 2011;</ref><ref type="bibr">Steinmetz and Sack, 2013)</ref> was to solve these two sub-problems independently. However, the important dependency between the two steps is ignored and errors caused by MD/NER will propagate to ED without possibility of recovery <ref type="bibr">(Sil and Yates, 2013;</ref><ref type="bibr">Luo et al., 2015)</ref>. We here advocate for models that address the end-to-end EL task, informally arguing that humans understand and generate text in a similar joint manner, discussing about entities which are gradually introduced, referenced under multiple names and evolving during time <ref type="bibr">(Ji et al., 2017)</ref>. Further, we emphasize the importance of the mutual dependency between MD and ED. First, numerous and more informative linkable spans found by MD obviously offer more contextual cues for ED. Second, finding the true entities appearing in a specific context encourages better mention boundaries, especially for multi-word mentions. For example, in the first sentence of <ref type="table" target="#tab_0">Table 1</ref>, understanding the presence of the entity Michelle Obama helps detecting its true mention "B. Obama's wife", as opposed to separately linking B. Obama and wife to less informative concepts.</p><p>We propose a simple, yet competitive, model for end-to-end EL. Getting inspiration from the recent works of <ref type="bibr">(Lee et al., 2017)</ref> and <ref type="bibr">(Ganea and Hofmann, 2017)</ref>, our model first generates all possible spans (mentions) that have at least one possible entity candidate. Then, each mention -candidate pair receives a context-aware compatibility score based on word and entity embeddings coupled with a neural attention and a global voting mechanisms. During training, we enforce the scores of gold entity -mention pairs to be higher than all possible scores of incorrect candidates or invalid mentions, thus jointly taking the ED and MD decisions.</p><p>Our contributions are:</p><p>• We address the end-to-end EL task using a simple model that conditions the "linkable" quality of a mention to the strongest context support of its best entity candidate. We do not require expensive manually annotated negative examples of non-linkable mentions. Moreover, we are able to train competitive models using little and only partially annotated documents (with named entities only such as the CoNLL-AIDA dataset).</p><p>• We are among the first to show that, with one single exception, engineered features can be fully replaced by neural embeddings automatically learned for the joint MD &amp; ED task.</p><p>• On the Gerbil 1 benchmarking platform, we empirically show significant gains for the endto-end EL task when test and training data 1 http://gerbil.aksw.org/gerbil/ come from the same domain. Morever, when testing datasets follow different annotation schemes or exhibit different statistics, our method is still effective in achieving state-ofthe-art or close performance, but needs to be coupled with a popular NER system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With few exceptions, MD/NER and ED are treated separately in the vast EL literature. Traditional NER models usually view the problem as a word sequence labeling that is modeled using conditional random fields on top of engineered features <ref type="bibr">(Finkel et al., 2005)</ref> or, more recently, using bi-LSTMs architectures <ref type="bibr">(Lample et al., 2016;</ref><ref type="bibr">Chiu and Nichols, 2016;</ref><ref type="bibr">Liu et al., 2017)</ref> capable of learning complex lexical and syntactic features.</p><p>In the context of ED, recent neural methods <ref type="bibr">(He et al., 2013;</ref><ref type="bibr" target="#b1">Sun et al., 2015;</ref><ref type="bibr" target="#b2">Yamada et al., 2016;</ref><ref type="bibr">Ganea and Hofmann, 2017;</ref><ref type="bibr">Le and Titov, 2018;</ref><ref type="bibr" target="#b3">Yang et al., 2018;</ref><ref type="bibr">Radhakrishnan et al., 2018)</ref> have established state-of-the-art results, outperforming engineered features based models. Context aware word, span and entity embeddings, together with neural similarity functions, are essential in these frameworks.</p><p>End-to-end EL is the realistic task and ultimate goal, but challenges in joint NER/MD and ED modeling arise from their different nature. Few previous methods tackle the joint task, where errors in one stage can be recovered by the next stage. One of the first attempts, (Sil and Yates, 2013) use a popular NER model to over-generate mentions and let the linking step to take the final decisions. However, their method is limited by the dependence on a good mention spotter and by the usage of handengineered features. It is also unclear how linking can improve their MD phase. Later, <ref type="bibr">(Luo et al., 2015)</ref> presented one of the most competitive joint MD and ED models leveraging semi-Conditional Random Fields (semi-CRF). However, there are several weaknesses in this work. First, the mutual task dependency is weak, being captured only by type-category correlation features. The other engineered features used in their model are either NER or ED specific. Second, while their probabilistic graphical model allows for tractable learning and inference, it suffers from high computational complexity caused by the usage of the cartesian product of all possible document span segmentations, NER categories and entity assignments. Another approach is J- <ref type="bibr">NERD (Nguyen et al., 2016)</ref> that addresses the end-to-end task using only engineered features and a probabilistic graphical model on top of sentence parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Joint Mention Detection and Entity Disambiguation</head><p>We formally introduce the tasks of interest. For EL, the input is a text document (or a query or tweet) given as a sequence D = {w 1 , . . . , w n } of words from a dictionary, w k ∈ W. The output of an EL model is a list of mention -entity pairs {(m i , e i )} i∈1,T , where each mention is a word subsequence of the input document, m = w q , . . . , w r , and each entity is an entry in a knowledge base KB (e.g. Wikipedia), e ∈ E. For the ED task, the list of entity mentions {m i } i=1,T that need to be disambiguated is additionally provided as input. The expected output is a list of corresponding</p><formula xml:id="formula_0">annotations {e i } i=1,T ∈ E T .</formula><p>Note that, in this work, we only link mentions that have a valid gold KB entity, setting referred in <ref type="bibr">(Röder et al., 2017)</ref> as InKB evaluation. Thus, we treat mentions referring to entities outside of the KB as "non-linkable". This is in line with few previous models, e.g. <ref type="bibr">(Luo et al., 2015;</ref><ref type="bibr">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b2">Yamada et al., 2016)</ref>. We leave the interesting setting of discovering out-of-KB entities as future work.</p><p>We now describe the components of our neural end-to-end EL model, depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. We aim for simplicity, but competitive accuracy.</p><p>Word and Char Embeddings. We use pretrained Word2Vec vectors <ref type="bibr">(Mikolov et al., 2013)</ref>. In addition, we train character embeddings that capture important word lexical information. <ref type="bibr">Following (Lample et al., 2016)</ref>, for each word independently, we use bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) on top of learnable char embeddings. These character LSTMs do not extend beyond single word boundaries, but they share the same parameters. Formally, let {z 1 , . . . , z L } be the character vectors of word w. We use the forward and backward LSTMs formulations defined recursively as in <ref type="bibr">(Lample et al., 2016)</ref>:</p><formula xml:id="formula_1">h f t = F W D − LST M (h f t−1 , z t )<label>(1)</label></formula><formula xml:id="formula_2">h b t = BKW D − LST M (h b t+1 , z t )</formula><p>Then, we form the character embedding of w is [h f L ; h b 1 ] from the hidden state of the forward LSTM corresponding to the last character concatenated with the hidden state of the backward LSTM corresponding to the first character. This is then concatenated with the pre-trained word embedding, forming the context-independent word-character embedding of w. We denote the sequence of these vectors as {v k } k∈1,n and depict it as the first neural layer in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Mention Representation. We find it crucial to make word embeddings aware of their local context, thus being informative for both mention boundary detection and entity disambiguation (leveraging contextual cues, e.g. "newspaper"). We thus encode context information into words using a bi-LSTM layer on top of the word-character embeddings {v k } k∈1,n . The hidden states of forward and backward LSTMs corresponding to each word are then concatenated into context-aware word embeddings, whose sequence is denoted as {x k } k∈1,n . Next, for each possible mention, we produce a fixed size representation inspired by <ref type="bibr">(Lee et al., 2017)</ref>. Given a mention m = w q , . . . , w r , we first concatenate the embeddings of the first, last and the "soft head" words of the mention:</p><formula xml:id="formula_3">g m = [x q ; x r ;x m ]<label>(2)</label></formula><p>The soft head embeddingx m is built using an attention mechanism on top of the mention's word embeddings, similar with (Lee et al., 2017):</p><formula xml:id="formula_4">α k = w α , x k a m k = exp(α k ) r t=q exp(α t ) (3) x m = r k=q a m k · v k</formula><p>However, we found the soft head embedding to only marginally improve results, probably due to the fact that most mentions are at most 2 words long. To learn non-linear interactions between the component word vectors, we project g m to a final mention representation with the same size as entity embeddings (see below) using a shallow feedforward neural network FFNN (a simple projection layer):</p><formula xml:id="formula_5">x m = FFNN 1 (g m )<label>(4)</label></formula><p>Entity Embeddings. We use fixed continuous entity representations, namely the pre-trained entity embeddings of (Ganea and Hofmann, 2017), due to their simplicity and compatibility with the pre-trained word vectors of <ref type="bibr">(Mikolov et al., 2013)</ref>. Briefly, these vectors are computed for each entity in isolation using the following exponential model that approximates the empirical conditional word-entity distributionp(w|e) obtained from cooccurrence counts.</p><formula xml:id="formula_6">exp( x w , y e ) w ∈W exp( x w , y e ) ≈p(w|e)<label>(5)</label></formula><p>Here, x w are fixed pre-trained word vectors and y e is the entity embedding to be trained. In practice, (Ganea and Hofmann, 2017) re-write this as a max-margin objective loss.</p><p>Candidate Selection. For each span m we select up to s entity candidates that might be referred by this mention. These are top entities based on an empirical probabilistic entity -map p(e|m) built by (Ganea and Hofmann, 2017) from Wikipedia hyperlinks, Crosswikis (Spitkovsky and Chang) and <ref type="bibr">YAGO (Hoffart et al., 2011)</ref> dictionaries. We denote by C(m) this candidate set and use it both at training and test time.</p><p>Final Local Score. For each span m that can possibly refer to an entity (i.e. |C(m)| ≥ 1) and for each of its entity candidates e j ∈ C(m), we compute a similarity score using embedding dotproduct that supposedly should capture useful information for both MD and ED decisions. We then combine it with the log-prior probability using a shallow FFNN, giving the context-aware entitymention score:</p><formula xml:id="formula_7">Ψ(e j , m) = FFNN 2 ([log p(e j |m); x m , y j ])<label>(6)</label></formula><p>Long Range Context Attention. In some cases, our model might be improved by explicitly capturing long context dependencies. To test this, we experimented with the attention model of (Ganea and Hofmann, 2017). This gives one context embedding per mention based on informative context words that are related to at least one of the candidate entities. We use this additional context embedding for computing dot-product similarity with any of the candidate entity embeddings. This value is fed as additional input of FFNN 2 in Eq. 6. We refer to this model as long range context attention.</p><p>Training. We assume a corpus with documents and gold entity -mention pairs G = {(m i , e * i )} i=1,K is available. At training time, for each input document we collect the set M of all (potentially overlapping) token spans m for which |C(m)| ≥ 1. We then train the parameters of our model using the following minimization procedure:</p><formula xml:id="formula_8">θ * = arg min θ m∈M e∈C(m) V (Ψ θ (e, m)) (7)</formula><p>where the violation term V enforces the scores of gold pairs to be linearly separable from scores of negative pairs, i.e.</p><formula xml:id="formula_9">V (Ψ(e, m)) = (8) max(0, γ − Ψ(e, m)), if(e, m) ∈ G max(0, Ψ(e, m)), otherwise</formula><p>Note that, in the absence of annotated negative examples of "non-linkable" mentions, we assume that all spans in M and their candidates that do not appear in G should not be linked. The model will be enforced to only output negative scores for all entity candidates of such mentions. We call all spans training the above setting. Our method can also be used to perform ED only, in which case we train only on gold mentions, i.e. M = {m|m ∈ G}. This is referred as gold spans training.</p><p>Inference. At test time, our method can be applied for both EL and ED only as follows. First, for each document in our validation or test sets, we select all possibly linkable token spans, i.e. M = {m| |C(m)| ≥ 1} for EL, or the input set of mentions M = {m|m ∈ G} for ED, respectively. Second, the best linking threshold δ is found on the validation set such that the micro F1 metric is maximized when only linking mention -entity pairs with Ψ score greater than δ. At test time, only entity -mention pairs with a score higher than δ are kept and sorted according to their Ψ scores; the final annotations are greedily produced based on this set such that only spans not overlapping with previously selected spans (of higher scores) are chosen.</p><p>Global Disambiguation Our current model is "local", i.e. performs disambiguation of each candidate span independently. To enhance it, we add an extra layer to our neural network that will promote coherence among linked and disambiguated entities inside the same document, i.e. the global disambiguation layer. Specifically, we compute a "global" mention-entity score based on which we produce the final annotations. We first define the set of mention-entity pairs that are allowed to participate in the global disambiguation voting, namely those that already have a high local score:</p><formula xml:id="formula_10">V G = {(m, e)|m ∈ M, e ∈ C(m), Ψ(e, m) ≥ γ }</formula><p>Since we initially consider all possible spans and for each span up to s candidate entities, this filtering step is important to avoid both undesired noise and exponential complexity for the EL task for which M is typically much bigger than for ED. The final "global" score G(e j , m) for entity candidate e j of mention m is given by the cosine similarity between the entity embedding and the normalized average of all other voting entities' embeddings (of the other mentions m ).</p><formula xml:id="formula_11">V m G = {e|(m , e) ∈ V G ∧ m = m} y m G = e∈V m G y e G(e j , m) = cos(y e j , y m G )</formula><p>This is combined with the local score, yielding</p><formula xml:id="formula_12">Φ(e j , m) = FFNN 3 ([Ψ(e j , m); G(e j , m)])</formula><p>The final loss function is now slightly modified. Specifically, we enforce the linear separability in two places: in Ψ(e, m) (exactly as before), but also in Φ(e, m), as follows</p><formula xml:id="formula_13">θ * = arg min θ (9) d∈D m∈M e∈C(m) V (Ψ θ (e, m)) + V (Φ θ (e, m))</formula><p>The inference procedure remains unchanged in this case, with the exception that it will only use the Φ(e, m) global score.</p><p>Coreference Resolution Heuristic. In a few cases we found important to be able to solve simple coreference resolution cases (e.g. "Alan" referring to "Alan Shearer"). These cases are difficult to handle by our candidate selection strategy. We thus adopt the simple heuristic descried in <ref type="bibr">(Ganea and Hofmann, 2017)</ref> and observed between 0.5% and 1% improvement on all datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets and Metrics. We used Wikipedia 2014 as our KB. We conducted experiments on the most important public EL datasets using the Gerbil platform <ref type="bibr">(Röder et al., 2017)</ref>. Datasets' statistics are provided in Tables 5 and 6 (Appendix). This benchmarking framework offers reliable and trustable evaluation and comparison with state of the art EL/ED methods on most of the public datasets for this task. It also shows how well different systems generalize to datasets from very different domains and annotation schemes compared to their training sets. Moreover, it offers evaluation metrics for the end-to-end EL task, as opposed to some works that only evaluate NER and ED separately, e.g. <ref type="bibr">(Luo et al., 2015)</ref>.</p><p>As previously explained, we do not use the NIL mentions (without a valid KB entity) and only compare against other systems using the InKB metrics.</p><p>For training we used the biggest publicly available EL dataset, AIDA/CoNLL <ref type="bibr">(Hoffart et al., 2011)</ref>, consisting of a training set of 18,448 linked mentions in 946 documents, a validation set of 4,791 mentions in 216 documents, and a test set of 4,485 mentions in 231 documents.</p><p>We report micro and macro InKB F1 scores for both EL and ED. For EL, these metrics are computed both in the strong matching and weak matching settings. The former requires exactly predicting the gold mention boundaries and their entity annotations, whereas the latter gives a perfect score to spans that just overlap with the gold mentions and are linked to the correct gold entities.</p><p>Baselines. We compare with popular and stateof-the-art EL and ED public systems, e.  <ref type="table">Table 3</ref>: AIDA A dataset: Gold mentions are split by the position they appear in the p(e|m) dictionary. In each cell, the upper value is the percentage of the gold mentions that were annotated with the correct entity (recall), whereas the lower value is the percentage of gold mentions for which our system's highest scored entity is the ground truth entity, but that might not be annotated in the end because its score is below the threshold δ.</p><p>300 dimensional, while 50 dimensional trainable character vectors were used. The char LSTMs have also hidden dimension of 50. Thus, word-character embeddings are 400 dimensional. The contextual LSTMs have hidden size of 150, resulting in 300 dimensional context-aware word vectors. We apply dropout on the concatenated word-character embeddings, on the output of the bidirectional context LSTM and on the entity embeddings used in Eq. 6. The three FFNNs in our model are simple projections without hidden layers (no improvements were obtained with deeper layers). For the long range context attention we used a word window size of K = 200 and keep top R = 10 words after the hard attention layer (notations from (Ganea and Hofmann, 2017)). We use at most s = 30 entity candidate per mention both at train and test time. γ is set to 0.2 without further investigations. γ is set to 0, but a value of 0.1 was giving similar results. For the loss optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 0.001. We perform early stopping by evaluating the model on the AIDA validation set each 10 minutes and stopping after 6 consecutive evaluations with no significant improvement in the macro F1 score. EL strong and weak matching results are presented in Tables 2 and 7 (Appendix).</p><p>We first note that our system outperforms all baselines on the end-to-end EL task on both AIDA-A (dev) and AIDA-B (test) datasets, which are the biggest EL datasets publicly available. Moreover, we surpass all competitors on both EL and ED by a large margin, at least 9%, showcasing the effectiveness of our method. We also outperform systems that optimize MD and ED separately, including our ED base model + att + global Stanford NER. This demonstrates the merit of joint MD + ED optimization.</p><p>In addition, one can observe that weak matching EL results are comparable with the strong matching results, showcasing that our method is very good at detecting mention boundaries.</p><p>At this point, our main goal was achieved: if enough training data is available with the same characteristics or annotation schemes as the test data, then our joint EL offers the best model. This is true not only when training on AIDA, but also for other types of datasets such as queries <ref type="table" target="#tab_0">(Table 11)</ref> or tweets <ref type="table" target="#tab_0">(Table 12</ref>). However, when testing data  <ref type="bibr">[3, and [5 cases</ref> illustrate the main source of errors. These are false negatives in which our model has the correct ground truth entity pair as the highest scored one for that mention, but since it is not confident enough (score &lt; γ) it decides not to annotate that mention. In this specific document these errors could probably be avoided easily with a better coreference resolution mechanism.</p><p>[3 and [4 cases illustrate that the gold standard can be problematic. Specifically, instead of annotating the whole span Korean War and linking it to the war of 1950, the gold annotation only include Korean and link it to the general entity of Korea_(country).</p><p>[2 is correctly annotated by our system but it is not included in the gold standard. <ref type="table">Table 4</ref>: Error analysis on a sample document. Green corresponds to true positive (correctly discovered and annotated mention), red to false negative (ground truth mention or entity that was not annotated) and orange to false positive (incorrect mention or entity annotation ). has different statistics or follows different conventions than the training data, our method is shown to work best in conjunction with a state-of-the-art NER system as it can be seen from the results of our ED base model + att + global Stanford NER for different datasets in <ref type="table" target="#tab_2">Table 2</ref>. It is expected that such a NER system designed with a broader generaliza-tion scheme in mind would help in this case, which is confirmed by our results on different datasets.</p><p>While the main focus of this paper is the endto-end EL and not the ED-only task, we do show ED results in Tables 8 and 9. We observe that our models are slightly behind recent top performing systems, but our unified EL -ED architecture has to deal with other challenges, e.g. being able to exchange global information between many more mentions at the EL stage, and is thus not suitable for expensive global ED strategies. We leave bridging this gap for ED as future work.</p><p>Additional results and insights are shown in the Appendix. <ref type="table">Table 3</ref> shows an ablation study of our method. One can see that the log p(e|m) prior is very helpful for correctly linking unambiguous mentions, but is introducing noise when gold entities are not frequent. For this category of rare entities, removing this prior completely will result in a significant improvement, but this is not a practical choice since the gold entity is unknown at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>Error Analysis. We conducted a qualitative experiment shown in <ref type="table">Table 4</ref>. We showcase correct annotations, as well as errors done by our system on the AIDA datasets. Inspecting the output annotations of our EL model, we discovered the remarkable property of not over-generating incorrect mentions, nor under-generating (missing) gold spans. We also observed that additional mentions generated by our model do correspond in the majority of time to actual KB entities, but are incorrectly forgotten from the gold annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented the first neural end-to-end entity linking model and show the benefit of jointly optimizing entity recognition and linking. Leveraging key components, namely word, entity and mention embeddings, we prove that engineered features can be almost completely replaced by modern neural networks. Empirically, on the established Gerbil benchmarking platform, we exhibit state-of-the-art performance for EL on the biggest public dataset, AIDA/CoNLL, also showing good generalization ability on other datasets with very different characteristics when combining our model with the popular Stanford NER system.</p><p>Our code is publicly available 2 .        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our global model architecture shown for the mention The New York Times. The final score is used for both the mention linking and entity disambiguation decisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Results and Discussion. The following models are used. i) Base model: only uses the mention local score and the log-prior. It does not use long range attention, nor global disambiguation. It does not use the head attention mechanism. ii) Base model + att: the Base Model plus Long Range Context Attention.ii) Base model + att + global: our Global Model (depicted infigure 1)iv) ED base model + att + global Stanford NER: our ED Global model that runs on top of the detected mentions of the Stanford NER system(Finkel et al., 2005).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples where MD may benefit from ED and viceversa. Each wrong MD decision</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>EL strong matching results on the Gerbil platform. Micro and Macro F1 scores are shown. We highlight the best and second best models, respectively. Training was done on AIDA-train set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SOCCER -[SHEARER ] NAMED AS [1 [1ENGLAND] CAPTAIN]. [ LONDON ] 1996-08-30 The world 's costliest footballer [ Alan Shearer ] was named as the new [England ] captain on Friday. The 26-year-old, who joined [Newcastle] for 15 million pounds sterling, takes over from [Tony Adams], who led the side during the [ European ] championship in June, and former captain [ David Platt ] . [2Adams] and [ Platt ] are both injured and will miss [England]'s opening [3[World Cup]] qualifier against [Moldova] on Sunday . [Shearer] takes the captaincy on a trial basis , but new coach [Glenn Hoddle] said he saw no reason why the former [Blackburn] and [Southampton] skipper should not make the post his own . "I 'm sure [4Alan] is the man for the job , " [Hoddle] said . [...] I spoke to [5Alan] he was up for it [...]. [Shearer] 's [Euro 96] striking partner [...]. Seoul ] 's unification ministry said . " ...I request the immediate repatriation of Kim In-so to [ North Korea ] where his family is waiting , " [1 North Korean ] Red Cross president Li Song-ho said in a telephone message to his southern couterpart , [2 Kang Young-hoon ] . Li said Kim had been critically ill with a cerebral haemorrhage . The message was distributed to the press by the [ South Korean ] unification ministry . Kim , an unrepentant communist , was captured during the [2 [3 Korean ] War ] and released after spending more than 30 years in a southern jail . He submitted a petition to the [ International Red Cross ] in 1993 asking for his repatriation . The domestic [ Yonhap ] news agency said the [ South Korean ] government would consider the northern demand only if the [3 North ] accepted [ Seoul ] 's requests , which include regular reunions of families split by the [4 [4 Korean ] War ] . Government officials were not available to comment . [ South Korea ] in 1993 unconditionally repatriated Li In-mo , a nothern partisan seized by the [5 South ] during the war and jailed for more than three decades.</figDesc><table><row><cell>1) Annotated document:</cell></row><row><cell>Analysis:</cell></row><row><cell>[1 is considered a false negative and the ground truth is the entity England_national_football_team. Our</cell></row><row><cell>annotation was for the span "ENGLAND CAPTAIN" and wrongly linked to the England_cricket_team.</cell></row><row><cell>[3 The ground truth here is 1998_FIFA_World_Cup whereas our model links it to FIFA_World_Cup.</cell></row><row><cell>[2,4,5 are correctly solved due to our coreference resolution heuristic.</cell></row><row><cell>2) Annotated document:</cell></row><row><cell>[ N. Korea ] urges [ S. Korea ] to return war veteran . [ SEOUL ] 1996-08-31 [ North Korea ] demanded</cell></row><row><cell>on Saturday that [ South Korea ] return a northern war veteran who has been in the [1 South ] since</cell></row><row><cell>the 1950-53 war , [ Analysis:</cell></row><row><cell>[1,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Erd'14: entity recognition and disambiguation challenge. In ACM SIGIR Forum, volume 48, pages 63-77. ACM.Diego Ceccarelli, Claudio Lucchese, Salvatore Orlando, Raffaele Perego, and Salvatore Trani. 2013.Dexter: an open source framework for entity linking.</figDesc><table><row><cell cols="3">Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai position of ground truth in p(e|m) AIDA Train the 7th international conference on semantic sys-AIDA A (dev) AIDA B (test)</cell></row><row><cell>Zhang, and Houfeng Wang. 2013. Learning entity</cell><cell>13704</cell><cell>tems, pages 1-8. ACM. 3390 3078</cell></row><row><cell cols="3">In Proceedings of the sixth international workshop on Exploiting semantic annotations in information retrieval, pages 17-20. ACM. Jason PC Chiu and Eric Nichols. 2016. Named entity tics, 4:357-370. Marco Cornolti, Paolo Ferragina, Massimiliano Cia-tion and disambiguation with rich linguistic features. Weikum. 2016. J-nerd: joint named entity recogni-Dat Ba Nguyen, Martin Theobald, and Gerhard tions of the Association for Computational Linguis-244. recognition with bidirectional lstm-cnns. Transac-75.6% 72.7% 69.8% 2339 655 651 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-12.9% 14% 14.8% rado, and Jeff Dean. 2013. Distributed representa-565 108 184 3.1% 2.3% 4.2% tions of words and phrases and their compositional-843 262 308 ity. In Advances in neural information processing 4.6% 5.6% 7% systems, pages 3111-3119. 686 247 187 Sepp Hochreiter and Jürgen Schmidhuber. 1997. ment, pages 545-554. ACM. conference on Information and knowledge manage-tion. In Proceedings of the 21st ACM international keyphrase overlap relatedness for entity disambigua-1 representation for entity disambiguation. In Pro-ceedings of the 51st Annual Meeting of the Associa-2 tion for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 30-34. 3 4-8 9+ 3.8% 5.3% 4.2% Long short-term memory. Neural computation, Andrea Moro, Alessandro Raganato, and Roberto Nav-Total number of non-NIL mentions 18137 4662 4408 9(8):1735-1780. Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum. 2012. Kore: Unambiguous mentions, i.e. 3575 936 833 igli. 2014. Entity linking meets word sense disam-Association for Computational Linguistics, 2:231-the unique candidate biguation: a unified approach. Transactions of the for which ground truth is 19.7% 20.1% 18.9%</cell></row><row><cell>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-</cell><cell></cell><cell>ramita, Stefan Rüd, and Hinrich Schütze. 2016. A piggyback system for joint entity mention detection Transactions of the Association for Computational Linguistics, 4:215-229.</cell></row><row><cell>iol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782-792. Association for Computational Lin-guistics.</cell><cell cols="2">and linking in web queries. In Proceedings of the 25th International Conference on World Wide Web, pages 567-578. International World Wide Web Con-ferences Steering Committee. Joachim Daiber, Max Jakob, Chris Hokamp, and Andrea Giovanni Nuzzolese, Anna Lisa Gentile, Valentina Presutti, Aldo Gangemi, Darío Garigliotti, and Roberto Navigli. 2015. Open knowledge extrac-tion challenge. In Semantic Web Evaluation Chal-lenge, pages 3-15. Springer.</cell></row><row><cell>Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, and Noah A Smith. 2017. Dynamic entity rep-resentations in neural language models. In Proceed-ings of the 2017 Conference on Empirical Methods</cell><cell cols="2">Pablo N Mendes. 2013. Improving efficiency and accuracy in multilingual entity extraction. In Pro-ceedings of the 9th International Conference on Se-mantic Systems, pages 121-124. ACM. Francesco Piccinno and Paolo Ferragina. 2014. From tagme to wat: a new entity annotator. In Proceed-ings of the first international workshop on Entity recognition &amp; disambiguation, pages 55-62. ACM.</cell></row><row><cell>in Natural Language Processing, pages 1830-1839.</cell><cell cols="2">Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Priya Radhakrishnan, Partha Talukdar, and Vasudeva</cell></row><row><cell>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</cell><cell></cell><cell>Marieke van Erp, Genevieve Gorrell, Raphaël Troncy, Johann Petrak, and Kalina Bontcheva. 2015. Analysis of named entity recognition and linking for tweets. Information Processing &amp; Management, Varma. 2018. Elden: Improved entity linking us-ing densified knowledge graphs. In Proceedings of the 2018 Conference of the North American Chap-ter of the Association for Computational Linguistics:</cell></row><row><cell>Guillaume Lample, Miguel Ballesteros, Sandeep Sub-ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of NAACL-HLT, pages 260-270. Phong Le and Ivan Titov. 2018. Improving entity link-</cell><cell cols="2">51(2):32-49. Human Language Technologies, Volume 1 (Long Pa-pers), volume 1, pages 1844-1853. MGJ van Erp, G Rizzo, and R Troncy. 2013. Learn-ing with the web: Spotting named entities on the in-Giuseppe Rizzo, Marieke van Erp, and Raphaël Troncy. Benchmarking the extraction and disambiguation of tersection of nerd and machine learning. In CEUR workshop proceedings, pages 27-30. named entities on the semantic web.</cell></row><row><cell>ing by modeling latent relations between mentions. arXiv preprint arXiv:1804.10637. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-moyer. 2017. End-to-end neural coreference reso-lution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188-197.</cell><cell cols="2">Jenny Rose Finkel, Trond Grenager, and Christopher Michael Röder, Ricardo Usbeck, and Axel-Cyrille Ngonga Ngomo. 2017. Gerbil-benchmarking Manning. 2005. Incorporating non-local informa-tion into information extraction systems by gibbs named entity recognition and linking consistently. Semantic Web, (Preprint):1-21. sampling. In Proceedings of the 43rd annual meet-ing on association for computational linguistics, Avirup Sil and Alexander Yates. 2013. Re-ranking for joint named-entity recognition and linking. In Pro-pages 363-370. Association for Computational Lin-guistics. ceedings of the 22nd ACM international conference</cell></row><row><cell>Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan Gui, Jian Peng, and Jiawei Han. 2017. Empower sequence labeling with task-aware neural language model. arXiv preprint arXiv:1709.04109. Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-iqing Nie. 2015. Joint entity recognition and disam-biguation. In Proceedings of the 2015 Conference</cell><cell cols="2">Octavian-Eugen Ganea, Marina Ganea, Aurelien Luc-on Conference on information &amp; knowledge manage-ment, pages 2369-2374. ACM. chi, Carsten Eickhoff, and Thomas Hofmann. 2016. René Speck and Axel-Cyrille Ngonga Ngomo. 2014. Probabilistic bag-of-hyperlinks model for entity Ensemble learning for named entity recognition. In linking. In Proceedings of the 25th International Conference on World Wide Web, pages 927-938. In-ternational World Wide Web Conferences Steering International semantic web conference, pages 519-534. Springer. Committee. Valentin I Spitkovsky and Angel X Chang. A cross-</cell></row><row><cell>ing, pages 879-888. on Empirical Methods in Natural Language Process-</cell><cell cols="2">Octavian-Eugen Ganea and Thomas Hofmann. 2017. lingual dictionary for english wikipedia concepts.</cell></row><row><cell></cell><cell cols="2">Deep joint entity disambiguation with local neural Nadine Steinmetz and Harald Sack. 2013. Semantic</cell></row><row><cell>Pablo N Mendes, Max Jakob, Andrés García-Silva, and</cell><cell></cell><cell>attention. In Proceedings of the 2017 Conference on multimedia information retrieval based on contex-</cell></row><row><cell>Christian Bizer. 2011. Dbpedia spotlight: shedding</cell><cell></cell><cell>Empirical Methods in Natural Language Processing, tual descriptions. In Extended Semantic Web Con-</cell></row><row><cell>light on the web of documents. In Proceedings of</cell><cell></cell><cell>pages 2619-2629. ference, pages 382-396. Springer.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics on the number of mentions for various settings and for all AIDA datasets.</figDesc><table><row><cell>Dataset</cell><cell>Topic</cell><cell>Type of annotations</cell><cell>Exhaustive annotations</cell><cell>Num Docs</cell><cell>Avg. Words/Doc.</cell><cell>Num InKB Anns</cell><cell>Num of gold entities</cell><cell>not in our KB</cell><cell>Recall 30 (%)</cell><cell>Recall 10 (%)</cell></row><row><cell>AIDA</cell><cell>news</cell><cell>NE</cell><cell></cell><cell cols="3">231 190 4483</cell><cell cols="2">5</cell><cell cols="2">98.3 95.7</cell></row><row><cell>AQUAINT</cell><cell>news</cell><cell>NE&amp;N</cell><cell></cell><cell cols="3">50 221 727</cell><cell cols="2">16</cell><cell cols="2">93.5 92.6</cell></row><row><cell>MSNBC</cell><cell>news</cell><cell>NE</cell><cell></cell><cell cols="3">20 544 653</cell><cell cols="2">3</cell><cell cols="2">92.8 90.3</cell></row><row><cell>ACE2004</cell><cell>news</cell><cell>NE</cell><cell></cell><cell cols="3">57 374 257</cell><cell cols="2">2</cell><cell cols="2">87.9 86.8</cell></row><row><cell>Derczynski (Derczynski et al., 2015)</cell><cell>tweets</cell><cell>NE</cell><cell></cell><cell cols="2">182 21</cell><cell>210</cell><cell cols="2">32</cell><cell cols="2">71.0 69.5</cell></row><row><cell>Microposts2016 dev (Rizzo et al.)</cell><cell>tweets</cell><cell>NE</cell><cell></cell><cell cols="2">100 14</cell><cell>251</cell><cell cols="2">50</cell><cell cols="2">59.8 59.8</cell></row><row><cell>N3-Reuters-128</cell><cell>news</cell><cell>NE</cell><cell></cell><cell cols="3">128 124 631</cell><cell cols="2">7</cell><cell cols="2">68.9 64.3</cell></row><row><cell>N3-RSS-500</cell><cell>news</cell><cell>NE</cell><cell></cell><cell cols="2">500 31</cell><cell>519</cell><cell cols="2">11</cell><cell cols="2">81.3 79.4</cell></row><row><cell>DBpedia Spotlight (Mendes et al., 2011)</cell><cell>news</cell><cell>NE&amp;N</cell><cell></cell><cell>58</cell><cell>29</cell><cell>330</cell><cell cols="2">7</cell><cell cols="2">90.6 90.3</cell></row><row><cell>KORE 50 (Hoffart et al., 2012)</cell><cell>mixed</cell><cell>NE</cell><cell></cell><cell>50</cell><cell>13</cell><cell>144</cell><cell cols="2">1</cell><cell cols="2">88.2 75.7</cell></row><row><cell>ERD2014 (Carmel et al., 2014)</cell><cell>queries</cell><cell>NE</cell><cell></cell><cell>91</cell><cell>3.5</cell><cell>59</cell><cell cols="2">1</cell><cell cols="2">72.9 72.9</cell></row><row><cell>Gerdaq (Cornolti et al., 2016)</cell><cell>queries</cell><cell>NE&amp;N</cell><cell></cell><cell cols="2">250 3.6</cell><cell>408</cell><cell cols="2">3</cell><cell cols="2">70.8 68.6</cell></row><row><cell>OKE (Nuzzolese et al., 2015)</cell><cell cols="6">wikipedia NE&amp;roles 55 26.6 288</cell><cell cols="2">3</cell><cell cols="2">81.9 81.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Test datasets statistics. Topic has the following possible values: news articles, twitter messages, search queries, or sentences extracted from Wikipedia articles. Type of annotations can be NE (Named Entities i.e. persons, locations, organizations) and/or N (Nouns i.e. dataset annotates even simple nouns). Exhaustive annotations means that these datasets annotate all entities that appear in their text and that fall into the selected categories of Type of Annotations. Recall 30/10 shows the percentage of the gold mentions for which their ground truth entity is among the first 30/10 candidate entities returned by our p(e|m) dictionary.</figDesc><table><row><cell>F1@MA</cell><cell>AIDA A</cell><cell>AIDA B</cell><cell>MSNBC</cell><cell>OKE-2015</cell><cell>OKE-2016</cell><cell>N3-Reuters-128</cell><cell>N3-RSS-500</cell><cell>Derczynski</cell><cell>KORE50</cell></row><row><cell>F1@MI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FREME</cell><cell>23.9</cell><cell>24.3</cell><cell>17.6</cell><cell>26.8</cell><cell>23.0</cell><cell>28.2</cell><cell>34.6</cell><cell>31.8</cell><cell>13.3</cell></row><row><cell></cell><cell>38.3</cell><cell>37.0</cell><cell>22.6</cell><cell>32.7</cell><cell>29.0</cell><cell>33.5</cell><cell>32.4</cell><cell>20.4</cell><cell>15.5</cell></row><row><cell>FOX</cell><cell>54.9</cell><cell>58.4</cell><cell>26.3</cell><cell>56.4</cell><cell>50.1</cell><cell>53.8</cell><cell>37.7</cell><cell>43.4</cell><cell>30.1</cell></row><row><cell></cell><cell>58.1</cell><cell>57.4</cell><cell>21.2</cell><cell>58.7</cell><cell>51.4</cell><cell>55.2</cell><cell>37.2</cell><cell>41.5</cell><cell>32.9</cell></row><row><cell>Babelfy</cell><cell>42.1</cell><cell>43.3</cell><cell>40.2</cell><cell>42.8</cell><cell>40.1</cell><cell>29.8</cell><cell>36.6</cell><cell>32.3</cell><cell>53.0</cell></row><row><cell></cell><cell>48.1</cell><cell>49.5</cell><cell>44.9</cell><cell>45.7</cell><cell>39.7</cell><cell>33.6</cell><cell>34.6</cell><cell>34.3</cell><cell>56.6</cell></row><row><cell>Entityclassifier.eu</cell><cell>44.9</cell><cell>45.3</cell><cell>47.2</cell><cell>32.7</cell><cell>35.7</cell><cell>31.8</cell><cell>24.5</cell><cell>18.1</cell><cell>27.1</cell></row><row><cell></cell><cell>47.0</cell><cell>47.6</cell><cell>48.9</cell><cell>33.0</cell><cell>33.6</cell><cell>35.3</cell><cell>24.1</cell><cell>19.3</cell><cell>30.1</cell></row><row><cell>Kea</cell><cell>38.7</cell><cell>42.1</cell><cell>35.0</cell><cell>53.9</cell><cell>54.9</cell><cell>22.1</cell><cell>25.9</cell><cell>33.2</cell><cell>46.5</cell></row><row><cell></cell><cell>42.9</cell><cell>44.7</cell><cell>36.3</cell><cell>54.4</cell><cell>55.2</cell><cell>22.4</cell><cell>24.7</cell><cell>29.2</cell><cell>50.5</cell></row><row><cell>DBpedia Spotlight</cell><cell>55.5</cell><cell>53.1</cell><cell>49.6</cell><cell>49.6</cell><cell>49.2</cell><cell>36.0</cell><cell>35.5</cell><cell>44.0</cell><cell>28.8</cell></row><row><cell></cell><cell>58.3</cell><cell>58.8</cell><cell>49.2</cell><cell>52.2</cell><cell>49.8</cell><cell>38.0</cell><cell>35.1</cell><cell>41.5</cell><cell>35.4</cell></row><row><cell>AIDA</cell><cell>69.0</cell><cell>72.3</cell><cell>66.0</cell><cell>61.7</cell><cell>43.3</cell><cell>44.7</cell><cell>47.3</cell><cell>41.5</cell><cell>52.9</cell></row><row><cell></cell><cell>72.6</cell><cell>73.5</cell><cell>68.1</cell><cell>65.8</cell><cell>47.2</cell><cell>48.5</cell><cell>47.5</cell><cell>34.8</cell><cell>58.6</cell></row><row><cell>WAT</cell><cell>69.7</cell><cell>71.6</cell><cell>66.7</cell><cell>58.4</cell><cell>55.2</cell><cell>54.4</cell><cell>47.6</cell><cell>47.1</cell><cell>39.9</cell></row><row><cell></cell><cell>73.3</cell><cell>73.6</cell><cell>68.9</cell><cell>60.7</cell><cell>56.6</cell><cell>58.3</cell><cell>46.5</cell><cell>43.6</cell><cell>52.2</cell></row><row><cell>Best baseline</cell><cell>69.7</cell><cell>72.3</cell><cell>66.7</cell><cell>61.7</cell><cell>55.2</cell><cell>54.4</cell><cell>47.6</cell><cell>47.1</cell><cell>53.0</cell></row><row><cell></cell><cell>73.3</cell><cell>73.6</cell><cell>68.9</cell><cell>65.8</cell><cell>56.6</cell><cell>58.3</cell><cell>47.5</cell><cell>43.6</cell><cell>58.6</cell></row><row><cell>base model</cell><cell>87.0</cell><cell>81.7</cell><cell>67.3</cell><cell>56.2</cell><cell>45.0</cell><cell>48.7</cell><cell>46.5</cell><cell>44.9</cell><cell>36.9</cell></row><row><cell></cell><cell>89.5</cell><cell>80.8</cell><cell>68.8</cell><cell>60.1</cell><cell>47.0</cell><cell>51.4</cell><cell>42.8</cell><cell>40.1</cell><cell>44.7</cell></row><row><cell>base model + att</cell><cell>87.1</cell><cell>82.4</cell><cell>72.6</cell><cell>59.0</cell><cell>49.2</cell><cell>51.0</cell><cell>48.4</cell><cell>49.3</cell><cell>38.0</cell></row><row><cell></cell><cell>89.3</cell><cell>82.6</cell><cell>72.6</cell><cell>63.0</cell><cell>51.6</cell><cell>54.7</cell><cell>44.6</cell><cell>44.4</cell><cell>45.1</cell></row><row><cell>base model + att + global</cell><cell>87.2</cell><cell>83.2</cell><cell>75.7</cell><cell>59.4</cell><cell>48.1</cell><cell>47.0</cell><cell>46.3</cell><cell>43.8</cell><cell>28.2</cell></row><row><cell></cell><cell>89.8</cell><cell>82.8</cell><cell>74.7</cell><cell>64.6</cell><cell>53.1</cell><cell>53.2</cell><cell>42.7</cell><cell>34.9</cell><cell>38.3</cell></row><row><cell>ED base model + att + global using Stanford NER mentions</cell><cell>76.0 80.5</cell><cell>73.9 75.0</cell><cell>74.7 74.8</cell><cell>65.3 68.9</cell><cell>57.7 59.3</cell><cell>54.9 56.9</cell><cell>48.6 46.1</cell><cell>50.2 44.3</cell><cell>41.9 47.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>EL weak matching results on the Gerbil platform. Micro and Macro F1 scores are shown. We highlight in red and blue the best and second best models, respectively. Training was done on AIDA-train set.</figDesc><table><row><cell>F1@MA</cell><cell>ACE2004</cell><cell>AIDA A</cell><cell>AIDA B</cell><cell>AQUAINT</cell><cell>MSNBC</cell><cell>DBpediaSpotlight</cell><cell>Derczynski</cell><cell>ERD2014</cell><cell>GERDAQ-Dev</cell><cell>GERDAQ-Test</cell><cell>KORE50</cell><cell>N3-Reuters-128</cell><cell>N3-RSS-500</cell><cell>OKE-2015</cell><cell>OKE-2016</cell></row><row><cell>F1@MI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AGDISTIS</cell><cell>78.1</cell><cell>49.2</cell><cell>54.2</cell><cell>58.9</cell><cell>72.9</cell><cell>35.3</cell><cell>50.8</cell><cell>63.2</cell><cell>21.4</cell><cell>22.8</cell><cell>30.0</cell><cell>68.5</cell><cell>51.4</cell><cell>61.4</cell><cell>59.6</cell></row><row><cell></cell><cell>65.6</cell><cell>55.6</cell><cell>54.9</cell><cell>60.4</cell><cell>74.0</cell><cell>39.3</cell><cell>44.5</cell><cell>32.0</cell><cell>17.8</cell><cell>20.3</cell><cell>33.3</cell><cell>63.8</cell><cell>51.8</cell><cell>62.0</cell><cell>58.8</cell></row><row><cell>FREME</cell><cell>73.5</cell><cell>25.3</cell><cell>25.7</cell><cell>43.7</cell><cell>17.9</cell><cell>37.9</cell><cell>48.3</cell><cell>71.3</cell><cell>37.6</cell><cell>33.4</cell><cell>13.8</cell><cell>30.2</cell><cell>45.5</cell><cell>25.0</cell><cell>27.4</cell></row><row><cell></cell><cell>60.3</cell><cell>33.0</cell><cell>40.3</cell><cell>57.6</cell><cell>24.3</cell><cell>52.5</cell><cell>38.2</cell><cell>51.8</cell><cell>44.2</cell><cell>39.6</cell><cell>17.4</cell><cell>31.7</cell><cell>43.9</cell><cell>33.3</cell><cell>35.0</cell></row><row><cell>FOX</cell><cell>38.6</cell><cell>55.5</cell><cell>59.6</cell><cell>0.0</cell><cell>16.2</cell><cell>11.7</cell><cell>51.2</cell><cell>51.7</cell><cell>11.2</cell><cell>10.6</cell><cell>26.9</cell><cell>56.6</cell><cell>52.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>58.9</cell><cell>58.0</cell><cell>0.0</cell><cell>13.3</cell><cell>15.9</cell><cell>44.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>31.1</cell><cell>58.1</cell><cell>53.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>ED results matched locally (not on Gerbil) using KB entity IDs. Micro and Macro F1 scores are shown. For the ED task there is a significant discrepancy between the local and the Gerbil scores which likely has two main causes: a different URL-based matching scheme used in Gerbil 3 and parsing or alignment errors.</figDesc><table><row><cell>Rec@MA</cell><cell>ACE2004</cell><cell>AQUAINT</cell></row><row><cell>Rec@MI</cell><cell></cell><cell></cell></row><row><cell>FREME</cell><cell>44.8</cell><cell>23.7</cell></row><row><cell></cell><cell>30.8</cell><cell>23.7</cell></row><row><cell>FOX</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Babelfy</cell><cell>10.0</cell><cell>34.9</cell></row><row><cell></cell><cell>17.8</cell><cell>35.8</cell></row><row><cell>Entityclassifier.eu</cell><cell>35.7</cell><cell>28.4</cell></row><row><cell></cell><cell>59.7</cell><cell>29.2</cell></row><row><cell>Kea</cell><cell>23.7</cell><cell>36.1</cell></row><row><cell></cell><cell>40.3</cell><cell>35.9</cell></row><row><cell>DBpedia Spotlight</cell><cell>39.1</cell><cell>45.1</cell></row><row><cell></cell><cell>60.5</cell><cell>45.2</cell></row><row><cell>AIDA</cell><cell>28.0</cell><cell>36.0</cell></row><row><cell></cell><cell>48.6</cell><cell>36.5</cell></row><row><cell>WAT</cell><cell>26.8</cell><cell>35.1</cell></row><row><cell></cell><cell>44.7</cell><cell>35.5</cell></row><row><cell>Best baseline</cell><cell>44.8</cell><cell>45.1</cell></row><row><cell></cell><cell>60.5</cell><cell>45.2</cell></row><row><cell>model+att+global</cell><cell>40.3</cell><cell>39.4</cell></row><row><cell></cell><cell>68.3</cell><cell>40.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>El strong matching results on the Gerbil platform. Micro and Macro recall scores are shown for ACE2004 and AQUAINT datasets that only annotate each entity once, thus F1 scores not being useful. We highlight in red and blue the best and second best models, respectively. Our models were trained on AIDA as before.</figDesc><table><row><cell>F1@MA</cell><cell>GERDAQ-Dev</cell><cell>GERDAQ-Test</cell><cell>DBpediaSpotlight</cell></row><row><cell>F1@MI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FREME</cell><cell>11.2</cell><cell>10.6</cell><cell>11.6</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>13.0</cell></row><row><cell>FOX</cell><cell>11.2</cell><cell>10.6</cell><cell>11.3</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>15.3</cell></row><row><cell>Babelfy</cell><cell>19.6</cell><cell>19.6</cell><cell>8.7</cell></row><row><cell></cell><cell>15.5</cell><cell>17.5</cell><cell>13.1</cell></row><row><cell>Entityclassifier.eu</cell><cell>11.2</cell><cell>10.6</cell><cell>18.3</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>23.1</cell></row><row><cell>Kea</cell><cell>40.2</cell><cell>40.2</cell><cell>42.0</cell></row><row><cell></cell><cell>40.2</cell><cell>43.7</cell><cell>42.6</cell></row><row><cell>DBpedia Spotlight</cell><cell>29.6</cell><cell>29.4</cell><cell>31.8</cell></row><row><cell></cell><cell>27.8</cell><cell>31.4</cell><cell>37.6</cell></row><row><cell>AIDA</cell><cell>11.2</cell><cell>10.6</cell><cell>14.4</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>19.1</cell></row><row><cell>WAT</cell><cell>12.9</cell><cell>12.6</cell><cell>14.5</cell></row><row><cell></cell><cell>3.3</cell><cell>6.0</cell><cell>17.0</cell></row><row><cell>Best baseline</cell><cell>40.2</cell><cell>40.2</cell><cell>42.0</cell></row><row><cell></cell><cell>40.2</cell><cell>43.7</cell><cell>42.6</cell></row><row><cell>model+att+global on gerdaq train</cell><cell>42.7</cell><cell>40.0</cell><cell>33.4</cell></row><row><cell></cell><cell>44.9</cell><cell>41.2</cell><cell>34.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>El strong matching results on the Gerbil platform. Micro and Macro F1 scores are shown. Training was done on Gerdaq train dataset. We highlight in red and blue the best and second best models, respectively.</figDesc><table><row><cell>F1@MA</cell><cell>MicropDev</cell><cell>MicropTest</cell></row><row><cell>F1@MI</cell><cell></cell><cell></cell></row><row><cell>FREME</cell><cell>3.9</cell><cell>77.9</cell></row><row><cell></cell><cell>3.8</cell><cell>3.7</cell></row><row><cell>FOX</cell><cell>6.1</cell><cell>71.9</cell></row><row><cell></cell><cell>7.5</cell><cell>3.7</cell></row><row><cell>Babelfy</cell><cell>6.2</cell><cell>52.5</cell></row><row><cell></cell><cell>9.5</cell><cell>2.8</cell></row><row><cell>Entityclassifier.eu</cell><cell>23.5</cell><cell>9.2</cell></row><row><cell></cell><cell>29.1</cell><cell>2.4</cell></row><row><cell>Kea</cell><cell>37.3</cell><cell>51.6</cell></row><row><cell></cell><cell>38.8</cell><cell>6.5</cell></row><row><cell>DBpedia Spotlight</cell><cell>29.1</cell><cell>13.6</cell></row><row><cell></cell><cell>34.0</cell><cell>5.2</cell></row><row><cell>AIDA</cell><cell>6.4</cell><cell>73.3</cell></row><row><cell></cell><cell>9.7</cell><cell>5.0</cell></row><row><cell>WAT</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Best baseline</cell><cell>37.3</cell><cell>77.9</cell></row><row><cell></cell><cell>38.8</cell><cell>6.5</cell></row><row><cell>model on micropost train</cell><cell>57.3</cell><cell>17.4</cell></row><row><cell></cell><cell>63.7</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>El strong matching results on the Gerbil platform when training on Micropost-train 2016. Micro and Macro F1 scores are shown. The majority of the errors are due to the newer KB version used in this dataset. We highlight in red and blue the best and second best models, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/dalab/end2end_ neural_el</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/dice-group/gerbil/ issues/98</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collective entity disambiguation with structured gradient tree boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ED results on the Gerbil platform. Micro and Macro F1 scores are shown. We highlight in red and blue the best and second best models, respectively. Training was done on AIDA-train set</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
